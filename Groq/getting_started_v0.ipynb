{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOePgWII4EG3mtICTgur0di"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9397b9d9d90945a883b2b3354184152e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_18ca7d25a368473eaee29b2c547a30f9","IPY_MODEL_efd9f065b3964f748018eee077b137b8","IPY_MODEL_1bcc4ecce8ec43029cb0c9aa26677c64"],"layout":"IPY_MODEL_4cc96fa9fbe4460eb8d123eefe0b259a"}},"18ca7d25a368473eaee29b2c547a30f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8d585a61915410aa3f4238edd1f2ee7","placeholder":"​","style":"IPY_MODEL_dd1474b37cdc44f9acddf4ee215d0e78","value":"Downloading readme: 100%"}},"efd9f065b3964f748018eee077b137b8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_22f5247b1c104509abcc39835384c2e2","max":147,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3346342d60424c149a7f72480df6e81e","value":147}},"1bcc4ecce8ec43029cb0c9aa26677c64":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_96db8b62e7594a44ab7610b8d9b3ea70","placeholder":"​","style":"IPY_MODEL_60fdb3e5fd1a43d19a37b4d7a3754bd5","value":" 147/147 [00:00&lt;00:00, 5.73kB/s]"}},"4cc96fa9fbe4460eb8d123eefe0b259a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8d585a61915410aa3f4238edd1f2ee7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd1474b37cdc44f9acddf4ee215d0e78":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"22f5247b1c104509abcc39835384c2e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3346342d60424c149a7f72480df6e81e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"96db8b62e7594a44ab7610b8d9b3ea70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60fdb3e5fd1a43d19a37b4d7a3754bd5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d387ef0c1ab42af92cd8b4d36a4a609":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8cb7520b1df943368de7661626722935","IPY_MODEL_4df0554587b041a0a689d34843336cd5","IPY_MODEL_d071f8c4f3a5402bb24a601c740961f9"],"layout":"IPY_MODEL_2866805a11b645a0812898aa7d3c798c"}},"8cb7520b1df943368de7661626722935":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8dc66d6d6b8d4f938e0d13546dbc97fd","placeholder":"​","style":"IPY_MODEL_0ca59533af054e67a8fd186f9d706099","value":"Downloading data: 100%"}},"4df0554587b041a0a689d34843336cd5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f5279ffe87d46c7a179e8dccfbfc7e6","max":76796,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0ee3680bf5fd461d8597e73d370b8a9d","value":76796}},"d071f8c4f3a5402bb24a601c740961f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ca7d05407c14f1195091e4ddce34101","placeholder":"​","style":"IPY_MODEL_c52c9444b750464f9da222bc93fe7c54","value":" 76.8k/76.8k [00:00&lt;00:00, 663kB/s]"}},"2866805a11b645a0812898aa7d3c798c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8dc66d6d6b8d4f938e0d13546dbc97fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ca59533af054e67a8fd186f9d706099":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f5279ffe87d46c7a179e8dccfbfc7e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ee3680bf5fd461d8597e73d370b8a9d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7ca7d05407c14f1195091e4ddce34101":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c52c9444b750464f9da222bc93fe7c54":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ffdce5ba5184051baf4e11f3b6a250c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3ab8a749f85348d7b8b2d1cc72069812","IPY_MODEL_fb661e96b0644a6981a606f1c2243883","IPY_MODEL_42078689440340db9a6b4092aaa1d87a"],"layout":"IPY_MODEL_dea412b9589b4de4986ea2e43ee2e260"}},"3ab8a749f85348d7b8b2d1cc72069812":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64fdaadc28f1463bbee2be94750cbc63","placeholder":"​","style":"IPY_MODEL_de6c09d1d82048e89eac328fbc8ae25c","value":"Generating train split: 100%"}},"fb661e96b0644a6981a606f1c2243883":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ee9e02adf8a457dadd54c33f4d2964d","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_56023b33fc74438cb908711770137ebe","value":25}},"42078689440340db9a6b4092aaa1d87a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9e8fc996fd34a0ab63f47efe97cb66b","placeholder":"​","style":"IPY_MODEL_2008535390814e8f9c35119b7db9f8a8","value":" 25/25 [00:00&lt;00:00, 520.45 examples/s]"}},"dea412b9589b4de4986ea2e43ee2e260":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64fdaadc28f1463bbee2be94750cbc63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de6c09d1d82048e89eac328fbc8ae25c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ee9e02adf8a457dadd54c33f4d2964d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56023b33fc74438cb908711770137ebe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f9e8fc996fd34a0ab63f47efe97cb66b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2008535390814e8f9c35119b7db9f8a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7GSjUF7t9rJ2","executionInfo":{"status":"ok","timestamp":1714782071543,"user_tz":240,"elapsed":27446,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}},"outputId":"13840644-91e1-401f-fa8e-026fbb088a4c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.6/867.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.7/116.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.9/312.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q langchain langchain-openai langchain-groq"]},{"cell_type":"code","source":["import os\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_groq import ChatGroq\n","from google.colab import userdata"],"metadata":{"id":"PZ7PJBRS-W5y","executionInfo":{"status":"ok","timestamp":1714782301046,"user_tz":240,"elapsed":125,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["os.environ['GROQ_API_KEY'] = userdata.get('groq')\n","os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n","os.environ['PINECONE_API_KEY'] = userdata.get('pinecone')"],"metadata":{"id":"fWzLL7fX_fcx","executionInfo":{"status":"ok","timestamp":1714785763595,"user_tz":240,"elapsed":1234,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["llm = ChatGroq(temperature=0,\n","               model_name = 'mixtral-8x7b-32768')"],"metadata":{"id":"3KKjEZmA_su1","executionInfo":{"status":"ok","timestamp":1714782592179,"user_tz":240,"elapsed":687,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["system = \"You are a helpful assistant.\"\n","human = \"{text}\"\n","\n","prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", system),\n","         (\"human\", human)\n","    ]\n","  )"],"metadata":{"id":"KL21n6vnArpu","executionInfo":{"status":"ok","timestamp":1714782686489,"user_tz":240,"elapsed":115,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["chain = prompt | llm\n","\n","query = \"Explain the importance of low latency LLMs.\"\n","\n","result = chain.invoke({\"text\": query})\n","\n","print(result.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vazCPKWpAdj3","executionInfo":{"status":"ok","timestamp":1714782749649,"user_tz":240,"elapsed":916,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}},"outputId":"cfc1335d-6d45-43ae-a8f8-faa4c27bb8b8"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Sure, I'd be happy to explain!\n","\n","LLM stands for \"Low Latency Logging,\" which is a method of recording and transmitting data with minimal delay. Low latency LLMs are particularly important in time-sensitive applications where real-time data processing is critical. Here are some reasons why low latency LLMs are important:\n","\n","1. Improved user experience: In applications such as online gaming or video conferencing, low latency LLMs ensure that data is transmitted and processed quickly, resulting in a smoother and more responsive user experience.\n","2. Increased efficiency: Low latency LLMs enable faster data processing, which can lead to increased efficiency in data-intensive applications such as financial trading or scientific simulations.\n","3. Enhanced security: In security-critical applications, low latency LLMs can help detect and respond to threats more quickly, reducing the risk of data breaches or other security incidents.\n","4. Better decision-making: In real-time decision-making scenarios, such as autonomous vehicles or industrial automation, low latency LLMs can provide the necessary data in near real-time, enabling faster and more accurate decision-making.\n","\n","Overall, low latency LLMs are essential in applications where timely and accurate data processing is critical. By minimizing delay and ensuring fast data transmission, low latency LLMs can improve user experience, increase efficiency, enhance security, and enable better decision-making.\n"]}]},{"cell_type":"markdown","source":["# Langchain Groq OpenAI - RAG"],"metadata":{"id":"dl-Gq_cLBMdA"}},{"cell_type":"markdown","source":["## Load the dataset: https://huggingface.co/docs/datasets/en/loading"],"metadata":{"id":"fNT4Ve7MFPeP"}},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xl30KWcKGK42","executionInfo":{"status":"ok","timestamp":1714784102637,"user_tz":240,"elapsed":11896,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}},"outputId":"191480fd-c7f6-4d56-df5e-ee8011e7d805"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Collecting huggingface-hub>=0.21.2 (from datasets)\n","  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.20.3\n","    Uninstalling huggingface-hub-0.20.3:\n","      Successfully uninstalled huggingface-hub-0.20.3\n","Successfully installed datasets-2.19.0 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\n"]}]},{"cell_type":"code","source":["from datasets import load_dataset\n","dataset = load_dataset('infoslack/mistral-7b-arxiv-paper-chunked', split='train')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":240,"referenced_widgets":["9397b9d9d90945a883b2b3354184152e","18ca7d25a368473eaee29b2c547a30f9","efd9f065b3964f748018eee077b137b8","1bcc4ecce8ec43029cb0c9aa26677c64","4cc96fa9fbe4460eb8d123eefe0b259a","c8d585a61915410aa3f4238edd1f2ee7","dd1474b37cdc44f9acddf4ee215d0e78","22f5247b1c104509abcc39835384c2e2","3346342d60424c149a7f72480df6e81e","96db8b62e7594a44ab7610b8d9b3ea70","60fdb3e5fd1a43d19a37b4d7a3754bd5","5d387ef0c1ab42af92cd8b4d36a4a609","8cb7520b1df943368de7661626722935","4df0554587b041a0a689d34843336cd5","d071f8c4f3a5402bb24a601c740961f9","2866805a11b645a0812898aa7d3c798c","8dc66d6d6b8d4f938e0d13546dbc97fd","0ca59533af054e67a8fd186f9d706099","5f5279ffe87d46c7a179e8dccfbfc7e6","0ee3680bf5fd461d8597e73d370b8a9d","7ca7d05407c14f1195091e4ddce34101","c52c9444b750464f9da222bc93fe7c54","8ffdce5ba5184051baf4e11f3b6a250c","3ab8a749f85348d7b8b2d1cc72069812","fb661e96b0644a6981a606f1c2243883","42078689440340db9a6b4092aaa1d87a","dea412b9589b4de4986ea2e43ee2e260","64fdaadc28f1463bbee2be94750cbc63","de6c09d1d82048e89eac328fbc8ae25c","8ee9e02adf8a457dadd54c33f4d2964d","56023b33fc74438cb908711770137ebe","f9e8fc996fd34a0ab63f47efe97cb66b","2008535390814e8f9c35119b7db9f8a8"]},"id":"RkNNJKm6BMR4","executionInfo":{"status":"ok","timestamp":1714784183301,"user_tz":240,"elapsed":4291,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}},"outputId":"a4b2c1ea-3b4f-4746-88b5-9b3939ed9982"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading readme:   0%|          | 0.00/147 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9397b9d9d90945a883b2b3354184152e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/76.8k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d387ef0c1ab42af92cd8b4d36a4a609"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/25 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ffdce5ba5184051baf4e11f3b6a250c"}},"metadata":{}}]},{"cell_type":"code","source":["dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EeOrHyaaA4TJ","executionInfo":{"status":"ok","timestamp":1714784189566,"user_tz":240,"elapsed":126,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}},"outputId":"e5a3795c-90f7-4c58-9e15-45503742c197"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n","    num_rows: 25\n","})"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["data = dataset.to_pandas()"],"metadata":{"id":"djVvVcz0Gjrb","executionInfo":{"status":"ok","timestamp":1714784222217,"user_tz":240,"elapsed":138,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["data.head(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":764},"id":"shspg7ZyGrp7","executionInfo":{"status":"ok","timestamp":1714784227950,"user_tz":240,"elapsed":344,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}},"outputId":"d936e95b-b58a-46e6-c2da-0ab114683d9f"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["          doi chunk-id                                              chunk  \\\n","0  2310.06825        0  Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...   \n","1  2310.06825        1  automated benchmarks. Our models are released ...   \n","2  2310.06825        2  GQA significantly accelerates the inference sp...   \n","3  2310.06825        3  Mistral 7B takes a significant step in balanci...   \n","4  2310.06825        4  parameters of the architecture are summarized ...   \n","\n","           id       title                                            summary  \\\n","0  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n","1  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n","2  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n","3  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n","4  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n","\n","                            source  \\\n","0  http://arxiv.org/pdf/2310.06825   \n","1  http://arxiv.org/pdf/2310.06825   \n","2  http://arxiv.org/pdf/2310.06825   \n","3  http://arxiv.org/pdf/2310.06825   \n","4  http://arxiv.org/pdf/2310.06825   \n","\n","                                             authors             categories  \\\n","0  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n","1  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n","2  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n","3  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n","4  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n","\n","                                             comment journal_ref  \\\n","0  Models and code are available at\\n  https://mi...        None   \n","1  Models and code are available at\\n  https://mi...        None   \n","2  Models and code are available at\\n  https://mi...        None   \n","3  Models and code are available at\\n  https://mi...        None   \n","4  Models and code are available at\\n  https://mi...        None   \n","\n","  primary_category published   updated  \\\n","0            cs.CL  20231010  20231010   \n","1            cs.CL  20231010  20231010   \n","2            cs.CL  20231010  20231010   \n","3            cs.CL  20231010  20231010   \n","4            cs.CL  20231010  20231010   \n","\n","                                          references  \n","0  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n","1  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n","2  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n","3  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n","4  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  "],"text/html":["\n","  <div id=\"df-fb8a5566-a296-4fe3-9f1c-3fbc0177be7a\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>doi</th>\n","      <th>chunk-id</th>\n","      <th>chunk</th>\n","      <th>id</th>\n","      <th>title</th>\n","      <th>summary</th>\n","      <th>source</th>\n","      <th>authors</th>\n","      <th>categories</th>\n","      <th>comment</th>\n","      <th>journal_ref</th>\n","      <th>primary_category</th>\n","      <th>published</th>\n","      <th>updated</th>\n","      <th>references</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2310.06825</td>\n","      <td>0</td>\n","      <td>Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...</td>\n","      <td>2310.06825</td>\n","      <td>Mistral 7B</td>\n","      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n","      <td>http://arxiv.org/pdf/2310.06825</td>\n","      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n","      <td>[cs.CL, cs.AI, cs.LG]</td>\n","      <td>Models and code are available at\\n  https://mi...</td>\n","      <td>None</td>\n","      <td>cs.CL</td>\n","      <td>20231010</td>\n","      <td>20231010</td>\n","      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2310.06825</td>\n","      <td>1</td>\n","      <td>automated benchmarks. Our models are released ...</td>\n","      <td>2310.06825</td>\n","      <td>Mistral 7B</td>\n","      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n","      <td>http://arxiv.org/pdf/2310.06825</td>\n","      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n","      <td>[cs.CL, cs.AI, cs.LG]</td>\n","      <td>Models and code are available at\\n  https://mi...</td>\n","      <td>None</td>\n","      <td>cs.CL</td>\n","      <td>20231010</td>\n","      <td>20231010</td>\n","      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2310.06825</td>\n","      <td>2</td>\n","      <td>GQA significantly accelerates the inference sp...</td>\n","      <td>2310.06825</td>\n","      <td>Mistral 7B</td>\n","      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n","      <td>http://arxiv.org/pdf/2310.06825</td>\n","      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n","      <td>[cs.CL, cs.AI, cs.LG]</td>\n","      <td>Models and code are available at\\n  https://mi...</td>\n","      <td>None</td>\n","      <td>cs.CL</td>\n","      <td>20231010</td>\n","      <td>20231010</td>\n","      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2310.06825</td>\n","      <td>3</td>\n","      <td>Mistral 7B takes a significant step in balanci...</td>\n","      <td>2310.06825</td>\n","      <td>Mistral 7B</td>\n","      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n","      <td>http://arxiv.org/pdf/2310.06825</td>\n","      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n","      <td>[cs.CL, cs.AI, cs.LG]</td>\n","      <td>Models and code are available at\\n  https://mi...</td>\n","      <td>None</td>\n","      <td>cs.CL</td>\n","      <td>20231010</td>\n","      <td>20231010</td>\n","      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2310.06825</td>\n","      <td>4</td>\n","      <td>parameters of the architecture are summarized ...</td>\n","      <td>2310.06825</td>\n","      <td>Mistral 7B</td>\n","      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n","      <td>http://arxiv.org/pdf/2310.06825</td>\n","      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n","      <td>[cs.CL, cs.AI, cs.LG]</td>\n","      <td>Models and code are available at\\n  https://mi...</td>\n","      <td>None</td>\n","      <td>cs.CL</td>\n","      <td>20231010</td>\n","      <td>20231010</td>\n","      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb8a5566-a296-4fe3-9f1c-3fbc0177be7a')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-fb8a5566-a296-4fe3-9f1c-3fbc0177be7a button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-fb8a5566-a296-4fe3-9f1c-3fbc0177be7a');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-33193296-e525-423b-a354-c61c1685c8eb\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-33193296-e525-423b-a354-c61c1685c8eb')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-33193296-e525-423b-a354-c61c1685c8eb button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"data","repr_error":"Out of range float values are not JSON compliant: nan"}},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["docs = data[['chunk', 'source']]\n","docs.head(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"17PKB1hQGs_V","executionInfo":{"status":"ok","timestamp":1714784334078,"user_tz":240,"elapsed":201,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}},"outputId":"c094675a-37ae-4859-b6e3-420ce04ca0e0"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                               chunk  \\\n","0  Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...   \n","1  automated benchmarks. Our models are released ...   \n","2  GQA significantly accelerates the inference sp...   \n","3  Mistral 7B takes a significant step in balanci...   \n","4  parameters of the architecture are summarized ...   \n","\n","                            source  \n","0  http://arxiv.org/pdf/2310.06825  \n","1  http://arxiv.org/pdf/2310.06825  \n","2  http://arxiv.org/pdf/2310.06825  \n","3  http://arxiv.org/pdf/2310.06825  \n","4  http://arxiv.org/pdf/2310.06825  "],"text/html":["\n","  <div id=\"df-e8d1f2aa-7c7c-42cc-8e39-c8c0303cbcbd\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>chunk</th>\n","      <th>source</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...</td>\n","      <td>http://arxiv.org/pdf/2310.06825</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>automated benchmarks. Our models are released ...</td>\n","      <td>http://arxiv.org/pdf/2310.06825</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>GQA significantly accelerates the inference sp...</td>\n","      <td>http://arxiv.org/pdf/2310.06825</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Mistral 7B takes a significant step in balanci...</td>\n","      <td>http://arxiv.org/pdf/2310.06825</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>parameters of the architecture are summarized ...</td>\n","      <td>http://arxiv.org/pdf/2310.06825</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e8d1f2aa-7c7c-42cc-8e39-c8c0303cbcbd')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-e8d1f2aa-7c7c-42cc-8e39-c8c0303cbcbd button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-e8d1f2aa-7c7c-42cc-8e39-c8c0303cbcbd');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-9892fcd6-b442-4c32-9be2-f8385ee71d6c\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9892fcd6-b442-4c32-9be2-f8385ee71d6c')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-9892fcd6-b442-4c32-9be2-f8385ee71d6c button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"docs","summary":"{\n  \"name\": \"docs\",\n  \"rows\": 25,\n  \"fields\": [\n    {\n      \"column\": \"chunk\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"and reasoning benchmarks.\\n4Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\\n3\\nFigure 4: Performance of Mistral 7B and different Llama models on a wide range of benchmarks . All\\nmodels were re-evaluated on all metrics with our evaluation pipeline for accurate comparison. Mistral 7B\\nsignificantly outperforms Llama 2 7B and Llama 2 13B on all benchmarks. It is also vastly superior to Llama 1\\n34B in mathematics, code generation, and reasoning benchmarks.\\nModel Modality MMLU HellaSwag WinoG PIQA Arc-e Arc-c NQ TriviaQA HumanEval MBPP MATH GSM8K\\nLLaMA 2 7B Pretrained 44.4% 77.1% 69.5% 77.9% 68.7% 43.2% 24.7% 63.8% 11.6% 26.1% 3.9% 16.0%\\nLLaMA 2 13B Pretrained 55.6% 80.7% 72.9% 80.8% 75.2% 48.8% 29.0% 69.6% 18.9% 35.4% 6.0% 34.3%\",\n          \"in implementing new features and integrating their solutions into ours. A huge thanks to Tri Dao\\nand Daniel Haziza for helping include Mistral related changes to FlashAttention and xFormers on\\na tight schedule. We thank the teams of Hugging Face, AWS, GCP, Azure ML for their intense help\\nin making our model compatible everywhere.\\n6\\nFigure 6: Human evaluation of Mistral 7B \\u2013 Instruct vs Llama 2 13B \\u2013 Chat Example. An example of\\nhuman evaluation from llmboxing.com . The question asks for recommendations of books in quantum physics.\\nLlama 2 13B \\u2013 Chat recommends a general physics book, while Mistral 7B \\u2013 Instruct recommends a more\\nrelevant book on quantum physics and describes in the contents in more detail.\\n7\\nReferences\\n[1]Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr\\u00f3n, and\\nSumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head\\ncheckpoints. arXiv preprint arXiv:2305.13245 , 2023.\\n[2]Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large\",\n          \"Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, L\\u00e9lio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\\u00e9e Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7\\u2013billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B \\u2013 Instruct, that surpasses Llama 2 13B \\u2013 chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"http://arxiv.org/pdf/2310.06825\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["## Build RAG pipeline"],"metadata":{"id":"Z5FflL6gHMvU"}},{"cell_type":"markdown","source":["### Load the documents"],"metadata":{"id":"St5E_bxPHRxS"}},{"cell_type":"code","source":["from langchain_community.document_loaders import DataFrameLoader\n","\n","loader = DataFrameLoader(docs, page_content_column='chunk')"],"metadata":{"id":"O1vx6ZSZHG8L","executionInfo":{"status":"ok","timestamp":1714784488968,"user_tz":240,"elapsed":142,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["documents = loader.load()"],"metadata":{"id":"V6pFZgXZHptW","executionInfo":{"status":"ok","timestamp":1714784498097,"user_tz":240,"elapsed":3,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["documents[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Bj_k15oHvA8","executionInfo":{"status":"ok","timestamp":1714784502817,"user_tz":240,"elapsed":2,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}},"outputId":"c90355e7-dc28-425d-eb09-cb9f5a4f8a4b"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(page_content='Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src', metadata={'source': 'http://arxiv.org/pdf/2310.06825'})"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["documents[0].page_content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178},"id":"bJIKAjQaHwJW","executionInfo":{"status":"ok","timestamp":1714784518445,"user_tz":240,"elapsed":129,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}},"outputId":"b8f087f3-211f-40e5-a477-8168d7337ed7"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["documents[0].metadata"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70UCQUpfHz8x","executionInfo":{"status":"ok","timestamp":1714784543305,"user_tz":240,"elapsed":135,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}},"outputId":"94a33bb4-c92a-4e3c-a429-e8ba0a4dd32a"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'source': 'http://arxiv.org/pdf/2310.06825'}"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["### Split the document - but the data is already in chunks"],"metadata":{"id":"Elwpe1gDIHec"}},{"cell_type":"code","source":["len(documents)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z7w_0wv_Ibeh","executionInfo":{"status":"ok","timestamp":1714784696159,"user_tz":240,"elapsed":3,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}},"outputId":"a9ed0ecd-d7ed-45af-94f4-4f488340cab3"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["25"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["### Create embeddings"],"metadata":{"id":"r3Gaw3iCIvGX"}},{"cell_type":"code","source":["from langchain_openai import OpenAIEmbeddings\n","\n","embeddings = OpenAIEmbeddings()"],"metadata":{"id":"ZYXA_ZEqIuln","executionInfo":{"status":"ok","timestamp":1714785014225,"user_tz":240,"elapsed":268,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":["### Load the embedded data to vector db"],"metadata":{"id":"niP2hlMHJ28L"}},{"cell_type":"code","source":["!pip install langchain-pinecone"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3CobS8AwI5qm","executionInfo":{"status":"ok","timestamp":1714785093560,"user_tz":240,"elapsed":7621,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}},"outputId":"7aa405b9-c070-479c-e217-3131502eef88"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain-pinecone\n","  Downloading langchain_pinecone-0.1.0-py3-none-any.whl (8.4 kB)\n","Requirement already satisfied: langchain-core<0.2.0,>=0.1.40 in /usr/local/lib/python3.10/dist-packages (from langchain-pinecone) (0.1.50)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-pinecone) (1.25.2)\n","Collecting pinecone-client<4.0.0,>=3.2.2 (from langchain-pinecone)\n","  Downloading pinecone_client-3.2.2-py3-none-any.whl (215 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.9/215.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain-pinecone) (6.0.1)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain-pinecone) (1.33)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain-pinecone) (0.1.54)\n","Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain-pinecone) (23.2)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain-pinecone) (2.7.1)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain-pinecone) (8.2.3)\n","Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4.0.0,>=3.2.2->langchain-pinecone) (2024.2.2)\n","Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4.0.0,>=3.2.2->langchain-pinecone) (4.66.2)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4.0.0,>=3.2.2->langchain-pinecone) (4.11.0)\n","Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4.0.0,>=3.2.2->langchain-pinecone) (2.0.7)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.40->langchain-pinecone) (2.4)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone) (3.10.3)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone) (2.31.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.40->langchain-pinecone) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.40->langchain-pinecone) (2.18.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.40->langchain-pinecone) (3.7)\n","Installing collected packages: pinecone-client, langchain-pinecone\n","Successfully installed langchain-pinecone-0.1.0 pinecone-client-3.2.2\n"]}]},{"cell_type":"code","source":["from langchain_pinecone import PineconeVectorStore\n","\n","index_name = 'groq-learning'\n","\n","index = PineconeVectorStore.from_documents(\n","    documents,\n","    embeddings,\n","    index_name=index_name\n",")"],"metadata":{"id":"zVNYZIBjJ-jz","executionInfo":{"status":"ok","timestamp":1714785813977,"user_tz":240,"elapsed":3689,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}}},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":["#### Similarity Search"],"metadata":{"id":"JY2jX0tTM4VL"}},{"cell_type":"code","source":["query = \"Explain what is so special about Mistral Model\"\n","\n","docs = index.similarity_search(query)\n","\n","docs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1bHUEGRmMCP0","executionInfo":{"status":"ok","timestamp":1714785914689,"user_tz":240,"elapsed":312,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}},"outputId":"a9f3b2ee-3c8b-4b0c-f6d0-22514d7b741a"},"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(page_content='preliminary demonstration that the base model can\\neasily be fine-tuned to achieve good performance.\\nIn Table 3, we observe that the resulting model,\\nMistral 7B – Instruct, exhibits superior performance compared to all 7B models on MT-Bench,\\nand is comparable to 13B – Chat models. An\\nindependent human evaluation was conducted on\\nhttps://llmboxing.com/leaderboard .\\nIn this evaluation, participants were provided with a set of questions along with anonymous responses\\nfrom two models and were asked to select their preferred response, as illustrated in Figure 6. As of\\nOctober 6, 2023, the outputs generated by Mistral 7B were preferred 5020 times, compared to 4143\\ntimes for Llama 2 13B.\\n4\\nFigure 5: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension for\\nMistral 7B and Llama 2 (7B/13B/70B) . Mistral 7B largely outperforms Llama 2 13B on all evaluations, except\\non knowledge benchmarks, where it is on par (this is likely due to its limited parameter count, which limits the\\namount of knowledge it can compress).\\n5 Adding guardrails for front-facing applications\\nThe ability to enforce guardrails when it comes to AI generation is important for front-facing applications. In this section, we highlight how to leverage system prompting to optionally enforce output', metadata={'source': 'http://arxiv.org/pdf/2310.06825'}),\n"," Document(page_content='automated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src\\nWebpage: https://mistral.ai/news/announcing-mistral-7b/\\n1 Introduction\\nIn the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model\\nperformance often necessitates an escalation in model size. However, this scaling tends to increase\\ncomputational costs and inference latency, thereby raising barriers to deployment in practical,\\nreal-world scenarios. In this context, the search for balanced models delivering both high-level\\nperformance and efficiency becomes critically essential. Our model, Mistral 7B, demonstrates that\\na carefully designed language model can deliver high performance while maintaining an efficient\\ninference. Mistral 7B outperforms the previous best 13B model (Llama 2, [ 26]) across all tested\\nbenchmarks, and surpasses the best 34B model (LLaMa 34B, [ 25]) in mathematics and code\\ngeneration. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20],\\nwithout sacrificing performance on non-code related benchmarks.\\nMistral 7B leverages grouped-query attention (GQA) [ 1], and sliding window attention (SWA) [ 6,3].\\nGQA significantly accelerates the inference speed, and also reduces the memory requirement during', metadata={'source': 'http://arxiv.org/pdf/2310.06825'}),\n"," Document(page_content='answer. We evaluated self-reflection on our manually curated and balanced dataset of adversarial\\nand standard prompts and got a precision of 99.4% for a recall of 95.6% (considering acceptable\\nprompts as positives).\\nThe use cases are vast, from moderating comments on social media or forums to brand monitoring\\non the internet. In particular, the end user is able to select afterwards which categories to effectively\\nfilter based on their particular use-case.\\n6 Conclusion\\nOur work on Mistral 7B demonstrates that language models may compress knowledge more than\\nwhat was previously thought. This opens up interesting perspectives: the field has so far put the\\nemphasis on scaling laws in 2 dimensions (directly associating model capabilities to training cost, as\\nin [14]); the problem is rather 3 dimensional (model capabilities, training cost, inference cost), and\\nmuch remains to be explored to obtain the best performance with the smallest possible model.\\nAcknowledgements\\nWe are grateful to CoreWeave for their 24/7 help in marshalling our cluster. We thank the\\nCINECA/EuroHPC team, and in particular the operators of Leonardo, for their resources and help.\\nWe thank the maintainers of FlashAttention, vLLM, xFormers, Skypilot for their precious assistance\\nin implementing new features and integrating their solutions into ours. A huge thanks to Tri Dao\\nand Daniel Haziza for helping include Mistral related changes to FlashAttention and xFormers on', metadata={'source': 'http://arxiv.org/pdf/2310.06825'}),\n"," Document(page_content='Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src', metadata={'source': 'http://arxiv.org/pdf/2310.06825'})]"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["query = \"Explain what is so special about Mistral Model\"\n","\n","docs = index.similarity_search(query, k = 2)\n","\n","docs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2sw1KPRYNFQg","executionInfo":{"status":"ok","timestamp":1714785936950,"user_tz":240,"elapsed":421,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}},"outputId":"d5017b8e-081d-4e78-a3f2-f49e43cd3412"},"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(page_content='preliminary demonstration that the base model can\\neasily be fine-tuned to achieve good performance.\\nIn Table 3, we observe that the resulting model,\\nMistral 7B – Instruct, exhibits superior performance compared to all 7B models on MT-Bench,\\nand is comparable to 13B – Chat models. An\\nindependent human evaluation was conducted on\\nhttps://llmboxing.com/leaderboard .\\nIn this evaluation, participants were provided with a set of questions along with anonymous responses\\nfrom two models and were asked to select their preferred response, as illustrated in Figure 6. As of\\nOctober 6, 2023, the outputs generated by Mistral 7B were preferred 5020 times, compared to 4143\\ntimes for Llama 2 13B.\\n4\\nFigure 5: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension for\\nMistral 7B and Llama 2 (7B/13B/70B) . Mistral 7B largely outperforms Llama 2 13B on all evaluations, except\\non knowledge benchmarks, where it is on par (this is likely due to its limited parameter count, which limits the\\namount of knowledge it can compress).\\n5 Adding guardrails for front-facing applications\\nThe ability to enforce guardrails when it comes to AI generation is important for front-facing applications. In this section, we highlight how to leverage system prompting to optionally enforce output', metadata={'source': 'http://arxiv.org/pdf/2310.06825'}),\n"," Document(page_content='automated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src\\nWebpage: https://mistral.ai/news/announcing-mistral-7b/\\n1 Introduction\\nIn the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model\\nperformance often necessitates an escalation in model size. However, this scaling tends to increase\\ncomputational costs and inference latency, thereby raising barriers to deployment in practical,\\nreal-world scenarios. In this context, the search for balanced models delivering both high-level\\nperformance and efficiency becomes critically essential. Our model, Mistral 7B, demonstrates that\\na carefully designed language model can deliver high performance while maintaining an efficient\\ninference. Mistral 7B outperforms the previous best 13B model (Llama 2, [ 26]) across all tested\\nbenchmarks, and surpasses the best 34B model (LLaMa 34B, [ 25]) in mathematics and code\\ngeneration. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20],\\nwithout sacrificing performance on non-code related benchmarks.\\nMistral 7B leverages grouped-query attention (GQA) [ 1], and sliding window attention (SWA) [ 6,3].\\nGQA significantly accelerates the inference speed, and also reduces the memory requirement during', metadata={'source': 'http://arxiv.org/pdf/2310.06825'})]"]},"metadata":{},"execution_count":50}]},{"cell_type":"markdown","source":["### Create a retriever"],"metadata":{"id":"Spmp_zruNa2R"}},{"cell_type":"code","source":["retriever = index.as_retriever(search_type = \"mmr\")"],"metadata":{"id":"giDXGh1eNONV","executionInfo":{"status":"ok","timestamp":1714786017226,"user_tz":240,"elapsed":111,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["matched_docs = retriever.get_relevant_documents(query)\n","\n","for i, d in enumerate(matched_docs):\n","  print(f\"\\n## Document {i}\\n\")\n","  print(d.page_content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vEuwNxzfNh5c","executionInfo":{"status":"ok","timestamp":1714786063807,"user_tz":240,"elapsed":918,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}},"outputId":"3923c7d0-ab06-4d01-af55-d13caa11c25b"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","## Document 0\n","\n","preliminary demonstration that the base model can\n","easily be fine-tuned to achieve good performance.\n","In Table 3, we observe that the resulting model,\n","Mistral 7B – Instruct, exhibits superior performance compared to all 7B models on MT-Bench,\n","and is comparable to 13B – Chat models. An\n","independent human evaluation was conducted on\n","https://llmboxing.com/leaderboard .\n","In this evaluation, participants were provided with a set of questions along with anonymous responses\n","from two models and were asked to select their preferred response, as illustrated in Figure 6. As of\n","October 6, 2023, the outputs generated by Mistral 7B were preferred 5020 times, compared to 4143\n","times for Llama 2 13B.\n","4\n","Figure 5: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension for\n","Mistral 7B and Llama 2 (7B/13B/70B) . Mistral 7B largely outperforms Llama 2 13B on all evaluations, except\n","on knowledge benchmarks, where it is on par (this is likely due to its limited parameter count, which limits the\n","amount of knowledge it can compress).\n","5 Adding guardrails for front-facing applications\n","The ability to enforce guardrails when it comes to AI generation is important for front-facing applications. In this section, we highlight how to leverage system prompting to optionally enforce output\n","\n","## Document 1\n","\n","parameters of the architecture are summarized in Table 1. Compared\n","to Llama, it introduces a few changes that we summarize below.\n","Sliding Window Attention. SWA exploits the stacked layers of a transformer to attend information beyond the window size W. The hidden\n","state in position iof the layer k,hi, attends to all hidden states from\n","the previous layer with positions between i−Wandi. Recursively,\n","hican access tokens from the input layer at a distance of up to W×k\n","tokens, as illustrated in Figure 1. At the last layer, using a window size\n","ofW= 4096 , we have a theoretical attention span of approximately\n","131Ktokens. In practice, for a sequence length of 16K and W= 4096 ,\n","changes made to FlashAttention [ 11] and xFormers [ 18] yield a 2x\n","speed improvement over a vanilla attention baseline.\n","Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling\n","buffer cache. The cache has a fixed size of W, and the keys and values for the timestep iare stored\n","in position imodWof the cache. As a result, when the position iis larger than W, past values\n","in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration\n","in Figure 2 for W= 3. On a sequence length of 32k tokens, this reduces the cache memory usage\n","\n","## Document 2\n","\n","answer. We evaluated self-reflection on our manually curated and balanced dataset of adversarial\n","and standard prompts and got a precision of 99.4% for a recall of 95.6% (considering acceptable\n","prompts as positives).\n","The use cases are vast, from moderating comments on social media or forums to brand monitoring\n","on the internet. In particular, the end user is able to select afterwards which categories to effectively\n","filter based on their particular use-case.\n","6 Conclusion\n","Our work on Mistral 7B demonstrates that language models may compress knowledge more than\n","what was previously thought. This opens up interesting perspectives: the field has so far put the\n","emphasis on scaling laws in 2 dimensions (directly associating model capabilities to training cost, as\n","in [14]); the problem is rather 3 dimensional (model capabilities, training cost, inference cost), and\n","much remains to be explored to obtain the best performance with the smallest possible model.\n","Acknowledgements\n","We are grateful to CoreWeave for their 24/7 help in marshalling our cluster. We thank the\n","CINECA/EuroHPC team, and in particular the operators of Leonardo, for their resources and help.\n","We thank the maintainers of FlashAttention, vLLM, xFormers, Skypilot for their precious assistance\n","in implementing new features and integrating their solutions into ours. A huge thanks to Tri Dao\n","and Daniel Haziza for helping include Mistral related changes to FlashAttention and xFormers on\n","\n","## Document 3\n","\n","Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping\n","large language models efficient. Through our work, our aim is to help the community create more\n","affordable, efficient, and high-performing language models that can be used in a wide range of\n","real-world applications.\n","2 Architectural details\n","Figure 1: Sliding Window Attention. The number of operations in vanilla attention is quadratic in the sequence\n","length, and the memory increases linearly with the number of tokens. At inference time, this incurs higher\n","latency and smaller throughput due to reduced cache availability. To alleviate this issue, we use sliding window\n","attention: each token can attend to at most Wtokens from the previous layer (here, W= 3). Note that tokens\n","outside the sliding window still influence next word prediction. At each attention layer, information can move\n","forward by Wtokens. Hence, after kattention layers, information can move forward by up to k×Wtokens.\n","Parameter Value\n","dim 4096\n","n_layers 32\n","head_dim 128\n","hidden_dim 14336\n","n_heads 32\n","n_kv_heads 8\n","window_size 4096\n","context_len 8192\n","vocab_size 32000\n","Table 1: Model architecture.Mistral 7B is based on a transformer architecture [ 27]. The main\n","parameters of the architecture are summarized in Table 1. Compared\n","to Llama, it introduces a few changes that we summarize below.\n"]}]},{"cell_type":"markdown","source":["### Load QA Chain"],"metadata":{"id":"6FVX7q8SNwxM"}},{"cell_type":"code","source":["from langchain.chains import RetrievalQA\n","from langchain.chains.question_answering import load_qa_chain"],"metadata":{"id":"GU1bFdinOAAJ","executionInfo":{"status":"ok","timestamp":1714786155520,"user_tz":240,"elapsed":179,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["llm = ChatGroq(temperature=0,\n","               model_name = 'mixtral-8x7b-32768')"],"metadata":{"id":"qkTFL0zKNtGN","executionInfo":{"status":"ok","timestamp":1714786156313,"user_tz":240,"elapsed":145,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}}},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":["#### Load QA Chain"],"metadata":{"id":"S9AvRUjOOX4N"}},{"cell_type":"code","source":["chain = load_qa_chain(llm, chain_type=\"stuff\")"],"metadata":{"id":"SGI-PdQ1N0ke","executionInfo":{"status":"ok","timestamp":1714786156454,"user_tz":240,"elapsed":142,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["query = \"Explain what is so special about Mistral Model\"\n","\n","similarity_docs = index.similarity_search(query)\n","\n","response = chain.run(question = query, input_documents = similarity_docs)"],"metadata":{"id":"tj6CjUBMN8Z6","executionInfo":{"status":"ok","timestamp":1714786206702,"user_tz":240,"elapsed":2301,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["response"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":196},"id":"R-gABnIeOPoF","executionInfo":{"status":"ok","timestamp":1714786209180,"user_tz":240,"elapsed":154,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}},"outputId":"8f9aa0e7-58ed-4eb5-ec36-50d1be607800"},"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Mistral is a 7-billion-parameter language model that is engineered for superior performance and efficiency. It outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. The Mistral model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. Additionally, Mistral has released a model fine-tuned to follow instructions, Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and automated benchmarks. The models are released under the Apache 2.0 license. Mistral 7B largely outperforms Llama 2 13B on all evaluations, except on knowledge benchmarks, where it is on par. This is likely due to its limited parameter count, which limits the amount of knowledge it can compress. The models have been evaluated on various benchmarks and have shown to have human-competitive performance, making it a special model in the field of NLP.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":59}]},{"cell_type":"markdown","source":["#### Retriever QA Chain"],"metadata":{"id":"i4T0-HDnOarh"}},{"cell_type":"code","source":["qa_chain = RetrievalQA.from_chain_type(\n","    llm = llm,\n","    chain_type=\"stuff\",\n","    retriever=retriever,\n","    return_source_documents=True\n",")"],"metadata":{"id":"1VgBE53QOQt5","executionInfo":{"status":"ok","timestamp":1714786267729,"user_tz":240,"elapsed":111,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["result = qa_chain({'query':query})"],"metadata":{"id":"NJYR5YJKOfDJ","executionInfo":{"status":"ok","timestamp":1714786291378,"user_tz":240,"elapsed":1834,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["result"],"metadata":{"id":"Wf09dFxDOzGU","executionInfo":{"status":"ok","timestamp":1714786352597,"user_tz":240,"elapsed":6,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}},"outputId":"1e1a3d3f-2d3d-4c49-ae0a-0ac91ed195ff","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'query': 'Explain what is so special about Mistral Model',\n"," 'result': 'Mistral 7B is a language model that stands out for its efficient balance between performance and cost. It is based on a transformer architecture with a sliding window attention mechanism, which allows each token to attend to a fixed number of tokens from the previous layer, reducing the operations required and memory usage. This mechanism also enables a larger theoretical attention span of approximately 131K tokens.\\n\\nAdditionally, Mistral 7B features a rolling buffer cache with a fixed size, reducing cache memory usage. It also demonstrates strong knowledge compression capabilities, outperforming other models in various evaluations such as MMLU, commonsense reasoning, and reading comprehension. Furthermore, it has the ability to enforce guardrails for front-facing applications, ensuring safe AI generation.\\n\\nIn terms of architecture, Mistral 7B has a hidden dimension of 14336, 32 layers, a head dimension of 128, and a window size of 4096, among other parameters. These characteristics make Mistral 7B a powerful and efficient language model suitable for a wide range of real-world applications.',\n"," 'source_documents': [Document(page_content='preliminary demonstration that the base model can\\neasily be fine-tuned to achieve good performance.\\nIn Table 3, we observe that the resulting model,\\nMistral 7B – Instruct, exhibits superior performance compared to all 7B models on MT-Bench,\\nand is comparable to 13B – Chat models. An\\nindependent human evaluation was conducted on\\nhttps://llmboxing.com/leaderboard .\\nIn this evaluation, participants were provided with a set of questions along with anonymous responses\\nfrom two models and were asked to select their preferred response, as illustrated in Figure 6. As of\\nOctober 6, 2023, the outputs generated by Mistral 7B were preferred 5020 times, compared to 4143\\ntimes for Llama 2 13B.\\n4\\nFigure 5: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension for\\nMistral 7B and Llama 2 (7B/13B/70B) . Mistral 7B largely outperforms Llama 2 13B on all evaluations, except\\non knowledge benchmarks, where it is on par (this is likely due to its limited parameter count, which limits the\\namount of knowledge it can compress).\\n5 Adding guardrails for front-facing applications\\nThe ability to enforce guardrails when it comes to AI generation is important for front-facing applications. In this section, we highlight how to leverage system prompting to optionally enforce output', metadata={'source': 'http://arxiv.org/pdf/2310.06825'}),\n","  Document(page_content='parameters of the architecture are summarized in Table 1. Compared\\nto Llama, it introduces a few changes that we summarize below.\\nSliding Window Attention. SWA exploits the stacked layers of a transformer to attend information beyond the window size W. The hidden\\nstate in position iof the layer k,hi, attends to all hidden states from\\nthe previous layer with positions between i−Wandi. Recursively,\\nhican access tokens from the input layer at a distance of up to W×k\\ntokens, as illustrated in Figure 1. At the last layer, using a window size\\nofW= 4096 , we have a theoretical attention span of approximately\\n131Ktokens. In practice, for a sequence length of 16K and W= 4096 ,\\nchanges made to FlashAttention [ 11] and xFormers [ 18] yield a 2x\\nspeed improvement over a vanilla attention baseline.\\nRolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling\\nbuffer cache. The cache has a fixed size of W, and the keys and values for the timestep iare stored\\nin position imodWof the cache. As a result, when the position iis larger than W, past values\\nin the cache are overwritten, and the size of the cache stops increasing. We provide an illustration\\nin Figure 2 for W= 3. On a sequence length of 32k tokens, this reduces the cache memory usage', metadata={'source': 'http://arxiv.org/pdf/2310.06825'}),\n","  Document(page_content='answer. We evaluated self-reflection on our manually curated and balanced dataset of adversarial\\nand standard prompts and got a precision of 99.4% for a recall of 95.6% (considering acceptable\\nprompts as positives).\\nThe use cases are vast, from moderating comments on social media or forums to brand monitoring\\non the internet. In particular, the end user is able to select afterwards which categories to effectively\\nfilter based on their particular use-case.\\n6 Conclusion\\nOur work on Mistral 7B demonstrates that language models may compress knowledge more than\\nwhat was previously thought. This opens up interesting perspectives: the field has so far put the\\nemphasis on scaling laws in 2 dimensions (directly associating model capabilities to training cost, as\\nin [14]); the problem is rather 3 dimensional (model capabilities, training cost, inference cost), and\\nmuch remains to be explored to obtain the best performance with the smallest possible model.\\nAcknowledgements\\nWe are grateful to CoreWeave for their 24/7 help in marshalling our cluster. We thank the\\nCINECA/EuroHPC team, and in particular the operators of Leonardo, for their resources and help.\\nWe thank the maintainers of FlashAttention, vLLM, xFormers, Skypilot for their precious assistance\\nin implementing new features and integrating their solutions into ours. A huge thanks to Tri Dao\\nand Daniel Haziza for helping include Mistral related changes to FlashAttention and xFormers on', metadata={'source': 'http://arxiv.org/pdf/2310.06825'}),\n","  Document(page_content='Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping\\nlarge language models efficient. Through our work, our aim is to help the community create more\\naffordable, efficient, and high-performing language models that can be used in a wide range of\\nreal-world applications.\\n2 Architectural details\\nFigure 1: Sliding Window Attention. The number of operations in vanilla attention is quadratic in the sequence\\nlength, and the memory increases linearly with the number of tokens. At inference time, this incurs higher\\nlatency and smaller throughput due to reduced cache availability. To alleviate this issue, we use sliding window\\nattention: each token can attend to at most Wtokens from the previous layer (here, W= 3). Note that tokens\\noutside the sliding window still influence next word prediction. At each attention layer, information can move\\nforward by Wtokens. Hence, after kattention layers, information can move forward by up to k×Wtokens.\\nParameter Value\\ndim 4096\\nn_layers 32\\nhead_dim 128\\nhidden_dim 14336\\nn_heads 32\\nn_kv_heads 8\\nwindow_size 4096\\ncontext_len 8192\\nvocab_size 32000\\nTable 1: Model architecture.Mistral 7B is based on a transformer architecture [ 27]. The main\\nparameters of the architecture are summarized in Table 1. Compared\\nto Llama, it introduces a few changes that we summarize below.', metadata={'source': 'http://arxiv.org/pdf/2310.06825'})]}"]},"metadata":{},"execution_count":66}]},{"cell_type":"code","source":["result['result']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178},"id":"BjvH5IzuOkY9","executionInfo":{"status":"ok","timestamp":1714786344918,"user_tz":240,"elapsed":5,"user":{"displayName":"Colab Shubham","userId":"04861373210350452225"}},"outputId":"080a2275-509c-46bf-c0b1-b5e0d395af45"},"execution_count":65,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Mistral 7B is a language model that stands out for its efficient balance between performance and cost. It is based on a transformer architecture with a sliding window attention mechanism, which allows each token to attend to a fixed number of tokens from the previous layer, reducing the operations required and memory usage. This mechanism also enables a larger theoretical attention span of approximately 131K tokens.\\n\\nAdditionally, Mistral 7B features a rolling buffer cache with a fixed size, reducing cache memory usage. It also demonstrates strong knowledge compression capabilities, outperforming other models in various evaluations such as MMLU, commonsense reasoning, and reading comprehension. Furthermore, it has the ability to enforce guardrails for front-facing applications, ensuring safe AI generation.\\n\\nIn terms of architecture, Mistral 7B has a hidden dimension of 14336, 32 layers, a head dimension of 128, and a window size of 4096, among other parameters. These characteristics make Mistral 7B a powerful and efficient language model suitable for a wide range of real-world applications.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":65}]},{"cell_type":"code","source":[],"metadata":{"id":"HsydLB2LOlV3"},"execution_count":null,"outputs":[]}]}
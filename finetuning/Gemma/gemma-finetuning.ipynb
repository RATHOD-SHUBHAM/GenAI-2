{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0126e17a-ec13-446c-bebc-05fd0f4afe2d",
   "metadata": {},
   "source": [
    "# Fine tuning Google Gemma 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b3c521-949c-4f04-a01e-cecdcbd96bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 18 11:21:32 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:61:00.0 Off |                  Off |\n",
      "|  0%   39C    P8              27W / 450W |      1MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6506f2-e379-4748-8e35-97ebba6c1293",
   "metadata": {},
   "source": [
    "# 6 Steps to Fine Tuning:\n",
    "    1. Setting up the environment - Install the required library.\n",
    "    2. Load the model and chat format.\n",
    "    3. Load and format the dataset.\n",
    "    4. LoRA Config.\n",
    "    5. Fine-Tuning.\n",
    "    6. Pushing the model to Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c32f3c4-b7fd-474d-9f61-e36af0e6becb",
   "metadata": {},
   "source": [
    "# Step 1: Install all the required library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0abf0203-b865-4a32-86a3-d9d921ab3bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -q -U accelerate==0.27.1\n",
    "!pip3 install -q -U peft==0.8.2 # Parametere effecient finetuning - LoRA Config\n",
    "!pip3 install -q -U bitsandbytes==0.42.0 # Load quantized version of model\n",
    "!pip3 install -q -U transformers==4.38.0\n",
    "!pip3 install -q -U trl==0.7.10 # Supervised finetuning\n",
    "!pip3 install -q -U datasets==2.17.0\n",
    "!pip3 install -q -U scipy\n",
    "!pip3 install -q -U tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c86b7d5-7ffd-437e-99cf-069aaea40641",
   "metadata": {},
   "source": [
    "### Import the necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19896eab-48f2-4f4c-8e5a-940d04bfe8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import(\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce433d40-31da-4b8d-a894-ea5c24366a62",
   "metadata": {},
   "source": [
    "# Log in to hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "189b91e4-59ea-48bb-a346-9becaa8bf528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e223b1f7-22e0-4d9d-9929-9ce621a32d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb6e24af53c4db3aa385949b5274d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0592211f-d017-4489-a2c2-081786633ec7",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b16671-42e0-49bc-a2ea-38b352318d40",
   "metadata": {},
   "source": [
    "## Dataset used\n",
    "[TokenBender/code_instructions_122k_alpaca_style](https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58ba25e1-9676-4889-a86c-3b72dfcce34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3af77ecb-e063-4e0a-80cb-d57c4e795130",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split= \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7589573e-04f1-4536-9e83-a59d088a7ef7",
   "metadata": {},
   "source": [
    "### Vizualize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30724d92-cd8a-4cda-bd36-f884d6c46fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'text', 'output', 'instruction'],\n",
       "    num_rows: 121959\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "032641c9-ebab-45f5-8072-b7fdf07c33e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "      <th>instruction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td># Python code\\ndef sum_sequence(sequence):\\n  ...</td>\n",
       "      <td>Create a function to calculate the sum of a se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>str1 = \"Hello \"\\nstr2 = \"world\"</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>def add_strings(str1, str2):\\n    \"\"\"This func...</td>\n",
       "      <td>Develop a function that will add two strings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>#include &lt;map&gt;\\n#include &lt;string&gt;\\n\\nclass Gro...</td>\n",
       "      <td>Design a data structure in C++ to store inform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[3, 1, 4, 5, 9, 0]</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>def bubble_sort(arr):\\n    n = len(arr)\\n \\n  ...</td>\n",
       "      <td>Implement a sorting algorithm to sort a given ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not applicable</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>import UIKit\\n\\nclass ExpenseViewController: U...</td>\n",
       "      <td>Design a Swift application for tracking expens...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             input  \\\n",
       "0                  [1, 2, 3, 4, 5]   \n",
       "1  str1 = \"Hello \"\\nstr2 = \"world\"   \n",
       "2                                    \n",
       "3               [3, 1, 4, 5, 9, 0]   \n",
       "4                   Not applicable   \n",
       "\n",
       "                                                text  \\\n",
       "0  Below is an instruction that describes a task....   \n",
       "1  Below is an instruction that describes a task....   \n",
       "2  Below is an instruction that describes a task....   \n",
       "3  Below is an instruction that describes a task....   \n",
       "4  Below is an instruction that describes a task....   \n",
       "\n",
       "                                              output  \\\n",
       "0  # Python code\\ndef sum_sequence(sequence):\\n  ...   \n",
       "1  def add_strings(str1, str2):\\n    \"\"\"This func...   \n",
       "2  #include <map>\\n#include <string>\\n\\nclass Gro...   \n",
       "3  def bubble_sort(arr):\\n    n = len(arr)\\n \\n  ...   \n",
       "4  import UIKit\\n\\nclass ExpenseViewController: U...   \n",
       "\n",
       "                                         instruction  \n",
       "0  Create a function to calculate the sum of a se...  \n",
       "1       Develop a function that will add two strings  \n",
       "2  Design a data structure in C++ to store inform...  \n",
       "3  Implement a sorting algorithm to sort a given ...  \n",
       "4  Design a Swift application for tracking expens...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset.to_pandas()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6365d697-1a82-44dd-8933-68f68b6f4708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'text', 'output', 'instruction'],\n",
       "    num_rows: 121959\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b016672-3558-4f59-aff2-93654b26de0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '[1, 2, 3, 4, 5]',\n",
       " 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: Create a function to calculate the sum of a sequence of integers. ### Input: [1, 2, 3, 4, 5] ### Output: # Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum',\n",
       " 'output': '# Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum',\n",
       " 'instruction': 'Create a function to calculate the sum of a sequence of integers.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9366438a-266d-4ce0-a0da-85f2a6904be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def7c370-dd43-4048-a7bb-5d23a6f00602",
   "metadata": {},
   "source": [
    "#### Each dataset will have datapoint that hold this information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b744a369-7f37-4a55-bf86-2b2717a49a81",
   "metadata": {},
   "source": [
    "```\n",
    "{\n",
    "  \"instruction\": \"Create a function to calculate the sum of a sequence of integers.\",\n",
    "  \"input\": \"[1, 2, 3, 4, 5]\",\n",
    "  \"output\": \"# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24b342d8-d4df-4b1f-bf4c-8ae2d761073a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '[1, 2, 3, 4, 5]', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: Create a function to calculate the sum of a sequence of integers. ### Input: [1, 2, 3, 4, 5] ### Output: # Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum', 'output': '# Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum', 'instruction': 'Create a function to calculate the sum of a sequence of integers.'}\n"
     ]
    }
   ],
   "source": [
    "for datapoint in dataset:\n",
    "    print(datapoint)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2d7d29-2529-4e64-b5f1-0a2f7eaded08",
   "metadata": {},
   "source": [
    "## Getting the right format\n",
    "Instruction Fintuning - Prepare the dataset under the format of \"prompt\" so the model can better understand :\n",
    "\n",
    "We will add a new colum called as prompt, that will store the gemma prompt.\n",
    "\n",
    "    1. The function generate_prompt : take the instruction and output and generate a prompt.\n",
    "    2. Shuffle the dataset.\n",
    "    3. Tokenizer the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c13c7fe-8207-4280-a23b-ef6c2992e0e9",
   "metadata": {},
   "source": [
    "#### Gemma format\n",
    "[Gemma Prompt Template](https://huggingface.co/google/gemma-7b-it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f73fe57-7b7f-47ab-9a16-789b06e3f9f5",
   "metadata": {},
   "source": [
    "```\n",
    "<start_of_turn>user What is your favorite condiment? <end_of_turn>\n",
    "<start_of_turn>model Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavor to whatever I'm cooking up in the kitchen!<end_of_turn>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55b24c30-088e-4819-ad87-188d15aca9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    # text from the dataset - we can specify anything we want\n",
    "    prefix_text = 'Below is a an instruction that describes a particular task, Write a response that appropriately completes the given request. \\n\\n'\n",
    "\n",
    "    if data_point['input']:\n",
    "        # If there are some input available\n",
    "        text = f\"\"\"<start_of_turn>user {prefix_text} {data_point['instruction']} Here is a input {data_point['input']} <end_of_turn>\\n<start_of_turn>model {data_point['output']} <end_of_turn>\"\"\"\n",
    "    else:\n",
    "        # If there is no input available\n",
    "        text = f\"\"\"<start_of_turn>user {prefix_text} {data_point['instruction']} <end_of_turn>\\n<start_of_turn>model {data_point['output']} <end_of_turn>\"\"\"\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41416b3f-886f-4a8f-978c-3115f6a12010",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column = []\n",
    "for data_point in dataset:\n",
    "    text = generate_prompt(data_point)\n",
    "    text_column.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19a87d6e-35e5-48f7-8578-eef4135abbf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start_of_turn>user Below is a an instruction that describes a particular task, Write a response that appropriately completes the given request. \\n\\n Create a function to calculate the sum of a sequence of integers. Here is a input [1, 2, 3, 4, 5] <end_of_turn>\\n<start_of_turn>model # Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum <end_of_turn>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_column[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a210bd7-76ff-46e1-81f9-14d812c952c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above code can also be written as list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "988fa714-bf40-4a6d-80fd-a80621a6ab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column = [generate_prompt(data_point) for data_point in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdf9d297-8190-4578-b8ff-be13914ffe74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start_of_turn>user Below is a an instruction that describes a particular task, Write a response that appropriately completes the given request. \\n\\n Create a function to calculate the sum of a sequence of integers. Here is a input [1, 2, 3, 4, 5] <end_of_turn>\\n<start_of_turn>model # Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum <end_of_turn>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_column[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "084b8f89-774b-463f-be8a-8f3210dd7497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add back to the dataset\n",
    "dataset = dataset.add_column(\"prompt\", text_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a47f58b-e76a-4352-970f-74e5141a9c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'text', 'output', 'instruction', 'prompt'],\n",
       "    num_rows: 121959\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0477e22c-7014-4ab2-ab7b-3b54921fc949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "      <th>instruction</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td># Python code\\ndef sum_sequence(sequence):\\n  ...</td>\n",
       "      <td>Create a function to calculate the sum of a se...</td>\n",
       "      <td>&lt;start_of_turn&gt;user Below is a an instruction ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>str1 = \"Hello \"\\nstr2 = \"world\"</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>def add_strings(str1, str2):\\n    \"\"\"This func...</td>\n",
       "      <td>Develop a function that will add two strings</td>\n",
       "      <td>&lt;start_of_turn&gt;user Below is a an instruction ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>#include &lt;map&gt;\\n#include &lt;string&gt;\\n\\nclass Gro...</td>\n",
       "      <td>Design a data structure in C++ to store inform...</td>\n",
       "      <td>&lt;start_of_turn&gt;user Below is a an instruction ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[3, 1, 4, 5, 9, 0]</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>def bubble_sort(arr):\\n    n = len(arr)\\n \\n  ...</td>\n",
       "      <td>Implement a sorting algorithm to sort a given ...</td>\n",
       "      <td>&lt;start_of_turn&gt;user Below is a an instruction ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not applicable</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>import UIKit\\n\\nclass ExpenseViewController: U...</td>\n",
       "      <td>Design a Swift application for tracking expens...</td>\n",
       "      <td>&lt;start_of_turn&gt;user Below is a an instruction ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             input  \\\n",
       "0                  [1, 2, 3, 4, 5]   \n",
       "1  str1 = \"Hello \"\\nstr2 = \"world\"   \n",
       "2                                    \n",
       "3               [3, 1, 4, 5, 9, 0]   \n",
       "4                   Not applicable   \n",
       "\n",
       "                                                text  \\\n",
       "0  Below is an instruction that describes a task....   \n",
       "1  Below is an instruction that describes a task....   \n",
       "2  Below is an instruction that describes a task....   \n",
       "3  Below is an instruction that describes a task....   \n",
       "4  Below is an instruction that describes a task....   \n",
       "\n",
       "                                              output  \\\n",
       "0  # Python code\\ndef sum_sequence(sequence):\\n  ...   \n",
       "1  def add_strings(str1, str2):\\n    \"\"\"This func...   \n",
       "2  #include <map>\\n#include <string>\\n\\nclass Gro...   \n",
       "3  def bubble_sort(arr):\\n    n = len(arr)\\n \\n  ...   \n",
       "4  import UIKit\\n\\nclass ExpenseViewController: U...   \n",
       "\n",
       "                                         instruction  \\\n",
       "0  Create a function to calculate the sum of a se...   \n",
       "1       Develop a function that will add two strings   \n",
       "2  Design a data structure in C++ to store inform...   \n",
       "3  Implement a sorting algorithm to sort a given ...   \n",
       "4  Design a Swift application for tracking expens...   \n",
       "\n",
       "                                              prompt  \n",
       "0  <start_of_turn>user Below is a an instruction ...  \n",
       "1  <start_of_turn>user Below is a an instruction ...  \n",
       "2  <start_of_turn>user Below is a an instruction ...  \n",
       "3  <start_of_turn>user Below is a an instruction ...  \n",
       "4  <start_of_turn>user Below is a an instruction ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset.to_pandas()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "011a6964-aa82-4bd4-9913-ea568317beca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '[1, 2, 3, 4, 5]', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: Create a function to calculate the sum of a sequence of integers. ### Input: [1, 2, 3, 4, 5] ### Output: # Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum', 'output': '# Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum', 'instruction': 'Create a function to calculate the sum of a sequence of integers.', 'prompt': '<start_of_turn>user Below is a an instruction that describes a particular task, Write a response that appropriately completes the given request. \\n\\n Create a function to calculate the sum of a sequence of integers. Here is a input [1, 2, 3, 4, 5] <end_of_turn>\\n<start_of_turn>model # Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum <end_of_turn>'}\n"
     ]
    }
   ],
   "source": [
    "for i in dataset:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fb5904-34df-46f7-ae40-f509fa22e327",
   "metadata": {},
   "source": [
    "# Define Paramters for FineTuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34143130-f393-45c8-80a5-6db36c0c8495",
   "metadata": {},
   "source": [
    "## 1. Bits and Bytes Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fa9dc95-0eaf-4e78-987e-da0249418942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base model\n",
    "bnb_4bit_compute_dtype = 'float16'\n",
    "\n",
    "# Quantized type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = 'nf4'\n",
    "\n",
    "# Activate nested quantization for 4 bit base model (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62de6f83-50b4-41a8-8191-0c28140040e8",
   "metadata": {},
   "source": [
    "### 2. QLoRA Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "982790c5-d421-4db8-8d16-2ba302d3af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e83fac-7f8d-49c0-a96b-ba5ffd03c3e9",
   "metadata": {},
   "source": [
    "### 3. Training Arguments Parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9a172997-d053-4eab-a797-8c7e9b442305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory where the model prediction and checkpoint will be stored\n",
    "output_dir = './results'\n",
    "\n",
    "# Number of training epochs\n",
    "# num_train_epochs = 2\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to true with A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias / LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# optimzer to use\n",
    "optim = \"paged_adamw_8bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (override num_train_epochs)\n",
    "# max_steps = -1\n",
    "max_steps = 200\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Save memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "save_strategy = \"epoch\"\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845a7df0-18a0-4dd8-8eee-6a662455abd3",
   "metadata": {},
   "source": [
    "### 4. SFT Parematers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d04593f8-d49c-4082-a080-f552855a0b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = None\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\" : 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d00ae1b-f0ca-43f0-bd2e-4d06c96fed03",
   "metadata": {},
   "source": [
    "# Load the Model and Tokenizer\n",
    "\n",
    "```\n",
    "model_id = \"google/gemma-7b-it\"\n",
    "model_id = \"google/gemma-7b\"\n",
    "model_id = \"google/gemma-2b-it\"\n",
    "model_id = \"google/gemma-2b\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e653831f-27f5-49d7-af44-71e4b2c5b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/gemma-2b-it'\n",
    "\n",
    "new_model_name = 'my-code-gemma-finetuned-it'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c0967d2-ca5b-4ca2-91f3-40d0afa0a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ea123c-2c6d-437b-b643-7ecad2229b30",
   "metadata": {},
   "source": [
    "### bnb config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b555b4d-6c8e-491c-8c4d-215c4266af72",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = use_4bit,\n",
    "    bnb_4bit_quant_type = bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype = compute_dtype,\n",
    "    bnb_4bit_use_double_quant = use_nested_quant\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3f3a228-4aba-4578-9640-403f033ab529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Our system supports bfloat16: accelerate training with bf16 = True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Check GPU compatiblity with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major , _ = torch.cuda.get_device_capability()\n",
    "\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Our system supports bfloat16: accelerate training with bf16 = True\")\n",
    "        print(\"=\" * 80)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d7444c-d989-45dc-9d87-e164ca89ca18",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e725ded-eb6c-4e65-b5d7-08d7e9f6cd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91cde9b67df9457f80c00bc2d7b5876a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = device_map\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd1a791-0824-42d0-91dd-34388ccc1113",
   "metadata": {},
   "source": [
    "### Load the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f849db6d-58f1-4e20-b499-969b633f03ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code = True\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_size = \"right\" # fix the weird overflow issue with fp16 training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ce1608-7e10-4e94-bdff-dd2695c0c3e3",
   "metadata": {},
   "source": [
    "### Test the downloaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9ee6bf4-3b77-432f-8f91-65fbe5267753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(query: str, model, tokenizer) -> str:\n",
    "    device = 'cuda:0'\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "        <start_of_turn>user\n",
    "        Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "        {query}\n",
    "        <end_of_turn>\\n\n",
    "        <start_of_turn>model\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = prompt_template.format(query = query)\n",
    "\n",
    "    encoded = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors = \"pt\",\n",
    "        add_special_tokens = True\n",
    "    )\n",
    "\n",
    "    model_inputs = encoded.to(device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens = 1000,\n",
    "        do_sample = True,\n",
    "        pad_token_id = tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    decoded = tokenizer.decode(\n",
    "        generated_ids[0],\n",
    "        skip_special_tokens = True\n",
    "    )\n",
    "\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f52f70e-f504-422c-981b-8f9390a864d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Write a python code for fibonacci series in python with recursion\"\n",
    "result = get_completion(query=query,\n",
    "                       model = model,\n",
    "                       tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35b6b379-2915-4dca-8931-d6e9f3d7d199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n        user\\n        Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n        Write a python code for fibonacci series in python with recursion\\n        \\n\\n        model\\n    Sure, here is the code for the fibonacci series in Python with recursion:\\n\\n```python\\ndef fibonacci(n):\\n    if n == 0:\\n        return 0\\n    elif n == 1:\\n        return 1\\n    else:\\n        return fibonacci(n-1) + fibonacci(n-2)\\n\\n\\n# Print the first 10 numbers in the Fibonacci sequence\\nfor i in range(10):\\n    print(fibonacci(i))\\n```\\n\\nThis code calculates the nth number in the Fibonacci sequence using recursion. It uses the base cases 0 and 1 to handle the starting numbers of the sequence. For all other values of n, it recursively calculates the previous two numbers (n-1 and n-2) and returns their sum.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21f9c865-1b73-4069-ad6c-545ae81f7e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        user\n",
      "        Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "        Write a python code for fibonacci series in python with recursion\n",
      "        \n",
      "\n",
      "        model\n",
      "    Sure, here is the code for the fibonacci series in Python with recursion:\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "\n",
      "# Print the first 10 numbers in the Fibonacci sequence\n",
      "for i in range(10):\n",
      "    print(fibonacci(i))\n",
      "```\n",
      "\n",
      "This code calculates the nth number in the Fibonacci sequence using recursion. It uses the base cases 0 and 1 to handle the starting numbers of the sequence. For all other values of n, it recursively calculates the previous two numbers (n-1 and n-2) and returns their sum.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1f88583-8615-455c-84e4-b5d52a5bda0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f083e53b-2261-48b6-affe-d83a30e5f3fc",
   "metadata": {},
   "source": [
    "# Tokenize the custom dataset as the model is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b66ce756-1aa1-4f86-acbc-cf6f2a1be732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'text', 'output', 'instruction', 'prompt'],\n",
       "    num_rows: 121959\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7c08a2f-efa7-4fbc-8353-0aac5147f13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=1234) # Shuffle the dataset\n",
    "\n",
    "dataset = dataset.map(lambda samples: tokenizer(samples['prompt']), batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33d613b6-8896-4381-a5d4-395c56e7d4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'text', 'output', 'instruction', 'prompt', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 121959\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "142907ce-3569-440a-b093-87323a382ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Not applicable', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: Create a MySQL table for a Recipe database. The table should contain fields for id (integer), title (string), and instructions (text). ### Input: Not applicable ### Output: CREATE TABLE recipes (\\n id INT NOT NULL AUTO_INCREMENT,\\n title VARCHAR (255) NOT NULL,\\n instructions TEXT,\\n PRIMARY KEY (id)\\n);', 'output': 'CREATE TABLE recipes (\\n id INT NOT NULL AUTO_INCREMENT,\\n title VARCHAR (255) NOT NULL,\\n instructions TEXT,\\n PRIMARY KEY (id)\\n);', 'instruction': 'Create a MySQL table for a Recipe database. The table should contain fields for id (integer), title (string), and instructions (text).', 'prompt': '<start_of_turn>user Below is a an instruction that describes a particular task, Write a response that appropriately completes the given request. \\n\\n Create a MySQL table for a Recipe database. The table should contain fields for id (integer), title (string), and instructions (text). Here is a input Not applicable <end_of_turn>\\n<start_of_turn>model CREATE TABLE recipes (\\n id INT NOT NULL AUTO_INCREMENT,\\n title VARCHAR (255) NOT NULL,\\n instructions TEXT,\\n PRIMARY KEY (id)\\n); <end_of_turn>', 'input_ids': [2, 106, 1645, 30641, 603, 476, 671, 14239, 674, 19306, 476, 3666, 6911, 235269, 15615, 476, 3590, 674, 47258, 56382, 573, 2764, 3853, 235265, 235248, 109, 7319, 476, 52281, 3037, 604, 476, 36096, 8746, 235265, 714, 3037, 1412, 2888, 7969, 604, 807, 591, 17055, 823, 4680, 591, 1973, 823, 578, 12027, 591, 1082, 846, 5698, 603, 476, 3772, 3232, 11629, 235248, 107, 108, 106, 2516, 77678, 16448, 24778, 591, 108, 807, 16256, 6544, 4179, 46218, 235298, 125170, 235269, 108, 4680, 92942, 591, 235284, 235308, 235308, 235275, 6544, 4179, 235269, 108, 12027, 1837, 235269, 108, 92199, 25976, 591, 539, 235275, 108, 594, 235248, 107], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "for i in dataset:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0816e1bb-53e9-4344-b519-7a01f2e12014",
   "metadata": {},
   "source": [
    "### Split the dataset into training and testing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ba452f2-8d72-4d48-95f4-57930c63afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0cb1658-ec54-4cfc-b444-47f28e2e6d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'text', 'output', 'instruction', 'prompt', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 97567\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'text', 'output', 'instruction', 'prompt', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 24392\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "057528c9-be4f-47b5-807a-a8f5fd62ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in dataset['train']:\n",
    "#     print(i)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb56a561-0f3a-4091-9eb7-85f02c5d98f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train']\n",
    "test_data = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0381ccba-ab67-4774-bfff-e4c99f1541b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'text', 'output', 'instruction', 'prompt', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 97567\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f602dbc6-82b2-4798-828e-c0e506ccc5e6",
   "metadata": {},
   "source": [
    "# Load the LoRA config\n",
    "\n",
    "Here comes the magic with peft! \n",
    "Let's load a PeftModel and specify that we are going to use low-rank adapters (LoRA) using get_peft_model utility function and then prepare_model_for_kbit_training method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0cef8385-29eb-4518-bf63-dc3b84286034",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9bdafc6d-3de7-4c14-a0e7-531e1a9688c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GemmaForCausalLM(\n",
      "  (model): GemmaModel(\n",
      "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-17): 18 x GemmaDecoderLayer(\n",
      "        (self_attn): GemmaAttention(\n",
      "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): GemmaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): GemmaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
      "          (act_fn): GELUActivation()\n",
      "        )\n",
      "        (input_layernorm): GemmaRMSNorm()\n",
      "        (post_attention_layernorm): GemmaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): GemmaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9218803-04bf-4778-a3a0-f3ea05756f80",
   "metadata": {},
   "source": [
    "### Get all the linear projection name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4dea02c-c281-4f1e-b5be-8423dfe50e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_liner_names(model):\n",
    "    cls = bnb.nn.Linear4bit # if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    \n",
    "    lora_module_names = set()\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "        if 'lm_head' in lora_module_names:\n",
    "            # Needed for 16 bit\n",
    "            lora_module_names.remove('lm_head')\n",
    "\n",
    "    return lora_module_names\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1626fc31-3388-4cc0-99bf-c8711808fc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'down_proj', 'v_proj', 'up_proj', 'k_proj', 'gate_proj', 'q_proj', 'o_proj'}\n"
     ]
    }
   ],
   "source": [
    "proj_modules = find_all_liner_names(model)\n",
    "\n",
    "print(proj_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "faeb4fa5-abb0-4b5b-b4f2-074d8fa6d3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LoRA config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "612d4808-ebc0-4374-8de2-cd4dbb57f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha = lora_alpha,\n",
    "    lora_dropout = lora_dropout,\n",
    "    r = lora_r,\n",
    "    target_modules = proj_modules,\n",
    "    bias = 'none',\n",
    "    task_type = 'CAUSAL_LM'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1473baf0-abed-4174-aee6-5dad3ae1859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import get_peft_model\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "daa91470-aab6-417b-85fe-93a9339f27f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GemmaForCausalLM(\n",
      "      (model): GemmaModel(\n",
      "        (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-17): 18 x GemmaDecoderLayer(\n",
      "            (self_attn): GemmaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): GemmaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): GemmaMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=16384, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=16384, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=16384, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): GELUActivation()\n",
      "            )\n",
      "            (input_layernorm): GemmaRMSNorm()\n",
      "            (post_attention_layernorm): GemmaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): GemmaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "05907f04-93a9-479c-af95-a6beb5333dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 78446592 | total: 2584619008 | Percentage: 3.0351%\n"
     ]
    }
   ],
   "source": [
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7999f52a-9e13-40ec-9074-cf6ce30298e4",
   "metadata": {},
   "source": [
    "# Set training parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "53f101c6-7d42-4a9f-aec4-2666a8551eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = transformers.TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    # num_train_epochs = num_train_epochs,\n",
    "    per_device_train_batch_size = per_device_train_batch_size,\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "    optim = optim,\n",
    "    save_steps = save_steps,\n",
    "    save_strategy= save_strategy,\n",
    "    logging_steps = logging_steps,\n",
    "    learning_rate = learning_rate,\n",
    "    weight_decay = weight_decay,\n",
    "    fp16 = fp16,\n",
    "    bf16 = bf16,\n",
    "    max_grad_norm = max_grad_norm,\n",
    "    max_steps = max_steps,\n",
    "    warmup_ratio = warmup_ratio,\n",
    "    group_by_length = group_by_length,\n",
    "    lr_scheduler_type = lr_scheduler_type,\n",
    "    report_to = \"tensorboard\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e530a2ee-7b21-4173-976d-8542069c5c13",
   "metadata": {},
   "source": [
    "# Set SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "37c820e5-1968-4388-a321-3e56b8039b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:223: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e471fe16ea5641d8989b37a3c339540e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/97567 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e09b3b153954fb4b6aa6b5d06b8e4ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24392 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:290: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = train_data,\n",
    "    eval_dataset = test_data,\n",
    "    peft_config = peft_config,\n",
    "    dataset_text_field = 'prompt',\n",
    "    max_seq_length = max_seq_length,\n",
    "    tokenizer = tokenizer,\n",
    "    args = training_arguments,\n",
    "    data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm = False),\n",
    "    packing = packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d985f8-abb4-4347-90b8-857160937478",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b1547701-ac0c-423d-82f0-76b6075ec629",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "140c22cb-b46c-4cfa-9b86-ee18e6b98adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 06:11, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.604900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.606200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.593700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.606000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.619600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.609500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.583600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.596000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.6024392223358155, metrics={'train_runtime': 374.3816, 'train_samples_per_second': 2.137, 'train_steps_per_second': 0.534, 'total_flos': 2011937953284096.0, 'train_loss': 0.6024392223358155, 'epoch': 0.01})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc08e44-185a-4878-8fe3-ef981fd22cb7",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b21c105f-3fc6-465f-ad75-5ecf48578ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "trainer.model.save_pretrained(new_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bb22db80-bbd5-4aea-a532-cf719b2a0ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('my-code-gemma-finetuned-it/tokenizer_config.json',\n",
       " 'my-code-gemma-finetuned-it/special_tokens_map.json',\n",
       " 'my-code-gemma-finetuned-it/tokenizer.json')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the tokenizer\n",
    "trainer.tokenizer.save_pretrained(new_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d87e656-c1c1-4bf2-a463-5fa0a7c6df97",
   "metadata": {},
   "source": [
    "# Check Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4f331b5b-130a-4e84-a7fd-450439130ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "# %reload_ext tensorboard\n",
    "%tensorboard --logdir results/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9fde8f34-ebc4-4fff-b47e-aa7d9718719e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: kill: (2054) - No such process\n"
     ]
    }
   ],
   "source": [
    "!kill 2054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e3ec2e3a-9c3b-48a6-af3b-36257e23c786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GemmaForCausalLM(\n",
       "      (model): GemmaModel(\n",
       "        (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-17): 18 x GemmaDecoderLayer(\n",
       "            (self_attn): GemmaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): GemmaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): GemmaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=16384, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=16384, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=16384, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): GELUActivation()\n",
       "            )\n",
       "            (input_layernorm): GemmaRMSNorm()\n",
       "            (post_attention_layernorm): GemmaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): GemmaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2a27ad-6cb0-42d5-8a18-1a189ac07ca8",
   "metadata": {},
   "source": [
    "# Merge the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0aa29e28-789f-4c77-99ec-fd5e37d312e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b28ea8b4b64b539f198f3c9a23b4e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage = True,\n",
    "    return_dict = True,\n",
    "    torch_dtype = torch.float16,\n",
    "    device_map = device_map,\n",
    ")\n",
    "\n",
    "\n",
    "merged_model = PeftModel.from_pretrained(base_model, new_model_name)\n",
    "merged_model = merged_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f02a13f-8718-47c6-8ae5-14402807e375",
   "metadata": {},
   "source": [
    "# Save the Megred Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b35b414b-8c12-4e02-8da8-1307d8d466b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.save_pretrained(\"merged_model\", safe_serialization = True)\n",
    "\n",
    "\n",
    "tokenizer.save_pretrained(\"merged_model\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75487f40-7e03-481c-a678-77e8a64fd5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the model and tokenizer to the hugging face model hub\n",
    "# !huggingface-cli login\n",
    "\n",
    "merged_model.push_to_hub(new_model_name, use_temp_dir = False, check_pr=True)\n",
    "\n",
    "tokenizer.push_to_hub(new_model_name,use_temp_dir = False, check_pr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1720e9cc-4c63-4559-a80d-6af686c1c42a",
   "metadata": {},
   "source": [
    "# Test the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f255f3cf-93af-4417-8f59-ff62d20e4205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f2f5d266-27f3-430d-baad-67dac12d2ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Write a python code for fibonacci series with recursion and explain the code\"\n",
    "\n",
    "result = get_completion(query=query,\n",
    "                       model = merged_model,\n",
    "                       tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "526f28a1-0453-4842-ae1b-0349dae3e3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        user\n",
      "        Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "        Write a python code for fibonacci series with recursion and explain the code\n",
      "        \n",
      "\n",
      "        model\n",
      "    def fibonacci(n) : \n",
      "        if n == 0 : \n",
      "            return 0\n",
      "        elif n == 1 : \n",
      "            return 1\n",
      "        else : \n",
      "            return(fibonacci(n-1) + fibonacci(n-2))\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    n = 10\n",
      "    print(\"Fibonacci Series (Recursive) with {}\".format(n))\n",
      "    print(\"Fibonacci for the given number is : {}\".format(fibonacci(n))) \n",
      "model Fibonacci is a sequence of numbers where each number is the sum of two previous numbers. The first two numbers in the sequence are 0 and 1 and in subsequent numbers, the first two add to whatever the rest of the numbers are.\n",
      "\n",
      "The recursion code is as follows:\n",
      "\n",
      "base case of an empty list. return 0\n",
      "\n",
      "When n = 1, return 1 (base case of a list of one).\n",
      "\n",
      "Return (fibonacci(n-1) + fibonacci(n-2)) if n > 1.\n",
      "\n",
      "The complexity of the algorithm is O(n) (the time taken to calculate the result is the same as the time taken to compute the n-th number in the Fibonacci sequence). \n",
      " getopt version 2.2.\n",
      "\n",
      "Options: -i <value>             => input:<value>\n",
      "Options: -r                     => recursion\n",
      "Options: --version <version>       => output:<version>\n",
      "Options:                                 => help\n",
      "-i value                          => input:<value>\n",
      "-r                               => recursion\n",
      "-v value                          => output:<value>\n",
      "-h                               => hour\n",
      "        .--version <version>            => output:<version>\n",
      "        .--hour                             => hour\n",
      "        .--hour <value>                      => hour\n",
      "        .--version <version>            => output:<version>\n",
      "        .--version                              => help\n",
      "        .<value>                               => help(option)\n",
      " \n",
      "fibonacci input: 10 \n",
      "Fibonacci for the given number is: [ 0, 1, 1, 2, 3, 5, 8, 13, 21, 34 ] \n",
      "The complexity of this function is O(n) because the calculations of its intermediate results get done completely with recursion. We can replace the recursive calculations with a loop if we want and still the overall time will remain the same. \n",
      "More about the solution.\n",
      "\n",
      "In this code, we have taken advantage of the natural properties of the Fibonacci sequence and have used the last n-1 numbers to calculate the present number. This reduces the time complexity of the recursive function. \n",
      "Code Complexity: O(n)\n",
      "The time complexity of this code is O(n) because the function takes advantage of the Fibonacci series and it uses the last n-1 numbers to calculate the latest number. \n",
      "Note: This solution works only for positive numbers because we keep subtracting one and two from the input number.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81f724b-5374-49a8-ba04-a9c150695806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

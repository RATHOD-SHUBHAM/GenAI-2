{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "74512fdc60f94a2a8f1b4018a8bd25df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e226948ab5644246b34379fd1aa4e071",
              "IPY_MODEL_c0738b4017984b1b8af9aed6b612fbe6",
              "IPY_MODEL_b6070e23d82949e3b49485e9a5e6cdfb"
            ],
            "layout": "IPY_MODEL_9695143d7f894f74a12d6e79c47fa52f"
          }
        },
        "e226948ab5644246b34379fd1aa4e071": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d90dc2bc2d2046f2b4ab9180a0221073",
            "placeholder": "​",
            "style": "IPY_MODEL_54b8921726984dde904cba24332c2516",
            "value": "Parsing nodes: 100%"
          }
        },
        "c0738b4017984b1b8af9aed6b612fbe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_016e8d551146454594ff8c29ebdc2621",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_63f2742484224a15aa57bfdd4342845f",
            "value": 30
          }
        },
        "b6070e23d82949e3b49485e9a5e6cdfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_369c6cedc84f4e4db57e5dd8e7bee9be",
            "placeholder": "​",
            "style": "IPY_MODEL_27c45ae265ef42e99f7a9757fb8c870a",
            "value": " 30/30 [00:00&lt;00:00, 327.55it/s]"
          }
        },
        "9695143d7f894f74a12d6e79c47fa52f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d90dc2bc2d2046f2b4ab9180a0221073": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54b8921726984dde904cba24332c2516": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "016e8d551146454594ff8c29ebdc2621": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63f2742484224a15aa57bfdd4342845f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "369c6cedc84f4e4db57e5dd8e7bee9be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27c45ae265ef42e99f7a9757fb8c870a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d595acf9dd5f4edea98d89913b3993d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f1aaae2abf7543d19f6663a9f718678c",
              "IPY_MODEL_df21ba774b494500a4940b99872f309f",
              "IPY_MODEL_097522dd50b74730a3d430c3cb281142"
            ],
            "layout": "IPY_MODEL_02ad6c46770b4cf1a8923a5624f86ed7"
          }
        },
        "f1aaae2abf7543d19f6663a9f718678c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de46581976734de4958de4685074e40b",
            "placeholder": "​",
            "style": "IPY_MODEL_4705e75c3c98432d8934dd1e4b4afc97",
            "value": "Generating embeddings: 100%"
          }
        },
        "df21ba774b494500a4940b99872f309f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18e0e1c44f3a4b2eb0b760be26dd4f70",
            "max": 41,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e43a135f3504eafa137470f521a5861",
            "value": 41
          }
        },
        "097522dd50b74730a3d430c3cb281142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_629b271f0bba477d94c765e5d3b01d91",
            "placeholder": "​",
            "style": "IPY_MODEL_86c6a6ce975a4c3fbbbbb7f478fb6720",
            "value": " 41/41 [00:00&lt;00:00, 44.51it/s]"
          }
        },
        "02ad6c46770b4cf1a8923a5624f86ed7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de46581976734de4958de4685074e40b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4705e75c3c98432d8934dd1e4b4afc97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18e0e1c44f3a4b2eb0b760be26dd4f70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e43a135f3504eafa137470f521a5861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "629b271f0bba477d94c765e5d3b01d91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86c6a6ce975a4c3fbbbbb7f478fb6720": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40f89c9af6b742388d3019874dffef66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d524358fb79e40eabc18d9a83296118f",
              "IPY_MODEL_2fdab3a9bbf44d07aefad61692d0afee",
              "IPY_MODEL_9695e1b8bce94bb5ba9c7e6f7b28d296"
            ],
            "layout": "IPY_MODEL_017c755f60b14f599b14329c6cdc6830"
          }
        },
        "d524358fb79e40eabc18d9a83296118f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab2c308c89d8439098e85d582161d0b8",
            "placeholder": "​",
            "style": "IPY_MODEL_1f70ac3dab614b7ab4c1ea8ab7029731",
            "value": "Parsing nodes: 100%"
          }
        },
        "2fdab3a9bbf44d07aefad61692d0afee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4290e8c38ccf481095140b5e1ef33a34",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_06f3b6f0b0ce44dfaa5fa70c94d80d40",
            "value": 30
          }
        },
        "9695e1b8bce94bb5ba9c7e6f7b28d296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_486fe2aa0d794bb4864679b5dcbed022",
            "placeholder": "​",
            "style": "IPY_MODEL_0337c31961cc4b4394d2008268d503f2",
            "value": " 30/30 [00:00&lt;00:00, 336.64it/s]"
          }
        },
        "017c755f60b14f599b14329c6cdc6830": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab2c308c89d8439098e85d582161d0b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f70ac3dab614b7ab4c1ea8ab7029731": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4290e8c38ccf481095140b5e1ef33a34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06f3b6f0b0ce44dfaa5fa70c94d80d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "486fe2aa0d794bb4864679b5dcbed022": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0337c31961cc4b4394d2008268d503f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74604c638ef34afbbbf7e0122be5a338": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4e6b6a8cf14425aaba4ff13d8cae030",
              "IPY_MODEL_5752fe7316934b6c9cf7d0737180b0cb",
              "IPY_MODEL_3567d45ff9db4cb08f7f85c48d2b58f4"
            ],
            "layout": "IPY_MODEL_cf15e7ae32404620aa4d984b7f9690ab"
          }
        },
        "d4e6b6a8cf14425aaba4ff13d8cae030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5dbe4b7029d54debbd1950dc81625895",
            "placeholder": "​",
            "style": "IPY_MODEL_01375b311eb34a12a0f95fde58784aac",
            "value": "Generating embeddings: 100%"
          }
        },
        "5752fe7316934b6c9cf7d0737180b0cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e752c88545c449298f67e5181fc0d47a",
            "max": 41,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8afdccab27f46e0a9dc7b1ac9f3e888",
            "value": 41
          }
        },
        "3567d45ff9db4cb08f7f85c48d2b58f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd836e12840a49e4bc73d66f76a6336d",
            "placeholder": "​",
            "style": "IPY_MODEL_31c30ea3edb543f995565d95834bab0b",
            "value": " 41/41 [00:00&lt;00:00, 54.00it/s]"
          }
        },
        "cf15e7ae32404620aa4d984b7f9690ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dbe4b7029d54debbd1950dc81625895": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01375b311eb34a12a0f95fde58784aac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e752c88545c449298f67e5181fc0d47a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8afdccab27f46e0a9dc7b1ac9f3e888": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd836e12840a49e4bc73d66f76a6336d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31c30ea3edb543f995565d95834bab0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3fb7f780124a4d46aee548ea5b2391ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d3e59e1175743a8a1910bf11eb24a94",
              "IPY_MODEL_e705e30ae822447ea5ab711d0a45255c",
              "IPY_MODEL_93fc3e587ef04fa8b1107b61a19504e5"
            ],
            "layout": "IPY_MODEL_f16fa3e21bbf427eb27d1c848cb8b541"
          }
        },
        "2d3e59e1175743a8a1910bf11eb24a94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2eaf873a7e94a328584a1d7372f0100",
            "placeholder": "​",
            "style": "IPY_MODEL_1db164bfadd0469ebf9feb3455b5a21a",
            "value": "Parsing nodes: 100%"
          }
        },
        "e705e30ae822447ea5ab711d0a45255c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b92cdfcae0f24b2a9bb7a7c7cb22addc",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d878849eba04fa28e0b41713c26f8cc",
            "value": 30
          }
        },
        "93fc3e587ef04fa8b1107b61a19504e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd087fd5e6a94e4ca5b8faff34d70526",
            "placeholder": "​",
            "style": "IPY_MODEL_9db13cf8c0224f2daf6b01cbdac52a77",
            "value": " 30/30 [00:00&lt;00:00, 327.46it/s]"
          }
        },
        "f16fa3e21bbf427eb27d1c848cb8b541": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2eaf873a7e94a328584a1d7372f0100": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1db164bfadd0469ebf9feb3455b5a21a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b92cdfcae0f24b2a9bb7a7c7cb22addc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d878849eba04fa28e0b41713c26f8cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd087fd5e6a94e4ca5b8faff34d70526": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9db13cf8c0224f2daf6b01cbdac52a77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d071af179c79418ab7b0893c37f3c0e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9de69c4486e4428690b2b53a909b60f2",
              "IPY_MODEL_852f30ec1cf54001bb1ad5b0bb8bd198",
              "IPY_MODEL_fa8a50460e01428a9c287100870a3dea"
            ],
            "layout": "IPY_MODEL_a59e7a38c60d49c8a5fdff7078c97b67"
          }
        },
        "9de69c4486e4428690b2b53a909b60f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53e0f3f9040d440790810025ee4c19b8",
            "placeholder": "​",
            "style": "IPY_MODEL_669096d91b3c4a4e912504aa254fe2e3",
            "value": "Generating embeddings: 100%"
          }
        },
        "852f30ec1cf54001bb1ad5b0bb8bd198": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_caeb244cb2524138a1b5375904d9d703",
            "max": 41,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d3612eced9d4dc5bf3a87232b7a7d49",
            "value": 41
          }
        },
        "fa8a50460e01428a9c287100870a3dea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef2f55172488444ebbe9fa8d3bf26ac0",
            "placeholder": "​",
            "style": "IPY_MODEL_2357671a72ff420e9bd4e5d0b2b91e2e",
            "value": " 41/41 [00:00&lt;00:00, 66.98it/s]"
          }
        },
        "a59e7a38c60d49c8a5fdff7078c97b67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53e0f3f9040d440790810025ee4c19b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "669096d91b3c4a4e912504aa254fe2e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "caeb244cb2524138a1b5375904d9d703": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d3612eced9d4dc5bf3a87232b7a7d49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef2f55172488444ebbe9fa8d3bf26ac0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2357671a72ff420e9bd4e5d0b2b91e2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRXDrRkx6GZ8",
        "outputId": "d6f76550-6777-4fed-bb6c-f7782debbd9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.10.5-py3-none-any.whl (5.6 kB)\n",
            "Collecting openai\n",
            "  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb\n",
            "  Downloading chromadb-0.4.22-py3-none-any.whl (509 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.0/509.0 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-agent-openai<0.2.0,>=0.1.0 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.1.1-py3-none-any.whl (12 kB)\n",
            "Collecting llama-index-core<0.11.0,>=0.10.0 (from llama-index)\n",
            "  Downloading llama_index_core-0.10.5-py3-none-any.whl (631 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m631.0/631.0 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-embeddings-openai<0.2.0,>=0.1.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.1.1-py3-none-any.whl (6.1 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-llms-openai<0.2.0,>=0.1.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.1.2-py3-none-any.whl (9.5 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.1-py3-none-any.whl (6.0 kB)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.1.1-py3-none-any.whl (4.3 kB)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.1.1-py3-none-any.whl (3.1 kB)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.1.3-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.0.3)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.109.2-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.27.1-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.25.2)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.4.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.1/41.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m117.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.17.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m120.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.22.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.22.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.22.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.6/105.6 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.1)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.60.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (23.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Collecting starlette<0.37.0,>=0.36.3 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.3-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (3.9.3)\n",
            "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.0->llama-index)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.0->llama-index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.0->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.6.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (3.2.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.5.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (9.4.0)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.0->llama-index)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.0->llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (4.12.3)\n",
            "Collecting bs4<0.0.3,>=0.0.2 (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Collecting pymupdf<2.0.0,>=1.23.21 (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index)\n",
            "  Downloading PyMuPDF-1.23.22-cp310-none-manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index)\n",
            "  Downloading pypdf-4.0.1-py3-none-any.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Collecting importlib-metadata<7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Collecting backoff<3.0.0,>=1.10.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.22.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.22.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.22.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.22.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl (14 kB)\n",
            "Collecting opentelemetry-instrumentation==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl (28 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.43b0-py3-none-any.whl (36 kB)\n",
            "Collecting opentelemetry-util-http==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.43b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.20.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (2.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.12.25)\n",
            "Collecting PyMuPDFb==1.23.22 (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.0->llama-index)\n",
            "  Downloading PyMuPDFb-1.23.22-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.0->llama-index) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.0->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.0->llama-index)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=2eee4e3757ef238a5355586b73664c7de62c5b1817eb49c29d33d073c480d235\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, mmh3, dirtyjson, websockets, uvloop, python-dotenv, pypdf, PyMuPDFb, pulsar-client, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, mypy-extensions, marshmallow, importlib-metadata, humanfriendly, httptools, h11, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, pymupdf, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, httpcore, coloredlogs, bs4, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, kubernetes, httpx, fastapi, dataclasses-json, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, openai, opentelemetry-instrumentation-fastapi, llama-index-legacy, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-embeddings-openai, chromadb, llama-index-program-openai, llama-index-multi-modal-llms-openai, llama-index-agent-openai, llama-index-question-gen-openai, llama-index\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 7.0.1\n",
            "    Uninstalling importlib-metadata-7.0.1:\n",
            "      Successfully uninstalled importlib-metadata-7.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyMuPDFb-1.23.22 asgiref-3.7.2 backoff-2.2.1 bcrypt-4.1.2 bs4-0.0.2 chroma-hnswlib-0.7.3 chromadb-0.4.22 coloredlogs-15.0.1 dataclasses-json-0.6.4 deprecated-1.2.14 dirtyjson-1.0.8 fastapi-0.109.2 h11-0.14.0 httpcore-1.0.3 httptools-0.6.1 httpx-0.26.0 humanfriendly-10.0 importlib-metadata-6.11.0 kubernetes-29.0.0 llama-index-0.10.5 llama-index-agent-openai-0.1.1 llama-index-core-0.10.5 llama-index-embeddings-openai-0.1.1 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.2 llama-index-multi-modal-llms-openai-0.1.1 llama-index-program-openai-0.1.1 llama-index-question-gen-openai-0.1.1 llama-index-readers-file-0.1.3 marshmallow-3.20.2 mmh3-4.1.0 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.17.0 openai-1.12.0 opentelemetry-api-1.22.0 opentelemetry-exporter-otlp-proto-common-1.22.0 opentelemetry-exporter-otlp-proto-grpc-1.22.0 opentelemetry-instrumentation-0.43b0 opentelemetry-instrumentation-asgi-0.43b0 opentelemetry-instrumentation-fastapi-0.43b0 opentelemetry-proto-1.22.0 opentelemetry-sdk-1.22.0 opentelemetry-semantic-conventions-0.43b0 opentelemetry-util-http-0.43b0 overrides-7.7.0 posthog-3.4.1 pulsar-client-3.4.0 pymupdf-1.23.22 pypdf-4.0.1 pypika-0.48.9 python-dotenv-1.0.1 starlette-0.36.3 tiktoken-0.6.0 typing-inspect-0.9.0 uvicorn-0.27.1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index openai chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "uEvMTo3l7rJc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "jLuryuzh8IBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8VOU0no8Hx1",
        "outputId": "95c5873a-8687-4acc-ee78-73aff2676e33"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /usr/local/lib/python3.10/dist-\n",
            "[nltk_data]     packages/llama_index/core/_static/nltk_cache...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /usr/local/lib/python3.10/dist-\n",
            "[nltk_data]     packages/llama_index/core/_static/nltk_cache...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader('/content/data').load_data()"
      ],
      "metadata": {
        "id": "EfbiYp6f74fk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kJghhV-8vP1",
        "outputId": "daf2a78f-64e7-4d12-a990-7f715507f841"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id_='26eb407c-eb49-4466-9b2c-97a2b354d9ae', embedding=None, metadata={'page_label': '1', 'file_name': 'attention.pdf', 'file_path': '/content/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7f57e13d-211e-4c39-8a4b-da625ddb0ab9', embedding=None, metadata={'page_label': '2', 'file_name': 'attention.pdf', 'file_path': '/content/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='07f404a4-122a-4a84-ab75-40841e5e2cc2', embedding=None, metadata={'page_label': '3', 'file_name': 'attention.pdf', 'file_path': '/content/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .\\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f551f012-165f-4478-ae71-86e3b580af76', embedding=None, metadata={'page_label': '4', 'file_name': 'attention.pdf', 'file_path': '/content/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='10ffea95-eef0-45e9-b7a3-24c298423136', embedding=None, metadata={'page_label': '5', 'file_name': 'attention.pdf', 'file_path': '/content/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\\n5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='055e9c25-50d5-4f07-890a-1503334f8c41', embedding=None, metadata={'page_label': '6', 'file_name': 'attention.pdf', 'file_path': '/content/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='aeb23fe6-9552-4f81-9be1-7f3992b40672', embedding=None, metadata={'page_label': '7', 'file_name': 'attention.pdf', 'file_path': '/content/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='length nis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate =d−0.5\\nmodel·min(step_num−0.5, step _num·warmup _steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a78d6b46-77e3-46c7-adaf-bfe1400a3992', embedding=None, metadata={'page_label': '8', 'file_name': 'attention.pdf', 'file_path': '/content/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0·1020\\nGNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\\nConvS2S [9] 25.16 40.46 9.6·10181.5·1020\\nMoE [32] 26.03 40.56 2.0·10191.2·1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018\\nTransformer (big) 28.4 41.8 2.3·1019\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='253d9282-e08f-4645-8da1-7f6ab2f845e1', embedding=None, metadata={'page_label': '9', 'file_name': 'attention.pdf', 'file_path': '/content/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dvPdrop ϵlstrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8021c7f1-c307-4884-baee-987fbf0e4a26', embedding=None, metadata={'page_label': '10', 'file_name': 'attention.pdf', 'file_path': '/content/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n10', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='222db050-f1ae-492b-9094-50fc588f6ef0', embedding=None, metadata={'page_label': '11', 'file_name': 'attention.pdf', 'file_path': '/content/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL , 2016.\\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n11', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cb2526f7-ef80-4017-bdfe-e80e80352eca', embedding=None, metadata={'page_label': '12', 'file_name': 'attention.pdf', 'file_path': '/content/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers) , pages 434–443. ACL, August 2013.\\n12', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='48941801-c41d-4c89-9f3e-51e6db2053d5', embedding=None, metadata={'page_label': '13', 'file_name': 'attention.pdf', 'file_path': '/content/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9094bd98-62a0-4e12-b98f-5120e084a380', embedding=None, metadata={'page_label': '14', 'file_name': 'attention.pdf', 'file_path': '/content/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='694801b2-729a-4c2e-991c-ba638b624026', embedding=None, metadata={'page_label': '15', 'file_name': 'attention.pdf', 'file_path': '/content/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6d6158cc-3963-405c-a0b5-5cc40de0e6f1', embedding=None, metadata={'page_label': '1', 'file_name': 'yoloWorld.pdf', 'file_path': '/content/data/yoloWorld.pdf', 'file_type': 'application/pdf', 'file_size': 4685106, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='YOLO-World: Real-Time Open-Vocabulary Object Detection\\nTianheng Cheng3,2,∗, Lin Song1,∗,✉, Yixiao Ge1,2,†, Wenyu Liu3, Xinggang Wang3,✉, Ying Shan1,2\\n∗equal contribution†project lead✉corresponding author\\n1Tencent AI Lab2ARC Lab, Tencent PCG\\n3School of EIC, Huazhong University of Science & Technology\\nCode & Models: YOLO-World\\nAbstract\\nThe You Only Look Once (YOLO) series of detectors\\nhave established themselves as efficient and practical tools.\\nHowever, their reliance on predefined and trained ob-\\nject categories limits their applicability in open scenar-\\nios. Addressing this limitation, we introduce YOLO-World,\\nan innovative approach that enhances YOLO with open-\\nvocabulary detection capabilities through vision-language\\nmodeling and pre-training on large-scale datasets. Specif-\\nically, we propose a new Re-parameterizable Vision-\\nLanguage Path Aggregation Network (RepVL-PAN) and\\nregion-text contrastive loss to facilitate the interaction be-\\ntween visual and linguistic information. Our method excels\\nin detecting a wide range of objects in a zero-shot man-\\nner with high efficiency. On the challenging LVIS dataset,\\nYOLO-World achieves 35.4 AP with 52.0 FPS on V100,\\nwhich outperforms many state-of-the-art methods in terms\\nof both accuracy and speed. Furthermore, the fine-tuned\\nYOLO-World achieves remarkable performance on several\\ndownstream tasks, including object detection and open-\\nvocabulary instance segmentation.\\n1. Introduction\\nObject detection has been a long-standing and fundamental\\nchallenge in computer vision with numerous applications in\\nimage understanding, robotics, and autonomous vehicles.\\nTremendous works [15, 26, 40, 42] have achieved signif-\\nicant breakthroughs in object detection with the develop-\\nment of deep neural networks. Despite the success of these\\nmethods, they remain limited as they only handle object de-\\ntection with a fixed vocabulary, e.g., 80 categories in the\\nCOCO [25] dataset. Once object categories are defined and\\nlabeled, trained detectors can only detect those specific cat-\\negories, thus limiting the ability and applicability of open\\n20×SpeedupFigure 1. Speed-and-Accuracy Curve. We compare YOLO-\\nWorld with recent open-vocabulary methods in terms of speed and\\naccuracy. All models are evaluated on the LVIS minival and in-\\nference speeds are measured on one NVIDIA V100 w/o TensorRT.\\nThe size of the circle represents the model’s size.\\nscenarios.\\nRecent works [7, 12, 45, 50, 55] have explored the\\nprevalent vision-language models [18, 36] to address open-\\nvocabulary detection [55] through distilling vocabulary\\nknowledge from language encoders, e.g., BERT [5]. How-\\never, these distillation-based methods are much limited due\\nto the scarcity of training data with a limited diversity of\\nvocabulary, e.g., OV-COCO [55] containing 48 base cate-\\ngories. Several methods [23, 29, 53, 54, 56] reformulate ob-\\nject detection training as region-level vision-language pre-\\ntraining and train open-vocabulary object detectors at scale.\\nHowever, those methods still struggle for detection in real-\\nworld scenarios, which suffer from two aspects: (1) heavy\\ncomputation burden and (2) complicated deployment for\\nedge devices. Previous works [23, 29, 53, 54, 56] have\\n1arXiv:2401.17270v2  [cs.CV]  2 Feb 2024', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9bb53237-c529-4d36-abbf-f9640d492f78', embedding=None, metadata={'page_label': '2', 'file_name': 'yoloWorld.pdf', 'file_path': '/content/data/yoloWorld.pdf', 'file_type': 'application/pdf', 'file_size': 4685106, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='demonstrated the promising performance of pre-training\\nlarge detectors while pre-training small detectors to en-\\ndow them with open recognition capabilities remains un-\\nexplored.\\nIn this paper, we present YOLO-World, aiming for\\nhigh-efficiency open-vocabulary object detection, and ex-\\nplore large-scale pre-training schemes to boost the tradi-\\ntional YOLO detectors to a new open-vocabulary world.\\nCompared to previous methods, the proposed YOLO-\\nWorld is remarkably efficient with high inference speed\\nand easy to deploy for downstream applications. Specif-\\nically, YOLO-World follows the standard YOLO archi-\\ntecture [19] and leverages the pre-trained CLIP [36] text\\nencoder to encode the input texts. We further propose\\nthe Re-parameterizable Vision-Language Path Aggregation\\nNetwork (RepVL-PAN) to connect text features and im-\\nage features for better visual-semantic representation. Dur-\\ning inference, the text encoder can be removed and the\\ntext embeddings can be re-parameterized into weights of\\nRepVL-PAN for efficient deployment. We further inves-\\ntigate the open-vocabulary pre-training scheme for YOLO\\ndetectors through region-text contrastive learning on large-\\nscale datasets, which unifies detection data, grounding data,\\nand image-text data into region-text pairs. The pre-trained\\nYOLO-World with abundant region-text pairs demonstrates\\na strong capability for large vocabulary detection and train-\\ning more data leads to greater improvements in open-\\nvocabulary capability.\\nIn addition, we explore a prompt-then-detect paradigm\\nto further improve the efficiency of open-vocabulary object\\ndetection in real-world scenarios. As illustrated in Fig. 2,\\ntraditional object detectors [15, 19, 22, 38–40, 49] con-\\ncentrate on the fixed-vocabulary (close-set) detection with\\npredefined and trained categories. While previous open-\\nvocabulary detectors [23, 29, 53, 56] encode the prompts of\\na user for online vocabulary with text encoders and detect\\nobjects. Notably, those methods tend to employ large de-\\ntectors with heavy backbones, e.g., Swin-L [31], to increase\\nthe open-vocabulary capacity. In contrast, the prompt-then-\\ndetect paradigm (Fig. 2 (c)) first encodes the prompts of a\\nuser to build an offline vocabulary and the vocabulary varies\\nwith different needs. Then, the efficient detector can infer\\nthe offline vocabulary on the fly without re-encoding the\\nprompts. For practical applications, once we have trained\\nthe detector, i.e., YOLO-World, we can pre-encode the\\nprompts or categories to build an offline vocabulary and\\nthen seamlessly integrate it into the detector.\\nOur main contributions can be summarized into three\\nfolds:\\n• We introduce the YOLO-World, a cutting-edge open-\\nvocabulary object detector with high efficiency for real-\\nworld applications.\\n• We propose a Re-parameterizable Vision-Language PANto connect vision and language features and an open-\\nvocabulary region-text contrastive pre-training scheme\\nfor YOLO-World.\\n• The proposed YOLO-World pre-trained on large-scale\\ndatasets demonstrates strong zero-shot performance and\\nachieves 35.4 AP on LVIS with 52.0 FPS. The pre-trained\\nYOLO-World can be easily adapted to downstream tasks,\\ne.g., open-vocabulary instance segmentation and referring\\nobject detection. Moreover, the pre-trained weights and\\ncodes of YOLO-World will be open-sourced to facilitate\\nmore practical applications.\\n2. Related Works\\n2.1. Traditional Object Detection\\nPrevalent object detection research concentrates on fixed-\\nvocabulary (close-set) detection, in which object detectors\\nare trained on datasets with pre-defined categories, e.g.,\\nCOCO dataset [25] and Objects365 dataset [43], and then\\ndetect objects within the fixed set of categories. During\\nthe past decades, the methods for traditional object de-\\ntection can be simply categorized into three groups, i.e.,\\nregion-based methods, pixel-based methods, and query-\\nbased methods. The region-based methods [10, 11, 15, 26,\\n41], such as Faster R-CNN [41], adopt a two-stage frame-\\nwork for proposal generation [41] and RoI-wise (Region-\\nof-Interest) classification and regression. The pixel-based\\nmethods [27, 30, 39, 46, 58] tend to be one-stage detec-\\ntors, which perform classification and regression over pre-\\ndefined anchors or pixels. DETR [1] first explores object\\ndetection through transformers [47] and inspires extensive\\nquery-based methods [61]. In terms of inference speed,\\nRedmon et al. presents YOLOs [37–39] which exploit sim-\\nple convolutional architectures for real-time object detec-\\ntion. Several works [9, 22, 32, 49, 52] propose various ar-\\nchitectures or designs for YOLO, including path aggrega-\\ntion networks [28], cross-stage partial networks [48], and\\nre-parameterization [6], which further improve both speed\\nand accuracy. In comparison to previous YOLOs, YOLO-\\nWorld in this paper aims to detect objects beyond the fixed\\nvocabulary with strong generalization ability.\\n2.2. Open-Vocabulary Object Detection\\nOpen-vocabulary object detection (OVD) [55] has emerged\\nas a new trend for modern object detection, which aims\\nto detect objects beyond the predefined categories. Early\\nworks [12] follow the standard OVD setting [55] by train-\\ning detectors on the base classes and evaluating the novel\\n(unknown) classes. Nevertheless, this open-vocabulary set-\\nting can evaluate the capability of detectors to detect and\\nrecognize novel objects, it is still limited for open sce-\\nnarios and lacks generalization ability to other domains\\ndue to training on the limited dataset and vocabulary.\\n2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9293d21a-3e99-41ad-a1b5-2d7190220985', embedding=None, metadata={'page_label': '3', 'file_name': 'yoloWorld.pdf', 'file_path': '/content/data/yoloWorld.pdf', 'file_type': 'application/pdf', 'file_size': 4685106, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='DetectorText Encoder(a) Traditional Object Detector(b) PreivousOpen-V ocabulary Detector(c) YOLO-WorldObject DetectorFixedVocabulary\\nText EncoderLarge Detector\\nText EncoderLightweight DetectorOfflineVocabulary\\nUserUserOnlineVocabularyRe-parameterize\\nUserTrain-onlyFigure 2. Comparison with Detection Paradigms. (a) Traditional Object Detector : These object detectors can only detect objects\\nwithin the fixed vocabulary pre-defined by the training datasets, e.g., 80 categories of COCO dataset [25]. The fixed vocabulary limits the\\nextension for open scenes. (b) Previous Open-Vocabulary Detectors: Previous methods tend to develop large and heavy detectors for\\nopen-vocabulary detection which intuitively have strong capacity. In addition, these detectors simultaneously encode images and texts as\\ninput for prediction, which is time-consuming for practical applications. (c) YOLO-World: We demonstrate the strong open-vocabulary\\nperformance of lightweight detectors, e.g., YOLO detectors [19, 39], which is of great significance for real-world applications. Rather than\\nusing online vocabulary, we present a prompt-then-detect paradigm for efficient inference, in which the user generates a series of prompts\\naccording to the need and the prompts will be encoded into an offline vocabulary. Then it can be re-parameterized as the model weights\\nfor deployment and further acceleration.\\nYOLO BackboneTextEncoder\\nA manand a womanare skiing with a dogcaption, noun phrases, category…\\nUserText Embeddings\\nMulti-scale Image FeaturesTextContrastive HeadBox HeadmanwomandogV ocabulary Embeddingsmanwomandog\\nUser’s  V ocabularyDogImage-aware Embeddings\\nMulti-scaleImage FeaturesTraining: Online VocabularyDeployment: Offline VocabularyVision-Language PAN\\nObject EmbeddingsRegion-Text Matching\\nInputImageExtract Nouns\\nFigure 3. Overall Architecture of YOLO-World. Compared to traditional YOLO detectors, YOLO-World as an open-vocabulary detector\\nadopts text as input. The Text Encoder first encodes the input text input text embeddings. Then the Image Encoder encodes the input image\\ninto multi-scale image features and the proposed RepVL-PAN exploits the multi-level cross-modality fusion for both image and text features.\\nFinally, YOLO-World predicts the regressed bounding boxes and the object embeddings for matching the categories or nouns that appeared\\nin the input text.\\nInspired by vision-language pre-training [18, 36], recent\\nworks [7, 21, 50, 59, 60] formulate open-vocabulary ob-\\nject detection as image-text matching and exploit large-\\nscale image-text data to increase the training vocabulary\\nat scale. GLIP [23] presents a pre-training framework for\\nopen-vocabulary detection based on phrase grounding and\\nevaluates in a zero-shot setting. Grounding DINO [29]\\nincorporates the grounded pre-training [23] into detection\\ntransformers [57] with cross-modality fusions.\\nSeveral methods [24, 53, 54, 56] unify detection datasets\\nand image-text datasets through region-text matching and\\npre-train detectors with large-scale image-text pairs, achiev-\\ning promising performance and generalization. However,these methods often use heavy detectors like ATSS [58]\\nor DINO [57] with Swin-L [31] as a backbone, lead-\\ning to high computational demands and deployment chal-\\nlenges. In contrast, we present YOLO-World, aiming for\\nefficient open-vocabulary object detection with real-time\\ninference and easier downstream application deployment.\\nDiffering from ZSD-YOLO [51], which also explores open-\\nvocabulary detection [55] with YOLO through language\\nmodel alignment, YOLO-World introduces a novel YOLO\\nframework with an effective pre-training strategy, enhanc-\\ning open-vocabulary performance and generalization.\\n3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8a46268a-9e1e-4f4f-b12d-a03c82a114fd', embedding=None, metadata={'page_label': '4', 'file_name': 'yoloWorld.pdf', 'file_path': '/content/data/yoloWorld.pdf', 'file_type': 'application/pdf', 'file_size': 4685106, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3. Method\\n3.1. Pre-training Formulation: Region-Text Pairs\\nThe traditional object detection methods, including the\\nYOLO-series [19], are trained with instance annotations\\nΩ ={Bi, ci}N\\ni=1, which consist of bounding boxes {Bi}\\nand category labels {ci}. In this paper, we reformulate the\\ninstance annotations as region-text pairs Ω ={Bi, ti}N\\ni=1,\\nwhere tiis the corresponding text for the region Bi. Specif-\\nically, the text tican be the category name, noun phrases,\\nor object descriptions. Moreover, YOLO-World adopts both\\nthe image Iand texts T(a set of nouns) as input and outputs\\npredicted boxes {ˆBk}and the corresponding object embed-\\ndings{ek}(ek∈RD).\\n3.2. Model Architecture\\nThe overall architecture of the proposed YOLO-World is il-\\nlustrated in Fig. 3, which consists of a YOLO detector , a\\nText Encoder , and a Re-parameterizable Vision-Language\\nPath Aggregation Network (RepVL-PAN). Given the input\\ntext, the text encoder in YOLO-World encodes the text into\\ntext embeddings. The image encoder in the YOLO detector\\nextracts the multi-scale features from the input image. Then\\nwe leverage the RepVL-PAN to enhance both text and im-\\nage representation by exploiting the cross-modality fusion\\nbetween image features and text embeddings.\\nYOLO Detector. YOLO-World is mainly developed\\nbased on YOLOv8 [19], which contains a Darknet back-\\nbone [19, 40] as the image encoder, a path aggregation net-\\nwork (PAN) for multi-scale feature pyramids, and a head\\nfor bounding box regression and object embeddings.\\nText Encoder. Given the text T, we adopt the Trans-\\nformer text encoder pre-trained by CLIP [36] to extract the\\ncorresponding text embeddings W=TextEncoder (T)∈\\nRC×D, where Cis the number of nouns and Dis the em-\\nbedding dimension. The CLIP text encoder offers better\\nvisual-semantic capabilities for connecting visual objects\\nwith texts compared to text-only language encoders [5].\\nWhen the input text is a caption or referring expression,\\nwe adopt the simple n-gram algorithm to extract the noun\\nphrases and then feed them into the text encoder.\\nText Contrastive Head. Following previous works [19],\\nwe adopt the decoupled head with two 3×3convs to regress\\nbounding boxes {bk}K\\nk=1and object embeddings {ek}K\\nk=1,\\nwhere Kdenotes the number of objects. We present a text\\ncontrastive head to obtain the object-text similarity sk,jby:\\nsk,j=α·L2-Norm (ek)·L2-Norm (wj)⊤+β, (1)\\nwhere L2-Norm (·)is the L2 normalization and wj∈W\\nis the j-th text embeddings. In addition, we add the affinetransformation with the learnable scaling factor αand shift-\\ning factor β. Both the L2 norms and the affine transforma-\\ntions are important for stabilizing the region-text training.\\nTraining with Online Vocabulary. During training, we\\nconstruct an online vocabulary Tfor each mosaic sample\\ncontaining 4 images. Specifically, we sample all positive\\nnouns involved in the mosaic images and randomly sam-\\nple some negative nouns from the corresponding dataset.\\nThe vocabulary for each mosaic sample contains at most M\\nnouns, and Mis set to 80 as default.\\nInference with Offline Vocabulary. At the inference\\nstage, we present a prompt-then-detect strategy with an of-\\nfline vocabulary for further efficiency. As shown in Fig. 3,\\nthe user can define a series of custom prompts, which might\\ninclude captions or categories. We then utilize the text en-\\ncoder to encode these prompts and obtain offline vocabu-\\nlary embeddings. The offline vocabulary allows for avoid-\\ning computation for each input and provides the flexibility\\nto adjust the vocabulary as needed.\\n3.3. Re-parameterizable Vision-Language PAN\\nFig. 4 shows the structure of the proposed RepVL-PAN\\nwhich follows the top-down and bottom-up paths in [19, 28]\\nto establish the feature pyramids {P3, P4, P5}with the\\nmulti-scale image features {C3, C4, C5}. Furthermore,\\nwe propose the Text-guided CSPLayer (T-CSPLayer) and\\nImage-Pooling Attention (I-Pooling Attention) to further\\nenhance the interaction between image features and text\\nfeatures, which can improve the visual-semantic represen-\\ntation for open-vocabulary capability. During inference, the\\noffline vocabulary embeddings can be re-parameterized into\\nweights of convolutional or linear layers for deployment.\\nText-guided CSPLayer. As Fig. 4 illustrates, the cross-\\nstage partial layers (CSPLayer) are utilized after the top-\\ndown or bottom-up fusion. We extend the CSPLayer\\n(also called C2f) of [19] by incorporating text guidance\\ninto multi-scale image features to form the Text-guided\\nCSPLayer. Specifically, given the text embeddings Wand\\nimage features Xl∈RH×W×D(l∈ {3,4,5}), we adopt\\nthemax-sigmoid attention after the last dark bottleneck\\nblock to aggregate text features into image features by:\\nX′\\nl=Xl·δ( max\\nj∈{1..C}(XlW⊤\\nj))⊤, (2)\\nwhere the updated X′\\nlis concatenated with the cross-stage\\nfeatures as output. The δindicates the sigmoid function.\\nImage-Pooling Attention. To enhance the text embed-\\ndings with image-aware information, we aggregate image\\n4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='690b897e-0201-4e2e-a586-8a204e5d6fa4', embedding=None, metadata={'page_label': '5', 'file_name': 'yoloWorld.pdf', 'file_path': '/content/data/yoloWorld.pdf', 'file_type': 'application/pdf', 'file_size': 4685106, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='manwomandog=\\nC3C4C5T-CSPLayerT-CSPLayerT-CSPLayerT-CSPLayerI-Pooling AttentionP3P4P5I-Pooling AttentionText Embeddings\\n×12×12×2×2Image-aware Embeddings\\nText to ImageImage to TextDark BottleneckMax-SigmoidCSTextSplitSCConcatT-CSPLayer (C2f Block)I-PoolingAttentionCMHCAText3×3Figure 4. Illustration of the RepVL-PAN. The proposed RepVL-\\nPAN adopts the Text-guided CSPLayer (T-CSPLayer) for injecting\\nlanguage information into image features and the Image Pooling\\nAttention (I-Pooling Attention) for enhancing image-aware text\\nembeddings.\\nfeatures to update the text embeddings by proposing the\\nImage-Pooling Attention. Rather than directly using cross-\\nattention on image features, we leverage max pooling on\\nmulti-scale features to obtain 3×3regions, resulting in a\\ntotal of 27 patch tokens ˜X∈R27×D. The text embeddings\\nare then updated by:\\nW′=W+MultiHead-Attention (W,˜X,˜X)(3)\\n3.4. Pre-training Schemes\\nIn this section, we present the training schemes for pre-\\ntraining YOLO-World on large-scale detection, grounding,\\nand image-text datasets.\\nLearning from Region-Text Contrastive Loss. Given\\nthe mosaic sample Iand texts T, YOLO-World outputs\\nKobject predictions {Bk, sk}K\\nk=1along with annotations\\nΩ ={Bi, ti}N\\ni=1. We follow [19] and leverage task-aligned\\nlabel assignment [8] to match the predictions with ground-\\ntruth annotations and assign each positive prediction with a\\ntext index as the classification label. Based on this vocabu-\\nlary, we construct the region-text contrastive loss Lconwith\\nregion-text pairs through cross entropy between object-text\\n(region-text) similarity and object-text assignments. In ad-\\ndition, we adopt IoU loss and distributed focal loss for\\nbounding box regression and the total training loss is de-\\nfined as: L(I) =Lcon+λI·(Liou+Ldfl),where λIis\\nan indicator factor and set to 1 when input image Iis from\\ndetection or grounding data and set to 0 when it is from\\nthe image-text data. Considering image-text datasets havenoisy boxes, we only calculate the regression loss for sam-\\nples with accurate bounding boxes.\\nPseudo Labeling with Image-Text Data. Rather than di-\\nrectly using image-text pairs for pre-training, we propose an\\nautomatic labeling approach to generate region-text pairs.\\nSpecifically, the labeling approach contains three steps: (1)\\nextract noun phrases : we first utilize the n-gram algo-\\nrithm to extract noun phrases from the text; (2) pseudo la-\\nbeling : we adopt a pre-trained open-vocabulary detector,\\ne.g., GLIP [23], to generate pseudo boxes for the given\\nnoun phrases for each image, thus providing the coarse\\nregion-text pairs. (3) filtering : We employ the pre-trained\\nCLIP [36] to evaluate the relevance of image-text pairs and\\nregion-text pairs, and filter the low-relevance pseudo an-\\nnotations and images. We further filter redundant bound-\\ning boxes by incorporating methods such as Non-Maximum\\nSuppression (NMS). We suggest the readers refer to the ap-\\npendix for the detailed approach. With the above approach,\\nwe sample and label 246k images from CC3M [44] with\\n821k pseudo annotations.\\n4. Experiments\\nIn this section, we demonstrate the effectiveness of the\\nproposed YOLO-World by pre-training it on large-scale\\ndatasets and evaluating YOLO-World in a zero-shot manner\\non both LVIS benchmark and COCO benchmark (Sec. 4.2).\\nWe also evaluate the fine-tuning performance of YOLO-\\nWorld on COCO, LVIS for object detection.\\n4.1. Implementation Details\\nThe YOLO-World is developed based on the MMYOLO\\ntoolbox [3] and the MMDetection toolbox [2]. Following\\n[19], we provide three variants of YOLO-World for differ-\\nent latency requirements, e.g., small (S), medium (M), and\\nlarge (L). We adopt the open-source CLIP [36] text encoder\\nwith pre-trained weights to encode the input text. Unless\\nspecified, we measure the inference speeds of all models on\\none NVIDIA V100 GPU without extra acceleration mecha-\\nnisms, e.g., FP16 or TensorRT.\\n4.2. Pre-training\\nExperimental Setup. At the pre-training stage, we adopt\\nthe AdamW optimizer [33] with an initial learning rate\\nof 0.002 and weight decay of 0.05. YOLO-World is pre-\\ntrained for 100 epochs on on 32 NVIDIA V100 GPUs with\\na total batch size of 512. During pre-training, we follow\\nprevious works [19] and adopt color augmentation, random\\naffine, random flip, and mosaic with 4 images for data aug-\\nmentation. The text encoder is frozen during pre-training.\\n5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='932d31b9-b083-473f-80a2-6d5064fe72eb', embedding=None, metadata={'page_label': '6', 'file_name': 'yoloWorld.pdf', 'file_path': '/content/data/yoloWorld.pdf', 'file_type': 'application/pdf', 'file_size': 4685106, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Dataset Type V ocab. Images Anno.\\nObjects365V1 [43] Detection 365 609k 9,621k\\nGQA [16] Grounding - 621k 3,681k\\nFlickr [35] Grounding - 149k 641k\\nCC3M †[44] Image-Text - 246k 821k\\nTable 1. Pre-training Data. The specifications of the datasets\\nused for pre-training YOLO-World.\\nPre-training Data. For pre-training YOLO-World, we\\nmainly adopt detection or grounding datasets including Ob-\\njects365 (V1) [43], GQA [16], Flickr30k [35], as specified\\nin Tab. 1. Following [23], we exclude the images from\\nthe COCO dataset in GoldG [20] (GQA and Flickr30k).\\nThe annotations of the detection datasets used for pre-\\ntraining contain both bounding boxes and categories or\\nnoun phrases. In addition, we also extend the pre-training\\ndata with image-text pairs, i.e., CC3M†[44], which we have\\nlabeled 246k images through the pseudo-labeling method\\ndiscussed in Sec. 3.4.\\nZero-shot Evaluation. After pre-training, we di-\\nrectly evaluate the proposed YOLO-World on the LVIS\\ndataset [13] in a zero-shot manner. The LVIS dataset\\ncontains 1203 object categories, which is much more\\nthan the categories of the pre-training detection datasets\\nand can measure the performance on large vocabulary\\ndetection. Following previous works [20, 23, 53, 54], we\\nmainly evaluate on LVIS minival [20] and report the\\nFixed AP [4] for comparison. The maximum number of\\npredictions is set to 1000.\\nMain Results on LVIS Object Detection. In Tab. 2, we\\ncompare the proposed YOLO-World with recent state-of-\\nthe-art methods [20, 29, 53, 54, 56] on LVIS benchmark in a\\nzero-shot manner. Considering the computation burden and\\nmodel parameters, we mainly compare with those methods\\nbased on lighter backbones, e.g., Swin-T [31]. Remarkably,\\nYOLO-World outperforms previous state-of-the-art meth-\\nods in terms of zero-shot performance and inference speed.\\nCompared to GLIP, GLIPv2, and Grounding DINO, which\\nincorporate more data, e.g., Cap4M (CC3M+SBU [34]),\\nYOLO-World pre-trained on O365 & GolG obtains bet-\\nter performance even with fewer model parameters. Com-\\npared to DetCLIP, YOLO-World achieves comparable per-\\nformance (35.4 v.s.34.4) while obtaining 20×increase in\\ninference speed. The experimental results also demonstrate\\nthat small models, e.g., YOLO-World-S with 13M parame-\\nters, can be used for vision-language pre-training and ob-\\ntain strong open-vocabulary capabilities.4.3. Ablation Experiments\\nWe provide extensive ablation studies to analyze YOLO-\\nWorld from two primary aspects, i.e., pre-training and ar-\\nchitecture. Unless specified, we mainly conduct ablation\\nexperiments based on YOLO-World-L and pre-train Ob-\\njects365 with zero-shot evaluation on LVIS minival .\\nPre-training Data. In Tab. 3, we evaluate the perfor-\\nmance of pre-training YOLO-World using different data.\\nCompared to the baseline trained on Objects365, adding\\nGQA can significantly improve performance with an 8.4\\nAP gain on LVIS. This improvement can be attributed to\\nthe richer textual information provided by the GQA dataset,\\nwhich can enhance the model’s ability to recognize large\\nvocabulary objects. Adding part of CC3M samples (8%\\nof the full datasets) can further bring 0.5 AP gain with 1.3\\nAP on rare objects. Tab. 3 demonstrates that adding more\\ndata can effectively improve the detection capabilities on\\nlarge-vocabulary scenarios. Furthermore, as the amount of\\ndata increases, the performance continues to improve, high-\\nlighting the benefits of leveraging larger and more diverse\\ndatasets for training.\\nAblations on RepVL-PAN. Tab. 4 demonstrates the ef-\\nfectiveness of the proposed RepVL-PAN of YOLO-World,\\nincluding Text-guided CSPLayers and Image Pooling At-\\ntention, for the zero-shot LVIS detection. Specifically, we\\nadopt two settings, i.e., (1) pre-training on O365 and (2)\\npre-training on O365 & GQA. Compared to O365 which\\nonly contains category annotations, GQA includes rich\\ntexts, particularly in the form of noun phrases. As shown\\nin Tab. 4, the proposed RepVL-PAN improves the base-\\nline (YOLOv8-PAN [19]) by 1.1 AP on LVIS, and the im-\\nprovements are remarkable in terms of the rare categories\\n(APr) of LVIS, which are hard to detect and recognize. In\\naddition, the improvements become more significant when\\nYOLO-World is pre-trained with the GQA dataset and ex-\\nperiments indicate that the proposed RepVL-PAN works\\nbetter with rich textual information.\\nText Encoders. In Tab. 5, we compare the performance\\nof using different text encoders, i.e., BERT-base [5] and\\nCLIP-base (ViT-base) [36]. We exploit two settings dur-\\ning pre-training, i.e., frozen and fine-tuned, and the learn-\\ning rate for fine-tuning text encoders is a 0.01×factor of\\nthe basic learning rate. As Tab. 5 shows, the CLIP text\\nencoder obtains superior results than BERT (+10.1 AP for\\nrare categories in LVIS), which is pre-trained with image-\\ntext pairs and has better capability for vision-centric embed-\\ndings. Fine-tuning BERT during pre-training brings signifi-\\ncant improvements (+3.7 AP) while fine-tuning CLIP leads\\nto a severe performance drop. We attribute the drop to that\\n6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d5bf604f-a59b-482b-b830-0b467d23c6ed', embedding=None, metadata={'page_label': '7', 'file_name': 'yoloWorld.pdf', 'file_path': '/content/data/yoloWorld.pdf', 'file_type': 'application/pdf', 'file_size': 4685106, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Method Backbone Params Pre-trained Data FPS AP AP rAPcAPf\\nMDETR [20] R-101 [14] 169M GoldG - 24.2 20.9 24.3 24.2\\nGLIP-T [23] Swin-T [31] 232M O365,GoldG 0.12 24.9 17.7 19.5 31.0\\nGLIP-T [23] Swin-T [31] 232M O365,GoldG,Cap4M 0.12 26.0 20.8 21.4 31.0\\nGLIPv2-T [56] Swin-T [31] 232M O365,GoldG 0.12 26.9 - - -\\nGLIPv2-T [56] Swin-T [31] 232M O365,GoldG,Cap4M 0.12 29.0 - - -\\nGrounding DINO-T [29] Swin-T [31] 172M O365,GoldG 1.5 25.6 14.4 19.6 32.2\\nGrounding DINO-T [29] Swin-T [31] 172M O365,GoldG,Cap4M 1.5 27.4 18.1 23.3 32.7\\nDetCLIP-T [53] Swin-T [31] 155M O365,GoldG 2.3 34.4 26.9 33.9 36.3\\nYOLO-World-S YOLOv8-S 13M (77M) O365,GoldG 74.1 (19.9) 26.2 19.1 23.6 29.8\\nYOLO-World-M YOLOv8-M 29M (92M) O365,GoldG 58.1 (18.5) 31.0 23.8 29.2 33.9\\nYOLO-World-L YOLOv8-L 48M (110M) O365,GoldG 52.0 (17.6) 35.0 27.1 32.8 38.3\\nYOLO-World-L YOLOv8-L 48M (110M) O365,GoldG,CC3M†52.0 (17.6) 35.4 27.6 34.1 38.0\\nTable 2. Zero-shot Evaluation on LVIS. We evaluate YOLO-World on LVIS minival [20] in a zero-shot manner. We report the Fixed\\nAP[4] for a fair comparison with recent methods.†denotes the pseudo-labeled CC3M in our setting, which contains 246k samples.\\nThe FPS is evaluated on one NVIDIA V100 GPU w/o TensorRT. The parameters and FPS of YOLO-World are evaluated for both the\\nre-parameterized version (w/o bracket) and the original version (w/ bracket).\\nPre-trained Data AP AP rAPcAPf\\nO365 23.5 16.2 21.1 27.0\\nO365,GQA 31.9 22.5 29.9 35.4\\nO365,GoldG 32.5 22.3 30.6 36.0\\nO365,GoldG,CC3M†33.0 23.6 32.0 35.5\\nTable 3. Ablations on Pre-training Data. We evaluate the zero-\\nshot performance on LVIS of pre-training YOLO-World with dif-\\nferent amounts of data.\\nGQA T →I I→T AP AP rAPcAPf\\n✗ ✗ ✗ 22.4 14.5 20.1 26.0\\n✗✓ ✗ 23.2 15.2 20.6 27.0\\n✗✓ ✓ 23.5 16.2 21.1 27.0\\n✓ ✗ ✗ 29.7 21.0 27.1 33.6\\n✓ ✓ ✓ 31.9 22.5 29.9 35.4\\nTable 4. Ablations on Re-parameterizable Vision-Language\\nPath Aggregation Network. We evaluate the zero-shot perfor-\\nmance on LVIS of the proposed Vision-Language Path Aggrega-\\ntion Network. T →I and I→T denote the Text-guided CSPLayers\\nand Image-Pooling Attention, respectively.\\nfine-tuning on O365 may degrade the generalization ability\\nof the pre-trained CLIP, which contains only 365 categories\\nand lacks abundant textual information.\\n4.4. Fine-tuning YOLO-World\\nIn this section, we further fine-tune YOLO-World for close-\\nset object detection on the COCO dataset and LVIS datasetText Encoder Frozen? AP AP rAPcAPf\\nBERT-base Frozen 14.6 3.4 10.7 20.0\\nBERT-base Fine-tune 18.3 6.6 14.6 23.6\\nCLIP-base Frozen 22.4 14.5 20.1 26.0\\nCLIP-base Fine-tune 19.3 8.6 15.7 24.8\\nTable 5. Text Encoder in YOLO-World. We ablate different text\\nencoders in YOLO-World through the zero-shot LVIS evaluation.\\nto demonstrate the effectiveness of the pre-training.\\nExperimental Setup. We use the pre-trained weights to\\ninitialize YOLO-World for fine-tuning. All models are fine-\\ntuned for 80 epochs with the AdamW optimizer and the ini-\\ntial learning rate is set to 0.0002. In addition, we fine-tune\\nthe CLIP text encoder with a learning factor of 0.01. For the\\nLVIS dataset, we follow previous works [7, 12, 60] and fine-\\ntune YOLO-World on the LVIS-base (common & frequent)\\nand evaluate it on the LVIS-novel (rare).\\nCOCO Object Detection. We compare the pre-trained\\nYOLO-World with previous YOLO detectors [19, 22, 49]\\nin Tab. 6. For fine-tuning YOLO-World on the COCO\\ndataset, we remove the proposed RepVL-PAN for fur-\\nther acceleration considering that the vocabulary size of\\nthe COCO dataset is small. In Tab. 6, it’s evident that\\nour approach can achieve decent zero-shot performance on\\nthe COCO dataset, which indicates that YOLO-World has\\nstrong generalization ability. Moreover, YOLO-World af-\\nter fine-tuning on the COCO train2017 demonstrates\\n7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a1b935b7-15d8-4613-8f83-e5cfa0e237ac', embedding=None, metadata={'page_label': '8', 'file_name': 'yoloWorld.pdf', 'file_path': '/content/data/yoloWorld.pdf', 'file_type': 'application/pdf', 'file_size': 4685106, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Method Pre-train AP AP 50AP75FPS\\nTraining from scratch.\\nYOLOv6-S [22] ✗ 43.7 60.8 47.0 442\\nYOLOv6-M [22] ✗ 48.4 65.7 52.7 277\\nYOLOv6-L [22] ✗ 50.7 68.1 54.8 166\\nYOLOv7-T [49] ✗ 37.5 55.8 40.2 404\\nYOLOv7-L [49] ✗ 50.9 69.3 55.3 182\\nYOLOv7-X [49] ✗ 52.6 70.6 57.3 131\\nYOLOv8-S [19] ✗ 44.4 61.2 48.1 386\\nYOLOv8-M [19] ✗ 50.5 67.3 55.0 238\\nYOLOv8-L [19] ✗ 52.9 69.9 57.7 159\\nZero-shot transfer.\\nYOLO-World-S O+G 37.6 52.3 40.7 -\\nYOLO-World-M O+G 42.8 58.3 46.4 -\\nYOLO-World-L O+G 44.4 59.8 48.3 -\\nYOLO-World-L O+G+C 45.1 60.7 48.9 -\\nFine-tuned w/ RepVL-PAN.\\nYOLO-World-S O+G 45.9 62.3 50.1 -\\nYOLO-World-M O+G 51.2 68.1 55.9 -\\nYOLO-World-L O+G+C 53.3 70.1 58.2 -\\nFine-tuned w/o RepVL-PAN.\\nYOLO-World-S O+G 45.7 62.3 49.9 373\\nYOLO-World-M O+G 50.7 67.2 55.1 231\\nYOLO-World-L O+G+C 53.3 70.3 58.1 156\\nTable 6. Comparison with YOLOs on COCO Object Detec-\\ntion. We fine-tune the YOLO-World on COCO train2017 and\\nevaluate on COCO val2017 . The results of YOLOv7 [49] and\\nYOLOv8 [19] are obtained from MMYOLO [3]. ‘O’, ‘G’, and ‘C’\\ndenote pertaining using Objects365, GoldG, and CC3M†, respec-\\ntively. The FPS is measured on one NVIDIA V100 w/ TensorRT.\\nhigher performance compared to previous methods trained\\nfrom scratch.\\nLVIS Object Detection. In Tab. 7, we evaluate the fine-\\ntuning performance of YOLO-World on the standard LVIS\\ndataset. Firstly, compared to the oracle YOLOv8s [19]\\ntrained on the full LVIS datasets, YOLO-World achieves\\nsignificant improvements, especially for larger models, e.g.,\\nYOLO-World-L outperforms YOLOv8-L by 7.2 AP and\\n10.2 AP r. The improvements can demonstrate the effec-\\ntiveness of the proposed pre-training strategy for large-\\nvocabulary detection. Moreover, YOLO-World, as an effi-\\ncient one-stage detector, outperforms previous state-of-the-\\nart two-stage methods [7, 12, 21, 50, 60] on the overall per-\\nformance without extra designs, e.g., learnable prompts [7]\\nor region-based alginments [12].Method AP AP r APcAPf\\nViLD [12] 27.8 16.7 26.5 34.2\\nRegionCLIP [59] 28.2 17.1 - -\\nDetic [60] 26.8 17.8 - -\\nFVLM [21] 24.2 18.6 - -\\nDetPro [7] 28.4 20.8 27.8 32.4\\nBARON [50] 29.5 23.2 29.3 32.5\\nYOLOv8-S 19.4 7.4 17.4 27.0\\nYOLOv8-M 23.1 8.4 21.3 31.5\\nYOLOv8-L 26.9 10.2 25.4 35.8\\nYOLO-World-S 23.9 12.8 20.4 32.7\\nYOLO-World-M 28.8 15.9 24.6 39.0\\nYOLO-World-L 34.1 20.4 31.1 43.5\\nTable 7. Comparison with Open-Vocabulary Detectors on\\nLVIS. We train YOLO-World on the LVIS-base (including com-\\nmon and frequent) report the bbox AP . The YOLO-v8 are trained\\non the full LVIS datasets (including base and novel) along with the\\nclass balanced sampling.\\n4.5. Open-Vocabulary Instance Segmentation\\nIn this section, we further fine-tune YOLO-World for\\nsegmenting objects under the open-vocabulary setting,\\nwhich can be termed open-vocabulary instance segmenta-\\ntion (OVIS). Previous methods [17] have explored OVIS\\nwith pseudo-labelling on novel objects. Differently, con-\\nsidering that YOLO-World has strong transfer and gener-\\nalization capabilities, we directly fine-tune YOLO-World\\non a subset of data with mask annotations and evaluate the\\nsegmentation performance under large-vocabulary settings.\\nSpecifically, we benchmark open-vocabulary instance seg-\\nmentation under two settings:\\n• (1) COCO to LVIS setting, we fine-tune YOLO-World on\\nthe COCO dataset (including 80 categories) with mask\\nannotations, under which the models need to transfer\\nfrom 80 categories to 1203 categories ( 80→1203 );\\n• (2) LVIS-base to LVIS setting, we fine-tune YOLO-World\\non the LVIS-base (including 866 categories, common &\\nfrequent) with mask annotations, under which the models\\nneed to transfer from 866 categories to 1203 categories\\n(866→1203 ).\\nWe evaluate the fine-tuned models on the standard LVIS\\nval2017 with 1203 categories, in which 337 rare cate-\\ngories are unseen and can be used to measure the open-\\nvocabulary performance.\\nResults. Tab. 8 shows the experimental results of extend-\\ning YOLO-World for open-vocabulary instance segmenta-\\ntion. Specifically, we adopt two fine-tuning strategies: (1)\\nonly fine-tuning the segmentation head and (2) fine-tuning\\n8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a7f8be4e-af45-49aa-aeb5-1ed785703eef', embedding=None, metadata={'page_label': '9', 'file_name': 'yoloWorld.pdf', 'file_path': '/content/data/yoloWorld.pdf', 'file_type': 'application/pdf', 'file_size': 4685106, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='all modules. Under strategy (1), the fine-tuned YOLO-\\nWorld still retains the zero-shot capabilities acquired from\\nthe pre-training stage, allowing it to generalize to unseen\\ncategories without additional fine-tuning. Strategy (2) en-\\nables YOLO-World fit the LVIS dataset better, but it may\\nresult in the degradation of the zero-shot capabilities.\\nTab. 8 shows the comparisons of fine-tuning YOLO-\\nWorld with different settings (COCO or LVIS-base) and\\ndifferent strategies (fine-tuning seg. head or fine-tuning\\nall). Firstly, fine-tuning on LVIS-base obtains better perfor-\\nmance compared to that based on COCO. However, the ra-\\ntios between AP and AP r(APr/AP) are nearly unchanged,\\ne.g., the ratios of YOLO-World on COCO and LVIS-base\\nare 76.5% and 74.3%, respectively. Considering that the\\ndetector is frozen, we attribute the performance gap to the\\nfact that the LVIS dataset provides more detailed and denser\\nsegmentation annotations, which are beneficial for learn-\\ning the segmentation head. When fine-tuning all mod-\\nules, YOLO-World obtains remarkable improvements on\\nLVIS, e.g., YOLO-World-L achieves 9.6 AP gain. However,\\nthe fine-tuning might degrade the open-vocabulary perfor-\\nmance and lead to a 0.6 box AP rdrop for YOLO-World-L.\\n4.6. Visualizations\\nWe provide the visualization results of pre-trained YOLO-\\nWorld-L under three settings: (a) we perform zero-shot\\ninference with LVIS categories; (b) we input the custom\\nprompts with fine-grained categories with attributes; (c) re-\\nferring detection. The visualizations also demonstrate that\\nYOLO-World has a strong generalization ability for open-\\nvocabulary scenarios along with referring ability.\\nZero-shot Inference on LVIS. Fig. 5 shows the visual-\\nization results based on the LVIS categories which are gen-\\nerated by the pre-trained YOLO-World-L in a zero-shot\\nmanner. The pre-trained YOLO-World exhibits strong zero-\\nshot transfer capabilities and is able to detect as many ob-\\njects as possible within the image.\\nInference with User’s Vocabulary. In Fig. 6, we explore\\nthe detection capabilities of YOLO-World with our defined\\ncategories. The visualization results demonstrate that the\\npre-trained YOLO-World-L also exhibits the capability for\\n(1) fine-grained detection ( i.e., detect the parts of one ob-\\nject) and (2) fine-grained classification ( i.e., distinguish dif-\\nferent sub-categories of objects.).\\nReferring Object Detection. In Fig. 7, we leverage some\\ndescriptive (discriminative) noun phrases as input, e.g., the\\nstanding person, to explore whether the model can locate\\nregions or objects in the image that match our given in-\\nput. The visualization results display the phrases and theircorresponding bounding boxes, demonstrating that the pre-\\ntrained YOLO-World has the referring or grounding capa-\\nbility. This ability can be attributed to the proposed pre-\\ntraining strategy with large-scale training data.\\n5. Conclusion\\nWe present YOLO-World, a cutting-edge real-time open-\\nvocabulary detector aiming to improve efficiency and open-\\nvocabulary capability in real-world applications. In this pa-\\nper, we have reshaped the prevalent YOLOs as a vision-\\nlanguage YOLO architecture for open-vocabulary pre-\\ntraining and detection and proposed RepVL-PAN, which\\nconnects vision and language information with the network\\nand can be re-parameterized for efficient deployment. We\\nfurther present the effective pre-training schemes with de-\\ntection, grounding and image-text data to endow YOLO-\\nWorld with a strong capability for open-vocabulary de-\\ntection. Experiments can demonstrate the superiority of\\nYOLO-World in terms of speed and open-vocabulary per-\\nformance and indicate the effectiveness of vision-language\\npre-training on small models, which is insightful for future\\nresearch. We hope YOLO-World can serve as a new bench-\\nmark for addressing real-world open-vocabulary detection.\\nReferences\\n[1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\\nto-end object detection with transformers. In ECCV , pages\\n213–229, 2020. 2\\n[2] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu\\nXiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,\\nJiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tian-\\nheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu,\\nJifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang,\\nChen Change Loy, and Dahua Lin. MMDetection: Open\\nmmlab detection toolbox and benchmark. arXiv preprint\\narXiv:1906.07155 , 2019. 5\\n[3] MMYOLO Contributors. MMYOLO: OpenMMLab YOLO\\nseries toolbox and benchmark. https://github.com/\\nopen-mmlab/mmyolo , 2022. 5, 8\\n[4] Achal Dave, Piotr Doll ´ar, Deva Ramanan, Alexander Kir-\\nillov, and Ross B. Girshick. Evaluating large-vocabulary\\nobject detectors: The devil is in the details. CoRR ,\\nabs/2102.01066, 2021. 6, 7\\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\\nToutanova. BERT: pre-training of deep bidirectional trans-\\nformers for language understanding. In NAACL-HLT , pages\\n4171–4186, 2019. 1, 4, 6\\n[6] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han,\\nGuiguang Ding, and Jian Sun. Repvgg: Making vgg-style\\nconvnets great again. In CVPR , pages 13733–13742, 2021.\\n2\\n[7] Yu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao,\\nand Guoqi Li. Learning to prompt for open-vocabulary ob-\\n9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='80de5355-316f-4641-98f8-15e32d029e85', embedding=None, metadata={'page_label': '10', 'file_name': 'yoloWorld.pdf', 'file_path': '/content/data/yoloWorld.pdf', 'file_type': 'application/pdf', 'file_size': 4685106, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Model Fine-tune Data Fine-tune Modules AP AP rAPcAPfAPbAPb\\nr\\nYOLO-World-M COCO Seg Head 12.3 9.1 10.9 14.6 22.3 16.2\\nYOLO-World-L COCO Seg Head 16.2 12.4 15.0 19.2 25.3 18.0\\nYOLO-World-M LVIS-base Seg Head 16.7 12.6 14.6 20.8 22.3 16.2\\nYOLO-World-L LVIS-base Seg Head 19.1 14.2 17.2 23.5 25.3 18.0\\nYOLO-World-M LVIS-base All 25.9 13.4 24.9 32.6 32.6 15.8\\nYOLO-World-L LVIS-base All 28.7 15.0 28.3 35.2 36.2 17.4\\nTable 8. Open-Vocabulary Instance Segmentation. We evaluate YOLO-World for open-vocabulary instance segmentation under the two\\nsettings. We fine-tune the segmentation head or all modules of YOLO-World and report Mask AP for comparison. APbdenotes the box\\nAP.\\nFigure 5. Visualization Results on Zero-shot Inference on LVIS. We adopt the pre-trained YOLO-World-L and infer with the LVIS\\nvocabulary (containing 1203 categories) on the COCO val2017 .\\nject detection with vision-language model. In CVPR , pages\\n14064–14073, 2022. 1, 3, 7, 8\\n[8] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R. Scott,\\nand Weilin Huang. TOOD: task-aligned one-stage object de-\\ntection. In ICCV , pages 3490–3499. IEEE, 2021. 5\\n[9] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian\\nSun. YOLOX: exceeding YOLO series in 2021. CoRR ,\\nabs/2107.08430, 2021. 2\\n[10] Ross B. Girshick. Fast R-CNN. In ICCV , pages 1440–1448,\\n2015. 2\\n[11] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra\\nMalik. Rich feature hierarchies for accurate object detection\\nand semantic segmentation. In CVPR , pages 580–587, 2014.\\n2\\n[12] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.\\nOpen-vocabulary object detection via vision and language\\nknowledge distillation. In ICLR , 2022. 1, 2, 7, 8\\n[13] Agrim Gupta, Piotr Doll ´ar, and Ross B. Girshick. LVIS: A\\ndataset for large vocabulary instance segmentation. In CVPR ,\\npages 5356–5364, 2019. 6\\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In CVPR ,\\npages 770–778, 2016. 7\\n[15] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross B.\\nGirshick. Mask R-CNN. In ICCV , pages 2980–2988, 2017.\\n1, 2\\n[16] Drew A. Hudson and Christopher D. Manning. GQA: A new\\ndataset for real-world visual reasoning and compositional\\nquestion answering. In CVPR , pages 6700–6709, 2019. 6\\n[17] Dat Huynh, Jason Kuen, Zhe Lin, Jiuxiang Gu, and Ehsan\\nElhamifar. Open-vocabulary instance segmentation via ro-\\nbust cross-modal pseudo-labeling. In CVPR , pages 7010–\\n7021, 2022. 8\\n[18] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\\nHieu Pham, Quoc V . Le, Yun-Hsuan Sung, Zhen Li, and Tom\\nDuerig. Scaling up visual and vision-language representation\\nlearning with noisy text supervision. In ICML , pages 4904–\\n4916, 2021. 1, 3\\n[19] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. Ultralyt-\\nics yolov8. https://github.com/ultralytics/\\nultralytics , 2023. 2, 3, 4, 5, 6, 7, 8\\n[20] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel\\n10', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5031eb95-9469-41bb-b84d-58ad4dce0066', embedding=None, metadata={'page_label': '11', 'file_name': 'yoloWorld.pdf', 'file_path': '/content/data/yoloWorld.pdf', 'file_type': 'application/pdf', 'file_size': 4685106, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='{men, women, boy, girl}{golden dog, black dog, spotted dog}{elephant, ear, leg, trunk, ivory}{grass, sky, zebra, trunk, tree}Figure 6. Visualization Results on User’s Vocabulary. We define the custom vocabulary for each input image and YOLO-World can\\ndetect the accurate regions according to the vocabulary. Images are obtained from COCO val2017 .\\nthe person in redthe brown animalthe tallest personperson with a white shirt\\nmoonthe standing personperson holding a toyperson holding a baseball batthe jumping person\\nFigure 7. Visualization Results on Referring Object Detection. We explore the capability of the pre-trained YOLO-World to detect\\nobjects with descriptive noun phrases. Images are obtained from COCO val2017 .\\nSynnaeve, Ishan Misra, and Nicolas Carion. MDETR - mod-\\nulated detection for end-to-end multi-modal understanding.\\nInICCV , pages 1760–1770, 2021. 6, 7\\n[21] Weicheng Kuo, Yin Cui, Xiuye Gu, A. J. Piergiovanni,\\nand Anelia Angelova. F-VLM: open-vocabulary object de-\\ntection upon frozen vision and language models. CoRR ,\\nabs/2209.15639, 2022. 3, 8\\n[22] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei\\nGeng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng,\\nWeiqiang Nie, Yiduo Li, Bo Zhang, Yufei Liang, Linyuan\\nZhou, Xiaoming Xu, Xiangxiang Chu, Xiaoming Wei, and\\nXiaolin Wei. Yolov6: A single-stage object detection frame-\\nwork for industrial applications. CoRR , abs/2209.02976,\\n2022. 2, 7, 8\\n[23] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\\nwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu\\nYuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and\\nJianfeng Gao. Grounded language-image pre-training. In\\nCVPR , pages 10955–10965, 2022. 1, 2, 3, 5, 6, 7, 13[24] Chuang Lin, Peize Sun, Yi Jiang, Ping Luo, Lizhen Qu, Gho-\\nlamreza Haffari, Zehuan Yuan, and Jianfei Cai. Learning\\nobject-language alignments for open-vocabulary object de-\\ntection. In ICLR , 2023. 3\\n[25] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James\\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and\\nC. Lawrence Zitnick. Microsoft COCO: common objects\\nin context. In Proceedings of the European Conference on\\nComputer Vision (ECCV) , pages 740–755, 2014. 1, 2, 3, 13\\n[26] Tsung-Yi Lin, Piotr Doll ´ar, Ross B. Girshick, Kaiming He,\\nBharath Hariharan, and Serge J. Belongie. Feature pyramid\\nnetworks for object detection. In CVPR , pages 936–944,\\n2017. 1, 2\\n[27] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He,\\nand Piotr Doll ´ar. Focal loss for dense object detection. In\\nICCV , pages 2999–3007, 2017. 2\\n[28] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.\\nPath aggregation network for instance segmentation. In\\nCVPR , pages 8759–8768, 2018. 2, 4\\n11', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9c6106d9-b1d7-4f9f-8678-ed376ad6e2cc', embedding=None, metadata={'page_label': '12', 'file_name': 'yoloWorld.pdf', 'file_path': '/content/data/yoloWorld.pdf', 'file_type': 'application/pdf', 'file_size': 4685106, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='[29] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\\nZhu, and Lei Zhang. Grounding DINO: marrying DINO with\\ngrounded pre-training for open-set object detection. CoRR ,\\nabs/2303.05499, 2023. 1, 2, 3, 6, 7\\n[30] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian\\nSzegedy, Scott E. Reed, Cheng-Yang Fu, and Alexander C.\\nBerg. SSD: single shot multibox detector. In ECCV , pages\\n21–37, 2016. 2\\n[31] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\\nHierarchical vision transformer using shifted windows. In\\nICCV , pages 9992–10002, 2021. 2, 3, 6, 7\\n[32] Xiang Long, Kaipeng Deng, Guanzhong Wang, Yang Zhang,\\nQingqing Dang, Yuan Gao, Hui Shen, Jianguo Ren, Shumin\\nHan, Errui Ding, and Shilei Wen. PP-YOLO: an effec-\\ntive and efficient implementation of object detector. CoRR ,\\nabs/2007.12099, 2020. 2\\n[33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\\nregularization. In ICLR , 2019. 5\\n[34] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.\\nIm2text: Describing images using 1 million captioned pho-\\ntographs. In NeurIPS , pages 1143–1151, 2011. 6\\n[35] Bryan A. Plummer, Liwei Wang, Chris M. Cervantes,\\nJuan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.\\nFlickr30k entities: Collecting region-to-phrase correspon-\\ndences for richer image-to-sentence models. Int. J. Comput.\\nVis., pages 74–93, 2017. 6\\n[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\\nKrueger, and Ilya Sutskever. Learning transferable visual\\nmodels from natural language supervision. In ICML , pages\\n8748–8763, 2021. 1, 2, 3, 4, 5, 6, 13\\n[37] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster,\\nstronger. In CVPR , pages 6517–6525, 2017. 2\\n[38] Joseph Redmon and Ali Farhadi. Yolov3: An incremental\\nimprovement. CoRR , abs/1804.02767, 2018. 2\\n[39] Joseph Redmon, Santosh Kumar Divvala, Ross B. Girshick,\\nand Ali Farhadi. You only look once: Unified, real-time ob-\\nject detection. In CVPR , pages 779–788, 2016. 2, 3\\n[40] Joseph Redmon, Santosh Kumar Divvala, Ross B. Girshick,\\nand Ali Farhadi. You only look once: Unified, real-time ob-\\nject detection. In CVPR , pages 779–788, 2016. 1, 2, 4\\n[41] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.\\nFaster R-CNN: towards real-time object detection with re-\\ngion proposal networks. IEEE Transactions on Pattern Anal-\\nysis and Machine Intelligence , pages 1137–1149, 2017. 2\\n[42] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.\\nFaster R-CNN: towards real-time object detection with re-\\ngion proposal networks. IEEE Transactions on Pattern Anal-\\nysis and Machine Intelligence , pages 1137–1149, 2017. 1\\n[43] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang\\nYu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365:\\nA large-scale, high-quality dataset for object detection. In\\nICCV , pages 8429–8438, 2019. 2, 6[44] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\\nage alt-text dataset for automatic image captioning. In ACL,\\npages 2556–2565, 2018. 5, 6, 13\\n[45] Cheng Shi and Sibei Yang. Edadet: Open-vocabulary ob-\\nject detection using early dense alignment. In ICCV , pages\\n15678–15688, 2023. 1\\n[46] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\\nfully convolutional one-stage object detection. In ICCV ,\\npages 9626–9635, 2019. 2\\n[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NeurIPS , pages\\n5998–6008, 2017. 2\\n[48] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu,\\nPing-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. Cspnet: A\\nnew backbone that can enhance learning capability of CNN.\\nInCVPRW , pages 1571–1580, 2020. 2\\n[49] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-\\nYuan Mark Liao. Yolov7: Trainable bag-of-freebies sets\\nnew state-of-the-art for real-time object detectors. In CVPR ,\\npages 7464–7475, 2023. 2, 7, 8\\n[50] Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, and\\nChen Change Loy. Aligning bag of regions for open-\\nvocabulary object detection. In CVPR , pages 15254–15264,\\n2023. 1, 3, 8\\n[51] Johnathan Xie and Shuai Zheng. ZSD-YOLO: zero-shot\\nYOLO detection using vision-language knowledgedistilla-\\ntion. CoRR , 2021. 3\\n[52] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang,\\nCheng Cui, Kaipeng Deng, Guanzhong Wang, Qingqing\\nDang, Shengyu Wei, Yuning Du, and Baohua Lai.\\nPP-YOLOE: an evolved version of YOLO. CoRR ,\\nabs/2203.16250, 2022. 2\\n[53] Lewei Yao, Jianhua Han, Youpeng Wen, Xiaodan Liang, Dan\\nXu, Wei Zhang, Zhenguo Li, Chunjing Xu, and Hang Xu.\\nDetclip: Dictionary-enriched visual-concept paralleled pre-\\ntraining for open-world detection. In NeurIPS , 2022. 1, 2, 3,\\n6, 7\\n[54] Lewei Yao, Jianhua Han, Xiaodan Liang, Dan Xu, Wei\\nZhang, Zhenguo Li, and Hang Xu. Detclipv2: Scal-\\nable open-vocabulary object detection pre-training via word-\\nregion alignment. In CVPR , pages 23497–23506, 2023. 1, 3,\\n6\\n[55] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-\\nFu Chang. Open-vocabulary object detection using captions.\\nInCVPR , pages 14393–14402, 2021. 1, 2, 3\\n[56] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun\\nChen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu\\nYuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Uni-\\nfying localization and vision-language understanding. In\\nNeurIPS , 2022. 1, 2, 3, 6, 7\\n[57] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun\\nZhu, Lionel M. Ni, and Heung-Yeung Shum. DINO: DETR\\nwith improved denoising anchor boxes for end-to-end object\\ndetection. In ICLR , 2023. 3\\n[58] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and\\nStan Z. Li. Bridging the gap between anchor-based and\\n12', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ab3e624c-bee4-4e5a-a0d3-9c4ef821fd47', embedding=None, metadata={'page_label': '13', 'file_name': 'yoloWorld.pdf', 'file_path': '/content/data/yoloWorld.pdf', 'file_type': 'application/pdf', 'file_size': 4685106, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='anchor-free detection via adaptive training sample selection.\\nInCVPR , pages 9756–9765, 2020. 2, 3\\n[59] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan\\nLi, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang\\nDai, Lu Yuan, Yin Li, and Jianfeng Gao. Regionclip:\\nRegion-based language-image pretraining. In CVPR , pages\\n16772–16782, 2022. 3, 8\\n[60] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp\\nKr¨ahenb ¨uhl, and Ishan Misra. Detecting twenty-thousand\\nclasses using image-level supervision. In ECCV , pages 350–\\n368, 2022. 3, 7, 8\\n[61] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\\nand Jifeng Dai. Deformable DETR: deformable transformers\\nfor end-to-end object detection. In ICLR , 2021. 2\\nA. Additional Details\\nA.1. Re-parameterization for RepVL-PAN\\nDuring inference on an offline vocabulary, we adopt re-\\nparameterization for RepVL-PAN for faster inference speed\\nand deployment. Firstly, we pre-compute the text embed-\\ndings W∈RC×Dthrough the text encoder.\\nRe-parameterize T-CSPLayer. For each T-CSPLayer in\\nRepVL-PAN, we can re-parameterize and simplify the pro-\\ncess of adding text guidance by reshaping the text embed-\\ndings W∈RC×D×1×1into the weights of a 1×1convo-\\nlution layer (or a linear layer), as follows:\\nX′=X⊙Sigmoid (max(Conv (X, W ),dim=1 )),(4)\\nwhere X× ∈RB×D×H×WandX′∈RB×D×H×Ware\\nthe input and output image features. ⊙is the matrix multi-\\nplication with reshape ortranspose .\\nRe-parameterize I-Pooling Attention. The I-Pooling\\nAttention can be re-parameterize or simplified by:\\n˜X=cat(MP(X3,3),MP(X4,3),MP(X5,3)),(5)\\nwhere cat is the concentration and MP(·, 3) denotes the\\nmax pooling for 3×3output features. {X3, X4, X5}are\\nthe multi-scale features in RepVL-PAN. ˜Xis flattened and\\nhas the shape of B×D×27. Then we can update the text\\nembeddings by:\\nW′=W+Softmax (W⊙˜X),dim=-1 )⊙W, (6)\\nA.2. Fine-tuning Details.\\nWe remove all T-CSPLayers and Image-Pooling Atten-\\ntion in RepVL-PAN when transferring YOLO-World to\\nCOCO [25] object detection, which only contains 80 cat-\\negories and has a relatively low dependency on visual-\\nlanguage interaction. During fine-tuning, we initializeYOLO-World using pre-trained weights. The learning rate\\nof fine-tuning is set to 0.0002 with the weight decay set to\\n0.05. After fine-tuning, we pre-compute the class text em-\\nbeddings with given COCO categories and store the embed-\\ndings into the weights of the classification layers.\\nB. Automatic Labeling on Large-scale Image-\\nText Data\\nIn this section, we add details procedures for labeling\\nregion-text pairs with large-scale image-text data, e.g.,\\nCC3M [44]. The overall labeling pipeline is illustrated in\\nFig. 8, which mainly consists of three procedures, i.e., (1)\\nextract object nouns, (2) pseudo labeling, and (3) filtering.\\nAs discussed in Sec. 3.4, we adopt the simple n-gram algo-\\nrithm to extract nouns from captions.\\nRegion-Text Proposals. After obtaining the set of object\\nnouns T={tk}Kfrom the first step, we leverage a pre-\\ntrained open-vocabulary detector, i.e., GLIP-L [23], to gen-\\nerate pseudo boxes {Bi}along with confidence scores {ci}:\\n{Bi, ti, ci}N\\ni=1=GLIP-Labeler (I, T), (7)\\nwhere {Bi, ti, ci}N\\ni=1are the coarse region-text proposals.\\nCLIP-based Re-scoring & Filtering. Considering the\\nregion-text proposals containing much noise, we present\\na restoring and filtering pipeline with the pre-trained\\nCLIP [36]. Given the input image I, caption T, and\\nthe coarse region-text proposals {Bi, ti, ci}N\\ni=1, the specific\\npipeline is listed as follows:\\n• (1) Compute Image-Text Score: we forward the image I\\nwith its caption Tinto CLIP and obtain the image-text\\nsimilarity score simg.\\n• (2) Compute Region-Text Score: we crop the region im-\\nages from the input image according to the region boxes\\n{Bi}. Then we forward the cropped images along with\\ntheir texts {ti}into CLIP and obtain the region-text simi-\\nlarity Sr={sr\\ni}N\\ni=1.\\n• (3) [Optional] Re-Labeling: we can forward each\\ncropped image with all nouns and assign the noun with\\nmaximum similarity, which can help correct the texts\\nwrongly labeled by GLIP.\\n• (4) Rescoring: we adopt the region-text similarity Srto\\nrescore the confidence scores ˜ci=pci∗sr\\ni.\\n• (5) Region-level Filtering: we first divide the region-text\\nproposals into different groups according to the texts and\\nthen perform non-maximum suppression (NMS) to fil-\\nter the duplicate predictions (the NMS threshold is set to\\n0.5). Then we filter out the proposals with low confidence\\nscores (the threshold is set to 0.3).\\n13', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8f575ce2-c16d-4f26-ae8c-7aa548fad504', embedding=None, metadata={'page_label': '14', 'file_name': 'yoloWorld.pdf', 'file_path': '/content/data/yoloWorld.pdf', 'file_type': 'application/pdf', 'file_size': 4685106, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Open-V ocabularyLabelerCLIP LabelerAutomatic Labeling Pipeline\\nimagePseudo LabelingCLIP-based Filteringimage, object nounsboxes\\nn-gramcaptionExtracting Object Nouns“A photography of a manand a woman”nouns\\nobjectsFigure 8. Labeling Pipeline for Image-Text Data We first leverage the simple n-gram to extract object nouns from the captions. We adopt\\na pre-trained open-vocabulary detector to generate pseudo boxes given the object nouns, which forms the coarse region-text proposals.\\nThen we use a pre-trained CLIP to rescore or relabel the boxes along with filtering.\\n• (6) Image-level Filtering: we compute the image-level\\nregion-text scores sregionby averaging the kept region-\\ntext scores. Then we obtain the image-level confidence\\nscore by s=√\\nsimg∗sregion and we keep the images\\nwith scores larger than 0.3.\\nThe thresholds mentioned above are empirically set accord-\\ning to the part of labeled results and the whole pipeline is\\nautomatic without human verification. Finally, the labeled\\nsamples are used for pre-training YOLO-World. We will\\nprovide the pseudo annotations of CC3M for further re-\\nsearch.\\nC. Pre-training YOLO-World at Scale\\nWhen pre-training small models, e.g., YOLO-World-S, a\\nnatural question we have is: how much capacity does a\\nsmall model have, and how much training data or what kind\\nof data does a small model need? To answer this question,\\nwe leverage different amounts of pseudo-labeled region-text\\npairs to pre-train YOLO-World. As shown in Tab. 9, adding\\nmore image-text samples can increase the zero-shot per-\\nformance of YOLO-World-S. Tab. 9 indicates: (1) adding\\nimage-text data can improve the overall zero-shot perfor-\\nmance of YOLO-World-S; (2) using an excessive amount\\nof pseudo-labeled data may have some negative effects for\\nsmall models (YOLO-World-S), though it can improve the\\non rare categories (AP r). However, using fine-grained an-\\nnotations (GoldG) for small models can provide significant\\nimprovements, which indicates that large-scale high-quality\\nannotated data can significantly enhance the capabilities of\\nsmall models. And Tab. 3 in the main text has shown that\\npre-training with the combination of fine-annotated data\\nand pseudo-annotated data can perform better. We will ex-\\nplore more about the data for pre-training small models or\\nYOLO detectors in future work.\\n14', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1e5b1a94-db31-4c89-9bfc-3d01984bd651', embedding=None, metadata={'page_label': '15', 'file_name': 'yoloWorld.pdf', 'file_path': '/content/data/yoloWorld.pdf', 'file_type': 'application/pdf', 'file_size': 4685106, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Method Pre-trained Data Samples AP AP rAPcAPf\\nYOLO-World-S O365 0.61M 16.3 9.2 14.1 20.1\\nYOLO-World-S O365+GoldG 1.38M 24.2 16.4 21.7 27.8\\nYOLO-World-S O365+CC3M-245k 0.85M 16.5 10.8 14.8 19.1\\nYOLO-World-S O365+CC3M-520k 1.13M 19.2 10.7 17.4 22.4\\nYOLO-World-S O365+CC3M-750k 1.36M 18.2 11.2 16.0 21.1\\nTable 9. Zero-shot Evaluation on LVIS. We evaluate the performance of pre-training YOLO-World-S with different amounts of data, the\\nimage-text data.\\n15', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mEuOqeq8X7q",
        "outputId": "85ae9dae-c1e1-4848-816e-580982727a80"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(id_='26eb407c-eb49-4466-9b2c-97a2b354d9ae', embedding=None, metadata={'page_label': '1', 'file_name': 'attention.pdf', 'file_path': '/content/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZT6y0w_8b4D",
        "outputId": "fe25bec8-ac3b-44a2-aeda-49650b45c513"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(id_='7f57e13d-211e-4c39-8a4b-da625ddb0ab9', embedding=None, metadata={'page_label': '2', 'file_name': 'attention.pdf', 'file_path': '/content/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXOtxRO38mZ4",
        "outputId": "b4260006-bc43-4acc-9925-2156f1599207"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(id_='1e5b1a94-db31-4c89-9bfc-3d01984bd651', embedding=None, metadata={'page_label': '15', 'file_name': 'yoloWorld.pdf', 'file_path': '/content/data/yoloWorld.pdf', 'file_type': 'application/pdf', 'file_size': 4685106, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Method Pre-trained Data Samples AP AP rAPcAPf\\nYOLO-World-S O365 0.61M 16.3 9.2 14.1 20.1\\nYOLO-World-S O365+GoldG 1.38M 24.2 16.4 21.7 27.8\\nYOLO-World-S O365+CC3M-245k 0.85M 16.5 10.8 14.8 19.1\\nYOLO-World-S O365+CC3M-520k 1.13M 19.2 10.7 17.4 22.4\\nYOLO-World-S O365+CC3M-750k 1.36M 18.2 11.2 16.0 21.1\\nTable 9. Zero-shot Evaluation on LVIS. We evaluate the performance of pre-training YOLO-World-S with different amounts of data, the\\nimage-text data.\\n15', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert to Index"
      ],
      "metadata": {
        "id": "SnBFEWX5NBuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "7wcurxpiNVpe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents=documents, show_progress=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "74512fdc60f94a2a8f1b4018a8bd25df",
            "e226948ab5644246b34379fd1aa4e071",
            "c0738b4017984b1b8af9aed6b612fbe6",
            "b6070e23d82949e3b49485e9a5e6cdfb",
            "9695143d7f894f74a12d6e79c47fa52f",
            "d90dc2bc2d2046f2b4ab9180a0221073",
            "54b8921726984dde904cba24332c2516",
            "016e8d551146454594ff8c29ebdc2621",
            "63f2742484224a15aa57bfdd4342845f",
            "369c6cedc84f4e4db57e5dd8e7bee9be",
            "27c45ae265ef42e99f7a9757fb8c870a",
            "d595acf9dd5f4edea98d89913b3993d0",
            "f1aaae2abf7543d19f6663a9f718678c",
            "df21ba774b494500a4940b99872f309f",
            "097522dd50b74730a3d430c3cb281142",
            "02ad6c46770b4cf1a8923a5624f86ed7",
            "de46581976734de4958de4685074e40b",
            "4705e75c3c98432d8934dd1e4b4afc97",
            "18e0e1c44f3a4b2eb0b760be26dd4f70",
            "0e43a135f3504eafa137470f521a5861",
            "629b271f0bba477d94c765e5d3b01d91",
            "86c6a6ce975a4c3fbbbbb7f478fb6720"
          ]
        },
        "id": "_iFAbME08rCE",
        "outputId": "af16da60-93f0-45f6-8ab4-427e5b579770"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Parsing nodes:   0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74512fdc60f94a2a8f1b4018a8bd25df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/41 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d595acf9dd5f4edea98d89913b3993d0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOKVHFWYNPtQ",
        "outputId": "913da088-047b-4ea6-c192-6a5139579463"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x7b81c3c67e20>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query Engine"
      ],
      "metadata": {
        "id": "elkMHBE-NsD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "4J80SaazNll9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Response"
      ],
      "metadata": {
        "id": "L5rEvvbnOGB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What is Yolo World\")"
      ],
      "metadata": {
        "id": "mOsSl5_VOAG0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iUPu6tMOVVz",
        "outputId": "cd74e960-ceb7-4f9e-a397-7ef03ad2b615"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Response(response='YOLO-World is an innovative approach that enhances YOLO (You Only Look Once) with open-vocabulary detection capabilities through vision-language modeling and pre-training on large-scale datasets. It introduces a new Re-parameterizable Vision-Language Path Aggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate the interaction between visual and linguistic information. This method excels in detecting a wide range of objects in a zero-shot manner with high efficiency, achieving notable performance on challenging datasets and downstream tasks such as object detection and open-vocabulary instance segmentation.', source_nodes=[NodeWithScore(node=TextNode(id_='c64087e5-342f-449b-bf88-2801b6d3aeaa', embedding=None, metadata={'page_label': '1', 'file_name': 'yoloWorld.pdf', 'file_path': '/content/data/yoloWorld.pdf', 'file_type': 'application/pdf', 'file_size': 4685106, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6d6158cc-3963-405c-a0b5-5cc40de0e6f1', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'yoloWorld.pdf', 'file_path': '/content/data/yoloWorld.pdf', 'file_type': 'application/pdf', 'file_size': 4685106, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, hash='bd4b7079cea816fc76682d7773776e58dd2127e5349d368478b462dcbaca2559'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='87708e5e-4e33-421e-881e-30c49ebd6647', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '15', 'file_name': 'attention.pdf', 'file_path': '/content/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, hash='8f785ac1974932ce435523b340ab0ea1ffad94e12bb0fa758aed7e3d6f38c67f'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='968b146c-7977-4e9a-8406-16c83269ff0d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='2a5d6e456b0e52f3c70c7dab33ca6e3a9cb904f22a9257333a8fdd8d08c43c7e')}, text='YOLO-World: Real-Time Open-Vocabulary Object Detection\\nTianheng Cheng3,2,∗, Lin Song1,∗,✉, Yixiao Ge1,2,†, Wenyu Liu3, Xinggang Wang3,✉, Ying Shan1,2\\n∗equal contribution†project lead✉corresponding author\\n1Tencent AI Lab2ARC Lab, Tencent PCG\\n3School of EIC, Huazhong University of Science & Technology\\nCode & Models: YOLO-World\\nAbstract\\nThe You Only Look Once (YOLO) series of detectors\\nhave established themselves as efficient and practical tools.\\nHowever, their reliance on predefined and trained ob-\\nject categories limits their applicability in open scenar-\\nios. Addressing this limitation, we introduce YOLO-World,\\nan innovative approach that enhances YOLO with open-\\nvocabulary detection capabilities through vision-language\\nmodeling and pre-training on large-scale datasets. Specif-\\nically, we propose a new Re-parameterizable Vision-\\nLanguage Path Aggregation Network (RepVL-PAN) and\\nregion-text contrastive loss to facilitate the interaction be-\\ntween visual and linguistic information. Our method excels\\nin detecting a wide range of objects in a zero-shot man-\\nner with high efficiency. On the challenging LVIS dataset,\\nYOLO-World achieves 35.4 AP with 52.0 FPS on V100,\\nwhich outperforms many state-of-the-art methods in terms\\nof both accuracy and speed. Furthermore, the fine-tuned\\nYOLO-World achieves remarkable performance on several\\ndownstream tasks, including object detection and open-\\nvocabulary instance segmentation.\\n1. Introduction\\nObject detection has been a long-standing and fundamental\\nchallenge in computer vision with numerous applications in\\nimage understanding, robotics, and autonomous vehicles.\\nTremendous works [15, 26, 40, 42] have achieved signif-\\nicant breakthroughs in object detection with the develop-\\nment of deep neural networks. Despite the success of these\\nmethods, they remain limited as they only handle object de-\\ntection with a fixed vocabulary, e.g., 80 categories in the\\nCOCO [25] dataset. Once object categories are defined and\\nlabeled, trained detectors can only detect those specific cat-\\negories, thus limiting the ability and applicability of open\\n20×SpeedupFigure 1. Speed-and-Accuracy Curve. We compare YOLO-\\nWorld with recent open-vocabulary methods in terms of speed and\\naccuracy. All models are evaluated on the LVIS minival and in-\\nference speeds are measured on one NVIDIA V100 w/o TensorRT.\\nThe size of the circle represents the model’s size.\\nscenarios.\\nRecent works [7, 12, 45, 50, 55] have explored the\\nprevalent vision-language models [18, 36] to address open-\\nvocabulary detection [55] through distilling vocabulary\\nknowledge from language encoders, e.g., BERT [5]. How-\\never, these distillation-based methods are much limited due\\nto the scarcity of training data with a limited diversity of\\nvocabulary, e.g., OV-COCO [55] containing 48 base cate-\\ngories. Several methods [23, 29, 53, 54, 56] reformulate ob-\\nject detection training as region-level vision-language pre-\\ntraining and train open-vocabulary object detectors at scale.\\nHowever, those methods still struggle for detection in real-\\nworld scenarios, which suffer from two aspects: (1) heavy\\ncomputation burden and (2) complicated deployment for\\nedge devices. Previous works [23, 29, 53, 54, 56] have\\n1arXiv:2401.17270v2  [cs.CV]  2 Feb 2024', start_char_idx=0, end_char_idx=3264, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8071089622845566), NodeWithScore(node=TextNode(id_='8018eac7-dac2-4193-b0a5-1ab38ba4f931', embedding=None, metadata={'page_label': '5', 'file_name': 'yoloWorld.pdf', 'file_path': '/content/data/yoloWorld.pdf', 'file_type': 'application/pdf', 'file_size': 4685106, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='690b897e-0201-4e2e-a586-8a204e5d6fa4', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '5', 'file_name': 'yoloWorld.pdf', 'file_path': '/content/data/yoloWorld.pdf', 'file_type': 'application/pdf', 'file_size': 4685106, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, hash='bb4ea7a7550a359b62cbff2e491514927f22203ac47f6c65b0ee90928ece8c89'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='00053e0d-29b2-4623-8e92-23ec87fe4095', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '5', 'file_name': 'yoloWorld.pdf', 'file_path': '/content/data/yoloWorld.pdf', 'file_type': 'application/pdf', 'file_size': 4685106, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, hash='384ec01e9388db49bd4f3e612495b59b0b676e44266c114881dfcd9a4cf92244'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='16a62ef5-6939-462f-acd1-e437a55c3c5e', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='7ab29369e11be476fe2326cfdd1973f8dded9fa8263cc1c621074ee7940d28a9')}, text='4. Experiments\\nIn this section, we demonstrate the effectiveness of the\\nproposed YOLO-World by pre-training it on large-scale\\ndatasets and evaluating YOLO-World in a zero-shot manner\\non both LVIS benchmark and COCO benchmark (Sec. 4.2).\\nWe also evaluate the fine-tuning performance of YOLO-\\nWorld on COCO, LVIS for object detection.\\n4.1. Implementation Details\\nThe YOLO-World is developed based on the MMYOLO\\ntoolbox [3] and the MMDetection toolbox [2]. Following\\n[19], we provide three variants of YOLO-World for differ-\\nent latency requirements, e.g., small (S), medium (M), and\\nlarge (L). We adopt the open-source CLIP [36] text encoder\\nwith pre-trained weights to encode the input text. Unless\\nspecified, we measure the inference speeds of all models on\\none NVIDIA V100 GPU without extra acceleration mecha-\\nnisms, e.g., FP16 or TensorRT.\\n4.2. Pre-training\\nExperimental Setup. At the pre-training stage, we adopt\\nthe AdamW optimizer [33] with an initial learning rate\\nof 0.002 and weight decay of 0.05. YOLO-World is pre-\\ntrained for 100 epochs on on 32 NVIDIA V100 GPUs with\\na total batch size of 512. During pre-training, we follow\\nprevious works [19] and adopt color augmentation, random\\naffine, random flip, and mosaic with 4 images for data aug-\\nmentation. The text encoder is frozen during pre-training.\\n5', start_char_idx=3067, end_char_idx=4382, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7954546329276995)], metadata={'c64087e5-342f-449b-bf88-2801b6d3aeaa': {'page_label': '1', 'file_name': 'yoloWorld.pdf', 'file_path': '/content/data/yoloWorld.pdf', 'file_type': 'application/pdf', 'file_size': 4685106, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, '8018eac7-dac2-4193-b0a5-1ab38ba4f931': {'page_label': '5', 'file_name': 'yoloWorld.pdf', 'file_path': '/content/data/yoloWorld.pdf', 'file_type': 'application/pdf', 'file_size': 4685106, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KyYN3kvOLcx",
        "outputId": "09b663cc-3d04-436e-c29b-21589290610d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLO-World is an innovative approach that enhances YOLO (You Only Look Once) with open-vocabulary detection capabilities through vision-language modeling and pre-training on large-scale datasets. It introduces a new Re-parameterizable Vision-Language Path Aggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate the interaction between visual and linguistic information. This method excels in detecting a wide range of objects in a zero-shot manner with high efficiency, achieving notable performance on challenging datasets and downstream tasks such as object detection and open-vocabulary instance segmentation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retriever Engine"
      ],
      "metadata": {
        "id": "Q8CLSvWVO4mV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This ingestion pipeline typically consists of three main stages:\n",
        "\n",
        "1. Load the data\n",
        "\n",
        "2. Transform the data\n",
        "\n",
        "3. Index and store the data"
      ],
      "metadata": {
        "id": "3UJBrJKSPWLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load data"
      ],
      "metadata": {
        "id": "yR1R3NIhW9ZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "documents = SimpleDirectoryReader('/content/data').load_data()"
      ],
      "metadata": {
        "id": "PROSa7YCO74z"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Transform data."
      ],
      "metadata": {
        "id": "FSfSOeqBW_bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX6ZIZmJW6kh",
        "outputId": "71c27d56-0163-4e76-b4a7-256faa80a89c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.7-py3-none-any.whl (815 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/815.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.6/815.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.9/815.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.20 (from langchain)\n",
            "  Downloading langchain_community-0.0.20-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.22 (from langchain)\n",
            "  Downloading langchain_core-0.1.23-py3-none-any.whl (241 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1,>=0.0.83 (from langchain)\n",
            "  Downloading langsmith-0.0.92-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.22->langchain) (3.7.1)\n",
            "Collecting langsmith<0.1,>=0.0.83 (from langchain)\n",
            "  Downloading langsmith-0.0.87-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.22->langchain) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Installing collected packages: jsonpointer, jsonpatch, langsmith, langchain-core, langchain-community, langchain\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.7 langchain-community-0.0.20 langchain-core-0.1.23 langsmith-0.0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")"
      ],
      "metadata": {
        "id": "_poG_FhXXORh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.text_splitter = text_splitter"
      ],
      "metadata": {
        "id": "P9aEhqvNX374"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Index data"
      ],
      "metadata": {
        "id": "eiu4woFiYIWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents=documents, show_progress=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "40f89c9af6b742388d3019874dffef66",
            "d524358fb79e40eabc18d9a83296118f",
            "2fdab3a9bbf44d07aefad61692d0afee",
            "9695e1b8bce94bb5ba9c7e6f7b28d296",
            "017c755f60b14f599b14329c6cdc6830",
            "ab2c308c89d8439098e85d582161d0b8",
            "1f70ac3dab614b7ab4c1ea8ab7029731",
            "4290e8c38ccf481095140b5e1ef33a34",
            "06f3b6f0b0ce44dfaa5fa70c94d80d40",
            "486fe2aa0d794bb4864679b5dcbed022",
            "0337c31961cc4b4394d2008268d503f2",
            "74604c638ef34afbbbf7e0122be5a338",
            "d4e6b6a8cf14425aaba4ff13d8cae030",
            "5752fe7316934b6c9cf7d0737180b0cb",
            "3567d45ff9db4cb08f7f85c48d2b58f4",
            "cf15e7ae32404620aa4d984b7f9690ab",
            "5dbe4b7029d54debbd1950dc81625895",
            "01375b311eb34a12a0f95fde58784aac",
            "e752c88545c449298f67e5181fc0d47a",
            "d8afdccab27f46e0a9dc7b1ac9f3e888",
            "bd836e12840a49e4bc73d66f76a6336d",
            "31c30ea3edb543f995565d95834bab0b"
          ]
        },
        "id": "b8OxbTNvX9Au",
        "outputId": "34c10e3a-9a64-41a2-fe59-81fdef5c69b0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Parsing nodes:   0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40f89c9af6b742388d3019874dffef66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/41 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74604c638ef34afbbbf7e0122be5a338"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Retriver"
      ],
      "metadata": {
        "id": "SgMcwpllaKHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core import get_response_synthesizer"
      ],
      "metadata": {
        "id": "HSuE72l9aF1B"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = VectorIndexRetriever(\n",
        "    index = index,\n",
        "    similarity_top_k= 3\n",
        ")"
      ],
      "metadata": {
        "id": "oTDT-JCIam_U"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_synthesizer = get_response_synthesizer()"
      ],
      "metadata": {
        "id": "gmVd6cgTavXC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Querying"
      ],
      "metadata": {
        "id": "cfazYFkxa3Lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor"
      ],
      "metadata": {
        "id": "nU0x07O5a2Y-"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = RetrieverQueryEngine(\n",
        "    retriever = retriever,\n",
        "    response_synthesizer = response_synthesizer,\n",
        "    node_postprocessors = [SimilarityPostprocessor(\n",
        "        similarity_cutoff = 0.8  # setting a threshold on the similarity score\n",
        "    )]\n",
        ")"
      ],
      "metadata": {
        "id": "myy2nH5QbKXz"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query"
      ],
      "metadata": {
        "id": "JymYy9--b8EN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What is the mechanism of attention is all you need?\")"
      ],
      "metadata": {
        "id": "CyScTKP8buic"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KNqivm9cE5h",
        "outputId": "af1b9116-5c3f-4b81-bbf1-f8ae4f2dcca4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Response(response='The mechanism of attention in \"Attention Is All You Need\" is based solely on attention mechanisms, eliminating the need for complex recurrent or convolutional neural networks that typically include an encoder and a decoder. The proposed network architecture, the Transformer, relies on attention mechanisms to establish connections between the encoder and decoder, without the use of recurrence and convolutions. This approach has been shown to be superior in quality, more parallelizable, and requires significantly less training time compared to traditional models.', source_nodes=[NodeWithScore(node=TextNode(id_='1991489c-5205-4011-92aa-ff6c2ea692b8', embedding=None, metadata={'page_label': '1', 'file_name': 'attention.pdf', 'file_path': '/content/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='691e137a-60f7-4233-b705-ab343b769603', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'attention.pdf', 'file_path': '/content/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, hash='7fd33cf5ece9f263c81698d21164ab8d56316781b6fc19d53e64d4b182bd9f4f'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='7be7b754-3e72-45a5-a4d1-89c449c5b804', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='495e14b856e9bf7cce65d459aa659c456c762fd0cbfe63c539eff1a72566bc1a')}, text='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023', start_char_idx=0, end_char_idx=2853, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8365392810090482), NodeWithScore(node=TextNode(id_='58b8d378-e6fa-4fb4-901d-6a2fe8ab6f70', embedding=None, metadata={'page_label': '13', 'file_name': 'attention.pdf', 'file_path': '/content/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='35ea1385-013a-427b-b1f2-cbcc54e83be7', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '13', 'file_name': 'attention.pdf', 'file_path': '/content/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, hash='7d069ef6aaee562bf32a79fff30e1a469504781752f872f798ac58fdffe689d6'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='0bcb901c-724b-412e-869f-caaf7209c6c1', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '12', 'file_name': 'attention.pdf', 'file_path': '/content/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, hash='800f353393a47f44c53b4173f76f76cd8463f801691b3fa71bc1f7080e2eacac'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='67e36c70-cb85-430e-8cac-952a2bebab7a', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='baa44ac133e6c69f689b8cbf9584b47e53004717783ac4d9feab44345a9d41b2')}, text='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13', start_char_idx=0, end_char_idx=812, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8030697609849488)], metadata={'1991489c-5205-4011-92aa-ff6c2ea692b8': {'page_label': '1', 'file_name': 'attention.pdf', 'file_path': '/content/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}, '58b8d378-e6fa-4fb4-901d-6a2fe8ab6f70': {'page_label': '13', 'file_name': 'attention.pdf', 'file_path': '/content/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-02-17', 'last_modified_date': '2024-02-17', 'last_accessed_date': '2024-02-17'}})"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xoC8KWDcFtk",
        "outputId": "57db988a-d9b1-46cc-e206-ff409b2c87f7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mechanism of attention in \"Attention Is All You Need\" is based solely on attention mechanisms, eliminating the need for complex recurrent or convolutional neural networks that typically include an encoder and a decoder. The proposed network architecture, the Transformer, relies on attention mechanisms to establish connections between the encoder and decoder, without the use of recurrence and convolutions. This approach has been shown to be superior in quality, more parallelizable, and requires significantly less training time compared to traditional models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "pprint(response.response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8SuzFlkcHUs",
        "outputId": "0b26d38b-2b9a-4de7-bff8-f9f9a83030d6"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('The mechanism of attention in \"Attention Is All You Need\" is based solely on '\n",
            " 'attention mechanisms, eliminating the need for complex recurrent or '\n",
            " 'convolutional neural networks that typically include an encoder and a '\n",
            " 'decoder. The proposed network architecture, the Transformer, relies on '\n",
            " 'attention mechanisms to establish connections between the encoder and '\n",
            " 'decoder, without the use of recurrence and convolutions. This approach has '\n",
            " 'been shown to be superior in quality, more parallelizable, and requires '\n",
            " 'significantly less training time compared to traditional models.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.response.pprint_utils import pprint_response\n",
        "\n",
        "pprint_response(response,show_source=True)\n",
        "print(\"\\n\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VwiPfz-cJkY",
        "outputId": "a864df5c-3f56-4f90-f657-0d9834ef814b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Response: The mechanism of attention in \"Attention Is All You\n",
            "Need\" is based solely on attention mechanisms, eliminating the need\n",
            "for complex recurrent or convolutional neural networks that typically\n",
            "include an encoder and a decoder. The proposed network architecture,\n",
            "the Transformer, relies on attention mechanisms to establish\n",
            "connections between the encoder and decoder, without the use of\n",
            "recurrence and convolutions. This approach has been shown to be\n",
            "superior in quality, more parallelizable, and requires significantly\n",
            "less training time compared to traditional models.\n",
            "______________________________________________________________________\n",
            "Source Node 1/2\n",
            "Node ID: 1991489c-5205-4011-92aa-ff6c2ea692b8\n",
            "Similarity: 0.8365392810090482\n",
            "Text: Provided proper attribution is provided, Google hereby grants\n",
            "permission to reproduce the tables and figures in this paper solely\n",
            "for use in journalistic or scholarly works. Attention Is All You Need\n",
            "Ashish Vaswani∗ Google Brain avaswani@google.comNoam Shazeer∗ Google\n",
            "Brain noam@google.comNiki Parmar∗ Google Research\n",
            "nikip@google.comJakob Uszkor...\n",
            "______________________________________________________________________\n",
            "Source Node 2/2\n",
            "Node ID: 58b8d378-e6fa-4fb4-901d-6a2fe8ab6f70\n",
            "Similarity: 0.8030697609849488\n",
            "Text: Attention Visualizations Input-Input Layer5 It is in this spirit\n",
            "that a majority of American governments have passed new laws since\n",
            "2009 making the registration or voting process more difficult . <EOS>\n",
            "<pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a\n",
            "majority of American governments have passed new laws since 2009\n",
            "making the regis...\n",
            "\n",
            "\n",
            "The mechanism of attention in \"Attention Is All You Need\" is based solely on attention mechanisms, eliminating the need for complex recurrent or convolutional neural networks that typically include an encoder and a decoder. The proposed network architecture, the Transformer, relies on attention mechanisms to establish connections between the encoder and decoder, without the use of recurrence and convolutions. This approach has been shown to be superior in quality, more parallelizable, and requires significantly less training time compared to traditional models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Storing and Loading index."
      ],
      "metadata": {
        "id": "hZ_C20OcekEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load data"
      ],
      "metadata": {
        "id": "-vnscbe5f9Oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader"
      ],
      "metadata": {
        "id": "lXOI3Gpmf80g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader('/content/data').load_data()"
      ],
      "metadata": {
        "id": "uP9FIf5-gC66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Chunk data"
      ],
      "metadata": {
        "id": "qiVuzLmXgMTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=100,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")"
      ],
      "metadata": {
        "id": "j493E91WgOjf"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.text_splitter = text_splitter"
      ],
      "metadata": {
        "id": "txjcBulcgUlX"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Index the data"
      ],
      "metadata": {
        "id": "azKQJaScgVWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage"
      ],
      "metadata": {
        "id": "UmiKc2iFeFUs"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the storage already exists\n",
        "\n",
        "PERSIST_DIR = \"storage\"\n",
        "\n",
        "if not os.path.exists(PERSIST_DIR):\n",
        "  index = VectorStoreIndex.from_documents(documents=documents, show_progress=True)\n",
        "\n",
        "  # Store it for later\n",
        "  index.storage_context.persist(persist_dir = PERSIST_DIR)\n",
        "\n",
        "else:\n",
        "  # Load the existing index\n",
        "  storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "  index = load_index_from_storage(storage_context=storage_context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "3fb7f780124a4d46aee548ea5b2391ec",
            "2d3e59e1175743a8a1910bf11eb24a94",
            "e705e30ae822447ea5ab711d0a45255c",
            "93fc3e587ef04fa8b1107b61a19504e5",
            "f16fa3e21bbf427eb27d1c848cb8b541",
            "d2eaf873a7e94a328584a1d7372f0100",
            "1db164bfadd0469ebf9feb3455b5a21a",
            "b92cdfcae0f24b2a9bb7a7c7cb22addc",
            "8d878849eba04fa28e0b41713c26f8cc",
            "fd087fd5e6a94e4ca5b8faff34d70526",
            "9db13cf8c0224f2daf6b01cbdac52a77",
            "d071af179c79418ab7b0893c37f3c0e3",
            "9de69c4486e4428690b2b53a909b60f2",
            "852f30ec1cf54001bb1ad5b0bb8bd198",
            "fa8a50460e01428a9c287100870a3dea",
            "a59e7a38c60d49c8a5fdff7078c97b67",
            "53e0f3f9040d440790810025ee4c19b8",
            "669096d91b3c4a4e912504aa254fe2e3",
            "caeb244cb2524138a1b5375904d9d703",
            "0d3612eced9d4dc5bf3a87232b7a7d49",
            "ef2f55172488444ebbe9fa8d3bf26ac0",
            "2357671a72ff420e9bd4e5d0b2b91e2e"
          ]
        },
        "id": "nahcj6q5ghER",
        "outputId": "45abf79b-5c00-4a71-aefc-d051b83a1ff9"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Parsing nodes:   0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3fb7f780124a4d46aee548ea5b2391ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/41 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d071af179c79418ab7b0893c37f3c0e3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Retriever"
      ],
      "metadata": {
        "id": "1cywwPgJh2TH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import get_response_synthesizer\n",
        "from llama_index.core.retrievers import VectorIndexRetriever"
      ],
      "metadata": {
        "id": "eLJMeCqmhvRL"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure retriever\n",
        "retriver = VectorIndexRetriever(\n",
        "    index = index,\n",
        "    similarity_top_k=3\n",
        ")"
      ],
      "metadata": {
        "id": "c39VtLd9iMd3"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure response synthesizer\n",
        "response_synthesizer = get_response_synthesizer()"
      ],
      "metadata": {
        "id": "g4NitXeGiVpi"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Query"
      ],
      "metadata": {
        "id": "7zcHt0UJioGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor"
      ],
      "metadata": {
        "id": "jyQwG_17ibNb"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = RetrieverQueryEngine(\n",
        "    retriever=retriever,\n",
        "    response_synthesizer=response_synthesizer,\n",
        "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.8)]\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "PA8jBBI_i7vH"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What is Yolo World\")"
      ],
      "metadata": {
        "id": "1LvNr6Ucj5q5"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.response.pprint_utils import pprint_response\n",
        "\n",
        "pprint_response(response=response, show_source=True)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81YdCvV4kMhp",
        "outputId": "ef5204af-41f7-408f-c6fb-59b0090dd261"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Response: YOLO-World is an innovative approach that enhances\n",
            "YOLO (You Only Look Once) with open-vocabulary detection capabilities\n",
            "through vision-language modeling and pre-training on large-scale\n",
            "datasets. It introduces a new Re-parameterizable Vision-Language Path\n",
            "Aggregation Network (RepVL-PAN) and region-text contrastive loss to\n",
            "facilitate the interaction between visual and linguistic information.\n",
            "This method excels in detecting a wide range of objects in a zero-shot\n",
            "manner with high efficiency.\n",
            "______________________________________________________________________\n",
            "Source Node 1/1\n",
            "Node ID: c561a577-43fb-467e-be74-6775ada88d32\n",
            "Similarity: 0.8073644574803355\n",
            "Text: YOLO-World: Real-Time Open-Vocabulary Object Detection Tianheng\n",
            "Cheng3,2,∗, Lin Song1,∗,✉, Yixiao Ge1,2,†, Wenyu Liu3, Xinggang\n",
            "Wang3,✉, Ying Shan1,2 ∗equal contribution†project lead✉corresponding\n",
            "author 1Tencent AI Lab2ARC Lab, Tencent PCG 3School of EIC, Huazhong\n",
            "University of Science & Technology Code & Models: YOLO-World Abstract\n",
            "The You Onl...\n",
            "\n",
            "\n",
            "YOLO-World is an innovative approach that enhances YOLO (You Only Look Once) with open-vocabulary detection capabilities through vision-language modeling and pre-training on large-scale datasets. It introduces a new Re-parameterizable Vision-Language Path Aggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate the interaction between visual and linguistic information. This method excels in detecting a wide range of objects in a zero-shot manner with high efficiency.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chroma."
      ],
      "metadata": {
        "id": "Hws_gIEMoV5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load Data"
      ],
      "metadata": {
        "id": "2saH35Ego8gP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader"
      ],
      "metadata": {
        "id": "VrBbkVcPkbN0"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader('/content/data').load_data()"
      ],
      "metadata": {
        "id": "FE895LajolEc"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Chunk Data"
      ],
      "metadata": {
        "id": "HtGPqLWfo_dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=100,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")"
      ],
      "metadata": {
        "id": "g3SOsB4noq_u"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.text_splitter = text_splitter"
      ],
      "metadata": {
        "id": "FOSDvcZmo6kE"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. index Data"
      ],
      "metadata": {
        "id": "ADecQ0h9p-MK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. Store Index"
      ],
      "metadata": {
        "id": "CJYcXTgEtPxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install llama-index-vector-stores-chroma"
      ],
      "metadata": {
        "id": "wCnDacpPva17"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from llama_index.core import VectorStoreIndex, StorageContext\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "# from llama_index.vector_stores.chroma.base import ChromaVectorStore"
      ],
      "metadata": {
        "id": "dzYV5Fa-o7JA"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 Steps are common for storing and loading.\n",
        "\n",
        "1. Initialize client.\n",
        "2. Get or Create client.\n",
        "3. Assign chroma as the vector_store to the context.\n",
        "\n",
        "But,\n",
        "\n",
        "To Store:\n",
        "*   VectorStoreIndex.from_documents()\n",
        "\n",
        "To Load:\n",
        "*   VectorStoreIndex.from_vector_store()\n",
        "\n"
      ],
      "metadata": {
        "id": "87w7Qw0Bx4Uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_path = \"chroma_db\"\n",
        "\n",
        "# Store Index\n",
        "if not os.path.exists(_path):\n",
        "  # Initialize client, setting path to save data\n",
        "  db = chromadb.PersistentClient(path = _path)\n",
        "\n",
        "  # create collection\n",
        "  chroma_collection = db.get_or_create_collection(\"Tutorial_basics\")\n",
        "\n",
        "  # Assign chroma as the vector_store to the context\n",
        "  vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "  storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "  # Create index\n",
        "  index = VectorStoreIndex.from_documents(\n",
        "      documents=documents,\n",
        "      storage_context=storage_context,\n",
        "      show_progress=True\n",
        "  )\n",
        "\n",
        "else:\n",
        "  # Load db\n",
        "  # Initialize Client\n",
        "  db = chromadb.PersistentClient(path = _path)\n",
        "\n",
        "  # get collection\n",
        "  chroma_collection = db.get_or_create_collection(\"Tutorial_basics\")\n",
        "\n",
        "  # Assign chroma as the vector_store to the context\n",
        "  vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "  storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "  # Load your index from stored vectors\n",
        "  index = VectorStoreIndex.from_vector_store(\n",
        "      vector_store=vector_store,\n",
        "      storage_context=storage_context,\n",
        "  )"
      ],
      "metadata": {
        "id": "2mJeZvohtp1P"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query without retrievers"
      ],
      "metadata": {
        "id": "8s_jlpiq2FR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a query engine\n",
        "query_engine = index.as_query_engine(show_source = True)\n",
        "\n",
        "response = query_engine.query(\"Explain Yolo World\")"
      ],
      "metadata": {
        "id": "l5fUbfZm1b1H"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.response.pprint_utils import pprint_response\n",
        "\n",
        "pprint_response(response=response,\n",
        "                show_source=True)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3eJaxYj3prf",
        "outputId": "4fdedf5a-c2ca-48ac-e3ea-7e5a3374b066"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Response: YOLO-World is an innovative approach that enhances the\n",
            "YOLO object detection system by incorporating open-vocabulary\n",
            "detection capabilities through vision-language modeling and pre-\n",
            "training on large-scale datasets. It introduces a new network called\n",
            "RepVL-PAN and a region-text contrastive loss to improve the\n",
            "interaction between visual and linguistic information. This method\n",
            "excels in detecting a wide range of objects in a zero-shot manner with\n",
            "high efficiency, achieving impressive performance on challenging\n",
            "datasets like LVIS. Additionally, YOLO-World can be fine-tuned for\n",
            "various downstream tasks such as object detection and open-vocabulary\n",
            "instance segmentation.\n",
            "______________________________________________________________________\n",
            "Source Node 1/2\n",
            "Node ID: 7082c9a7-775b-457d-92a7-5f188c4e62d7\n",
            "Similarity: 0.686536938601568\n",
            "Text: YOLO-World: Real-Time Open-Vocabulary Object Detection Tianheng\n",
            "Cheng3,2,∗, Lin Song1,∗,✉, Yixiao Ge1,2,†, Wenyu Liu3, Xinggang\n",
            "Wang3,✉, Ying Shan1,2 ∗equal contribution†project lead✉corresponding\n",
            "author 1Tencent AI Lab2ARC Lab, Tencent PCG 3School of EIC, Huazhong\n",
            "University of Science & Technology Code & Models: YOLO-World Abstract\n",
            "The You Onl...\n",
            "______________________________________________________________________\n",
            "Source Node 2/2\n",
            "Node ID: f62dd16a-42a7-4420-babc-fff1b573f500\n",
            "Similarity: 0.6717468781310421\n",
            "Text: 4. Experiments In this section, we demonstrate the effectiveness\n",
            "of the proposed YOLO-World by pre-training it on large-scale datasets\n",
            "and evaluating YOLO-World in a zero-shot manner on both LVIS benchmark\n",
            "and COCO benchmark (Sec. 4.2). We also evaluate the fine-tuning\n",
            "performance of YOLO- World on COCO, LVIS for object detection. 4.1.\n",
            "Implement...\n",
            "\n",
            "\n",
            "YOLO-World is an innovative approach that enhances the YOLO object detection system by incorporating open-vocabulary detection capabilities through vision-language modeling and pre-training on large-scale datasets. It introduces a new network called RepVL-PAN and a region-text contrastive loss to improve the interaction between visual and linguistic information. This method excels in detecting a wide range of objects in a zero-shot manner with high efficiency, achieving impressive performance on challenging datasets like LVIS. Additionally, YOLO-World can be fine-tuned for various downstream tasks such as object detection and open-vocabulary instance segmentation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geRaTEub2T2L",
        "outputId": "368f8b19-8c04-4a4b-9075-829440c6a511"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLO-World is an innovative approach that enhances the YOLO object detection system by incorporating open-vocabulary detection capabilities through vision-language modeling and pre-training on large-scale datasets. It introduces a new network called RepVL-PAN and a region-text contrastive loss to improve the interaction between visual and linguistic information. This method excels in detecting a wide range of objects in a zero-shot manner with high efficiency, achieving impressive performance on challenging datasets like LVIS. Additionally, YOLO-World can be fine-tuned for various downstream tasks such as object detection and open-vocabulary instance segmentation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Retriever"
      ],
      "metadata": {
        "id": "s2LB1nwrzVdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import get_response_synthesizer\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor"
      ],
      "metadata": {
        "id": "VDSxDeqizKxc"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confiure retriever\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=3\n",
        ")"
      ],
      "metadata": {
        "id": "ftN4fBGqzuOE"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure respose synthesizer\n",
        "response_synthesizer=get_response_synthesizer()"
      ],
      "metadata": {
        "id": "43-5oo6T0ED1"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assemble query engine\n",
        "query_engine = RetrieverQueryEngine(\n",
        "    retriever=retriever,\n",
        "    response_synthesizer=response_synthesizer,\n",
        "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.4)]\n",
        ")"
      ],
      "metadata": {
        "id": "PvA8ZvoC0L8F"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Query"
      ],
      "metadata": {
        "id": "SWcMRPZr0dNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What is Yolo World\")"
      ],
      "metadata": {
        "id": "K3Ztp6hy0bDy"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# response"
      ],
      "metadata": {
        "id": "i0alVtaP1nKM"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.response.pprint_utils import pprint_response\n",
        "\n",
        "pprint_response(response=response,\n",
        "                show_source=True)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sc1rt06e0mAK",
        "outputId": "73537e29-dabe-4d80-d0da-a9cfeb9809a1"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Response: YOLO-World is an innovative approach that enhances\n",
            "YOLO (You Only Look Once) with open-vocabulary detection capabilities\n",
            "through vision-language modeling and pre-training on large-scale\n",
            "datasets. It introduces a new Re-parameterizable Vision-Language Path\n",
            "Aggregation Network (RepVL-PAN) and region-text contrastive loss to\n",
            "facilitate the interaction between visual and linguistic information.\n",
            "YOLO-World excels in detecting a wide range of objects in a zero-shot\n",
            "manner with high efficiency, achieving significant performance on\n",
            "challenging datasets like LVIS.\n",
            "______________________________________________________________________\n",
            "Source Node 1/3\n",
            "Node ID: 7082c9a7-775b-457d-92a7-5f188c4e62d7\n",
            "Similarity: 0.6802662150694498\n",
            "Text: YOLO-World: Real-Time Open-Vocabulary Object Detection Tianheng\n",
            "Cheng3,2,∗, Lin Song1,∗,✉, Yixiao Ge1,2,†, Wenyu Liu3, Xinggang\n",
            "Wang3,✉, Ying Shan1,2 ∗equal contribution†project lead✉corresponding\n",
            "author 1Tencent AI Lab2ARC Lab, Tencent PCG 3School of EIC, Huazhong\n",
            "University of Science & Technology Code & Models: YOLO-World Abstract\n",
            "The You Onl...\n",
            "______________________________________________________________________\n",
            "Source Node 2/3\n",
            "Node ID: f62dd16a-42a7-4420-babc-fff1b573f500\n",
            "Similarity: 0.6642539836031883\n",
            "Text: 4. Experiments In this section, we demonstrate the effectiveness\n",
            "of the proposed YOLO-World by pre-training it on large-scale datasets\n",
            "and evaluating YOLO-World in a zero-shot manner on both LVIS benchmark\n",
            "and COCO benchmark (Sec. 4.2). We also evaluate the fine-tuning\n",
            "performance of YOLO- World on COCO, LVIS for object detection. 4.1.\n",
            "Implement...\n",
            "______________________________________________________________________\n",
            "Source Node 3/3\n",
            "Node ID: 73063120-9b91-4d03-bffa-6a4fa2f93ce9\n",
            "Similarity: 0.6552768345752994\n",
            "Text: demonstrated the promising performance of pre-training large\n",
            "detectors while pre-training small detectors to en- dow them with open\n",
            "recognition capabilities remains un- explored. In this paper, we\n",
            "present YOLO-World, aiming for high-efficiency open-vocabulary object\n",
            "detection, and ex- plore large-scale pre-training schemes to boost the\n",
            "tradi- ti...\n",
            "\n",
            "\n",
            "YOLO-World is an innovative approach that enhances YOLO (You Only Look Once) with open-vocabulary detection capabilities through vision-language modeling and pre-training on large-scale datasets. It introduces a new Re-parameterizable Vision-Language Path Aggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate the interaction between visual and linguistic information. YOLO-World excels in detecting a wide range of objects in a zero-shot manner with high efficiency, achieving significant performance on challenging datasets like LVIS.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XEd1L-3O04kR"
      },
      "execution_count": 89,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1tp8iLtNlIh",
        "outputId": "10185616-6f77-4db4-8256-21c52a278527"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.4/124.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet langchain langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n",
        "os.environ[\"OPENAI_API_VERSION\"] = \"2024-05-01-preview\"\n",
        "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"This form the Azure portal - model deployment\"\n",
        "# os.environ[\"AZURE_OPENAI_API_KEY\"] = 'This form the Azure portal - model deployment'\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://mobility-sage-openai-services.openai.azure.com/\"\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = '9bbf92a17be24c66881eaa5a1c6aa77d'"
      ],
      "metadata": {
        "id": "7ENeTEuXN-pu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "6JF1hkhqPqbJ"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = AzureChatOpenAI(\n",
        "            temperature=1,\n",
        "            azure_deployment=\"gpt-4o\",\n",
        "        )"
      ],
      "metadata": {
        "id": "s0zqB-jsPsRa"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Router Chain\n",
        "\n",
        "A good way to imagine this is if you have multiple subchains, each of which is specialized for a particular type of input, you could have a router chain, which first decides which subchain to pass it to and then passes it to that chain."
      ],
      "metadata": {
        "id": "RxgMV0r_WiS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In simple terms if we want out prompt to wear multiple hats or assume differnt roles based on the user input, we can use the router chain."
      ],
      "metadata": {
        "id": "7WocXSZX6Yvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create multiple prompts\n",
        "\n",
        "physics_template = \"\"\"\n",
        "  You are a very smart physics professor. \\\n",
        "  You are great at answering questions about physics in a concise\\\n",
        "  and easy to understand manner. \\\n",
        "  When you don't know the answer to a question you admit\\\n",
        "  that you don't know.\n",
        "\n",
        "  Here is a question:\n",
        "  {input}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "math_template = \"\"\"\n",
        "  You are a very good mathematician. \\\n",
        "  You are great at answering math questions. \\\n",
        "  You are so good because you are able to break down \\\n",
        "  hard problems into their component parts,\n",
        "  answer the component parts, and then put them together\\\n",
        "  to answer the broader question.\n",
        "\n",
        "  Here is a question:\n",
        "  {input}\n",
        "\"\"\"\n",
        "\n",
        "history_template = \"\"\"\n",
        "  You are a very good historian. \\\n",
        "  You have an excellent knowledge of and understanding of people,\\\n",
        "  events and contexts from a range of historical periods. \\\n",
        "  You have the ability to think, reflect, debate, discuss and \\\n",
        "  evaluate the past. You have a respect for historical evidence\\\n",
        "  and the ability to make use of it to support your explanations \\\n",
        "  and judgements.\n",
        "\n",
        "  Here is a question:\n",
        "  {input}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "computerscience_template = \"\"\"\n",
        "  You are a successful computer scientist.\\\n",
        "  You have a passion for creativity, collaboration,\\\n",
        "  forward-thinking, confidence, strong problem-solving capabilities,\\\n",
        "  understanding of theories and algorithms, and excellent communication \\\n",
        "  skills. You are great at answering coding questions. \\\n",
        "  You are so good because you know how to solve a problem by \\\n",
        "  describing the solution in imperative steps \\\n",
        "  that a machine can easily interpret and you know how to \\\n",
        "  choose a solution that has a good balance between \\\n",
        "  time complexity and space complexity.\n",
        "\n",
        "  Here is a question:\n",
        "  {input}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "n7P1_OZM6hwc"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a prompt descriptor\n",
        "\n",
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"physics\",\n",
        "        \"description\": \"Good for answering questions about physics\",\n",
        "        \"prompt_template\": physics_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"math\",\n",
        "        \"description\": \"Good for answering math questions\",\n",
        "        \"prompt_template\": math_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"History\",\n",
        "        \"description\": \"Good for answering history questions\",\n",
        "        \"prompt_template\": history_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"computer science\",\n",
        "        \"description\": \"Good for answering computer science questions\",\n",
        "        \"prompt_template\": computerscience_template\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "25VDGNgq6pUC"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the Destination chains.\n",
        "\n",
        "## These are the chains that will be called by the router chain."
      ],
      "metadata": {
        "id": "4KYaunQ_7hAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "destination_chain = {}\n",
        "\n",
        "for p_info in prompt_infos:\n",
        "  name = p_info['name']\n",
        "  prompt_template = p_info['prompt_template']\n",
        "\n",
        "  prompt = ChatPromptTemplate.from_template(\n",
        "      template=prompt_template\n",
        "  )\n",
        "\n",
        "  chain = LLMChain(\n",
        "      llm = llm,\n",
        "      prompt = prompt\n",
        "  )\n",
        "\n",
        "  destination_chain[name] = chain\n",
        "\n"
      ],
      "metadata": {
        "id": "S6hkrrCD7Zn8"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "destination_chain.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQYbBIA_EA8w",
        "outputId": "7bfa5b3b-afba-4a41-84cf-5b855a41440d"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['physics', 'math', 'History', 'computer science'])"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "default_prompt = ChatPromptTemplate.from_template(\n",
        "    \"{input}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "4aBBq5tkEIiC"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "default_chain = LLMChain(\n",
        "    llm = llm,\n",
        "    prompt = default_prompt,\n",
        "    output_key = \"text\"\n",
        ")"
      ],
      "metadata": {
        "id": "k6x8blojB6B9"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "default_prompt = PromptTemplate.from_template(\"{blackhole}\")\n",
        "default_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKVSPzqe8gO0",
        "outputId": "949c55eb-6caa-40ac-d4a3-423f06d7c750"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['blackhole'], template='{blackhole}')"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "destinations = [f\"{p['name']} : {p['description']}\" for p in prompt_infos]\n",
        "destinations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJQDF5vw9kTF",
        "outputId": "2ea757f6-ddfe-4184-dbf8-2b4f0f676d7a"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['physics : Good for answering questions about physics',\n",
              " 'math : Good for answering math questions',\n",
              " 'History : Good for answering history questions',\n",
              " 'computer science : Good for answering computer science questions']"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "destinations_str = '\\n'.join(destinations)\n",
        "destinations_str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "TgakX4wx-MQY",
        "outputId": "b12e8bc5-11a1-4250-ce61-b9ef583980f8"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'physics : Good for answering questions about physics\\nmath : Good for answering math questions\\nHistory : Good for answering history questions\\ncomputer science : Good for answering computer science questions'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Router Chain."
      ],
      "metadata": {
        "id": "X_JQaqLe9K5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
        "# from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE"
      ],
      "metadata": {
        "id": "hVJfiJZ89BKX"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Custom Multi Prompt Router Template"
      ],
      "metadata": {
        "id": "fEicUiPPMVgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"\n",
        "  Given a raw text input to a \\\n",
        "  language model select the model prompt best suited for the input. \\\n",
        "  You will be given the names of the available prompts and a \\\n",
        "  description of what the prompt is best suited for. \\\n",
        "  You may also revise the original input if you think that revising\\\n",
        "  it will ultimately lead to a better response from the language model.\n",
        "\n",
        "  << FORMATTING >>\n",
        "  Return a markdown code snippet with a JSON object formatted to look like:\n",
        "  ```json\n",
        "  {{{{\n",
        "      \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
        "      \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "  }}}}\n",
        "  ```\n",
        "\n",
        "  REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "  names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
        "  well suited for any of the candidate prompts.\n",
        "  REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "  if you don't think any modifications are needed.\n",
        "\n",
        "  << CANDIDATE PROMPTS >>\n",
        "  {destinations}\n",
        "\n",
        "  << INPUT >>\n",
        "  {{input}}\n",
        "\n",
        "  << OUTPUT (remember to include the ```json)>>\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "F26lSp6uMGPs"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations = destinations_str\n",
        ")\n",
        "\n",
        "print(router_template) # built in  MULTI_PROMPT_ROUTER_TEMPLATE from langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFrtXBsR-Twh",
        "outputId": "98e73ccf-84d8-4400-903c-b61e4d38ac59"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Given a raw text input to a   language model select the model prompt best suited for the input.   You will be given the names of the available prompts and a   description of what the prompt is best suited for.   You may also revise the original input if you think that revising  it will ultimately lead to a better response from the language model.\n",
            "\n",
            "  << FORMATTING >>\n",
            "  Return a markdown code snippet with a JSON object formatted to look like:\n",
            "  ```json\n",
            "  {{\n",
            "      \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
            "      \"next_inputs\": string \\ a potentially modified version of the original input\n",
            "  }}\n",
            "  ```\n",
            "\n",
            "  REMEMBER: \"destination\" MUST be one of the candidate prompt   names specified below OR it can be \"DEFAULT\" if the input is not  well suited for any of the candidate prompts.\n",
            "  REMEMBER: \"next_inputs\" can just be the original input   if you don't think any modifications are needed.\n",
            "\n",
            "  << CANDIDATE PROMPTS >>\n",
            "  physics : Good for answering questions about physics\n",
            "math : Good for answering math questions\n",
            "History : Good for answering history questions\n",
            "computer science : Good for answering computer science questions\n",
            "\n",
            "  << INPUT >>\n",
            "  {input}\n",
            "\n",
            "  << OUTPUT (remember to include the ```json)>>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=['input'],\n",
        "    output_parser=RouterOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "Tft9URQU-f4b"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "router_chain = LLMRouterChain.from_llm(\n",
        "    llm=llm,\n",
        "    prompt=router_prompt\n",
        ")"
      ],
      "metadata": {
        "id": "Vq-RKN2pBkfD"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = MultiPromptChain(\n",
        "    router_chain=router_chain,\n",
        "    destination_chains=destination_chain,\n",
        "    default_chain=default_chain,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "RMkV56AHBYJc"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run('Explain Generative AI')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "mUwtDbFCI_4v",
        "outputId": "061545af-aa16-430b-baab-863e8ade0953"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "computer science: {'input': 'Explain Generative AI'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Generative AI refers to a class of artificial intelligence systems designed to create new, original content. Unlike traditional AI, which performs tasks such as classification or prediction based on existing data, generative AI aims to generate new examples from learned patterns. This can include text, images, music, and more. Let's delve into the specifics.\\n\\n### Key Components\\n\\n1. **Models and Architectures**:\\n    - **Generative Adversarial Networks (GANs)**: Comprises two neural networks, a generator, and a discriminator, which work against each other. The generator creates content, and the discriminator evaluates it, pushing the generator to create more realistic content over time.\\n    - **Variational Autoencoders (VAEs)**: Involves an encoder that compresses input data into a latent space and a decoder that reconstructs data from the latent space. Useful for generating new, similar data points.\\n    - **Transformers**: Especially popular in natural language processing, transformers like GPT (Generative Pre-trained Transformer) leverage self-attention mechanisms to generate coherent and contextually relevant text.\\n\\n2. **Training Data**:\\n    - The quality and quantity of training data significantly impact the effectiveness of a generative AI model. Diverse data helps the model generalize better and produce more realistic outputs.\\n\\n### Applications\\n\\n1. **Text Generation**:\\n    - **Chatbots**: Generate human-like responses in customer service or personal assistant applications.\\n    - **Creative Writing**: Assist authors by generating text that can act as inspiration or filler.\\n  \\n2. **Image Generation**:\\n    - **Art Creation**: AI Art applications like DeepArt or DALL-E can create artwork or manipulate existing images.\\n    - **Gaming**: Generate textures, environments, or even entire game levels procedurally.\\n\\n3. **Music Composition**:\\n    - AI can generate new melodies, harmonies, or entire compositions in various genres.\\n\\n4. **Synthetic Data Generation**:\\n    - For privacy-preserving purposes, generative models can create synthetic datasets that retain essential characteristics of the original data but without exposing sensitive information.\\n\\n### Practical Example: Text Generation with GPT-3\\n\\nConsider the task of generating a descriptive paragraph of a scenic landscape:\\n\\n**Input Prompt**:\\n```\\nGenerate a descriptive paragraph about a scenic landscape featuring mountains, lakes, and forests in autumn.\\n```\\n\\n**Generated Output**:\\n```\\nNestled in the heart of the valley, the serene landscape is a masterpiece of nature’s autumn palette. Majestic mountains cloaked in the last green of the season rise proudly, their peaks dusted with the first snow of the coming winter. Below, a crystal-clear lake mirrors the vivid hues of the surrounding trees, with their leaves ranging from fiery red to burnt orange and golden yellow. The air is crisp, filled with the earthy scent of fallen leaves and the distant sound of a babbling brook weaving through the dense forest at the mountain's base.\\n```\\n\\n### Challenges\\n\\n1. **Quality Control**:\\n    - Generative models can sometimes produce biased, incorrect, or nonsensical outputs. Rigorous evaluation and fine-tuning are necessary.\\n  \\n2. **Computational Resources**:\\n    - Training sophisticated models like GANs or large transformers requires significant computational power.\\n\\n3. **Ethics and Misuse**:\\n    - Generative AI can be misused for creating deepfakes, misinformation, or plagiarized content. Establishing ethical guidelines and robust detection mechanisms is crucial.\\n\\n### Conclusion\\n\\nGenerative AI represents a significant leap in AI capabilities, enabling machines to create, rather than simply analyze. As the technology continues to mature, its potential applications will likely expand, offering new tools for creativity, data synthesis, and beyond. Balancing innovation with ethical considerations will be key to harnessing the power of generative AI responsibly.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run('What is a black hole !!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "suz3LrprMs64",
        "outputId": "f522c4f9-1f29-425c-ed8d-4fc73f1760aa"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "physics: {'input': 'What is a black hole?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"A black hole is a region of space where gravity is so strong that nothing, not even light, can escape from it. This extreme gravitational pull is a result of a large amount of mass being concentrated into a very small area. \\n\\nThe boundary surrounding a black hole is called the event horizon. Once something crosses this boundary, it cannot escape the black hole's gravitational pull. The core of a black hole is known as the singularity, where density becomes infinite and the laws of physics as we currently understand them break down.\\n\\nBlack holes can form from the remnants of massive stars that have undergone supernova explosions, and there are also supermassive black holes at the centers of most galaxies, including our own Milky Way.\\n\\nIf you have any more specific questions about black holes, feel free to ask!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cTAEprwcM2VA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
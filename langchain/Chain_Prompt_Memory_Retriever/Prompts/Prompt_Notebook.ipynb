{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RZztgktvYov",
        "outputId": "673f9e0d-e548-4498-fa52-9e6323070b95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.4/124.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet langchain langchain-openai langchain_core"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "x098q3Tewj7G"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Talk to LLM Directly\n"
      ],
      "metadata": {
        "id": "aCLsUQU9wc0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "messages = [\n",
        "    (\"system\", \"You are a helpful assistant that translates English to German.\"),\n",
        "    (\"human\", \"Translate this sentence from English to German. I love programming.\"),\n",
        "]\n",
        "\n",
        "llm.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UF78JGK2wfmc",
        "outputId": "7e2ec233-3b42-4f6d-80da-056528916020"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Ich liebe Programmieren.', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 34, 'total_tokens': 39}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-16b49fa8-96ff-4846-9c86-ada684544090-0', usage_metadata={'input_tokens': 34, 'output_tokens': 5, 'total_tokens': 39})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Prompt Template."
      ],
      "metadata": {
        "id": "uxqC0rl1wSML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 2 ways to have a prompt template.\n",
        "\n",
        "1.   ChatPromptTemplate\n",
        "2.   PromptTemplate\n",
        "\n"
      ],
      "metadata": {
        "id": "RPebjEFT0K2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ChatPromptTemplate"
      ],
      "metadata": {
        "id": "0ARxQXvr1w9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
        "        ),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# chaining\n",
        "chain = prompt | llm\n",
        "\n",
        "# Running the chain\n",
        "chain.invoke(\n",
        "    {\n",
        "        \"input_language\" : \"English\",\n",
        "        \"output_language\" : \"German\",\n",
        "        \"input\" : \"I Love you\",\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubFnw1d6vhKU",
        "outputId": "95b40669-2ecc-4b98-d9cf-6796b3bb47ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Ich liebe dich.', response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 25, 'total_tokens': 29}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-267a93e9-bab9-479f-8e74-59f2f557e137-0', usage_metadata={'input_tokens': 25, 'output_tokens': 4, 'total_tokens': 29})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Template\n",
        "\n",
        "\n",
        "\n",
        "1.   Template.\n",
        "2.   Prompt.\n",
        "3.   Chain.\n",
        "\n"
      ],
      "metadata": {
        "id": "rfNtqgrM10FD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "#### 1] With one input.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vkgQwj0l3n3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "template = '''\n",
        "  Act as a naming consultant for new companies.\n",
        "  What is a good name for a company that makes {products} ?\n",
        "'''\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=['products'],\n",
        "    template = template\n",
        ")\n",
        "\n",
        "prompt.format(products = 'Socks')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9Ihlh2m2yZvA",
        "outputId": "2cc72804-f50f-4e31-bd34-b5d219d5501e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  Act as a naming consultant for new companies.\\n  What is a good name for a company that makes Socks ?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "\n",
        "chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt = prompt,\n",
        "    verbose = True\n",
        ")\n",
        "\n",
        "\n",
        "chain.invoke({\n",
        "    'products' : \"socks\"\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcQqhwUe2gXP",
        "outputId": "ea41fa0e-b373-4f25-9317-dc7738678090"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "  Act as a naming consultant for new companies.\n",
            "  What is a good name for a company that makes socks ?\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'products': 'socks', 'text': '\"CozyFeet Creations\"'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2] With Multiple input."
      ],
      "metadata": {
        "id": "TuSwFhNy3m3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = '''\n",
        "  Tell me a {adjective} joke about {content}.\n",
        "'''\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=['adjective', 'content'],\n",
        "    template = template\n",
        ")\n",
        "\n",
        "prompt.format(adjective = 'Funny', content = 'Cats')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Lg6QbDlL3LTe",
        "outputId": "6ccece2e-6c05-474b-e726-ef9e332c6689"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  Tell me a Funny joke about Cats.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "\n",
        "chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt = prompt,\n",
        "    verbose = True\n",
        ")\n",
        "\n",
        "\n",
        "chain.invoke({\n",
        "    'adjective' : 'Funny',\n",
        "    'content' : 'Cats'\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00sFXFPm6soO",
        "outputId": "688c6b4d-170c-4cca-b153-581f2e77b66b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "  Tell me a Funny joke about Cats.\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'adjective': 'Funny',\n",
              " 'content': 'Cats',\n",
              " 'text': 'Why was the cat sitting on the computer?\\n\\nBecause it wanted to keep an eye on the mouse!'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few Shot Template.\n",
        "\n",
        "\n",
        "\n",
        "1.   Example.\n",
        "2.   Example Template.\n",
        "3.   Example Prompt.\n",
        "\n",
        "4.   Prefix.\n",
        "5.   Suffix.\n",
        "\n",
        "6.   Few Shot Prompt Template.\n",
        "7.   Chain.\n",
        "\n"
      ],
      "metadata": {
        "id": "VjM5fiAdGJCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "7uc5OBR661EZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature = 0)"
      ],
      "metadata": {
        "id": "XQ3gWLyiHTVv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"query\": \"How are you?\",\n",
        "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
        "    }, {\n",
        "        \"query\": \"What time is it?\",\n",
        "        \"answer\": \"It's time to get a watch.\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "rGaNyUXHHftj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_template = \"\"\"\n",
        "  user: {query}\n",
        "  AI: {answer}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7zXCyyyJHpXO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt = PromptTemplate(\n",
        "    input_variables=['query', 'answer'],\n",
        "    template=example_template\n",
        ")"
      ],
      "metadata": {
        "id": "cXDzfigrIFVL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"\"\"\n",
        "  The following are excerpts from conversations with an AI\n",
        "    assistant. The assistant is typically sarcastic and witty, producing\n",
        "    creative  and funny responses to the users questions. Here are some\n",
        "    examples:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nZyhfcZuHv75"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "suffix = \"\"\"\n",
        "  User: {query}\n",
        "  AI:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7j0JajtGH1AR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=['query'],\n",
        "    example_separator='\\n\\n'\n",
        ")"
      ],
      "metadata": {
        "id": "2pQSXWSoH5nv"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the meaning of life\"\n",
        "\n",
        "chain = LLMChain(llm=llm,\n",
        "                 prompt = few_shot_prompt_template,\n",
        "                 verbose = False)\n",
        "\n",
        "chain.invoke({'query' : query})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4V8rQIgIP9s",
        "outputId": "c41e94f5-1c9e-45dd-9d44-371a25d07411"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What is the meaning of life',\n",
              " 'text': \"I'm not sure, but I heard it involves a lot of coffee and memes.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n3B6HnHyJXZk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
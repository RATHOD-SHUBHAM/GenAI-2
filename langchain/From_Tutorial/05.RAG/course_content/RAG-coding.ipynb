{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7c8fd44",
   "metadata": {},
   "source": [
    "# Indexing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e748d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Springer Series in Statistics\\nTrevor Hastie\\nRobert TibshiraniJerome FriedmanSpringer Series in Statistics\\nThe Elements of\\nStatistical Learning\\nData Mining, Inference, and Prediction\\nThe Elements of Statistical LearningDuring the past decade there has been an explosion in computation and information tech-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 0}),\n",
       " Document(page_content='nology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining,machine learning, and bioinformatics. Many of these tools have common underpinnings butare often expressed with different terminology. This book describes the important ideas inthese areas in a common conceptual framework.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 0}),\n",
       " Document(page_content='While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples are given, with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book’s coverage is broad, from supervised learning(prediction) to unsupervised learning. The many topics include neural networks, supportvector machines, classification trees and boosting—the first comprehensive treatment of thistopic', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 0}),\n",
       " Document(page_content='in any book.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 0}),\n",
       " Document(page_content='This major new edition features many topics not covered in the original, including graphical\\nmodels, random forests, ensemble methods, least angle regression & path algorithms for thelasso, non-negative matrix factorization, and spectral clustering. There is also a chapter onmethods for “wide” data (p bigger than n), including multiple testing and false discovery rates.\\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 0}),\n",
       " Document(page_content='Stanford University. They are prominent researchers in this area: Hastie and Tibshiranideveloped generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS andinvented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of thevery successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 0}),\n",
       " Document(page_content='gradient boosting.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 0}),\n",
       " Document(page_content='›springer.comSTATISTICS\\nisbn 978-0-387-84857-0Trevor Hastie • Robert Tibshirani • Jerome Friedman\\nThe Elements of Statictical Learning\\nHastie • Tibshirani • Friedman\\nSecond Edition', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 0}),\n",
       " Document(page_content='This is page v\\nPrinter: Opaque this\\nTo our parents:\\nValerie and Patrick Hastie\\nVera and Sami Tibshirani\\nFlorence and Harry Friedman\\nand to our families:\\nSamantha, Timothy, and Lynda\\nCharlie, Ryan, Julie, and Cheryl\\nMelanie, Dora, Monika, and Ildiko', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 1}),\n",
       " Document(page_content='vi', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 2}),\n",
       " Document(page_content='This is page vii\\nPrinter: Opaque this\\nPreface to the Second Edition\\nIn God we trust, all others bring data.\\n–William Edwards Deming (1900-1993)1\\nWe have been gratiﬁed by the popularity of the ﬁrst edition of The\\nElements of Statistical Learning. This, along with the fast pace of research\\nin the statistical learning ﬁeld, motivated us to update our book with a\\nsecond edition.\\nWe have added four new chapters and updated some of the existi ng', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 3}),\n",
       " Document(page_content='chapters. Because many readers are familiar with the layout of the ﬁrst\\nedition, we have tried to change it as little as possible. Her e is a summary\\nof the main changes:\\n1On the Web, this quote has been widely attributed to both Deming and R obert W.\\nHayden; however Professor Hayden told us that he can claim no credit for th is quote,\\nand ironically we could ﬁnd no “data” conﬁrming that Deming actual ly said this.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 3}),\n",
       " Document(page_content='viii Preface to the Second Edition\\nChapter What’s new\\n1.Introduction\\n2.Overview of Supervised Learning\\n3.Linear Methods for Regression LAR algorithm and generaliza tions\\nof the lasso\\n4.Linear Methods for Classiﬁcation Lasso path for logistic re gression\\n5.Basis Expansions and Regulariza-\\ntionAdditional illustrations of RKHS\\n6.Kernel Smoothing Methods\\n7.Model Assessment and Selection Strengths and pitfalls of cr oss-\\nvalidation\\n8.Model Inference and Averaging\\n9.Additive Models, Trees, and', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 4}),\n",
       " Document(page_content='Related Methods\\n10.Boosting and Additive Trees New example from ecology; some\\nmaterial split oﬀ to Chapter 16.\\n11.Neural Networks Bayesian neural nets and the NIPS\\n2003 challenge\\n12.Support Vector Machines and\\nFlexible DiscriminantsPath algorithm for SVM classiﬁer\\n13.Prototype Methods and\\nNearest-Neighbors\\n14.Unsupervised Learning Spectral clustering, kernel PCA,\\nsparse PCA, non-negative matrix\\nfactorization archetypal analysis,\\nnonlinear dimension reduction,\\nGoogle page rank algorithm, a', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 4}),\n",
       " Document(page_content='direct approach to ICA\\n15.Random Forests New\\n16.Ensemble Learning New\\n17.Undirected Graphical Models New\\n18.High-Dimensional Problems New\\nSome further notes:\\n•Our ﬁrst edition was unfriendly to colorblind readers; in pa rticular,\\nwe tended to favor red/greencontrasts which are particularly trou-\\nblesome. We have changed the color palette in this edition to a large\\nextent, replacing the above with an orange/bluecontrast.\\n•We have changed the name of Chapter 6 from “Kernel Methods” to', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 4}),\n",
       " Document(page_content='“Kernel Smoothing Methods”, to avoid confusion with the mac hine-\\nlearningkernelmethodthatisdiscussedinthecontextofsu pportvec-\\ntor machines (Chapter 12) and more generally in Chapters 5 an d 14.\\n•In the ﬁrst edition, the discussion of error-rate estimatio n in Chap-\\nter 7 was sloppy, as we did not clearly diﬀerentiate the notio ns of\\nconditional error rates (conditional on the training set) a nd uncondi-\\ntional rates. We have ﬁxed this in the new edition.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 4}),\n",
       " Document(page_content='Preface to the Second Edition ix\\n•Chapters 15 and 16 follow naturally from Chapter 10, and the c hap-\\nters are probably best read in that order.\\n•In Chapter 17, we have not attempted a comprehensive treatme nt\\nof graphical models, and discuss only undirected models and some\\nnew methods for their estimation. Due to a lack of space, we ha ve\\nspeciﬁcally omitted coverage of directed graphical models .\\n•Chapter 18 explores the “ p≫N” problem, which is learning in high-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 5}),\n",
       " Document(page_content='dimensional feature spaces. These problems arise in many ar eas, in-\\ncluding genomic and proteomic studies, and document classi ﬁcation.\\nWe thank the many readers who have found the (too numerous) er rors in\\nthe ﬁrst edition. We apologize for those and have done our bes t to avoid er-\\nrorsinthisnewedition.WethankMarkSegal,BalaRajaratna m,andLarry\\nWasserman for comments on some of the new chapters, and many S tanford\\ngraduate and post-doctoral students who oﬀered comments, i n particular', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 5}),\n",
       " Document(page_content='Mohammed AlQuraishi, John Boik, Holger Hoeﬂing, Arian Male ki, Donal\\nMcMahon, Saharon Rosset, Babak Shababa, Daniela Witten, Ji Zhu and\\nHui Zou. We thank John Kimmel for his patience in guiding us th rough this\\nnew edition. RT dedicates this edition to the memory of Anna M cPhee.\\nTrevor Hastie\\nRobert Tibshirani\\nJerome Friedman\\nStanford, California\\nAugust 2008', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 5}),\n",
       " Document(page_content='x Preface to the Second Edition', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 6}),\n",
       " Document(page_content='This is page xi\\nPrinter: Opaque this\\nPreface to the First Edition\\nWe are drowning in information and starving for knowledge.\\n–Rutherford D. Roger\\nThe ﬁeld of Statistics is constantly challenged by the probl ems that science\\nandindustrybringstoitsdoor.Intheearlydays,theseprob lemsoftencame\\nfrom agricultural and industrial experiments and were rela tively small in\\nscope. With the advent of computers and the information age, statistical', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 7}),\n",
       " Document(page_content='problems have exploded both in size and complexity. Challen ges in the\\nareas of data storage, organization and searching have led t o the new ﬁeld\\nof “data mining”; statistical and computational problems i n biology and\\nmedicine have created “bioinformatics.” Vast amounts of da ta are being\\ngenerated in many ﬁelds, and the statistician’s job is to mak e sense of it\\nall: to extract important patterns and trends, and understa nd “what the\\ndata says.” We call this learning from data .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 7}),\n",
       " Document(page_content='The challenges in learning from data have led to a revolution in the sta-\\ntisticalsciences.Sincecomputationplayssuchakeyrole, itisnotsurprising\\nthat much of this new development has been done by researcher s in other\\nﬁelds such as computer science and engineering.\\nThe learning problems that we consider can be roughly catego rized as\\neithersupervised orunsupervised . In supervised learning, the goal is to pre-\\ndict the value of an outcome measure based on a number of input measures;', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 7}),\n",
       " Document(page_content='in unsupervised learning, there is no outcome measure, and t he goal is to\\ndescribe the associations and patterns among a set of input m easures.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 7}),\n",
       " Document(page_content='xii Preface to the First Edition\\nThis book is our attempt to bring together many of the importa nt new\\nideas in learning, and explain them in a statistical framewo rk. While some\\nmathematical details are needed, we emphasize the methods a nd their con-\\nceptual underpinnings rather than their theoretical prope rties. As a result,\\nwe hope that this book will appeal not just to statisticians b ut also to\\nresearchers and practitioners in a wide variety of ﬁelds.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 8}),\n",
       " Document(page_content='Just as we have learned a great deal from researchers outside of the ﬁeld\\nof statistics, our statistical viewpoint may help others to better understand\\ndiﬀerent aspects of learning:\\nThere is no true interpretation of anything; interpretatio n is a\\nvehicle in the service of human comprehension. The value of\\ninterpretation is in enabling others to fruitfully think ab out an\\nidea.\\n–Andreas Buja\\nWe would like to acknowledge the contribution of many people to the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 8}),\n",
       " Document(page_content='conception and completion of this book. David Andrews, Leo B reiman,\\nAndreas Buja, John Chambers, Bradley Efron, Geoﬀrey Hinton , Werner\\nStuetzle, and John Tukey have greatly inﬂuenced our careers . Balasub-\\nramanian Narasimhan gave us advice and help on many computat ional\\nproblems, and maintained an excellent computing environme nt. Shin-Ho\\nBang helped in the production of a number of the ﬁgures. Lee Wi lkinson\\ngavevaluabletipsoncolorproduction.IlanaBelitskaya,E vaCantoni,Maya', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 8}),\n",
       " Document(page_content='Gupta,MichaelJordan,ShantiGopatam,RadfordNeal,Jorge Picazo,Bog-\\ndan Popescu, Olivier Renaud, Saharon Rosset, John Storey, J i Zhu, Mu\\nZhu, two reviewers and many students read parts of the manusc ript and\\noﬀered helpful suggestions. John Kimmel was supportive, pa tient and help-\\nful at every phase; MaryAnn Brickner and Frank Ganz headed a s uperb\\nproduction team at Springer. Trevor Hastie would like to tha nk the statis-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 8}),\n",
       " Document(page_content='tics department at the University of Cape Town for their hosp itality during\\nthe ﬁnal stages of this book. We gratefully acknowledge NSF a nd NIH for\\ntheir support of this work. Finally, we would like to thank ou r families and\\nour parents for their love and support.\\nTrevor Hastie\\nRobert Tibshirani\\nJerome Friedman\\nStanford, California\\nMay 2001\\nThe quiet statisticians have changed our world; not by disco v-\\nering new facts or technical developments, but by changing t he', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 8}),\n",
       " Document(page_content='ways that we reason, experiment and form our opinions ....\\n–Ian Hacking', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 8}),\n",
       " Document(page_content='This is page xiii\\nPrinter: Opaque this\\nContents\\nPreface to the Second Edition vii\\nPreface to the First Edition xi\\n1 Introduction 1\\n2 Overview of Supervised Learning 9\\n2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.2 Variable Types and Terminology . . . . . . . . . . . . . . 9\\n2.3 Two Simple Approaches to Prediction:\\nLeast Squares and Nearest Neighbors . . . . . . . . . . . 11\\n2.3.1 Linear Models and Least Squares . . . . . . . . 11', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 9}),\n",
       " Document(page_content='2.3.2 Nearest-Neighbor Methods . . . . . . . . . . . . 14\\n2.3.3 From Least Squares to Nearest Neighbors . . . . 16\\n2.4 Statistical Decision Theory . . . . . . . . . . . . . . . . . 18\\n2.5 Local Methods in High Dimensions . . . . . . . . . . . . . 22\\n2.6 Statistical Models, Supervised Learning\\nand Function Approximation . . . . . . . . . . . . . . . . 28\\n2.6.1 A Statistical Model\\nfor the Joint Distribution Pr( X,Y) . . . . . . . 28\\n2.6.2 Supervised Learning . . . . . . . . . . . . . . . . 29', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 9}),\n",
       " Document(page_content='2.6.3 Function Approximation . . . . . . . . . . . . . 29\\n2.7 Structured Regression Models . . . . . . . . . . . . . . . 32\\n2.7.1 Diﬃculty of the Problem . . . . . . . . . . . . . 32', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 9}),\n",
       " Document(page_content='xiv Contents\\n2.8 Classes of Restricted Estimators . . . . . . . . . . . . . . 33\\n2.8.1 Roughness Penalty and Bayesian Methods . . . 34\\n2.8.2 Kernel Methods and Local Regression . . . . . . 34\\n2.8.3 Basis Functions and Dictionary Methods . . . . 35\\n2.9 Model Selection and the Bias–Variance Tradeoﬀ . . . . . 37\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 39\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\\n3 Linear Methods for Regression 43', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 10}),\n",
       " Document(page_content='3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 43\\n3.2 Linear Regression Models and Least Squares . . . . . . . 44\\n3.2.1 Example: Prostate Cancer . . . . . . . . . . . . 49\\n3.2.2 The Gauss–Markov Theorem . . . . . . . . . . . 51\\n3.2.3 Multiple Regression\\nfrom Simple Univariate Regression . . . . . . . . 52\\n3.2.4 Multiple Outputs . . . . . . . . . . . . . . . . . 56\\n3.3 Subset Selection . . . . . . . . . . . . . . . . . . . . . . . 57', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 10}),\n",
       " Document(page_content='3.3.1 Best-Subset Selection . . . . . . . . . . . . . . . 57\\n3.3.2 Forward- and Backward-Stepwise Selection . . . 58\\n3.3.3 Forward-Stagewise Regression . . . . . . . . . . 60\\n3.3.4 Prostate Cancer Data Example (Continued) . . 61\\n3.4 Shrinkage Methods . . . . . . . . . . . . . . . . . . . . . . 61\\n3.4.1 Ridge Regression . . . . . . . . . . . . . . . . . 61\\n3.4.2 The Lasso . . . . . . . . . . . . . . . . . . . . . 68\\n3.4.3 Discussion: Subset Selection, Ridge Regression', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 10}),\n",
       " Document(page_content='and the Lasso . . . . . . . . . . . . . . . . . . . 69\\n3.4.4 Least Angle Regression . . . . . . . . . . . . . . 73\\n3.5 Methods Using Derived Input Directions . . . . . . . . . 79\\n3.5.1 Principal Components Regression . . . . . . . . 79\\n3.5.2 Partial Least Squares . . . . . . . . . . . . . . . 80\\n3.6 Discussion: A Comparison of the Selection\\nand Shrinkage Methods . . . . . . . . . . . . . . . . . . . 82\\n3.7 Multiple Outcome Shrinkage and Selection . . . . . . . . 84', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 10}),\n",
       " Document(page_content='3.8 More on the Lasso and Related Path Algorithms . . . . . 86\\n3.8.1 Incremental Forward Stagewise Regression . . . 86\\n3.8.2 Piecewise-Linear Path Algorithms . . . . . . . . 89\\n3.8.3 The Dantzig Selector . . . . . . . . . . . . . . . 89\\n3.8.4 The Grouped Lasso . . . . . . . . . . . . . . . . 90\\n3.8.5 Further Properties of the Lasso . . . . . . . . . . 91\\n3.8.6 Pathwise Coordinate Optimization . . . . . . . . 92\\n3.9 Computational Considerations . . . . . . . . . . . . . . . 93', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 10}),\n",
       " Document(page_content='Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 94\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 10}),\n",
       " Document(page_content='Contents xv\\n4 Linear Methods for Classiﬁcation 101\\n4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 101\\n4.2 Linear Regression of an Indicator Matrix . . . . . . . . . 103\\n4.3 Linear Discriminant Analysis . . . . . . . . . . . . . . . . 106\\n4.3.1 Regularized Discriminant Analysis . . . . . . . . 112\\n4.3.2 Computations for LDA . . . . . . . . . . . . . . 113\\n4.3.3 Reduced-Rank Linear Discriminant Analysis . . 113', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 11}),\n",
       " Document(page_content='4.4 Logistic Regression . . . . . . . . . . . . . . . . . . . . . . 119\\n4.4.1 Fitting Logistic Regression Models . . . . . . . . 120\\n4.4.2 Example: South African Heart Disease . . . . . 122\\n4.4.3 Quadratic Approximations and Inference . . . . 124\\n4.4.4L1Regularized Logistic Regression . . . . . . . . 125\\n4.4.5 Logistic Regression or LDA? . . . . . . . . . . . 127\\n4.5 Separating Hyperplanes . . . . . . . . . . . . . . . . . . . 129\\n4.5.1 Rosenblatt’s Perceptron Learning Algorithm . . 130', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 11}),\n",
       " Document(page_content='4.5.2 Optimal Separating Hyperplanes . . . . . . . . . 132\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 135\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\\n5 Basis Expansions and Regularization 139\\n5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 139\\n5.2 Piecewise Polynomials and Splines . . . . . . . . . . . . . 141\\n5.2.1 Natural Cubic Splines . . . . . . . . . . . . . . . 144', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 11}),\n",
       " Document(page_content='5.2.2 Example:SouthAfricanHeartDisease(Continued)146\\n5.2.3 Example: Phoneme Recognition . . . . . . . . . 148\\n5.3 Filtering and Feature Extraction . . . . . . . . . . . . . . 150\\n5.4 Smoothing Splines . . . . . . . . . . . . . . . . . . . . . . 151\\n5.4.1 Degrees of Freedom and Smoother Matrices . . . 153\\n5.5 Automatic Selection of the Smoothing Parameters . . . . 15 6\\n5.5.1 Fixing the Degrees of Freedom . . . . . . . . . . 158\\n5.5.2 The Bias–Variance Tradeoﬀ . . . . . . . . . . . . 158', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 11}),\n",
       " Document(page_content='5.6 Nonparametric Logistic Regression . . . . . . . . . . . . . 161\\n5.7 Multidimensional Splines . . . . . . . . . . . . . . . . . . 162\\n5.8 Regularization and Reproducing Kernel Hilbert Spaces . 167\\n5.8.1 Spaces of Functions Generated by Kernels . . . 168\\n5.8.2 Examples of RKHS . . . . . . . . . . . . . . . . 170\\n5.9 Wavelet Smoothing . . . . . . . . . . . . . . . . . . . . . 174\\n5.9.1 Wavelet Bases and the Wavelet Transform . . . 176', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 11}),\n",
       " Document(page_content='5.9.2 Adaptive Wavelet Filtering . . . . . . . . . . . . 179\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 181\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\\nAppendix: Computational Considerations for Splines . . . . . . 186\\nAppendix:B-splines . . . . . . . . . . . . . . . . . . . . . 186\\nAppendix: Computations for Smoothing Splines . . . . . 189', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 11}),\n",
       " Document(page_content='xvi Contents\\n6 Kernel Smoothing Methods 191\\n6.1 One-Dimensional Kernel Smoothers . . . . . . . . . . . . 192\\n6.1.1 Local Linear Regression . . . . . . . . . . . . . . 194\\n6.1.2 Local Polynomial Regression . . . . . . . . . . . 197\\n6.2 Selecting the Width of the Kernel . . . . . . . . . . . . . 198\\n6.3 Local Regression in IRp. . . . . . . . . . . . . . . . . . . 200\\n6.4 Structured Local Regression Models in IRp. . . . . . . . 201\\n6.4.1 Structured Kernels . . . . . . . . . . . . . . . . . 203', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 12}),\n",
       " Document(page_content='6.4.2 Structured Regression Functions . . . . . . . . . 203\\n6.5 Local Likelihood and Other Models . . . . . . . . . . . . 205\\n6.6 Kernel Density Estimation and Classiﬁcation . . . . . . . 20 8\\n6.6.1 Kernel Density Estimation . . . . . . . . . . . . 208\\n6.6.2 Kernel Density Classiﬁcation . . . . . . . . . . . 210\\n6.6.3 The Naive Bayes Classiﬁer . . . . . . . . . . . . 210\\n6.7 Radial Basis Functions and Kernels . . . . . . . . . . . . 212', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 12}),\n",
       " Document(page_content='6.8 Mixture Models for Density Estimation and Classiﬁcatio n 214\\n6.9 Computational Considerations . . . . . . . . . . . . . . . 216\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 216\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\\n7 Model Assessment and Selection 219\\n7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 219\\n7.2 Bias, Variance and Model Complexity . . . . . . . . . . . 219', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 12}),\n",
       " Document(page_content='7.3 The Bias–Variance Decomposition . . . . . . . . . . . . . 223\\n7.3.1 Example: Bias–Variance Tradeoﬀ . . . . . . . . 226\\n7.4 Optimism of the Training Error Rate . . . . . . . . . . . 228\\n7.5 Estimates of In-Sample Prediction Error . . . . . . . . . . 230\\n7.6 The Eﬀective Number of Parameters . . . . . . . . . . . . 232\\n7.7 The Bayesian Approach and BIC . . . . . . . . . . . . . . 233\\n7.8 Minimum Description Length . . . . . . . . . . . . . . . . 235', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 12}),\n",
       " Document(page_content='7.9 Vapnik–Chervonenkis Dimension . . . . . . . . . . . . . . 237\\n7.9.1 Example (Continued) . . . . . . . . . . . . . . . 239\\n7.10 Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . 241\\n7.10.1K-Fold Cross-Validation . . . . . . . . . . . . . 241\\n7.10.2 The Wrong and Right Way\\nto Do Cross-validation . . . . . . . . . . . . . . . 245\\n7.10.3 Does Cross-Validation Really Work? . . . . . . . 247\\n7.11 Bootstrap Methods . . . . . . . . . . . . . . . . . . . . . 249', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 12}),\n",
       " Document(page_content='7.11.1 Example (Continued) . . . . . . . . . . . . . . . 252\\n7.12 Conditional or Expected Test Error? . . . . . . . . . . . . 254\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 257\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\\n8 Model Inference and Averaging 261\\n8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 261', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 12}),\n",
       " Document(page_content='Contents xvii\\n8.2 The Bootstrap and Maximum Likelihood Methods . . . . 261\\n8.2.1 A Smoothing Example . . . . . . . . . . . . . . 261\\n8.2.2 Maximum Likelihood Inference . . . . . . . . . . 265\\n8.2.3 Bootstrap versus Maximum Likelihood . . . . . 267\\n8.3 Bayesian Methods . . . . . . . . . . . . . . . . . . . . . . 267\\n8.4 Relationship Between the Bootstrap\\nand Bayesian Inference . . . . . . . . . . . . . . . . . . . 271\\n8.5 The EM Algorithm . . . . . . . . . . . . . . . . . . . . . 272', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 13}),\n",
       " Document(page_content='8.5.1 Two-Component Mixture Model . . . . . . . . . 272\\n8.5.2 The EM Algorithm in General . . . . . . . . . . 276\\n8.5.3 EM as a Maximization–Maximization Procedure 277\\n8.6 MCMC for Sampling from the Posterior . . . . . . . . . . 279\\n8.7 Bagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\\n8.7.1 Example: Trees with Simulated Data . . . . . . 283\\n8.8 Model Averaging and Stacking . . . . . . . . . . . . . . . 288\\n8.9 Stochastic Search: Bumping . . . . . . . . . . . . . . . . . 290', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 13}),\n",
       " Document(page_content='Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 292\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293\\n9 Additive Models, Trees, and Related Methods 295\\n9.1 Generalized Additive Models . . . . . . . . . . . . . . . . 295\\n9.1.1 Fitting Additive Models . . . . . . . . . . . . . . 297\\n9.1.2 Example: Additive Logistic Regression . . . . . 299\\n9.1.3 Summary . . . . . . . . . . . . . . . . . . . . . . 304', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 13}),\n",
       " Document(page_content='9.2 Tree-Based Methods . . . . . . . . . . . . . . . . . . . . . 305\\n9.2.1 Background . . . . . . . . . . . . . . . . . . . . 305\\n9.2.2 Regression Trees . . . . . . . . . . . . . . . . . . 307\\n9.2.3 Classiﬁcation Trees . . . . . . . . . . . . . . . . 308\\n9.2.4 Other Issues . . . . . . . . . . . . . . . . . . . . 310\\n9.2.5 Spam Example (Continued) . . . . . . . . . . . 313\\n9.3 PRIM: Bump Hunting . . . . . . . . . . . . . . . . . . . . 317\\n9.3.1 Spam Example (Continued) . . . . . . . . . . . 320', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 13}),\n",
       " Document(page_content='9.4 MARS: Multivariate Adaptive Regression Splines . . . . . 3 21\\n9.4.1 Spam Example (Continued) . . . . . . . . . . . 326\\n9.4.2 Example (Simulated Data) . . . . . . . . . . . . 327\\n9.4.3 Other Issues . . . . . . . . . . . . . . . . . . . . 328\\n9.5 Hierarchical Mixtures of Experts . . . . . . . . . . . . . . 329\\n9.6 Missing Data . . . . . . . . . . . . . . . . . . . . . . . . . 332\\n9.7 Computational Considerations . . . . . . . . . . . . . . . 334', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 13}),\n",
       " Document(page_content='Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 334\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\\n10 Boosting and Additive Trees 337\\n10.1 Boosting Methods . . . . . . . . . . . . . . . . . . . . . . 337\\n10.1.1 Outline of This Chapter . . . . . . . . . . . . . . 340', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 13}),\n",
       " Document(page_content='xviii Contents\\n10.2 Boosting Fits an Additive Model . . . . . . . . . . . . . . 341\\n10.3 Forward Stagewise Additive Modeling . . . . . . . . . . . 342\\n10.4 Exponential Loss and AdaBoost . . . . . . . . . . . . . . 343\\n10.5 Why Exponential Loss? . . . . . . . . . . . . . . . . . . . 345\\n10.6 Loss Functions and Robustness . . . . . . . . . . . . . . . 346\\n10.7 “Oﬀ-the-Shelf” Procedures for Data Mining . . . . . . . . 35 0\\n10.8 Example: Spam Data . . . . . . . . . . . . . . . . . . . . 352', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 14}),\n",
       " Document(page_content='10.9 Boosting Trees . . . . . . . . . . . . . . . . . . . . . . . . 353\\n10.10 Numerical Optimization via Gradient Boosting . . . . . . 358\\n10.10.1 Steepest Descent . . . . . . . . . . . . . . . . . . 358\\n10.10.2 Gradient Boosting . . . . . . . . . . . . . . . . . 359\\n10.10.3 Implementations of Gradient Boosting . . . . . . 360\\n10.11 Right-Sized Trees for Boosting . . . . . . . . . . . . . . . 361\\n10.12 Regularization . . . . . . . . . . . . . . . . . . . . . . . . 364', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 14}),\n",
       " Document(page_content='10.12.1 Shrinkage . . . . . . . . . . . . . . . . . . . . . . 364\\n10.12.2 Subsampling . . . . . . . . . . . . . . . . . . . . 365\\n10.13 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . 367\\n10.13.1 Relative Importance of Predictor Variables . . . 367\\n10.13.2 Partial Dependence Plots . . . . . . . . . . . . . 369\\n10.14 Illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . 371\\n10.14.1 California Housing . . . . . . . . . . . . . . . . . 371', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 14}),\n",
       " Document(page_content='10.14.2 New Zealand Fish . . . . . . . . . . . . . . . . . 375\\n10.14.3 Demographics Data . . . . . . . . . . . . . . . . 379\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 380\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384\\n11 Neural Networks 389\\n11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 389\\n11.2 Projection Pursuit Regression . . . . . . . . . . . . . . . 389', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 14}),\n",
       " Document(page_content='11.3 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 392\\n11.4 Fitting Neural Networks . . . . . . . . . . . . . . . . . . . 395\\n11.5 Some Issues in Training Neural Networks . . . . . . . . . 397\\n11.5.1 Starting Values . . . . . . . . . . . . . . . . . . . 397\\n11.5.2 Overﬁtting . . . . . . . . . . . . . . . . . . . . . 398\\n11.5.3 Scaling of the Inputs . . . . . . . . . . . . . . . 398\\n11.5.4 Number of Hidden Units and Layers . . . . . . . 400', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 14}),\n",
       " Document(page_content='11.5.5 Multiple Minima . . . . . . . . . . . . . . . . . . 400\\n11.6 Example: Simulated Data . . . . . . . . . . . . . . . . . . 401\\n11.7 Example: ZIP Code Data . . . . . . . . . . . . . . . . . . 404\\n11.8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 408\\n11.9 Bayesian Neural Nets and the NIPS 2003 Challenge . . . 409\\n11.9.1 Bayes, Boosting and Bagging . . . . . . . . . . . 410\\n11.9.2 Performance Comparisons . . . . . . . . . . . . 412', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 14}),\n",
       " Document(page_content='11.10 Computational Considerations . . . . . . . . . . . . . . . 414\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 415', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 14}),\n",
       " Document(page_content='Contents xix\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415\\n12 Support Vector Machines and\\nFlexible Discriminants 417\\n12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 417\\n12.2 The Support Vector Classiﬁer . . . . . . . . . . . . . . . . 417\\n12.2.1 Computing the Support Vector Classiﬁer . . . . 420\\n12.2.2 Mixture Example (Continued) . . . . . . . . . . 421\\n12.3 Support Vector Machines and Kernels . . . . . . . . . . . 423', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 15}),\n",
       " Document(page_content='12.3.1 Computing the SVM for Classiﬁcation . . . . . . 423\\n12.3.2 The SVM as a Penalization Method . . . . . . . 426\\n12.3.3 Function Estimation and Reproducing Kernels . 428\\n12.3.4 SVMs and the Curse of Dimensionality . . . . . 431\\n12.3.5 A Path Algorithm for the SVM Classiﬁer . . . . 432\\n12.3.6 Support Vector Machines for Regression . . . . . 434\\n12.3.7 Regression and Kernels . . . . . . . . . . . . . . 436\\n12.3.8 Discussion . . . . . . . . . . . . . . . . . . . . . 438', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 15}),\n",
       " Document(page_content='12.4 Generalizing Linear Discriminant Analysis . . . . . . . . 4 38\\n12.5 Flexible Discriminant Analysis . . . . . . . . . . . . . . . 440\\n12.5.1 Computing the FDA Estimates . . . . . . . . . . 444\\n12.6 Penalized Discriminant Analysis . . . . . . . . . . . . . . 446\\n12.7 Mixture Discriminant Analysis . . . . . . . . . . . . . . . 449\\n12.7.1 Example: Waveform Data . . . . . . . . . . . . . 451\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 455', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 15}),\n",
       " Document(page_content='Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455\\n13 Prototype Methods and Nearest-Neighbors 459\\n13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 459\\n13.2 Prototype Methods . . . . . . . . . . . . . . . . . . . . . 459\\n13.2.1K-means Clustering . . . . . . . . . . . . . . . . 460\\n13.2.2 Learning Vector Quantization . . . . . . . . . . 462\\n13.2.3 Gaussian Mixtures . . . . . . . . . . . . . . . . . 463', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 15}),\n",
       " Document(page_content='13.3k-Nearest-Neighbor Classiﬁers . . . . . . . . . . . . . . . 463\\n13.3.1 Example: A Comparative Study . . . . . . . . . 468\\n13.3.2 Example: k-Nearest-Neighbors\\nand Image Scene Classiﬁcation . . . . . . . . . . 470\\n13.3.3 Invariant Metrics and Tangent Distance . . . . . 471\\n13.4 Adaptive Nearest-Neighbor Methods . . . . . . . . . . . . 475\\n13.4.1 Example . . . . . . . . . . . . . . . . . . . . . . 478\\n13.4.2 Global Dimension Reduction\\nfor Nearest-Neighbors . . . . . . . . . . . . . . . 479', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 15}),\n",
       " Document(page_content='13.5 Computational Considerations . . . . . . . . . . . . . . . 480\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 481\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 15}),\n",
       " Document(page_content='xx Contents\\n14 Unsupervised Learning 485\\n14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 485\\n14.2 Association Rules . . . . . . . . . . . . . . . . . . . . . . 487\\n14.2.1 Market Basket Analysis . . . . . . . . . . . . . . 488\\n14.2.2 The Apriori Algorithm . . . . . . . . . . . . . . 489\\n14.2.3 Example: Market Basket Analysis . . . . . . . . 492\\n14.2.4 Unsupervised as Supervised Learning . . . . . . 495\\n14.2.5 Generalized Association Rules . . . . . . . . . . 497', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 16}),\n",
       " Document(page_content='14.2.6 Choice of Supervised Learning Method . . . . . 499\\n14.2.7 Example: Market Basket Analysis (Continued) . 499\\n14.3 Cluster Analysis . . . . . . . . . . . . . . . . . . . . . . . 501\\n14.3.1 Proximity Matrices . . . . . . . . . . . . . . . . 503\\n14.3.2 Dissimilarities Based on Attributes . . . . . . . 503\\n14.3.3 Object Dissimilarity . . . . . . . . . . . . . . . . 505\\n14.3.4 Clustering Algorithms . . . . . . . . . . . . . . . 507\\n14.3.5 Combinatorial Algorithms . . . . . . . . . . . . 507', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 16}),\n",
       " Document(page_content='14.3.6K-means . . . . . . . . . . . . . . . . . . . . . . 509\\n14.3.7 Gaussian Mixtures as Soft K-means Clustering . 510\\n14.3.8 Example: Human Tumor Microarray Data . . . 512\\n14.3.9 Vector Quantization . . . . . . . . . . . . . . . . 514\\n14.3.10K-medoids . . . . . . . . . . . . . . . . . . . . . 515\\n14.3.11 Practical Issues . . . . . . . . . . . . . . . . . . 518\\n14.3.12 Hierarchical Clustering . . . . . . . . . . . . . . 520\\n14.4 Self-Organizing Maps . . . . . . . . . . . . . . . . . . . . 528', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 16}),\n",
       " Document(page_content='14.5 Principal Components, Curves and Surfaces . . . . . . . . 53 4\\n14.5.1 Principal Components . . . . . . . . . . . . . . . 534\\n14.5.2 Principal Curves and Surfaces . . . . . . . . . . 541\\n14.5.3 Spectral Clustering . . . . . . . . . . . . . . . . 544\\n14.5.4 Kernel Principal Components . . . . . . . . . . . 547\\n14.5.5 Sparse Principal Components . . . . . . . . . . . 550\\n14.6 Non-negative Matrix Factorization . . . . . . . . . . . . . 553', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 16}),\n",
       " Document(page_content='14.6.1 Archetypal Analysis . . . . . . . . . . . . . . . . 554\\n14.7 Independent Component Analysis\\nand Exploratory Projection Pursuit . . . . . . . . . . . . 557\\n14.7.1 Latent Variables and Factor Analysis . . . . . . 558\\n14.7.2 Independent Component Analysis . . . . . . . . 560\\n14.7.3 Exploratory Projection Pursuit . . . . . . . . . . 565\\n14.7.4 A Direct Approach to ICA . . . . . . . . . . . . 565\\n14.8 Multidimensional Scaling . . . . . . . . . . . . . . . . . . 570', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 16}),\n",
       " Document(page_content='14.9 Nonlinear Dimension Reduction\\nand Local Multidimensional Scaling . . . . . . . . . . . . 572\\n14.10 The Google PageRank Algorithm . . . . . . . . . . . . . 576\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 578\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 16}),\n",
       " Document(page_content='Contents xxi\\n15 Random Forests 587\\n15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 587\\n15.2 Deﬁnition of Random Forests . . . . . . . . . . . . . . . . 587\\n15.3 Details of Random Forests . . . . . . . . . . . . . . . . . 592\\n15.3.1 Out of Bag Samples . . . . . . . . . . . . . . . . 592\\n15.3.2 Variable Importance . . . . . . . . . . . . . . . . 593\\n15.3.3 Proximity Plots . . . . . . . . . . . . . . . . . . 595\\n15.3.4 Random Forests and Overﬁtting . . . . . . . . . 596', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 17}),\n",
       " Document(page_content='15.4 Analysis of Random Forests . . . . . . . . . . . . . . . . . 597\\n15.4.1 Variance and the De-Correlation Eﬀect . . . . . 597\\n15.4.2 Bias . . . . . . . . . . . . . . . . . . . . . . . . . 600\\n15.4.3 Adaptive Nearest Neighbors . . . . . . . . . . . 601\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 602\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603\\n16 Ensemble Learning 605', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 17}),\n",
       " Document(page_content='16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 605\\n16.2 Boosting and Regularization Paths . . . . . . . . . . . . . 607\\n16.2.1 Penalized Regression . . . . . . . . . . . . . . . 607\\n16.2.2 The “Bet on Sparsity” Principle . . . . . . . . . 610\\n16.2.3 Regularization Paths, Over-ﬁtting and Margins . 613\\n16.3 Learning Ensembles . . . . . . . . . . . . . . . . . . . . . 616\\n16.3.1 Learning a Good Ensemble . . . . . . . . . . . . 617', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 17}),\n",
       " Document(page_content='16.3.2 Rule Ensembles . . . . . . . . . . . . . . . . . . 622\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 623\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624\\n17 Undirected Graphical Models 625\\n17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 625\\n17.2 Markov Graphs and Their Properties . . . . . . . . . . . 627\\n17.3 Undirected Graphical Models for Continuous Variables . 630\\n17.3.1 Estimation of the Parameters', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 17}),\n",
       " Document(page_content='when the Graph Structure is Known . . . . . . . 631\\n17.3.2 Estimation of the Graph Structure . . . . . . . . 635\\n17.4 Undirected Graphical Models for Discrete Variables . . . 638\\n17.4.1 Estimation of the Parameters\\nwhen the Graph Structure is Known . . . . . . . 639\\n17.4.2 Hidden Nodes . . . . . . . . . . . . . . . . . . . 641\\n17.4.3 Estimation of the Graph Structure . . . . . . . . 642\\n17.4.4 Restricted Boltzmann Machines . . . . . . . . . 643', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 17}),\n",
       " Document(page_content='Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645\\n18 High-Dimensional Problems: p≫N 649\\n18.1 When pis Much Bigger than N. . . . . . . . . . . . . . 649', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 17}),\n",
       " Document(page_content='xxii Contents\\n18.2 Diagonal Linear Discriminant Analysis\\nand Nearest Shrunken Centroids . . . . . . . . . . . . . . 651\\n18.3 Linear Classiﬁers with Quadratic Regularization . . . . . 654\\n18.3.1 Regularized Discriminant Analysis . . . . . . . . 656\\n18.3.2 Logistic Regression\\nwith Quadratic Regularization . . . . . . . . . . 657\\n18.3.3 The Support Vector Classiﬁer . . . . . . . . . . 657\\n18.3.4 Feature Selection . . . . . . . . . . . . . . . . . . 658', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 18}),\n",
       " Document(page_content='18.3.5 Computational Shortcuts When p≫N. . . . . 659\\n18.4 Linear Classiﬁers with L1Regularization . . . . . . . . . 661\\n18.4.1 Application of Lasso\\nto Protein Mass Spectroscopy . . . . . . . . . . 664\\n18.4.2 The Fused Lasso for Functional Data . . . . . . 666\\n18.5 Classiﬁcation When Features are Unavailable . . . . . . . 6 68\\n18.5.1 Example: String Kernels\\nand Protein Classiﬁcation . . . . . . . . . . . . . 668\\n18.5.2 Classiﬁcation and Other Models Using', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 18}),\n",
       " Document(page_content='Inner-Product Kernels and Pairwise Distances . 670\\n18.5.3 Example: Abstracts Classiﬁcation . . . . . . . . 672\\n18.6 High-Dimensional Regression:\\nSupervised Principal Components . . . . . . . . . . . . . 674\\n18.6.1 Connection to Latent-Variable Modeling . . . . 678\\n18.6.2 Relationship with Partial Least Squares . . . . . 680\\n18.6.3 Pre-Conditioning for Feature Selection . . . . . 681\\n18.7 Feature Assessment and the Multiple-Testing Problem . . 683', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 18}),\n",
       " Document(page_content='18.7.1 The False Discovery Rate . . . . . . . . . . . . . 687\\n18.7.2 Asymmetric Cutpoints and the SAM Procedure 690\\n18.7.3 A Bayesian Interpretation of the FDR . . . . . . 692\\n18.8 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . 693\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 694\\nReferences 699\\nAuthor Index 729\\nIndex 737', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 18}),\n",
       " Document(page_content='This is page 1\\nPrinter: Opaque this\\n1\\nIntroduction\\nStatistical learning plays a key role in many areas of science, ﬁnance and\\nindustry. Here are some examples of learning problems:\\n•Predict whether a patient, hospitalized due to a heart attac k, will\\nhave a second heart attack. The prediction is to be based on de mo-\\ngraphic, diet and clinical measurements for that patient.\\n•Predict the price of a stock in 6 months from now, on the basis o f\\ncompany performance measures and economic data.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 19}),\n",
       " Document(page_content='•Identify the numbers in a handwritten ZIP code, from a digiti zed\\nimage.\\n•Estimate the amount of glucose in the blood of a diabetic pers on,\\nfrom the infrared absorption spectrum of that person’s bloo d.\\n•Identify the risk factors for prostate cancer, based on clin ical and\\ndemographic variables.\\nThe science of learning plays a key role in the ﬁelds of statis tics, data\\nmining and artiﬁcial intelligence, intersecting with area s of engineering and\\nother disciplines.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 19}),\n",
       " Document(page_content='This book is about learning from data. In a typical scenario, we have\\nan outcome measurement, usually quantitative (such as a sto ck price) or\\ncategorical (such as heart attack/no heart attack), that we wish to predict\\nbased on a set of features (such as diet and clinical measurements). We\\nhave atraining set of data, in which we observe the outcome and feature', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 19}),\n",
       " Document(page_content='2 1. Introduction\\nTABLE 1.1. Average percentage of words or characters in an email messag e\\nequal to the indicated word or character. We have chosen the w ords and characters\\nshowing the largest diﬀerence between spamandemail.\\ngeorge you your hp free hpl ! our re edu remove\\nspam 0.00 2.26 1.38 0.02 0.52 0.01 0.51 0.51 0.13 0.01 0.28\\nemail 1.27 1.27 0.44 0.90 0.07 0.43 0.11 0.18 0.42 0.29 0.01\\nmeasurements for a set of objects (such as people). Using thi s data we build', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 20}),\n",
       " Document(page_content='a prediction model, or learner, which will enable us to predict the outcome\\nfor new unseen objects. A good learner is one that accurately predicts such\\nan outcome.\\nThe examples above describe what is called the supervised learning prob-\\nlem. It is called “supervised” because of the presence of the outcome vari-\\nable to guide the learning process. In the unsupervised learning problem ,\\nwe observe only the features and have no measurements of the o utcome.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 20}),\n",
       " Document(page_content='Our task is rather to describe how the data are organized or cl ustered. We\\ndevote most of this book to supervised learning; the unsuper vised problem\\nis less developed in the literature, and is the focus of Chapt er 14.\\nHere are some examples of real learning problems that are dis cussed in\\nthis book.\\nExample 1: Email Spam\\nThe data for this example consists of information from 4601 e mail mes-\\nsages, in a study to try to predict whether the email was junk e mail, or', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 20}),\n",
       " Document(page_content='“spam.” The objective was to design an automatic spam detect or that\\ncould ﬁlter out spam before clogging the users’ mailboxes. F or all 4601\\nemail messages, the true outcome (email type) emailorspamis available,\\nalong with the relative frequencies of 57 of the most commonl y occurring\\nwords and punctuation marks in the email message. This is a su pervised\\nlearning problem, with the outcome the class variable email/spam. It is also\\ncalled aclassiﬁcation problem.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 20}),\n",
       " Document(page_content='Table 1.1 lists the words and characters showing the largest average\\ndiﬀerence between spamandemail.\\nOur learning method has to decide which features to use and ho w: for\\nexample, we might use a rule such as\\nif (%george<0.6) & (%you>1.5) then spam\\nelseemail.\\nAnother form of a rule might be:\\nif (0.2·%you−0.3·%george)>0 then spam\\nelseemail.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 20}),\n",
       " Document(page_content='1. Introduction 3\\nlpsa−1 1 2 3 4\\nooo ooo ooo oo o ooooo o oooo oooooooooo ooooooooooooooo oo ooooooooooooo oo oooo ooooooooooooooooooooooooooooo\\noo o ooo ooo oo o oooooooooooooooooooo ooooo o ooooooooo oo ooooooo oooooo oo oooo ooooooooooooooooooooooooooooo40 50 60 70 80\\noo o ooo ooo oo o oooooo o ooooooooooooo ooooooooooooooo o o o oooooo oo oooo oo oooo oooooooooooooo ooooooooooooooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='oo o ooo o o o oo ooooo o oo o o o ooooo ooo oo o oo o oo oo oo o o ooo o o oo ooo o o o ooooo o o o ooo o o o o oooo oo oo ooooo o o oo o o oooooo0.0 0.4 0.8\\noo o ooo ooo oo o oooooooooooooooooooo oooooo o ooooooo o oo oooooooooooo o o o oooo oo o o oooo oo o ooo o oo o o ooo o oooooo\\noo o ooo ooo oo o ooooooooo o o o oooooo oo oooo o o oooooooo o oo ooooo ooooooo o oo oooo ooo ooooooo o ooooooo oooo ooooooo6.0 7.0 8.0 9.0', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='oo o ooo ooo oo o oooooooooooooooooooo oooooooo o ooooo o oo ooooooooooooo oo oooo ooooo o ooooooooo o ooooooooooooo\\n0 1 2 3 4 5oo o ooo ooo oo o ooooooooooo o oo o o o ooo oooooooo o ooooo o oo o o o oooooooooo oo oo o o oo ooo o ooo o oooooooo oooo oooooo o−1 1 2 3 4ooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\noooo\\noo\\nooo\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\noo\\nooo\\noo\\nooo\\nooo\\nlcavol\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\noooo\\noo\\nooo\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\noo\\nooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='ooo\\nooooo\\noooo\\noooo\\noo\\nooo\\noo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\noooo\\noo\\nooo\\nooooo\\noo\\nooo\\nooo\\noo\\no oo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooo\\noo oo\\noo\\nooo\\noo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\nooo o\\noo\\no oo\\nooooo\\noo\\no oo\\nooo\\noo\\nooo\\nooo\\no o\\nooo\\nooo\\no oooo\\noo oo\\noooo\\noo\\nooo\\noo\\nooo\\no oo\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\noooo\\noo\\nooo\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\no o\\nooo\\nooo\\no oooo\\noo o o\\noo o o\\noo\\nooo\\noo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\nooo o\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='ooo\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\no o\\nooo\\nooo\\nooooo\\noo o o\\noooo\\noo\\nooo\\noo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\noooo\\noo\\no oo\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooo\\nooo o\\noo\\nooo\\noo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\noooo\\noo\\no oo\\nooooo\\noo\\no oo\\nooo\\noo\\nooo\\nooo\\no o\\noo o\\nooo\\nooooo\\noo oo\\noooo\\noo\\nooo\\noo\\nooo\\no oo\\noo\\noooooo o\\nooo\\nooo\\nooo\\noo\\nooooo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\noooo\\noooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\nooo\\nooo\\noo\\noooooo o\\nooo\\nooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='ooo\\noo\\nooooo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\noooo\\noooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\nooo\\nooolweight\\noo\\noooooo o\\nooo\\nooo\\nooo\\noo\\nooooo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\noooo\\no ooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\noo o\\nooo\\noo\\noooooo o\\noo o\\nooo\\nooo\\noo\\no oooo\\nooo\\nooo\\no\\noooo\\no\\noo\\noooo o\\noooo\\no ooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\nooo\\nooo\\noo\\noooooo o\\nooo\\nooo\\nooo\\noo\\nooooo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\nooo o\\noooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='ooo oooo\\noo\\noo\\nooo\\nooo\\noo\\noooooo o\\nooo\\nooo\\nooo\\noo\\no oo oo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\nooo o\\noooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\nooo\\nooo\\noo\\noooooo o\\nooo\\nooo\\nooo\\noo\\nooooo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\nooo o\\noooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooo oooo\\noo\\noo\\nooo\\nooo\\n2.5 3.5 4.5oo\\noooooo o\\nooo\\nooo\\nooo\\noo\\nooo oo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\nooo o\\noooo\\noo\\noo\\nooo\\noo\\nooo\\noo oooo\\no\\noo\\noo\\noo o\\noo\\nooooooo\\noo\\noo\\nooo\\nooo40 50 60 70 80ooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\noooooooo oo o oo\\noooo\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='ooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\noo oooooo oo o oo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\noooooooo oo o oo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\noooo\\nage\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\no o oooooo oo o oo\\noo oo\\noo\\nooo\\no oo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='ooo o\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\noooooooo oo o oo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\no o oooooo oo o oo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\noooooooo oo o oo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\noooooooo oo o oo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='ooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\noo o ooooo\\no ooo\\no o o oo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\nooo\\noo\\noooo\\no\\no oo o o ooo\\no o oo\\no o ooo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\no oo\\noo\\no o oo\\no\\no o o oo ooo\\no o oo\\no o o oo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\no oo\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='o o oo\\no\\no o o o o ooo\\no ooo\\no o o oo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\nooo\\noo\\no o oo\\nolbph\\no o o o o ooo\\no o oo\\no o o oo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\no oo\\noo\\no o oo\\no\\no o o o o ooo\\no o oo\\no o o oo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\no oo\\noo\\no ooo\\no\\no o o o o ooo\\no o oo\\no o o oo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='o oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\no oo\\noo\\no o oo\\no\\n−1 0 1 2o o o o o ooo\\no o oo\\no o o oo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\no oo\\noo\\no o oo\\no0.0 0.4 0.8oo o ooo o o o oo o o o o o o o o o o o o o o o o o o o o o o o o o o oo\\no o o o o o oo\\no o o o o o o o o o o o o oo\\noo\\no o o o o oo\\noo oo o\\no oo\\no o oo\\no oo\\nooo o\\noooooo o', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='o oo o o o o o o o o o o o oo o oo o o o o o o o o o o o o o o o o o o oo\\no o o o o ooo\\no o o o o o o o o o o o o oo\\noo\\no o oo o oo\\noo o o o\\no oo\\no o oo\\no oo\\noo o o\\nooo o o o o\\no o o oo o o o o o o o o o o o oo o o oo o oo o oo o o o o oo o o ooo\\noo oo o o oo\\no o o o o o o o oo o o o oo\\noo\\noo oo o oo\\noo o o o\\no oo\\no o oo\\no oo\\noo o o\\nooo o o oo\\no o o o o o o o o oo o o o o o o oo o o o o o o o oo o o o o o o o o o oo\\no o o o o o oo\\no o o o o o o o o oo o o oo\\noo\\no o o o ooo\\noo o o o\\no oo\\noo oo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='o oo\\nooo o\\noo oo o o o\\no o o o o o o oo o o oo o o o oo o oo oo o o o o o o o o o oo o oo oo\\no o o oo o oo\\no o oo o ooo oo o o o oo\\noo\\no o o oo oo\\noo o o o\\nooo\\no o oo\\no oo\\noo o o\\noo o o o o o\\nsvi\\no o o o o o o o o o o o o o o o o o o o o oo oo o o o o oo o o o o o ooo\\no o o o o o oo\\no o o o o o oo o o o o o oo\\noo\\no o o o o oo\\noo o o o\\no oo\\no o oo\\no oo\\noo o o\\noo o oo o o\\no o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o oo\\no oo o o o oo\\no o o o o o o o o o o o o oo\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='o o o o o oo\\noo oo o\\no oo\\no o oo\\no oo\\noo o o\\noo o o o o o\\no o o o o o o o o o o o o o o o o o o o o o o oo o o o oo o o o o oo o oo\\no oo oo o oo\\no o o o o o o o o o o o o oo\\noo\\no o oo ooo\\noo oo o\\no oo\\no o oo\\no oo\\noo o o\\noo o o o oo\\noo o ooo o o o oo oo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no o oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\no o o oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\no oooo\\noo\\no oo o o o o o o o o oo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no o oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\no o o oooo\\nooo\\no\\nooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='ooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\no oooo\\noo\\no o o oo o o o o o o oo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no ooo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\no o o oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\no oooo\\noo\\no o o o o o o o o oo oo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no o oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\no o o oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\no oooo\\noo\\no o o o o o o oo o o oo\\noo\\nooo\\no ooo\\noo\\no oo\\nooo\\no\\no o oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\no o o oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\no oo\\noo\\noo\\nooo\\no\\no oooo\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='o o o o o o o o o o o oo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no o oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\no o o oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\no oooo\\noo\\nlcp\\no o o o o o o o o o o oo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no o oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\no o o oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\no oooo\\noo\\n−1 0 1 2 3o o o o o o o o o o o oo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no o oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\no o o oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\no oooo\\noo6.0 7.0 8.0 9.0ooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='ooo o o o oo oo o o\\noo\\no o o oo\\noo\\no oo o o\\no o o o o ooo\\no o oo\\no\\noo o oo\\no\\no oo\\noo o o o o\\noo o\\nooo\\no\\noo o o\\noo o o oo\\no o o o o o o o oo\\no o\\nooo o\\noooooo o\\no oo\\no o o o o o o o oo o o\\noo\\no o o oo\\noo\\no oo o o\\no o o o o ooo\\no o oo\\no\\noo ooo\\no\\no oo\\noo o o o o\\noo o\\nooo\\no\\noo oo\\noo o o oo\\no o o o o o o o oo\\no o\\noo o o\\nooo o o o o\\no oo\\noo o o o o o o oo o o\\noo\\no o o oo\\noo\\no ooo o\\no o o oo ooo\\no o oo\\no\\noo o oo\\no\\no oo\\noo o o oo\\noo o\\nooo\\no\\noo oo\\noo o o oo\\no o o o o o o o oo\\no o\\noo o o\\nooo o o oo\\no oo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='o o o o o o oo oo o o\\noo\\no o o oo\\noo\\no ooo o\\no o o o o ooo\\no o oo\\no\\noo o oo\\no\\no oo\\noo o o o o\\noo o\\nooo\\no\\noo o o\\noo o o oo\\no o o o o oo o oo\\no o\\nooo o\\noo oo o o o\\no oo\\no o o o oo o o oo o o\\noo\\no o ooo\\noo\\no oo o o\\no o o oo ooo\\no o oo\\no\\noo o oo\\no\\no oo\\nooo o oo\\noo o\\nooo\\no\\noo o o\\noo o ooo\\no o ooo o o o oo\\no o\\noo o o\\noo o o o o o\\no oo\\no o o o o o o o oo o o\\noo\\no o o oo\\noo\\no oo o o\\no o o o o ooo\\no ooo\\no\\noo o oo\\no\\no oo\\noo o o o o\\noo o\\nooo\\no\\noo o o\\noo oo oo\\no o o o oo o o oo\\no o\\noo o o\\noo o o o o o\\no oo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='o o o o o o o o oo o o\\noo\\no o o oo\\noo\\no oo o o\\no o o o o ooo\\no o oo\\no\\noo o oo\\no\\no oo\\noo oo o o\\noo o\\nooo\\no\\noo o o\\noo oo oo\\no o o o o o o o oo\\no o\\noo o o\\noo o oo o ogleason\\no oo\\no o o o o o o o oo o o\\noo\\no o o oo\\noo\\no oo o o\\no o o o o ooo\\no o oo\\no\\noo o oo\\no\\no oo\\noo o o o o\\noo o\\nooo\\no\\noo oo\\noo o o oo\\no o o o o o o o oo\\no o\\noo o o\\noo o o o oo\\n0 1 2 3 4 5ooo\\nooo o o o oo oo\\no ooo\\no o o oo\\noo\\no oo\\noo\\no o o o o ooo oo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='o oo\\no o o o o o o o oo\\no ooo\\no o o oo\\noo\\no oo\\noo\\no o o o o ooo oo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\n2.5 3.5 4.5o oo\\noo o o o o o o oo\\no ooo\\no o o oo\\noo\\no oo\\noo\\no o o oo ooo oo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\no oo\\no o o o o o oo oo\\no ooo\\no o o oo\\noo\\no oo\\noo\\no o o o o ooo oo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\n−1 0 1 2o oo\\no o o o oo o o oo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='o ooo\\no o ooo\\noo\\no oo\\noo\\no o o oo ooo oo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\no oo\\no o o o o o o o oo\\no ooo\\no o o oo\\noo\\no oo\\noo\\no o o o o ooo oo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\n−1 0 1 2 3o oo\\no o o o o o o o oo\\no ooo\\no o o oo\\noo\\no oo\\noo\\no o o o o ooo oo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\no oo\\no o o o o o o o oo\\no ooo\\no o o oo\\noo\\no oo\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='o o o o o ooo oo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\n0 20 60 100\\n0 20 60 100pgg45\\nFIGURE 1.1. Scatterplot matrix of the prostate cancer data. The ﬁrst row s hows\\nthe response against each of the predictors in turn. Two of th e predictors, sviand\\ngleason, are categorical.\\nFor this problem not all errors are equal; we want to avoid ﬁlt ering out\\ngood email, while letting spam get through is not desirable b ut less serious', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='in its consequences. We discuss a number of diﬀerent methods for tackling\\nthis learning problem in the book.\\nExample 2: Prostate Cancer\\nThe data for this example, displayed in Figure 1.11, come from a study\\nby Stamey et al. (1989) that examined the correlation betwee n the level of\\n1There was an error in these data in the ﬁrst edition of this book. Subj ect 32 had\\na value of 6.1 for lweight, which translates to a 449 gm prostate! The correct value is', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='44.9 gm. We are grateful to Prof. Stephen W. Link for alerting us to this error.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='4 1. Introduction\\nFIGURE 1.2. Examples of handwritten digits from U.S. postal envelopes.\\nprostate speciﬁc antigen (PSA) and a number of clinical meas ures, in 97\\nmen who were about to receive a radical prostatectomy.\\nThe goal is to predict the log of PSA ( lpsa) from a number of measure-\\nments including log cancer volume ( lcavol), log prostate weight lweight,\\nage, log of benign prostatic hyperplasia amount lbph, seminal vesicle in-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 22}),\n",
       " Document(page_content='vasionsvi, log of capsular penetration lcp, Gleason score gleason, and\\npercent of Gleason scores 4 or 5 pgg45. Figure 1.1 is a scatterplot matrix\\nof the variables. Some correlations with lpsaare evident, but a good pre-\\ndictive model is diﬃcult to construct by eye.\\nThis is a supervised learning problem, known as a regression problem ,\\nbecause the outcome measurement is quantitative.\\nExample 3: Handwritten Digit Recognition\\nThe data from this example come from the handwritten ZIP code s on', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 22}),\n",
       " Document(page_content='envelopes from U.S. postal mail. Each image is a segment from a ﬁve digit\\nZIP code, isolating a single digit. The images are 16 ×16 eight-bit grayscale\\nmaps, with each pixel ranging in intensity from 0 to 255. Some sample\\nimages are shown in Figure 1.2.\\nThe images have been normalized to have approximately the sa me size\\nand orientation. The task is to predict, from the 16 ×16 matrix of pixel\\nintensities, the identity of each image (0 ,1,...,9) quickly and accurately. If', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 22}),\n",
       " Document(page_content='it is accurate enough, the resulting algorithm would be used as part of an\\nautomatic sorting procedure for envelopes. This is a classi ﬁcation problem\\nfor which the error rate needs to be kept very low to avoid misd irection of', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 22}),\n",
       " Document(page_content='1. Introduction 5\\nmail. In order to achieve this low error rate, some objects ca n be assigned\\nto a “don’t know” category, and sorted instead by hand.\\nExample 4: DNA Expression Microarrays\\nDNA stands for deoxyribonucleic acid, and is the basic mater ial that makes\\nup human chromosomes. DNA microarrays measure the expressi on of a\\ngene in a cell by measuring the amount of mRNA (messenger ribo nucleic\\nacid) present for that gene. Microarrays are considered a br eakthrough', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 23}),\n",
       " Document(page_content='technology in biology, facilitating the quantitative stud y of thousands of\\ngenes simultaneously from a single sample of cells.\\nHere is how a DNA microarray works. The nucleotide sequences for a few\\nthousand genes are printed on a glass slide. A target sample a nd a reference\\nsample are labeled with red and green dyes, and each are hybri dized with\\nthe DNA on the slide. Through ﬂuoroscopy, the log (red/green ) intensities', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 23}),\n",
       " Document(page_content='of RNA hybridizing at each site is measured. The result is a fe w thousand\\nnumbers, typically ranging from say −6to6, measuring the expressionlevel\\nof each gene in the target relative to the reference sample. P ositive values\\nindicate higher expression in the target versus the referen ce, and vice versa\\nfor negative values.\\nA gene expression dataset collects together the expression values from a\\nseries of DNA microarray experiments, with each column repr esenting an', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 23}),\n",
       " Document(page_content='experiment.Therearethereforeseveralthousandrowsrepr esentingindivid-\\nual genes, and tens of columns representing samples: in the p articular ex-\\nample of Figure 1.3 there are 6830 genes (rows) and 64 samples (columns),\\nalthough for clarity only a random sample of 100 rows are show n. The ﬁg-\\nure displays the data set as a heat map, ranging from green (ne gative) to\\nred (positive). The samples are 64 cancer tumors from diﬀere nt patients.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 23}),\n",
       " Document(page_content='The challenge here is to understand how the genes and samples are or-\\nganized. Typical questions include the following:\\n(a) which samples are most similar to each other, in terms of t heir expres-\\nsion proﬁles across genes?\\n(b) which genes are most similar to each other, in terms of the ir expression\\nproﬁles across samples?\\n(c) do certain genes show very high (or low) expression for ce rtain cancer\\nsamples?\\nWe could view this task as a regression problem, with two cate gorical', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 23}),\n",
       " Document(page_content='predictor variables—genes and samples—with the response var iable being\\nthe level of expression. However, it is probably more useful to view it as\\nunsupervised learning problem. For example, for question (a) above, we\\nthink of the samples as points in 6830–dimensional space, wh ich we want\\ntoclustertogether in some way.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 23}),\n",
       " Document(page_content='6 1. Introduction', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 24}),\n",
       " Document(page_content='SID42354SID31984SID301902SIDW128368SID375990SID360097SIDW325120ESTsChr.10SIDW365099SID377133SID381508SIDW308182SID380265SIDW321925ESTsChr.15SIDW362471SIDW417270SIDW298052SID381079SIDW428642TUPLE1TUP1ERLUMENSIDW416621SID43609ESTsSID52979SIDW357197SIDW366311ESTsSMALLNUCSIDW486740ESTsSID297905SID485148SID284853ESTsChr.15SID200394SIDW322806ESTsChr.2SIDW257915SID46536SIDW488221ESTsChr.5SID280066SIDW376394ESTsChr.15SIDW321854WASWiskottHYPOTHETICALSIDW376776SIDW205716SID239012SIDW203464HLACLASSISIDW51', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 24}),\n",
       " Document(page_content='0534SIDW279664SIDW201620SID297117SID377419SID114241ESTsCh31SIDW376928SIDW310141SIDW298203PTPRCSID289414SID127504ESTsChr.3SID305167SID488017SIDW296310ESTsChr.6SID47116MITOCHONDRIAL60ChrSIDW376586HomosapiensSIDW487261SIDW470459SID167117SIDW31489SID375812DNAPOLYMERSID377451ESTsChr.1MYBPROTOSID471915ESTsSIDW469884HumanmRNASIDW377402ESTsSID207172RASGTPASESID325394H.sapiensmRNAGNALSID73161SIDW380102SIDW299104BREAST', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 24}),\n",
       " Document(page_content='RENAL\\nMELANOMAMELANOMA\\nMCF7D-repro\\nCOLONCOLON\\nK562B-repro\\nCOLON\\nNSCLC\\nLEUKEMIA\\nRENAL\\nMELANOMA\\nBREAST\\nCNSCNS\\nRENAL\\nMCF7A-repro\\nNSCLC\\nK562A-repro\\nCOLON\\nCNS\\nNSCLCNSCLC\\nLEUKEMIA\\nCNS\\nOVARIAN\\nBREAST\\nLEUKEMIA\\nMELANOMAMELANOMA\\nOVARIANOVARIAN\\nNSCLC\\nRENAL\\nBREAST\\nMELANOMA\\nOVARIANOVARIAN\\nNSCLC\\nRENAL\\nBREAST\\nMELANOMA\\nLEUKEMIA\\nCOLON\\nBREAST\\nLEUKEMIA\\nCOLON\\nCNS\\nMELANOMA\\nNSCLC\\nPROSTATE\\nNSCLC\\nRENALRENAL\\nNSCLC\\nRENAL\\nLEUKEMIA\\nOVARIAN\\nPROSTATE\\nCOLON\\nBREAST\\nRENAL', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 24}),\n",
       " Document(page_content='UNKNOWNFIGURE 1.3. DNA microarray data: expression matrix of 6830genes (rows)\\nand64samples (columns), for the human tumor data. Only a random sample\\nof100rows are shown. The display is a heat map, ranging from bright g reen\\n(negative, under expressed) to bright red (positive, over e xpressed). Missing values\\nare gray. The rows and columns are displayed in a randomly chosen order.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 24}),\n",
       " Document(page_content='1. Introduction 7\\nWho Should Read this Book\\nThis book is designed for researchers and students in a broad variety of\\nﬁelds: statistics, artiﬁcial intelligence, engineering, ﬁnance and others. We\\nexpect that the reader will have had at least one elementary c ourse in\\nstatistics, covering basic topics including linear regres sion.\\nWe have not attempted to write a comprehensive catalog of lea rning\\nmethods, but rather to describe some of the most important te chniques.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 25}),\n",
       " Document(page_content='Equally notable, we describe the underlying concepts and co nsiderations\\nby which a researcher can judge a learning method. We have tri ed to write\\nthis book in an intuitive fashion, emphasizing concepts rat her than math-\\nematical details.\\nAsstatisticians,ourexpositionwillnaturallyreﬂectour backgroundsand\\nareas of expertise. However in the past eight years we have be en attending\\nconferences in neural networks, data mining and machine lea rning, and our', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 25}),\n",
       " Document(page_content='thinking has been heavily inﬂuenced by these exciting ﬁelds . This inﬂuence\\nis evident in our current research, and in this book.\\nHow This Book is Organized\\nOur view is that one must understand simple methods before tr ying to\\ngrasp more complex ones. Hence, after giving an overview of t he supervis-\\ning learning problem in Chapter 2 , we discuss linear methods for regression\\nand classiﬁcation in Chapters 3 and4. InChapter 5 we describe splines,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 25}),\n",
       " Document(page_content='wavelets and regularization/penalization methods for a si ngle predictor,\\nwhileChapter 6 covers kernel methods and local regression. Both of these\\nsets of methods are important building blocks for high-dime nsional learn-\\ning techniques. Model assessment and selection is the topic ofChapter 7 ,\\ncovering the concepts of bias and variance, overﬁtting and m ethods such as\\ncross-validation for choosing models. Chapter 8 discusses model inference', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 25}),\n",
       " Document(page_content='and averaging, including an overview of maximum likelihood , Bayesian in-\\nference and the bootstrap, the EM algorithm, Gibbs sampling and bagging,\\nA related procedure called boosting is the focus of Chapter 10 .\\nInChapters 9–13 we describe a series of structured methods for su-\\npervised learning, with Chapters 9 and 11 covering regression and Chap-\\nters 12 and 13 focusing on classiﬁcation. Chapter 14 describes methods for\\nunsupervised learning. Two recently proposed techniques, random forests', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 25}),\n",
       " Document(page_content='and ensemble learning, are discussed in Chapters 15 and 16 . We describe\\nundirected graphical models in Chapter 17 and ﬁnally we study high-\\ndimensional problems in Chapter 18 .\\nAt the end of each chapter we discuss computational considerations im-\\nportant for data mining applications, including how the com putations scale\\nwith the number of observations and predictors. Each chapte r ends with\\nBibliographic Notes giving background references for the material.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 25}),\n",
       " Document(page_content='8 1. Introduction\\nWe recommend that Chapters 1–4 be ﬁrst read in sequence. Chap ter 7\\nshould also be considered mandatory, as it covers central co ncepts that\\npertain to all learning methods. With this in mind, the rest o f the book\\ncan be read sequentially, or sampled, depending on the reade r’s interest.\\nThe symbol\\n indicates a technically diﬃcult section, one that can\\nbe skipped without interrupting the ﬂow of the discussion.\\nBook Website\\nThe website for this book is located at', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 26}),\n",
       " Document(page_content='http://www-stat.stanford.edu/ElemStatLearn\\nIt contains a number of resources, including many of the data sets used in\\nthis book.\\nNote for Instructors\\nWe have successively used the ﬁrst edition of this book as the basis for a\\ntwo-quartercourse,andwiththeadditionalmaterialsinth issecondedition,\\nitcouldevenbeusedforathree-quartersequence.Exercise sareprovidedat\\nthe end of each chapter. It is important for students to have a ccess to good', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 26}),\n",
       " Document(page_content='software tools for these topics. We used the R and S-PLUS prog ramming\\nlanguages in our courses.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 26}),\n",
       " Document(page_content='This is page 9\\nPrinter: Opaque this\\n2\\nOverview of Supervised Learning\\n2.1 Introduction\\nThe ﬁrst three examples described in Chapter 1 have several c omponents\\nin common. For each there is a set of variables that might be de noted as\\ninputs, which are measured or preset. These have some inﬂuence on on e or\\nmoreoutputs. For each example the goal is to use the inputs to predict the\\nvalues of the outputs. This exercise is called supervised learning .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 27}),\n",
       " Document(page_content='We have used the more modern language of machine learning. In the\\nstatistical literature the inputs are often called the predictors , a term we\\nwill use interchangeably with inputs, and more classically theindependent\\nvariables .Inthepatternrecognitionliteraturetheterm featuresispreferred,\\nwhich we use as well. The outputs are called the responses , or classically\\nthedependent variables .\\n2.2 Variable Types and Terminology\\nThe outputs vary in nature among the examples. In the glucose prediction', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 27}),\n",
       " Document(page_content='example, the output is a quantitative measurement, where some measure-\\nments are bigger than others, and measurements close in valu e are close\\nin nature. In the famous Iris discrimination example due to R . A. Fisher,\\nthe output is qualitative (species of Iris) and assumes values in a ﬁnite set\\nG={Virginica ,SetosaandVersicolor}. In the handwritten digit example\\nthe output is one of 10 diﬀerent digit classes:G={0,1,...,9}. In both of', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 27}),\n",
       " Document(page_content='10 2. Overview of Supervised Learning\\nthese there is no explicit ordering in the classes, and in fac t often descrip-\\ntive labels rather than numbers are used to denote the classe s. Qualitative\\nvariables are also referred to as categorical ordiscretevariables as well as\\nfactors.\\nFor both types of outputs it makes sense to think of using the i nputs to\\npredict the output. Given some speciﬁc atmospheric measure ments today\\nand yesterday, we want to predict the ozone level tomorrow. G iven the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 28}),\n",
       " Document(page_content='grayscale values for the pixels of the digitized image of the handwritten\\ndigit, we want to predict its class label.\\nThis distinction in output type has led to a naming conventio n for the\\nprediction tasks: regression when we predict quantitative outputs, and clas-\\nsiﬁcation when we predict qualitative outputs. We will see that these t wo\\ntasks have a lot in common, and in particular both can be viewe d as a task\\nin function approximation.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 28}),\n",
       " Document(page_content='Inputs also vary in measurement type; we can have some of each of qual-\\nitative and quantitative input variables. These have also l ed to distinctions\\nin the types of methods that are used for prediction: some met hods are\\ndeﬁned most naturally for quantitative inputs, some most na turally for\\nqualitative and some for both.\\nA third variable type is ordered categorical , such as small, medium and\\nlarge, where there is an ordering between the values, but no metric notion', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 28}),\n",
       " Document(page_content='is appropriate (the diﬀerence between medium and small need not be the\\nsame as that between large and medium). These are discussed f urther in\\nChapter 4.\\nQualitative variables are typically represented numerica lly by codes. The\\neasiest case is when there are only two classes or categories , such as “suc-\\ncess” or “failure,” “survived” or “died.” These are often re presented by a\\nsingle binary digit or bit as 0 or 1, or else by −1 and 1. For reasons that will', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 28}),\n",
       " Document(page_content='become apparent, such numeric codes are sometimes referred to astargets.\\nWhen there are more than two categories, several alternativ es are available.\\nThe most useful and commonly used coding is via dummy variables . Here a\\nK-level qualitative variable is represented by a vector of Kbinary variables\\nor bits, only one of which is “on” at a time. Although more comp act coding\\nschemes are possible, dummy variables are symmetric in the l evels of the\\nfactor.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 28}),\n",
       " Document(page_content='We will typically denote an input variable by the symbol X. IfXis\\na vector, its components can be accessed by subscripts Xj. Quantitative\\noutputs will be denoted by Y, and qualitative outputs by G(for group).\\nWe use uppercase letters such as X,YorGwhen referring to the generic\\naspects of a variable. Observed values are written in lowerc ase; hence the\\nith observed value of Xis written as xi(wherexiis again a scalar or\\nvector). Matrices are represented by bold uppercase letter s; for example, a', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 28}),\n",
       " Document(page_content='set ofNinputp-vectorsxi, i= 1,...,Nwould be represented by the N×p\\nmatrixX. In general, vectors will not be bold, except when they have N\\ncomponents; this convention distinguishes a p-vector of inputs xifor the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 28}),\n",
       " Document(page_content='2.3 Least Squares and Nearest Neighbors 11\\nith observation from the N-vectorxjconsisting of all the observations on\\nvariableXj. Since all vectors are assumed to be column vectors, the ith\\nrow ofXisxT\\ni, the vector transpose of xi.\\nFor the moment we can loosely state the learning task as follo ws: given\\nthe value of an input vector X, make a good prediction of the output Y,\\ndenoted by ˆY(pronounced “y-hat”). If Ytakes values in IR then so should', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 29}),\n",
       " Document(page_content='ˆY; likewise for categorical outputs, ˆGshould take values in the same set G\\nassociated with G.\\nFor a two-class G, one approach is to denote the binary coded target\\nasY, and then treat it as a quantitative output. The predictions ˆYwill\\ntypically lie in [0 ,1], and we can assign to ˆGthe class label according to\\nwhether ˆy >0.5. This approach generalizes to K-level qualitative outputs\\nas well.\\nWe need data to construct prediction rules, often a lot of it. We thus', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 29}),\n",
       " Document(page_content='suppose we have available a set of measurements ( xi,yi) or (xi,gi), i=\\n1,...,N,knownasthe training data ,withwhichtoconstructourprediction\\nrule.\\n2.3 Two Simple Approaches to Prediction: Least\\nSquares and Nearest Neighbors\\nIn this section we develop two simple but powerful predictio n methods: the\\nlinear model ﬁt by least squares and the k-nearest-neighbor prediction rule.\\nThelinearmodelmakeshugeassumptionsaboutstructureand yieldsstable', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 29}),\n",
       " Document(page_content='but possibly inaccurate predictions. The method of k-nearest neighbors\\nmakes very mild structural assumptions: its predictions ar e often accurate\\nbut can be unstable.\\n2.3.1 Linear Models and Least Squares\\nThe linear model has been a mainstay of statistics for the pas t 30 years\\nand remains one of our most important tools. Given a vector of inputs\\nXT= (X1,X2,...,X p), we predict the output Yvia the model\\nˆY=ˆβ0+p∑\\nj=1Xjˆβj. (2.1)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 29}),\n",
       " Document(page_content='The term ˆβ0is the intercept, also known as the biasin machine learning.\\nOften it is convenient to include the constant variable 1 in X, include ˆβ0in\\nthe vector of coeﬃcients ˆβ, and then write the linear model in vector form\\nas an inner product\\nˆY=XTˆβ, (2.2)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 29}),\n",
       " Document(page_content='12 2. Overview of Supervised Learning\\nwhereXTdenotes vector or matrix transpose ( Xbeing a column vector).\\nHere we are modeling a single output, so ˆYis a scalar; in general ˆYcan be\\naK–vector, in which case βwould be a p×Kmatrix of coeﬃcients. In the\\n(p+ 1)-dimensional input–output space, ( X,ˆY) represents a hyperplane.\\nIf the constant is included in X, then the hyperplane includes the origin\\nand is a subspace; if not, it is an aﬃne set cutting the Y-axis at the point', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 30}),\n",
       " Document(page_content='(0,ˆβ0). From now on we assume that the intercept is included in ˆβ.\\nViewed as a function over the p-dimensional input space, f(X) =XTβ\\nis linear, and the gradient f′(X) =βis a vector in input space that points\\nin the steepest uphill direction.\\nHow do we ﬁt the linear model to a set of training data? There ar e\\nmany diﬀerent methods, but by far the most popular is the meth od of\\nleast squares . In this approach, we pick the coeﬃcients βto minimize the\\nresidual sum of squares\\nRSS(β) =N∑', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 30}),\n",
       " Document(page_content='i=1(yi−xT\\niβ)2. (2.3)\\nRSS(β) is a quadratic function of the parameters, and hence its min imum\\nalways exists, but may not be unique. The solution is easiest to characterize\\nin matrix notation. We can write\\nRSS(β) = (y−Xβ)T(y−Xβ), (2.4)\\nwhereXis anN×pmatrix with each row an input vector, and yis an\\nN-vector of the outputs in the training set. Diﬀerentiating w .r.t.βwe get\\nthenormal equations\\nXT(y−Xβ) = 0. (2.5)\\nIfXTXis nonsingular, then the unique solution is given by\\nˆβ= (XTX)−1XTy, (2.6)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 30}),\n",
       " Document(page_content='and the ﬁtted value at the ith inputxiis ˆyi= ˆy(xi) =xT\\niˆβ. At an arbi-\\ntrary input x0the prediction is ˆ y(x0) =xT\\n0ˆβ. The entire ﬁtted surface is\\ncharacterized by the pparameters ˆβ. Intuitively, it seems that we do not\\nneed a very large data set to ﬁt such a model.\\nLet’s look at an example of the linear model in a classiﬁcatio n context.\\nFigure 2.1 shows a scatterplot of training data on a pair of in putsX1and\\nX2. The data are simulated, and for the present the simulation m odel is', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 30}),\n",
       " Document(page_content='not important. The output class variable Ghas the values BLUEorORANGE,\\nand is represented as such in the scatterplot. There are 100 p oints in each\\nof the two classes. The linear regression model was ﬁt to thes e data, with\\nthe response Ycoded as 0 for BLUEand 1 for ORANGE. The ﬁtted values ˆY\\nare converted to a ﬁtted class variable ˆGaccording to the rule\\nˆG={\\nORANGE ifˆY >0.5,\\nBLUE ifˆY≤0.5.(2.7)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 30}),\n",
       " Document(page_content='2.3 Least Squares and Nearest Neighbors 13\\nLinear Regression of 0/1 Response', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='.. . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . .. .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nFIGURE 2.1. A classiﬁcation example in two dimensions. The classes are code d\\nas a binary variable ( BLUE= 0,ORANGE= 1), and then ﬁt by linear regression.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='The line is the decision boundary deﬁned by xTˆβ= 0.5. The orange shaded region\\ndenotes that part of input space classiﬁed as ORANGE, while the blue region is\\nclassiﬁed as BLUE.\\nThe set of points in IR2classiﬁed as ORANGEcorresponds to{x:xTˆβ >0.5},\\nindicated in Figure 2.1, and the two predicted classes are se parated by the\\ndecision boundary {x:xTˆβ= 0.5}, which is linear in this case. We see\\nthat for these data there are several misclassiﬁcations on b oth sides of the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='decisionboundary.Perhapsourlinearmodelistoorigid—or aresucherrors\\nunavoidable? Remember that these are errors on the training data itself,\\nand we have not said where the constructed data came from. Con sider the\\ntwo possible scenarios:\\nScenario 1: The training data in each class were generated from bivariat e\\nGaussian distributions with uncorrelated components and d iﬀerent\\nmeans.\\nScenario 2: Thetrainingdataineachclasscamefromamixtureof10low-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='variance Gaussian distributions, with individual means th emselves\\ndistributed as Gaussian.\\nA mixture of Gaussians is best described in terms of the gener ative\\nmodel. One ﬁrst generates a discrete variable that determin es which of', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='14 2. Overview of Supervised Learning\\nthe component Gaussians to use, and then generates an observ ation from\\nthe chosen density. In the case of one Gaussian per class, we w ill see in\\nChapter 4 that a linear decision boundary is the best one can d o, and that\\nour estimate is almost optimal. The region of overlap is inev itable, and\\nfuture data to be predicted will be plagued by this overlap as well.\\nIn the case of mixtures of tightly clustered Gaussians the st ory is dif-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 32}),\n",
       " Document(page_content='ferent. A linear decision boundary is unlikely to be optimal , and in fact is\\nnot. The optimal decision boundary is nonlinear and disjoin t, and as such\\nwill be much more diﬃcult to obtain.\\nWe now look at another classiﬁcation and regression procedu re that is\\nin some sense at the opposite end of the spectrum to the linear model, and\\nfar better suited to the second scenario.\\n2.3.2 Nearest-Neighbor Methods\\nNearest-neighbor methods use those observations in the tra ining setTclos-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 32}),\n",
       " Document(page_content='est in input space to xto form ˆY. Speciﬁcally, the k-nearest neighbor ﬁt\\nforˆYis deﬁned as follows:\\nˆY(x) =1\\nk∑\\nxi∈Nk(x)yi, (2.8)\\nwhereNk(x) is the neighborhood of xdeﬁned by the kclosest points xiin\\nthe training sample. Closeness implies a metric, which for t he moment we\\nassume is Euclidean distance. So, in words, we ﬁnd the kobservations with\\nxiclosest toxin input space, and average their responses.\\nIn Figure 2.2 we use the same training data as in Figure 2.1, an d use', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 32}),\n",
       " Document(page_content='15-nearest-neighbor averaging of the binary coded respons e as the method\\nof ﬁtting. Thus ˆYis the proportion of ORANGE’s in the neighborhood, and\\nso assigning class ORANGEtoˆGifˆY >0.5 amounts to a majority vote in\\nthe neighborhood. The colored regions indicate all those po ints in input\\nspace classiﬁed as BLUEorORANGEby such a rule, in this case found by\\nevaluating the procedure on a ﬁne grid in input space. We see t hat the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 32}),\n",
       " Document(page_content='decision boundaries that separate the BLUEfrom the ORANGEregions are far\\nmore irregular, and respond to local clusters where one clas s dominates.\\nFigure 2.3 shows the results for 1-nearest-neighbor classi ﬁcation: ˆYis\\nassigned the value yℓof the closest point xℓtoxin the training data. In\\nthis case the regions of classiﬁcation can be computed relat ively easily, and\\ncorrespond to a Voronoi tessellation of the training data. Each point xi', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 32}),\n",
       " Document(page_content='has an associated tile bounding the region for which it is the closest input\\npoint. For all points xin the tile, ˆG(x) =gi. The decision boundary is even\\nmore irregular than before.\\nThe method of k-nearest-neighbor averaging is deﬁned in exactly the\\nsame way for regression of a quantitative output Y, althoughk= 1 would\\nbe an unlikely choice.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 32}),\n",
       " Document(page_content='2.3 Least Squares and Nearest Neighbors 15\\n15-Nearest Neighbor Classifier', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='... .. .. .. .. . .. . .. . .. . . . .. . . . . .. . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nFIGURE 2.2. The same classiﬁcation example in two dimensions as in Fig-\\nure 2.1. The classes are coded as a binary variable (BLUE= 0,ORANGE= 1)and', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='then ﬁt by 15-nearest-neighbor averaging as in (2.8). The predicted clas s is hence\\nchosen by majority vote amongst the 15-nearest neighbors.\\nIn Figure 2.2 we see that far fewer training observations are misclassiﬁed\\nthan in Figure 2.1. This should not give us too much comfort, t hough, since\\nin Figure 2.3 noneof the training data are misclassiﬁed. A little thought\\nsuggests that for k-nearest-neighbor ﬁts, the error on the training data', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='should be approximately an increasing function of k, and will always be 0\\nfork= 1. An independent test set would give us a more satisfactory means\\nfor comparing the diﬀerent methods.\\nIt appears that k-nearest-neighbor ﬁts have a single parameter, the num-\\nber of neighbors k, compared to the pparameters in least-squares ﬁts. Al-\\nthough this is the case, we will see that the eﬀective number of parameters\\nofk-nearest neighbors is N/kand is generally bigger than p, and decreases', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='with increasing k. To get an idea of why, note that if the neighborhoods\\nwere nonoverlapping, there would be N/kneighborhoods and we would ﬁt\\none parameter (a mean) in each neighborhood.\\nIt is also clear that we cannot use sum-of-squared errors on t he training\\nset as a criterion for picking k, since we would always pick k= 1! It would\\nseem thatk-nearest-neighbor methods would be more appropriate for th e\\nmixture Scenario 2 described above, while for Gaussian data the decision', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='boundaries of k-nearest neighbors would be unnecessarily noisy.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='16 2. Overview of Supervised Learning\\n1−Nearest Neighbor Classifier\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\nooooo\\nooo\\no\\no\\nooo\\noooooo\\noo\\no\\noo\\nooo\\no\\nFIGURE 2.3. The same classiﬁcation example in two dimensions as in Fig-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 34}),\n",
       " Document(page_content='ure 2.1. The classes are coded as a binary variable (BLUE= 0,ORANGE= 1), and\\nthen predicted by 1-nearest-neighbor classiﬁcation.\\n2.3.3 From Least Squares to Nearest Neighbors\\nThe linear decision boundary from least squares is very smoo th, and ap-\\nparently stable to ﬁt. It does appear to rely heavily on the as sumption\\nthat a linear decision boundary is appropriate. In language we will develop\\nshortly, it has low variance and potentially high bias.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 34}),\n",
       " Document(page_content='On the other hand, the k-nearest-neighbor procedures do not appear to\\nrelyonanystringentassumptionsabouttheunderlyingdata ,andcanadapt\\nto any situation. However, any particular subregion of the d ecision bound-\\nary depends on a handful of input points and their particular positions,\\nand is thus wiggly and unstable—high variance and low bias.\\nEach method has its own situations for which it works best; in particular\\nlinear regression is more appropriate for Scenario 1 above, while nearest', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 34}),\n",
       " Document(page_content='neighbors are more suitable for Scenario 2. The time has come to expose\\nthe oracle! The data in fact were simulated from a model somew here be-\\ntween the two, but closer to Scenario 2. First we generated 10 meansmk\\nfrom a bivariate Gaussian distribution N((1,0)T,I) and labeled this class\\nBLUE. Similarly, 10 more were drawn from N((0,1)T,I) and labeled class\\nORANGE. Then for each class we generated 100 observations as follow s: for', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 34}),\n",
       " Document(page_content='each observation, we picked an mkat random with probability 1 /10, and', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 34}),\n",
       " Document(page_content='2.3 Least Squares and Nearest Neighbors 17\\nDegrees of Freedom − N/kTest Error\\n0.10 0.15 0.20 0.25 0.30\\n  2   3   5   8  12  18  29  67 200151 101  69  45  31  21  11   7   5   3   1\\nTrain\\nTest\\nBayesk −  Number of Nearest Neighbors\\nLinear\\nFIGURE 2.4. Misclassiﬁcation curves for the simulation example used in Fig -\\nures 2.1, 2.2 and 2.3. A single training sample of size 200was used, and a test\\nsample of size 10,000. The orange curves are test and the blue are training er-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 35}),\n",
       " Document(page_content='ror fork-nearest-neighbor classiﬁcation. The results for linear regr ession are the\\nbigger orange and blue squares at three degrees of freedom. Th e purple line is the\\noptimal Bayes error rate.\\nthen generated a N(mk,I/5), thus leading to a mixture of Gaussian clus-\\nters for each class. Figure 2.4 shows the results of classify ing 10,000 new\\nobservations generated from the model. We compare the resul ts for least\\nsquares and those for k-nearest neighbors for a range of values of k.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 35}),\n",
       " Document(page_content='A large subset of the most popular techniques in use today are variants of\\nthese two simple procedures. In fact 1-nearest-neighbor, t he simplest of all,\\ncaptures a large percentage of the market for low-dimension al problems.\\nThe following list describes some ways in which these simple procedures\\nhave been enhanced:\\n•Kernel methods use weights that decrease smoothly to zero wi th dis-\\ntancefromthetargetpoint,ratherthantheeﬀective0 /1weightsused\\nbyk-nearest neighbors.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 35}),\n",
       " Document(page_content='•In high-dimensional spaces the distance kernels are modiﬁe d to em-\\nphasize some variable more than others.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 35}),\n",
       " Document(page_content='18 2. Overview of Supervised Learning\\n•Local regression ﬁts linear models by locally weighted leas t squares,\\nrather than ﬁtting constants locally.\\n•Linear models ﬁt to a basis expansion of the original inputs a llow\\narbitrarily complex models.\\n•Projection pursuit and neural network models consist of sum s of non-\\nlinearly transformed linear models.\\n2.4 Statistical Decision Theory\\nIn this section we develop a small amount of theory that provi des a frame-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 36}),\n",
       " Document(page_content='work for developing models such as those discussed informal ly so far. We\\nﬁrst consider the case of a quantitative output, and place ou rselves in the\\nworld of random variables and probability spaces. Let X∈IRpdenote a\\nreal valued random input vector, and Y∈IR a real valued random out-\\nput variable, with joint distribution Pr( X,Y). We seek a function f(X)\\nfor predicting Ygiven values of the input X. This theory requires a loss', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 36}),\n",
       " Document(page_content='functionL(Y,f(X)) for penalizing errors in prediction, and by far the most\\ncommon and convenient is squared error loss :L(Y,f(X)) = (Y−f(X))2.\\nThis leads us to a criterion for choosing f,\\nEPE(f) = E(Y−f(X))2(2.9)\\n=∫\\n[y−f(x)]2Pr(dx,dy), (2.10)\\nthe expected (squared) prediction error . By conditioning1onX, we can\\nwrite EPE as\\nEPE(f) = EXEY|X(\\n[Y−f(X)]2|X)\\n(2.11)\\nand we see that it suﬃces to minimize EPE pointwise:\\nf(x) = argmincEY|X(\\n[Y−c]2|X=x)\\n. (2.12)\\nThe solution is\\nf(x) = E(Y|X=x), (2.13)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 36}),\n",
       " Document(page_content='the conditional expectation, also known as the regression function. Thus\\nthe best prediction of Yat any point X=xis the conditional mean, when\\nbest is measured by average squared error.\\nThe nearest-neighbor methods attempt to directly implemen t this recipe\\nusing the training data. At each point x, we might ask for the average of all\\n1Conditioning here amounts to factoring the joint density Pr( X,Y) = Pr(Y|X)Pr(X)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 36}),\n",
       " Document(page_content='where Pr( Y|X) = Pr(Y,X)/Pr(X), and splitting up the bivariate integral accordingly.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 36}),\n",
       " Document(page_content='2.4 Statistical Decision Theory 19\\nthoseyis with input xi=x. Since there is typically at most one observation\\nat any point x, we settle for\\nˆf(x) = Ave(yi|xi∈Nk(x)), (2.14)\\nwhere “Ave” denotes average, and Nk(x) is the neighborhood containing\\nthekpoints inTclosest tox. Two approximations are happening here:\\n•expectation is approximated by averaging over sample data;\\n•conditioning at a point is relaxed to conditioning on some re gion\\n“close” to the target point.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 37}),\n",
       " Document(page_content='For large training sample size N, the points in the neighborhood are likely\\nto be close to x, and askgets large the average will get more stable.\\nIn fact, under mild regularity conditions on the joint proba bility distri-\\nbution Pr(X,Y), one can show that as N,k→∞such thatk/N→0,\\nˆf(x)→E(Y|X=x). In light of this, why look further, since it seems\\nwe have a universal approximator? We often do not have very la rge sam-\\nples. If the linear or some more structured model is appropri ate, then we', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 37}),\n",
       " Document(page_content='can usually get a more stable estimate than k-nearest neighbors, although\\nsuch knowledge has to be learned from the data as well. There a re other\\nproblems though, sometimes disastrous. In Section 2.5 we se e that as the\\ndimensionpgets large, so does the metric size of the k-nearest neighbor-\\nhood. So settling for nearest neighborhood as a surrogate fo r conditioning\\nwill fail us miserably. The convergence above still holds, b ut therateof\\nconvergence decreases as the dimension increases.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 37}),\n",
       " Document(page_content='How does linear regressionﬁt intothis framework? Thesimpl est explana-\\ntion is that one assumes that the regression function f(x) is approximately\\nlinear in its arguments:\\nf(x)≈xTβ. (2.15)\\nThisisamodel-basedapproach—wespecifyamodelfortheregr essionfunc-\\ntion. Plugging this linear model for f(x) into EPE (2.9) and diﬀerentiating\\nwe can solve for βtheoretically:\\nβ= [E(XXT)]−1E(XY). (2.16)\\nNote we have notconditioned on X; rather we have used our knowledge', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 37}),\n",
       " Document(page_content='of the functional relationship to poolover values of X. The least squares\\nsolution (2.6) amounts to replacing the expectation in (2.1 6) by averages\\nover the training data.\\nSo bothk-nearest neighbors and least squares end up approximating\\nconditional expectations by averages. But they diﬀer drama tically in terms\\nof model assumptions:\\n•Least squares assumes f(x) is well approximated by a globally linear\\nfunction.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 37}),\n",
       " Document(page_content='20 2. Overview of Supervised Learning\\n•k-nearest neighbors assumes f(x) is well approximated by a locally\\nconstant function.\\nAlthough the latter seems more palatable, we have already se en that we\\nmay pay a price for this ﬂexibility.\\nMany of the more modern techniques described in this book are model\\nbased, although far more ﬂexible than the rigid linear model . For example,\\nadditive models assume that\\nf(X) =p∑\\nj=1fj(Xj). (2.17)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 38}),\n",
       " Document(page_content='Thisretainstheadditivity ofthelinearmodel,buteachcoo rdinatefunction\\nfjis arbitrary. It turns out that the optimal estimate for the a dditive model\\nuses techniques such as k-nearest neighbors to approximate univariate con-\\nditional expectations simultaneously for each of the coordinate functions.\\nThus the problems of estimating a conditional expectation i n high dimen-\\nsionsaresweptawayinthiscasebyimposingsome(oftenunre alistic)model\\nassumptions, in this case additivity.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 38}),\n",
       " Document(page_content='Are we happy with the criterion (2.11)? What happens if we rep lace the\\nL2loss function with the L1: E|Y−f(X)|? The solution in this case is the\\nconditional median,\\nˆf(x) = median( Y|X=x), (2.18)\\nwhich is a diﬀerent measure of location, and its estimates ar e more robust\\nthan those for the conditional mean. L1criteria have discontinuities in\\ntheir derivatives, which have hindered their widespread us e. Other more\\nresistant loss functions will be mentioned in later chapter s, but squared', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 38}),\n",
       " Document(page_content='error is analytically convenient and the most popular.\\nWhat do we do when the output is a categorical variable G? The same\\nparadigm works here, except we need a diﬀerent loss function for penalizing\\nprediction errors. An estimate ˆGwill assume values in G, the set of possible\\nclasses. Our loss function can be represented by a K×KmatrixL, where\\nK= card(G).Lwill be zero on the diagonal and nonnegative elsewhere,\\nwhereL(k,ℓ) is the price paid for classifying an observation belonging to', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 38}),\n",
       " Document(page_content='classGkasGℓ. Most often we use the zero–one loss function, where all\\nmisclassiﬁcations are charged a single unit. The expected p rediction error\\nis\\nEPE = E[L(G,ˆG(X))], (2.19)\\nwhere again the expectation is taken with respect to the join t distribution\\nPr(G,X). Again we condition, and can write EPE as\\nEPE = E XK∑\\nk=1L[Gk,ˆG(X)]Pr(Gk|X) (2.20)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 38}),\n",
       " Document(page_content='2.4 Statistical Decision Theory 21\\nBayes Optimal Classifier', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='... .. . . .. . . . .. . . . . .. . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='.. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nFIGURE 2.5. The optimal Bayes decision boundary for the simulation examp le\\nof Figures 2.1, 2.2 and 2.3. Since the generating density is k nown for each class,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='this boundary can be calculated exactly (Exercise 2.2).\\nand again it suﬃces to minimize EPE pointwise:\\nˆG(x) = argming∈GK∑\\nk=1L(Gk,g)Pr(Gk|X=x). (2.21)\\nWith the 0–1 loss function this simpliﬁes to\\nˆG(x) = argming∈G[1−Pr(g|X=x)] (2.22)\\nor simply\\nˆG(x) =Gkif Pr(Gk|X=x) = max\\ng∈GPr(g|X=x).(2.23)\\nThis reasonable solution is known as the Bayes classiﬁer , and says that\\nwe classify to the most probable class, using the conditiona l (discrete) dis-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='tribution Pr( G|X). Figure 2.5 shows the Bayes-optimal decision boundary\\nfor our simulation example. The error rate of the Bayes class iﬁer is called\\ntheBayes rate .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='22 2. Overview of Supervised Learning\\nAgain we see that the k-nearest neighbor classiﬁer directly approximates\\nthis solution—a majority vote in a nearest neighborhood amou nts to ex-\\nactly this, except that conditional probability at a point i s relaxed to con-\\nditional probability within a neighborhood of a point, and p robabilities are\\nestimated by training-sample proportions.\\nSuppose for a two-class problem we had taken the dummy-varia ble ap-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 40}),\n",
       " Document(page_content='proach and coded Gvia a binary Y, followed by squared error loss estima-\\ntion. Then ˆf(X) = E(Y|X) = Pr(G=G1|X) ifG1corresponded to Y= 1.\\nLikewise for a K-class problem, E( Yk|X) = Pr(G=Gk|X). This shows\\nthat our dummy-variable regression procedure, followed by classiﬁcation to\\nthe largest ﬁtted value, is another way of representing the B ayes classiﬁer.\\nAlthough this theory is exact, in practice problems can occu r, depending', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 40}),\n",
       " Document(page_content='on the regression model used. For example, when linear regre ssion is used,\\nˆf(X) need not be positive, and we might be suspicious about using it as\\nan estimate of a probability. We will discuss a variety of app roaches to\\nmodeling Pr( G|X) in Chapter 4.\\n2.5 Local Methods in High Dimensions\\nWe have examined two learning techniques for prediction so f ar: the stable\\nbut biased linear model and the less stable but apparently le ss biased class', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 40}),\n",
       " Document(page_content='ofk-nearest-neighbor estimates. It would seem that with a reas onably large\\nset of training data, we could always approximate the theore tically optimal\\nconditional expectation by k-nearest-neighbor averaging, since we should\\nbe able to ﬁnd a fairly large neighborhood of observations cl ose to any x\\nand average them. This approach and our intuition breaks dow n in high\\ndimensions, and the phenomenon is commonly referred to as th ecurse', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 40}),\n",
       " Document(page_content='of dimensionality (Bellman, 1961). There are many manifestations of this\\nproblem, and we will examine a few here.\\nConsiderthenearest-neighborprocedureforinputsunifor mlydistributed\\nin ap-dimensional unit hypercube, as in Figure 2.6. Suppose we se nd out a\\nhypercubical neighborhood about a target point to capture a fractionrof\\nthe observations. Since this corresponds to a fraction rof the unit volume,\\ntheexpectededgelengthwillbe ep(r) =r1/p.Intendimensions e10(0.01) =', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 40}),\n",
       " Document(page_content='0.63 ande10(0.1) = 0.80, while the entire range for each input is only 1 .0.\\nSo to capture 1% or 10% of the data to form a local average, we mu st cover\\n63% or 80% of the range of each input variable. Such neighborh oods are no\\nlonger “local.” Reducing rdramatically does not help much either, since\\nthe fewer observations we average, the higher is the varianc e of our ﬁt.\\nAnother consequence of the sparse sampling in high dimensio ns is that', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 40}),\n",
       " Document(page_content='all sample points are close to an edge of the sample. Consider Ndata points\\nuniformly distributed in a p-dimensional unit ball centered at the origin.\\nSuppose we consider a nearest-neighbor estimate at the orig in. The median', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 40}),\n",
       " Document(page_content='2.5 Local Methods in High Dimensions 23\\n1\\n10Unit Cube\\nFraction of VolumeDistance\\n0.0 0.2 0.4 0.60.0 0.2 0.4 0.6 0.8 1.0p=1p=2p=3p=10\\nNeighborhood\\nFIGURE 2.6. The curse of dimensionality is well illustrated by a subcubica l\\nneighborhood for uniform data in a unit cube. The ﬁgure on the right shows the\\nside-length of the subcube needed to capture a fraction rof the volume of the data,\\nfor diﬀerent dimensions p. In ten dimensions we need to cover 80%of the range', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 41}),\n",
       " Document(page_content='of each coordinate to capture 10%of the data.\\ndistance from the origin to the closest data point is given by the expression\\nd(p,N) =(\\n1−1\\n21/N)1/p\\n(2.24)\\n(Exercise 2.3). A more complicated expression exists for th e mean distance\\nto the closest point. For N= 500,p= 10 ,d(p,N)≈0.52, more than\\nhalfwaytotheboundary.Hencemostdatapointsarecloserto theboundary\\nof the sample space than to any other data point. The reason th at this', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 41}),\n",
       " Document(page_content='presents a problem is that prediction is much more diﬃcult ne ar the edges\\nof the training sample. One must extrapolate from neighbori ng sample\\npoints rather than interpolate between them.\\nAnother manifestation of the curse is that the sampling dens ity is pro-\\nportional to N1/p, wherepis the dimension of the input space and Nis the\\nsample size. Thus, if N1= 100 represents a dense sample for a single input\\nproblem, then N10= 10010is the sample size required for the same sam-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 41}),\n",
       " Document(page_content='pling density with 10 inputs. Thus in high dimensions all fea sible training\\nsamples sparsely populate the input space.\\nLet us construct another uniform example. Suppose we have 10 00 train-\\ning examples xigenerated uniformly on [ −1,1]p. Assume that the true\\nrelationship between XandYis\\nY=f(X) =e−8||X||2,\\nwithout any measurement error. We use the 1-nearest-neighb or rule to\\npredicty0at the test-point x0= 0. Denote the training set by T. We can', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 41}),\n",
       " Document(page_content='24 2. Overview of Supervised Learning\\ncompute the expected prediction error at x0for our procedure, averaging\\nover all such samples of size 1000. Since the problem is deter ministic, this\\nis the mean squared error (MSE) for estimating f(0):\\nMSE(x0) = E T[f(x0)−ˆy0]2\\n= ET[ˆy0−ET(ˆy0)]2+[ET(ˆy0)−f(x0)]2\\n= Var T(ˆy0)+Bias2(ˆy0). (2.25)\\nFigure 2.7 illustrates the setup. We have broken down the MSE into two\\ncomponents that will become familiar as we proceed: varianc e and squared', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 42}),\n",
       " Document(page_content='bias.Suchadecompositionisalwayspossibleandoftenusef ul,andisknown\\nas thebias–variance decomposition . Unless the nearest neighbor is at 0,\\nˆy0will be smaller than f(0) in this example, and so the average estimate\\nwill be biased downward. The variance is due to the sampling v ariance of\\nthe 1-nearest neighbor. In low dimensions and with N= 1000, the nearest\\nneighbor is very close to 0, and so both the bias and variance a re small. As', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 42}),\n",
       " Document(page_content='the dimension increases, the nearest neighbor tends to stra y further from\\nthe target point, and both bias and variance are incurred. By p= 10, for\\nmore than 99% of the samples the nearest neighbor is a distanc e greater\\nthan 0.5 from the origin. Thus as pincreases, the estimate tends to be 0\\nmore often than not, and hence the MSE levels oﬀ at 1 .0, as does the bias,\\nand the variance starts dropping (an artifact of this exampl e).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 42}),\n",
       " Document(page_content='Although this is a highly contrived example, similar phenom ena occur\\nmore generally. The complexity of functions of many variabl es can grow\\nexponentially with the dimension, and if we wish to be able to estimate\\nsuch functions with the same accuracy as function in low dime nsions, then\\nwe need the size of our training set to grow exponentially as w ell. In this\\nexample, the function is a complex interaction of all pvariables involved.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 42}),\n",
       " Document(page_content='The dependence of the bias term on distance depends on the tru th, and\\nit need not always dominate with 1-nearest neighbor. For exa mple, if the\\nfunction always involves only a few dimensions as in Figure 2 .8, then the\\nvariance can dominate instead.\\nSuppose, on the other hand, that we know that the relationshi p between\\nYandXis linear,\\nY=XTβ+ε, (2.26)\\nwhereε∼N(0,σ2) and we ﬁt the model by least squares to the train-\\ning data. For an arbitrary test point x0, we have ˆy0=xT\\n0ˆβ, which can', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 42}),\n",
       " Document(page_content='be written as ˆ y0=xT\\n0β+∑N\\ni=1ℓi(x0)εi, whereℓi(x0) is theith element\\nofX(XTX)−1x0. Since under this model the least squares estimates are', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 42}),\n",
       " Document(page_content='2.5 Local Methods in High Dimensions 25\\nXf(X)\\n-1.0 -0.5 0.0 0.5 1.00.0 0.2 0.4 0.6 0.8 1.0•1-NN in One Dimension\\nX1X2\\n-1.0 -0.5 0.0 0.5 1.0-1.0 -0.5 0.0 0.5 1.0•••\\n•••\\n••\\n•\\n•••\\n•••\\n••\\n••\\n•\\n•\\n•••\\n••1-NN in One vs. Two Dimensions\\nDimensionAverage Distance to Nearest Neighbor\\n2 4 6 8 100.0 0.2 0.4 0.6 0.8••••••••••Distance to 1-NN vs. Dimension\\nDimensionMse\\n2 4 6 8 100.0 0.2 0.4 0.6 0.8 1.0• •••••••••\\n• •••••••• • • •••••••••MSE vs. Dimension\\n•MSE\\n•Variance\\n•Sq. Bias', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 43}),\n",
       " Document(page_content='FIGURE 2.7. A simulation example, demonstrating the curse of dimensional-\\nity and its eﬀect on MSE, bias and variance. The input feature s are uniformly\\ndistributed in [−1,1]pforp= 1,...,10The top left panel shows the target func-\\ntion (no noise) in IR:f(X) =e−8||X||2, and demonstrates the error that 1-nearest\\nneighbor makes in estimating f(0). The training point is indicated by the blue tick\\nmark. The top right panel illustrates why the radius of the 1-nearest neighborhood', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 43}),\n",
       " Document(page_content='increases with dimension p. The lower left panel shows the average radius of the\\n1-nearest neighborhoods. The lower-right panel shows the MSE , squared bias and\\nvariance curves as a function of dimension p.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 43}),\n",
       " Document(page_content='26 2. Overview of Supervised Learning\\nXf(X)\\n-1.0 -0.5 0.0 0.5 1.00 1 2 3 4•1-NN in One Dimension\\nDimensionMSE\\n2 4 6 8 100.0 0.05 0.10 0.15 0.20 0.25••••••••••\\n••••••••••\\n• •• • ••••••MSE  vs. Dimension\\n•MSE\\n•Variance\\n•Sq. Bias\\nFIGURE 2.8. A simulation example with the same setup as in Figure 2.7. Here\\nthe function is constant in all but one dimension: f(X) =1\\n2(X1+ 1)3. The\\nvariance dominates.\\nunbiased, we ﬁnd that\\nEPE(x0) = E y0|x0ET(y0−ˆy0)2\\n= Var(y0|x0)+ET[ˆy0−ETˆy0]2+[ETˆy0−xT\\n0β]2', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 44}),\n",
       " Document(page_content='= Var(y0|x0)+Var T(ˆy0)+Bias2(ˆy0)\\n=σ2+ETxT\\n0(XTX)−1x0σ2+02. (2.27)\\nHere we have incurred an additional variance σ2in the prediction error,\\nsince our target is not deterministic. There is no bias, and t he variance\\ndepends on x0. IfNis large andTwere selected at random, and assuming\\nE(X) = 0, then XTX→NCov(X) and\\nEx0EPE(x0)∼Ex0xT\\n0Cov(X)−1x0σ2/N+σ2\\n= trace[Cov( X)−1Cov(x0)]σ2/N+σ2\\n=σ2(p/N)+σ2. (2.28)\\nHere we see that the expected EPE increases linearly as a func tion ofp,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 44}),\n",
       " Document(page_content='with slope σ2/N. IfNis large and/or σ2is small, this growth in vari-\\nance is negligible (0 in the deterministic case). By imposin g some heavy\\nrestrictions on the class of models being ﬁtted, we have avoi ded the curse\\nof dimensionality. Some of the technical details in (2.27) a nd (2.28) are\\nderived in Exercise 2.5.\\nFigure 2.9 compares 1-nearest neighbor vs. least squares in two situa-\\ntions, both of which have the form Y=f(X) +ε,Xuniform as before,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 44}),\n",
       " Document(page_content='andε∼N(0,1). The sample size is N= 500. For the orange curve, f(x)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 44}),\n",
       " Document(page_content='2.5 Local Methods in High Dimensions 27\\nDimensionEPE Ratio\\n2 4 6 8 101.6 1.7 1.8 1.9 2.0 2.1••••••••••••••••••••Expected Prediction Error of 1NN vs. OLS\\n•Linear\\n•Cubic\\nFIGURE 2.9. The curves show the expected prediction error (at x0= 0) for\\n1-nearest neighbor relative to least squares for the model Y=f(X)+ε. For the\\norange curve, f(x) =x1, while for the blue curve f(x) =1\\n2(x1+1)3.\\nis linear in the ﬁrst coordinate, for the blue curve, cubic as in Figure 2.8.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 45}),\n",
       " Document(page_content='Shown is the relative EPE of 1-nearest neighbor to least squa res, which\\nappears to start at around 2 for the linear case. Least square s is unbiased\\nin this case, and as discussed above the EPE is slightly above σ2= 1.\\nThe EPE for 1-nearest neighbor is always above 2, since the va riance of\\nˆf(x0) in this case is at least σ2, and the ratio increases with dimension as\\nthe nearest neighbor strays from the target point. For the cu bic case, least', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 45}),\n",
       " Document(page_content='squares is biased, which moderates the ratio. Clearly we cou ld manufacture\\nexamples where the bias of least squares would dominate the v ariance, and\\nthe 1-nearest neighbor would come out the winner.\\nBy relying on rigid assumptions, the linear model has no bias at all and\\nnegligible variance, while the error in 1-nearest neighbor is substantially\\nlarger. However, if the assumptions are wrong, all bets are o ﬀ and the\\n1-nearest neighbor may dominate. We will see that there is a w hole spec-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 45}),\n",
       " Document(page_content='trum of models between the rigid linear models and the extrem ely ﬂexible\\n1-nearest-neighbor models, each with their own assumption s and biases,\\nwhich have been proposed speciﬁcally to avoid the exponenti al growth in\\ncomplexity of functions in high dimensions by drawing heavi ly on these\\nassumptions.\\nBefore we delve more deeply, let us elaborate a bit on the conc ept of\\nstatistical models and see how they ﬁt into the prediction framework.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 45}),\n",
       " Document(page_content='28 2. Overview of Supervised Learning\\n2.6 Statistical Models, Supervised Learning and\\nFunction Approximation\\nOur goal is to ﬁnd a useful approximation ˆf(x) to the function f(x) that\\nunderlies the predictive relationship between the inputs a nd outputs. In the\\ntheoretical setting of Section 2.4, we saw that squared erro r loss lead us\\nto the regression function f(x) = E(Y|X=x) for a quantitative response.\\nThe class of nearest-neighbor methods can be viewed as direc t estimates', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 46}),\n",
       " Document(page_content='of this conditional expectation, but we have seen that they c an fail in at\\nleast two ways:\\n•if the dimension of the input space is high, the nearest neigh bors need\\nnot be close to the target point, and can result in large error s;\\n•if special structure is known to exist, this can be used to red uce both\\nthe bias and the variance of the estimates.\\nWe anticipate using other classes of models for f(x), in many cases specif-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 46}),\n",
       " Document(page_content='ically designed to overcome the dimensionality problems, a nd here we dis-\\ncuss a framework for incorporating them into the prediction problem.\\n2.6.1 A Statistical Model for the Joint Distribution Pr(X,Y)\\nSuppose in fact that our data arose from a statistical model\\nY=f(X)+ε, (2.29)\\nwhere the random error εhas E(ε) = 0 and is independent of X. Note that\\nfor this model, f(x) = E(Y|X=x), and in fact the conditional distribution\\nPr(Y|X) depends on Xonlythrough the conditional mean f(x).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 46}),\n",
       " Document(page_content='The additive error model is a useful approximation to the tru th. For\\nmost systems the input–output pairs ( X,Y) will not have a deterministic\\nrelationship Y=f(X). Generally there will be other unmeasured variables\\nthatalsocontributeto Y,includingmeasurementerror.Theadditivemodel\\nassumes that we can capture all these departures from a deter ministic re-\\nlationship via the error ε.\\nFor some problems a deterministic relationship does hold. M any of the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 46}),\n",
       " Document(page_content='classiﬁcation problems studied in machine learning are of t his form, where\\nthe response surface can be thought of as a colored map deﬁned in IRp.\\nThe training data consist of colored examples from the map {xi,gi}, and\\nthe goal is to be able to color any point. Here the function is d eterministic,\\nand the randomness enters through the xlocation of the training points.\\nFor the moment we will not pursue such problems, but will see t hat they', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 46}),\n",
       " Document(page_content='can be handled by techniques appropriate for the error-base d models.\\nThe assumption in (2.29) that the errors are independent and identically\\ndistributedisnotstrictlynecessary,butseemstobeatthe backofourmind', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 46}),\n",
       " Document(page_content='2.6 Statistical Models, Supervised Learning and Function Ap proximation 29\\nwhen we average squared errors uniformly in our EPE criterio n. With such\\na model it becomes natural to use least squares as a data crite rion for\\nmodel estimation as in (2.1). Simple modiﬁcations can be mad e to avoid\\nthe independence assumption; for example, we can have Var( Y|X=x) =\\nσ(x), and now both the mean and variance depend on X. In general the\\nconditional distribution Pr( Y|X) can depend on Xin complicated ways,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 47}),\n",
       " Document(page_content='but the additive error model precludes these.\\nSo far we have concentrated on the quantitative response. Ad ditive error\\nmodels are typically not used for qualitative outputs G; in this case the tar-\\nget function p(X)isthe conditional density Pr( G|X), and this is modeled\\ndirectly. For example, for two-class data, it is often reaso nable to assume\\nthat the data arise from independent binary trials, with the probability of\\none particular outcome being p(X), and the other 1 −p(X). Thus ifYis', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 47}),\n",
       " Document(page_content='the 0–1 coded version of G, then E(Y|X=x) =p(x), but the variance\\ndepends on xas well: Var( Y|X=x) =p(x)[1−p(x)].\\n2.6.2 Supervised Learning\\nBefore we launch into more statistically oriented jargon, w e present the\\nfunction-ﬁtting paradigm from a machine learning point of v iew. Suppose\\nfor simplicity that the errors are additive and that the mode lY=f(X)+ε\\nis a reasonable assumption. Supervised learning attempts t o learnfby\\nexample through a teacher. One observes the system under study, both', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 47}),\n",
       " Document(page_content='the inputs and outputs, and assembles a training set of observations T=\\n(xi,yi), i= 1,...,N. The observed input values to the system xiare also\\nfed into an artiﬁcial system, known as a learning algorithm ( usually a com-\\nputer program), which also produces outputs ˆf(xi) in response to the in-\\nputs. The learning algorithm has the property that it can mod ify its in-\\nput/output relationship ˆfin response to diﬀerences yi−ˆf(xi) between the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 47}),\n",
       " Document(page_content='original and generated outputs. This process is known as learning by exam-\\nple. Upon completion of the learning process the hope is that the artiﬁcial\\nand real outputs will be close enough to be useful for all sets of inputs likely\\nto be encountered in practice.\\n2.6.3 Function Approximation\\nThe learning paradigm of the previous section has been the mo tivation\\nfor research into the supervised learning problem in the ﬁel ds of machine', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 47}),\n",
       " Document(page_content='learning (with analogies to human reasoning) and neural net works (with\\nbiological analogies to the brain). The approach taken in ap plied mathe-\\nmatics and statistics has been from the perspective of funct ion approxima-\\ntion and estimation. Here the data pairs {xi,yi}are viewed as points in a\\n(p+1)-dimensional Euclidean space. The function f(x) has domain equal\\nto thep-dimensional input subspace, and is related to the data via a model', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 47}),\n",
       " Document(page_content='30 2. Overview of Supervised Learning\\nsuch asyi=f(xi)+εi. For convenience in this chapter we will assume the\\ndomain is IRp, ap-dimensional Euclidean space, although in general the\\ninputs can be of mixed type. The goal is to obtain a useful appr oximation\\ntof(x) for allxin some region of IRp, given the representations in T.\\nAlthough somewhat less glamorous than the learning paradig m, treating\\nsupervised learning as a problem in function approximation encourages the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 48}),\n",
       " Document(page_content='geometrical concepts of Euclidean spaces and mathematical concepts of\\nprobabilistic inference to be applied to the problem. This i s the approach\\ntaken in this book.\\nMany of the approximations we will encounter have associate d a set of\\nparameters θthat can be modiﬁed to suit the data at hand. For example,\\nthe linear model f(x) =xTβhasθ=β. Another class of useful approxi-\\nmators can be expressed as linear basis expansions\\nfθ(x) =K∑\\nk=1hk(x)θk, (2.30)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 48}),\n",
       " Document(page_content='where thehkare a suitable set of functions or transformations of the inp ut\\nvectorx. Traditional examples are polynomial and trigonometric ex pan-\\nsions, where for example hkmight bex2\\n1,x1x2\\n2, cos(x1) and so on. We\\nalso encounter nonlinear expansions, such as the sigmoid tr ansformation\\ncommon to neural network models,\\nhk(x) =1\\n1+exp(−xTβk). (2.31)\\nWe can use least squares to estimate the parameters θinfθas we did\\nfor the linear model, by minimizing the residual sum-of-squ ares\\nRSS(θ) =N∑', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 48}),\n",
       " Document(page_content='i=1(yi−fθ(xi))2(2.32)\\nas a function of θ. This seems a reasonable criterion for an additive error\\nmodel. In terms of function approximation, we imagine our pa rameterized\\nfunction as a surface in p+ 1 space, and what we observe are noisy re-\\nalizations from it. This is easy to visualize when p= 2 and the vertical\\ncoordinate is the output y, as in Figure 2.10. The noise is in the output\\ncoordinate, so we ﬁnd the set of parameters such that the ﬁtte d surface', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 48}),\n",
       " Document(page_content='gets as close to the observed points as possible, where close is measured by\\nthe sum of squared vertical errors in RSS( θ).\\nFor the linear model we get a simple closed form solution to th e mini-\\nmization problem. This is also true for the basis function me thods, if the\\nbasis functions themselves do not have any hidden parameter s. Otherwise\\nthe solution requires either iterative methods or numerica l optimization.\\nWhile least squares is generally very convenient, it is not t he only crite-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 48}),\n",
       " Document(page_content='rion used and in some cases would not make much sense. A more ge neral', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 48}),\n",
       " Document(page_content='2.6 Statistical Models, Supervised Learning and Function Ap proximation 31\\n•••\\n•••\\n••••\\n••\\n•••\\n••••\\n•••\\n••\\n•••\\n•\\n•\\n••\\n••• ••••\\n••\\n•••\\n•••\\n•\\n••\\n••\\n•••\\n•\\n••\\n•••\\n•\\n•••\\n••••\\n•••\\n•\\n••\\nFIGURE 2.10. Least squares ﬁtting of a function of two inputs. The paramet ers\\noffθ(x)are chosen so as to minimize the sum-of-squared vertical err ors.\\nprinciple for estimation is maximum likelihood estimation . Suppose we have\\na random sample yi, i= 1,...,Nfrom a density Pr θ(y) indexed by some', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 49}),\n",
       " Document(page_content='parameters θ. The log-probability of the observed sample is\\nL(θ) =N∑\\ni=1logPrθ(yi). (2.33)\\nThe principle of maximum likelihood assumes that the most re asonable\\nvalues forθare those for which the probability of the observed sample is\\nlargest. Least squares for the additive error model Y=fθ(X) +ε, with\\nε∼N(0,σ2), is equivalent to maximum likelihood using the conditiona l\\nlikelihood\\nPr(Y|X,θ) =N(fθ(X),σ2). (2.34)\\nSo although the additional assumption of normality seems mo re restrictive,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 49}),\n",
       " Document(page_content='the results are the same. The log-likelihood of the data is\\nL(θ) =−N\\n2log(2π)−Nlogσ−1\\n2σ2N∑\\ni=1(yi−fθ(xi))2,(2.35)\\nand the only term involving θis the last, which is RSS( θ) up to a scalar\\nnegative multiplier.\\nA more interesting example is the multinomial likelihood fo r the regres-\\nsion function Pr( G|X) for a qualitative output G. Suppose we have a model\\nPr(G=Gk|X=x) =pk,θ(x), k= 1,...,Kfor the conditional probabil-\\nity of each class given X, indexed by the parameter vector θ. Then the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 49}),\n",
       " Document(page_content='32 2. Overview of Supervised Learning\\nlog-likelihood (also referred to as the cross-entropy) is\\nL(θ) =N∑\\ni=1logpgi,θ(xi), (2.36)\\nand when maximized it delivers values of θthat best conform with the data\\nin this likelihood sense.\\n2.7 Structured Regression Models\\nWehaveseenthatalthoughnearest-neighborandotherlocal methodsfocus\\ndirectly on estimating the function at a point, they face pro blems in high\\ndimensions. They may also be inappropriate even in low dimen sions in', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 50}),\n",
       " Document(page_content='cases where more structured approaches can make more eﬃcien t use of the\\ndata. This section introduces classes of such structured ap proaches. Before\\nwe proceed, though, we discuss further the need for such clas ses.\\n2.7.1 Diﬃculty of the Problem\\nConsider the RSS criterion for an arbitrary function f,\\nRSS(f) =N∑\\ni=1(yi−f(xi))2. (2.37)\\nMinimizing (2.37) leads to inﬁnitely many solutions: any fu nctionˆfpassing\\nthrough the training points ( xi,yi) is a solution. Any particular solution', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 50}),\n",
       " Document(page_content='chosen might be a poor predictor at test points diﬀerent from the training\\npoints. If there are multiple observation pairs xi,yiℓ, ℓ= 1,...,N iat each\\nvalue ofxi, the risk is limited. In this case, the solutions pass throug h\\nthe average values of the yiℓat eachxi; see Exercise 2.6. The situation is\\nsimilar to the one we have already visited in Section 2.4; ind eed, (2.37) is\\nthe ﬁnite sample version of (2.11) on page 18. If the sample si zeNwere', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 50}),\n",
       " Document(page_content='suﬃciently large such that repeats were guaranteed and dens ely arranged,\\nit would seem that these solutions might all tend to the limit ing conditional\\nexpectation.\\nIn order to obtain useful results for ﬁnite N, we must restrict the eligible\\nsolutions to (2.37) to a smaller set of functions. How to deci de on the\\nnature of the restrictions is based on considerations outsi de of the data.\\nTheserestrictions aresometimes encodedviatheparametri crepresentation', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 50}),\n",
       " Document(page_content='offθ, or may be built into the learning method itself, either impl icitly or\\nexplicitly. These restricted classes of solutions are the m ajor topic of this\\nbook. One thing should be clear, though. Any restrictions im posed onf\\nthat lead to a unique solution to (2.37) do not really remove t he ambiguity', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 50}),\n",
       " Document(page_content='2.8 Classes of Restricted Estimators 33\\ncaused by the multiplicity of solutions. There are inﬁnitel y many possible\\nrestrictions, each leading to a unique solution, so the ambi guity has simply\\nbeen transferred to the choice of constraint.\\nIn general the constraints imposed by most learning methods can be\\ndescribed as complexity restrictions of one kind or another. This usually\\nmeans some kind of regular behavior in small neighborhoods o f the input', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 51}),\n",
       " Document(page_content='space. That is, for all input points xsuﬃciently close to each other in\\nsome metric, ˆfexhibits some special structure such as nearly constant,\\nlinear or low-order polynomial behavior. The estimator is t hen obtained by\\naveraging or polynomial ﬁtting in that neighborhood.\\nThe strength of the constraint is dictated by the neighborho od size. The\\nlarger the size of the neighborhood, the stronger the constr aint, and the\\nmore sensitive the solution is to the particular choice of co nstraint. For', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 51}),\n",
       " Document(page_content='example, local constant ﬁts in inﬁnitesimally small neighb orhoods is no\\nconstraint at all; local linear ﬁts in very large neighborho ods is almost a\\nglobally linear model, and is very restrictive.\\nThe nature of the constraint depends on the metric used. Some methods,\\nsuch as kernel and local regression and tree-based methods, directly specify\\nthe metric and size of the neighborhood. The nearest-neighb or methods\\ndiscussed so far are based on the assumption that locally the function is', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 51}),\n",
       " Document(page_content='constant;closetoatargetinput x0,thefunctiondoesnotchangemuch,and\\nso close outputs can be averaged to produce ˆf(x0). Other methods such\\nas splines, neural networks and basis-function methods imp licitly deﬁne\\nneighborhoods of local behavior. In Section 5.4.1 we discus s the concept\\nof anequivalent kernel (see Figure 5.8 on page 157), which describes this\\nlocal dependence for any method linear in the outputs. These equivalent', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 51}),\n",
       " Document(page_content='kernels in many cases look just like the explicitly deﬁned we ighting kernels\\ndiscussed above—peaked at the target point and falling away s moothly\\naway from it.\\nOne fact should be clear by now. Any method that attempts to pr o-\\nduce locally varying functions in small isotropic neighbor hoods will run\\ninto problems in high dimensions—again the curse of dimensio nality. And\\nconversely, all methods that overcome the dimensionality p roblems have an', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 51}),\n",
       " Document(page_content='associated—andoftenimplicitoradaptive—metricformeasur ingneighbor-\\nhoods, which basically does not allow the neighborhood to be simultane-\\nously small in all directions.\\n2.8 Classes of Restricted Estimators\\nThe variety of nonparametric regressiontechniques or lear ning methods fall\\nintoanumberofdiﬀerentclassesdependingonthenatureoft herestrictions\\nimposed. These classes are not distinct, and indeed some met hods fall in\\nseveral classes. Here we give a brief summary, since detaile d descriptions', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 51}),\n",
       " Document(page_content='34 2. Overview of Supervised Learning\\nare given in later chapters. Each of the classes has associat ed with it one\\nor more parameters, sometimes appropriately called smoothing parameters,\\nthat control the eﬀective size of the local neighborhood. He re we describe\\nthree broad classes.\\n2.8.1 Roughness Penalty and Bayesian Methods\\nHere the class of functions is controlled by explicitly pena lizing RSS( f)\\nwith a roughness penalty\\nPRSS(f;λ) = RSS(f)+λJ(f). (2.38)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 52}),\n",
       " Document(page_content='The user-selected functional J(f) will be large for functions fthat vary too\\nrapidly over small regions of input space. For example, the p opularcubic\\nsmoothing spline for one-dimensional inputs is the solution to the penalized\\nleast-squares criterion\\nPRSS(f;λ) =N∑\\ni=1(yi−f(xi))2+λ∫\\n[f′′(x)]2dx. (2.39)\\nThe roughness penalty here controls large values of the seco nd derivative\\noff, and the amount of penalty is dictated by λ≥0. Forλ= 0 no penalty', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 52}),\n",
       " Document(page_content='is imposed, and any interpolating function will do, while fo rλ=∞only\\nfunctions linear in xare permitted.\\nPenalty functionals Jcan be constructed for functions in any dimension,\\nand special versions can be created to impose special struct ure. For ex-\\nample, additive penalties J(f) =∑p\\nj=1J(fj) are used in conjunction with\\nadditive functions f(X) =∑p\\nj=1fj(Xj) to create additive models with\\nsmooth coordinate functions. Similarly, projection pursuit regression mod-\\nels havef(X) =∑M\\nm=1gm(αT', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 52}),\n",
       " Document(page_content='mX) for adaptively chosen directions αm, and\\nthe functions gmcan each have an associated roughness penalty.\\nPenalty function, or regularization methods, express our prior belief that\\nthe type of functions we seek exhibit a certain type of smooth behavior, and\\nindeed can usually be cast in a Bayesian framework. The penal tyJcorre-\\nsponds to a log-prior, and PRSS( f;λ) the log-posterior distribution, and\\nminimizing PRSS( f;λ) amounts to ﬁnding the posterior mode. We discuss', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 52}),\n",
       " Document(page_content='roughness-penalty approaches in Chapter 5 and the Bayesian paradigm in\\nChapter 8.\\n2.8.2 Kernel Methods and Local Regression\\nThesemethodscanbethoughtofasexplicitlyprovidingesti matesofthere-\\ngression function or conditional expectation by specifyin g the nature of the\\nlocal neighborhood, and of the class of regular functions ﬁt ted locally. The\\nlocal neighborhood is speciﬁed by a kernel function Kλ(x0,x) which assigns', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 52}),\n",
       " Document(page_content='2.8 Classes of Restricted Estimators 35\\nweights to points xin a region around x0(see Figure 6.1 on page 192). For\\nexample, the Gaussian kernel has a weight function based on t he Gaussian\\ndensity function\\nKλ(x0,x) =1\\nλexp[\\n−||x−x0||2\\n2λ]\\n(2.40)\\nand assigns weights to points that die exponentially with th eir squared\\nEuclidean distance from x0. The parameter λcorresponds to the variance\\nof the Gaussian density, and controls the width of the neighb orhood. The', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 53}),\n",
       " Document(page_content='simplest form of kernel estimate is the Nadaraya–Watson wei ghted average\\nˆf(x0) =∑N\\ni=1Kλ(x0,xi)yi∑N\\ni=1Kλ(x0,xi). (2.41)\\nIn general we can deﬁne a local regression estimate of f(x0) asfˆθ(x0),\\nwhereˆθminimizes\\nRSS(fθ,x0) =N∑\\ni=1Kλ(x0,xi)(yi−fθ(xi))2, (2.42)\\nandfθis some parameterized function, such as a low-order polynom ial.\\nSome examples are:\\n•fθ(x) =θ0, the constant function; this results in the Nadaraya–\\nWatson estimate in (2.41) above.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 53}),\n",
       " Document(page_content='•fθ(x) =θ0+θ1xgives the popular local linear regression model.\\nNearest-neighbor methods can be thought of as kernel method s having a\\nmore data-dependent metric. Indeed, the metric for k-nearest neighbors is\\nKk(x,x0) =I(||x−x0||≤||x(k)−x0||),\\nwherex(k)is the training observation ranked kth in distance from x0, and\\nI(S) is the indicator of the set S.\\nThesemethodsofcourseneedtobemodiﬁedinhighdimensions ,toavoid\\nthe curse of dimensionality. Various adaptations are discu ssed in Chapter 6.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 53}),\n",
       " Document(page_content='2.8.3 Basis Functions and Dictionary Methods\\nThis class of methods includes the familiar linear and polyn omial expan-\\nsions, but more importantly a wide variety of more ﬂexible mo dels. The\\nmodel forfis a linear expansion of basis functions\\nfθ(x) =M∑\\nm=1θmhm(x), (2.43)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 53}),\n",
       " Document(page_content='36 2. Overview of Supervised Learning\\nwhere each of the hmis a function of the input x, and the term linear here\\nrefers to the action of the parameters θ. This class covers a wide variety of\\nmethods. In some cases the sequence of basis functions is pre scribed, such\\nas a basis for polynomials in xof total degree M.\\nForone-dimensional x,polynomialsplinesofdegree Kcanberepresented\\nby an appropriate sequence of Mspline basis functions, determined in turn', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 54}),\n",
       " Document(page_content='byM−K−1knots.Theseproducefunctionsthatarepiecewisepolynomials\\nof degreeKbetween the knots, and joined up with continuity of degree\\nK−1 at the knots. As an example consider linear splines, or piec ewise\\nlinear functions. One intuitively satisfying basis consis ts of the functions\\nb1(x) = 1,b2(x) =x, andbm+2(x) = (x−tm)+,m= 1,...,M−2,\\nwheretmis themth knot, and z+denotes positive part. Tensor products\\nof spline bases can be used for inputs with dimensions larger than one', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 54}),\n",
       " Document(page_content='(see Section 5.2, and the CART and MARS models in Chapter 9.) T he\\nparameterMcontrols the degree of the polynomial or the number of knots\\nin the case of splines.\\nRadial basis functions are symmetric p-dimensional kernels located at\\nparticular centroids,\\nfθ(x) =M∑\\nm=1Kλm(µm,x)θm; (2.44)\\nfor example, the Gaussian kernel Kλ(µ,x) =e−||x−µ||2/2λis popular.\\nRadial basis functions have centroids µmand scales λmthat have to\\nbe determined. The spline basis functions have knots. In gen eral we would', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 54}),\n",
       " Document(page_content='like the data to dictate them as well. Including these as para meters changes\\nthe regression problem from a straightforward linear probl em to a combi-\\nnatorially hard nonlinear problem. In practice, shortcuts such as greedy\\nalgorithms or two stage processes are used. Section 6.7 desc ribes some such\\napproaches.\\nA single-layer feed-forward neural network model with line ar output\\nweights can be thought of as an adaptive basis function metho d. The model\\nhas the form\\nfθ(x) =M∑\\nm=1βmσ(αT', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 54}),\n",
       " Document(page_content='mx+bm), (2.45)\\nwhereσ(x) = 1/(1 +e−x) is known as the activation function. Here, as\\nin the projection pursuit model, the directions αmand thebiastermsbm\\nhavetobedetermined,andtheirestimationisthemeatofthe computation.\\nDetails are given in Chapter 11.\\nThese adaptively chosen basis function methods are also kno wn asdictio-\\nnarymethods, where one has available a possibly inﬁnite set or di ctionary\\nDof candidate basis functions from which to choose, and model s are built', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 54}),\n",
       " Document(page_content='up by employing some kind of search mechanism.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 54}),\n",
       " Document(page_content='2.9 Model Selection and the Bias–Variance Tradeoﬀ 37\\n2.9 Model Selection and the Bias–Variance\\nTradeoﬀ\\nAll the models described above and many others discussed in l ater chapters\\nhave asmoothing orcomplexity parameter that has to be determined:\\n•the multiplier of the penalty term;\\n•the width of the kernel;\\n•or the number of basis functions.\\nInthecaseofthesmoothingspline,theparameter λindexesmodelsranging\\nfrom a straight line ﬁt to the interpolating model. Similarl y a local degree-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 55}),\n",
       " Document(page_content='mpolynomial model ranges between a degree- mglobal polynomial when\\nthe window size is inﬁnitely large, to an interpolating ﬁt wh en the window\\nsize shrinks to zero. This means that we cannot use residual s um-of-squares\\non the training data to determine these parameters as well, s ince we would\\nalways pick those that gave interpolating ﬁts and hence zero residuals. Such\\na model is unlikely to predict future data well at all.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 55}),\n",
       " Document(page_content='Thek-nearest-neighbor regression ﬁt ˆfk(x0) usefully illustrates the com-\\npeting forces that aﬀect the predictive ability of such appr oximations. Sup-\\npose the data arise from a model Y=f(X) +ε, with E(ε) = 0 and\\nVar(ε) =σ2. For simplicity here we assume that the values of xiin the\\nsample are ﬁxed in advance (nonrandom). The expected predic tion error\\natx0, also known as testorgeneralization error, can be decomposed:\\nEPEk(x0) = E[(Y−ˆfk(x0))2|X=x0]\\n=σ2+[Bias2(ˆfk(x0))+Var T(ˆfk(x0))] (2.46)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 55}),\n",
       " Document(page_content='=σ2+[\\nf(x0)−1\\nkk∑\\nℓ=1f(x(ℓ))]2\\n+σ2\\nk.(2.47)\\nThe subscripts in parentheses ( ℓ) indicate the sequence of nearest neighbors\\ntox0.\\nThere are three terms in this expression. The ﬁrst term σ2is their-\\nreducible error—the variance of the new test target—and is beyond our\\ncontrol, even if we know the true f(x0).\\nThe second and third terms are under our control, and make up t he\\nmean squared error ofˆfk(x0) in estimating f(x0), which is broken down', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 55}),\n",
       " Document(page_content='into a bias component and a variance component. The bias term is the\\nsquared diﬀerence between the true mean f(x0) and the expected value of\\nthe estimate—[E T(ˆfk(x0))−f(x0)]2—where the expectation averages the\\nrandomness in the training data. This term will most likely i ncrease with\\nk, if the true function is reasonably smooth. For small kthe few closest\\nneighbors will have values f(x(ℓ)) close tof(x0), so their average should', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 55}),\n",
       " Document(page_content='38 2. Overview of Supervised Learning\\nHigh Bias\\nLow VarianceLow Bias\\nHigh VariancePrediction Error\\nModel ComplexityTraining SampleTest Sample\\nLow High\\nFIGURE 2.11. Test and training error as a function of model complexity.\\nbe close to f(x0). Askgrows, the neighbors are further away, and then\\nanything can happen.\\nThe variance term is simply the variance of an average here, a nd de-\\ncreases as the inverse of k. So askvaries, there is a bias–variance tradeoﬀ.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 56}),\n",
       " Document(page_content='More generally, as the model complexity of our procedureis increased, the\\nvariance tends to increase and the squared bias tends to decr ease. The op-\\nposite behavior occurs as the model complexity is decreased . Fork-nearest\\nneighbors, the model complexity is controlled by k.\\nTypically we would like to choose our model complexity to tra de bias\\noﬀ with variance in such a way as to minimize the test error. An obvious\\nestimate of test error is the training error1\\nN∑\\ni(yi−ˆyi)2. Unfortunately', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 56}),\n",
       " Document(page_content='training error is not a good estimate of test error, as it does not properly\\naccount for model complexity.\\nFigure 2.11 shows the typical behavior of the test and traini ng error, as\\nmodel complexity is varied. The training error tends to decr ease whenever\\nwe increase the model complexity, that is, whenever we ﬁt the data harder.\\nHowever with too much ﬁtting, the model adapts itself too clo sely to the\\ntraining data, and will not generalize well (i.e., have larg e test error). In', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 56}),\n",
       " Document(page_content='that case the predictions ˆf(x0) will have large variance, as reﬂected in the\\nlast term of expression (2.46). In contrast, if the model is n ot complex\\nenough, it will underﬁt and may have large bias, again resulting in poor\\ngeneralization. In Chapter 7 we discuss methods for estimat ing the test\\nerror of a prediction method, and hence estimating the optim al amount of\\nmodel complexity for a given prediction method and training set.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 56}),\n",
       " Document(page_content='Exercises 39\\nBibliographic Notes\\nSome good general books on the learning problem are Duda et al . (2000),\\nBishop(1995),(Bishop, 2006),Ripley(1996),Cherkasskya ndMulier(2007)\\nand Vapnik (1996). Parts of this chapter are based on Friedma n (1994b).\\nExercises\\nEx. 2.1Suppose each of K-classes has an associated target tk, which is a\\nvector of all zeros, except a one in the kth position. Show that classifying to\\nthe largest element of ˆ yamounts to choosing the closest target, min k||tk−', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 57}),\n",
       " Document(page_content='ˆy||, if the elements of ˆ ysum to one.\\nEx. 2.2Show how to compute the Bayes decision boundary for the simul a-\\ntion example in Figure 2.5.\\nEx. 2.3Derive equation (2.24).\\nEx. 2.4The edge eﬀect problem discussed on page 23 is not peculiar to\\nuniform sampling from bounded domains. Consider inputs dra wn from a\\nspherical multinormal distribution X∼N(0,Ip). The squared distance\\nfrom any sample point to the origin has a χ2\\npdistribution with mean p.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 57}),\n",
       " Document(page_content='Consider a prediction point x0drawn from this distribution, and let a=\\nx0/||x0||be an associated unit vector. Let zi=aTxibe the projection of\\neach of the training points on this direction.\\nShow that the ziare distributed N(0,1) with expected squared distance\\nfrom the origin 1, while the target point has expected square d distancep\\nfrom the origin.\\nHence forp= 10, a randomly drawn test point is about 3 .1 standard\\ndeviations from the origin, while all the training points ar e on average', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 57}),\n",
       " Document(page_content='one standard deviation along direction a. So most prediction points see\\nthemselves as lying on the edge of the training set.\\nEx. 2.5\\n(a) Derive equation (2.27). The last line makes use of (3.8) t hrough a\\nconditioning argument.\\n(b) Derive equation (2.28), making use of the cyclicproperty of the trace\\noperator [trace( AB) = trace(BA)], and its linearity (which allows us\\nto interchange the order of trace and expectation).\\nEx. 2.6Consider a regression problem with inputs xiand outputs yi, and a', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 57}),\n",
       " Document(page_content='parameterized model fθ(x) to be ﬁt by least squares. Show that if there are\\nobservations with tiedoridentical values ofx, then the ﬁt can be obtained\\nfrom a reduced weighted least squares problem.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 57}),\n",
       " Document(page_content='40 2. Overview of Supervised Learning\\nEx. 2.7Suppose we have a sample of Npairsxi,yidrawn i.i.d. from the\\ndistribution characterized as follows:\\nxi∼h(x),the design density\\nyi=f(xi)+εi, fis the regression function\\nεi∼(0,σ2) (mean zero, variance σ2)\\nWe construct an estimator for flinearin theyi,\\nˆf(x0) =N∑\\ni=1ℓi(x0;X)yi,\\nwhere the weights ℓi(x0;X) do not depend on the yi, but do depend on the\\nentire training sequence of xi, denoted here by X.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 58}),\n",
       " Document(page_content='(a)Showthatlinearregressionand k-nearest-neighborregressionaremem-\\nbersofthisclassofestimators.Describeexplicitlythewe ightsℓi(x0;X)\\nin each of these cases.\\n(b) Decompose the conditional mean-squared error\\nEY|X(f(x0)−ˆf(x0))2\\nintoaconditionalsquaredbiasandaconditionalvariancec omponent.\\nLikeX,Yrepresents the entire training sequence of yi.\\n(c) Decompose the (unconditional) mean-squared error\\nEY,X(f(x0)−ˆf(x0))2\\ninto a squared bias and a variance component.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 58}),\n",
       " Document(page_content='(d) Establish a relationship between the squared biases and variances in\\nthe above two cases.\\nEx. 2.8Compare the classiﬁcation performance of linear regressio n andk–\\nnearest neighbor classiﬁcation on the zipcodedata. In particular, consider\\nonly the 2’s and3’s, andk= 1,3,5,7 and 15. Show both the training and\\ntest error for each choice. The zipcodedata are available from the book\\nwebsitewww-stat.stanford.edu/ElemStatLearn .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 58}),\n",
       " Document(page_content='Ex. 2.9Consider a linear regression model with pparameters, ﬁt by least\\nsquares to a set of training data ( x1,y1),...,(xN,yN) drawn at random\\nfrom a population. Let ˆβbe the least squares estimate. Suppose we have\\nsome test data (˜ x1,˜y1),...,(˜xM,˜yM) drawn at random from the same pop-\\nulation as the training data. If Rtr(β) =1\\nN∑N\\n1(yi−βTxi)2andRte(β) =\\n1\\nM∑M\\n1(˜yi−βT˜xi)2, prove that\\nE[Rtr(ˆβ)]≤E[Rte(ˆβ)],', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 58}),\n",
       " Document(page_content='Exercises 41\\nwhere the expectations are over all that is random in each exp ression. [This\\nexercisewasbroughttoourattentionbyRyanTibshirani,fr omahomework\\nassignment given by Andrew Ng.]', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 59}),\n",
       " Document(page_content='42 2. Overview of Supervised Learning', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 60}),\n",
       " Document(page_content='This is page 43\\nPrinter: Opaque this\\n3\\nLinear Methods for Regression\\n3.1 Introduction\\nA linear regression model assumes that the regression funct ion E(Y|X) is\\nlinear in the inputs X1,...,X p. Linear models were largely developed in\\nthe precomputer age of statistics, but even in today’s compu ter era there\\nare still good reasons to study and use them. They are simple a nd often\\nprovide an adequate and interpretable description of how th e inputs aﬀect', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 61}),\n",
       " Document(page_content='the output. For prediction purposes they can sometimes outp erform fancier\\nnonlinear models, especially in situations with small numb ers of training\\ncases,lowsignal-to-noiseratioorsparsedata.Finally,l inearmethodscanbe\\nappliedtotransformationsoftheinputsandthisconsidera blyexpandstheir\\nscope. These generalizations are sometimes called basis-f unction methods,\\nand are discussed in Chapter 5.\\nIn this chapter we describe linear methods for regression, w hile in the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 61}),\n",
       " Document(page_content='nextchapter wediscusslinear methods for classiﬁcation. O nsometopics we\\ngo into considerable detail, as it is our ﬁrm belief that an un derstanding\\nof linear methods is essential for understanding nonlinear ones. In fact,\\nmany nonlinear techniques are direct generalizations of th e linear methods\\ndiscussed here.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 61}),\n",
       " Document(page_content='44 3. Linear Methods for Regression\\n3.2 Linear Regression Models and Least Squares\\nAsintroducedinChapter2,wehaveaninputvector XT= (X1,X2,...,X p),\\nand want to predict a real-valued output Y. The linear regression model\\nhas the form\\nf(X) =β0+p∑\\nj=1Xjβj. (3.1)\\nThe linear model either assumes that the regression functio n E(Y|X) is\\nlinear, or that the linear model is a reasonable approximati on. Here the\\nβj’s are unknown parameters or coeﬃcients, and the variables Xjcan come\\nfrom diﬀerent sources:', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 62}),\n",
       " Document(page_content='•quantitative inputs;\\n•transformations of quantitative inputs, such as log, squar e-root or\\nsquare;\\n•basisexpansions,suchas X2=X2\\n1,X3=X3\\n1,leadingtoapolynomial\\nrepresentation;\\n•numeric or “dummy” coding of the levels of qualitative input s. For\\nexample, if Gis a ﬁve-level factor input, we might create Xj, j=\\n1,...,5,such thatXj=I(G=j). Together this group of Xjrepre-\\nsents the eﬀect of Gby a set of level-dependent constants, since in∑5\\nj=1Xjβj, one of the Xjs is one, and the others are zero.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 62}),\n",
       " Document(page_content='•interactions between variables, for example, X3=X1·X2.\\nNo matter the source of the Xj, the model is linear in the parameters.\\nTypically we have a set of training data ( x1,y1)...(xN,yN) from which\\nto estimate the parameters β. Eachxi= (xi1,xi2,...,x ip)Tis a vector\\nof feature measurements for the ith case. The most popular estimation\\nmethodis least squares ,inwhichwepickthecoeﬃcients β= (β0,β1,...,β p)T\\nto minimize the residual sum of squares\\nRSS(β) =N∑\\ni=1(yi−f(xi))2\\n=N∑\\ni=1(\\nyi−β0−p∑', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 62}),\n",
       " Document(page_content='j=1xijβj)2\\n. (3.2)\\nFrom a statistical point of view, this criterion is reasonab le if the training\\nobservations ( xi,yi) represent independent random draws from their popu-\\nlation. Even if the xi’s were not drawn randomly, the criterion is still valid\\nif theyi’s are conditionally independent given the inputs xi. Figure 3.1\\nillustrates the geometry of least-squares ﬁtting in the IRp+1-dimensional', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 62}),\n",
       " Document(page_content='3.2 Linear Regression Models and Least Squares 45\\n•• •\\n••••\\n••••\\n•••\\n••\\n•••\\n•\\n••\\n••\\n•••\\n••••••\\n••••\\n•••\\n••\\n••\\n•\\n••\\n••\\n••••\\n••\\n••\\n•\\n•••\\n•\\n••••\\n••••\\n••\\n••\\nX1X2Y\\nFIGURE 3.1. Linear least squares ﬁtting with X∈IR2. We seek the linear\\nfunction of Xthat minimizes the sum of squared residuals from Y.\\nspace occupied by the pairs ( X,Y). Note that (3.2) makes no assumptions\\nabout the validity of model (3.1); it simply ﬁnds the best lin ear ﬁt to the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 63}),\n",
       " Document(page_content='data. Least squares ﬁtting is intuitively satisfying no mat ter how the data\\narise; the criterion measures the average lack of ﬁt.\\nHow do we minimize (3.2)? Denote by XtheN×(p+ 1) matrix with\\neach row an input vector (with a 1 in the ﬁrst position), and si milarly let\\nybe theN-vector of outputs in the training set. Then we can write the\\nresidual sum-of-squares as\\nRSS(β) = (y−Xβ)T(y−Xβ). (3.3)\\nThis is a quadratic function in the p+1 parameters. Diﬀerentiating with\\nrespect toβwe obtain\\n∂RSS', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 63}),\n",
       " Document(page_content='∂β=−2XT(y−Xβ)\\n∂2RSS\\n∂β∂βT= 2XTX.(3.4)\\nAssuming (for the moment) that Xhas full column rank, and hence XTX\\nis positive deﬁnite, we set the ﬁrst derivative to zero\\nXT(y−Xβ) = 0 (3.5)\\nto obtain the unique solution\\nˆβ= (XTX)−1XTy. (3.6)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 63}),\n",
       " Document(page_content='46 3. Linear Methods for Regression\\nx1x2y\\nˆ y\\nFIGURE 3.2. TheN-dimensional geometry of least squares regression with two\\npredictors. The outcome vector yis orthogonally projected onto the hyperplane\\nspanned by the input vectors x1andx2. The projection ˆyrepresents the vector\\nof the least squares predictions\\nThe predicted values at an input vector x0are given by ˆf(x0) = (1 :x0)Tˆβ;\\nthe ﬁtted values at the training inputs are\\nˆy=Xˆβ=X(XTX)−1XTy, (3.7)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 64}),\n",
       " Document(page_content='where ˆyi=ˆf(xi). The matrix H=X(XTX)−1XTappearing in equation\\n(3.7) is sometimes called the “hat” matrix because it puts th e hat on y.\\nFigure3.2showsadiﬀerentgeometricalrepresentationoft heleastsquares\\nestimate,thistimeinIRN.Wedenotethecolumnvectorsof Xbyx0,x1,...,xp,\\nwithx0≡1. For much of what follows, this ﬁrst column is treated like a ny\\nother. These vectors span a subspace of IRN, also referred to as the column\\nspace of X. We minimize RSS( β) =∥y−Xβ∥2by choosing ˆβso that the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 64}),\n",
       " Document(page_content='residual vector y−ˆyis orthogonal to this subspace. This orthogonality is\\nexpressed in (3.5), and the resulting estimate ˆyis hence the orthogonal pro-\\njectionofyonto this subspace. The hat matrix Hcomputes the orthogonal\\nprojection, and hence it is also known as a projection matrix .\\nIt might happen that the columns of Xare not linearly independent, so\\nthatXis not of full rank. This would occur, for example, if two of th e\\ninputs were perfectly correlated, (e.g., x2= 3x1). ThenXTXis singular', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 64}),\n",
       " Document(page_content='and the least squares coeﬃcients ˆβare not uniquely deﬁned. However,\\nthe ﬁtted values ˆy=Xˆβare still the projection of yonto the column\\nspace of X; there is just more than one way to express that projection\\nin terms of the column vectors of X. The non-full-rank case occurs most\\noftenwhenoneormorequalitative inputsarecodedinaredun dantfashion.\\nThere is usually a natural way to resolve the non-unique repr esentation,\\nby recoding and/or dropping redundant columns in X. Most regression', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 64}),\n",
       " Document(page_content='software packages detect these redundancies and automatic ally implement', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 64}),\n",
       " Document(page_content='3.2 Linear Regression Models and Least Squares 47\\nsome strategy for removing them. Rank deﬁciencies can also o ccur in signal\\nand image analysis, where the number of inputs pcan exceed the number\\nof training cases N. In this case, the features are typically reduced by\\nﬁltering or else the ﬁtting is controlled by regularization (Section 5.2.3 and\\nChapter 18).\\nUp to now we have made minimal assumptions about the true dist ribu-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 65}),\n",
       " Document(page_content='tion of the data. In order to pin down the sampling properties ofˆβ, we now\\nassume that the observations yiare uncorrelated and have constant vari-\\nanceσ2, and that the xiare ﬁxed (non random). The variance–covariance\\nmatrix of the least squares parameter estimates is easily de rived from (3.6)\\nand is given by\\nVar(ˆβ) = (XTX)−1σ2. (3.8)\\nTypically one estimates the variance σ2by\\nˆσ2=1\\nN−p−1N∑\\ni=1(yi−ˆyi)2.\\nTheN−p−1 rather than Nin the denominator makes ˆ σ2an unbiased\\nestimate of σ2: E(ˆσ2) =σ2.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 65}),\n",
       " Document(page_content='To draw inferences about the parameters and the model, addit ional as-\\nsumptions are needed. We now assume that (3.1) is the correct model for\\nthe mean; that is, the conditional expectation of Yis linear inX1,...,X p.\\nWealsoassumethatthedeviationsof Yarounditsexpectationareadditive\\nand Gaussian. Hence\\nY= E(Y|X1,...,X p)+ε\\n=β0+p∑\\nj=1Xjβj+ε, (3.9)\\nwhere the error εis a Gaussian random variable with expectation zero and\\nvarianceσ2, writtenε∼N(0,σ2).\\nUnder (3.9), it is easy to show that', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 65}),\n",
       " Document(page_content='ˆβ∼N(β,(XTX)−1σ2). (3.10)\\nThis is a multivariate normal distribution with mean vector and variance–\\ncovariance matrix as shown. Also\\n(N−p−1)ˆσ2∼σ2χ2\\nN−p−1, (3.11)\\na chi-squared distribution with N−p−1 degrees of freedom. In addition ˆβ\\nand ˆσ2are statistically independent. We use these distributiona l properties\\nto form tests of hypothesis and conﬁdence intervals for the p arametersβj.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 65}),\n",
       " Document(page_content='48 3. Linear Methods for Regression\\nZTail Probabilities\\n2.0 2.2 2.4 2.6 2.8 3.00.01 0.02 0.03 0.04 0.05 0.06t30\\nt100\\nnormal\\nFIGURE 3.3. The tail probabilities Pr(|Z|>z)for three distributions, t30,t100\\nand standard normal. Shown are the appropriate quantiles for t esting signiﬁcance\\nat thep= 0.05and0.01levels. The diﬀerence between tand the standard normal\\nbecomes negligible for Nbigger than about 100.\\nTo test the hypothesis that a particular coeﬃcient βj= 0, we form the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 66}),\n",
       " Document(page_content='standardized coeﬃcient or Z-score\\nzj=ˆβj\\nˆσ√vj, (3.12)\\nwherevjisthejthdiagonalelementof( XTX)−1.Underthenullhypothesis\\nthatβj= 0,zjis distributed as tN−p−1(atdistribution with N−p−1\\ndegrees of freedom), and hence a large (absolute) value of zjwill lead to\\nrejection of this null hypothesis. If ˆ σis replaced by a known value σ, then\\nzjwould have a standard normal distribution. The diﬀerence be tween the\\ntail quantiles of a t-distribution and a standard normal become negligible', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 66}),\n",
       " Document(page_content='as the sample size increases, and so we typically use the norm al quantiles\\n(see Figure 3.3).\\nOften we need to test for the signiﬁcance of groups of coeﬃcie nts simul-\\ntaneously. For example, to test if a categorical variable wi thklevels can\\nbe excluded from a model, we need to test whether the coeﬃcien ts of the\\ndummy variables used to represent the levels can all be set to zero. Here\\nwe use the Fstatistic,\\nF=(RSS0−RSS1)/(p1−p0)\\nRSS1/(N−p1−1), (3.13)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 66}),\n",
       " Document(page_content='whereRSS 1istheresidualsum-of-squaresfortheleastsquaresﬁtofth ebig-\\nger model with p1+1 parameters, and RSS 0the same for the nested smaller\\nmodel with p0+1 parameters, having p1−p0parameters constrained to be', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 66}),\n",
       " Document(page_content='3.2 Linear Regression Models and Least Squares 49\\nzero. TheFstatistic measures the change in residual sum-of-squares p er\\nadditional parameter in the bigger model, and it is normaliz ed by an esti-\\nmate ofσ2. Under the Gaussian assumptions, and the null hypothesis th at\\nthe smaller model is correct, the Fstatistic will have a Fp1−p0,N−p1−1dis-\\ntribution. It can be shown (Exercise 3.1) that the zjin (3.12) are equivalent\\nto theFstatistic for dropping the single coeﬃcient βjfrom the model. For', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 67}),\n",
       " Document(page_content='largeN, the quantiles of Fp1−p0,N−p1−1approach those of χ2\\np1−p0/(p1−p0).\\nSimilarly, we can isolate βjin (3.10) to obtain a 1 −2αconﬁdence interval\\nforβj:\\n(ˆβj−z(1−α)v1\\n2\\njˆσ,ˆβj+z(1−α)v1\\n2\\njˆσ). (3.14)\\nHerez(1−α)is the 1−αpercentile of the normal distribution:\\nz(1−0.025)= 1.96,\\nz(1−.05)= 1.645,etc.\\nHence the standard practice of reporting ˆβ±2·se(ˆβ) amounts to an ap-\\nproximate 95% conﬁdence interval. Even if the Gaussian erro r assumption', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 67}),\n",
       " Document(page_content='does not hold, this interval will be approximately correct, with its coverage\\napproaching 1−2αas the sample size N→∞.\\nIn a similar fashion we can obtain an approximate conﬁdence s et for the\\nentire parameter vector β, namely\\nCβ={β|(ˆβ−β)TXTX(ˆβ−β)≤ˆσ2χ2\\np+1(1−α)}, (3.15)\\nwhereχ2\\nℓ(1−α)is the 1−αpercentile of the chi-squared distribution on ℓ\\ndegrees of freedom: for example, χ2\\n5(1−0.05)= 11.1,χ2\\n5(1−0.1)= 9.2. This\\nconﬁdence set for βgenerates a corresponding conﬁdence set for the true', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 67}),\n",
       " Document(page_content='functionf(x) =xTβ, namely{xTβ|β∈Cβ}(Exercise 3.2; see also Fig-\\nure 5.4 in Section 5.2.2 for examples of conﬁdence bands for f unctions).\\n3.2.1 Example: Prostate Cancer\\nThe data for this example come from a study by Stamey et al. (19 89). They\\nexamined the correlation between the level of prostate-spe ciﬁc antigen and\\na number of clinical measures in men who were about to receive a radical\\nprostatectomy. The variables are log cancer volume ( lcavol), log prostate', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 67}),\n",
       " Document(page_content='weight ( lweight),age, log of the amount of benign prostatic hyperplasia\\n(lbph), seminal vesicle invasion ( svi), log of capsular penetration ( lcp),\\nGleason score ( gleason), and percent of Gleason scores 4 or 5 ( pgg45).\\nThe correlation matrix of the predictors given in Table 3.1 s hows many\\nstrong correlations. Figure 1.1 (page 3) of Chapter 1 is a sca tterplot matrix\\nshowing every pairwise plot between the variables. We see th atsviis a', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 67}),\n",
       " Document(page_content='binary variable, and gleasonis an ordered categorical variable. We see, for', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 67}),\n",
       " Document(page_content='50 3. Linear Methods for Regression\\nTABLE 3.1. Correlations of predictors in the prostate cancer data.\\nlcavol lweight age lbph svi lcp gleason\\nlweight 0.300\\nage0.286 0.317\\nlbph0.063 0.437 0.287\\nsvi0.593 0.181 0.129 −0.139\\nlcp0.692 0.157 0.173 −0.089 0.671\\ngleason 0.426 0.024 0.366 0.033 0.307 0.476\\npgg45 0.483 0.074 0.276 −0.030 0.481 0.663 0.757\\nTABLE 3.2. Linear model ﬁt to the prostate cancer data. The Z score is the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 68}),\n",
       " Document(page_content='coeﬃcient divided by its standard error (3.12). Roughly a Zscore larger than two\\nin absolute value is signiﬁcantly nonzero at the p= 0.05level.\\nTerm Coeﬃcient Std. Error ZScore\\nIntercept 2.46 0.09 27.60\\nlcavol 0.68 0.13 5.37\\nlweight 0.26 0.10 2.75\\nage−0.14 0.10 −1.40\\nlbph 0.21 0.10 2.06\\nsvi 0.31 0.12 2.47\\nlcp−0.29 0.15 −1.87\\ngleason−0.02 0.15 −0.15\\npgg45 0.27 0.15 1.74\\nexample, that both lcavolandlcpshow a strong relationship with the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 68}),\n",
       " Document(page_content='response lpsa, and with each other. We need to ﬁt the eﬀects jointly to\\nuntangle the relationships between the predictors and the r esponse.\\nWe ﬁt a linear model to the log of prostate-speciﬁc antigen, lpsa, after\\nﬁrst standardizing the predictors to have unit variance. We randomly split\\nthe dataset into a training set of size 67 and a test set of size 30. We ap-\\nplied least squares estimation to the training set, produci ng the estimates,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 68}),\n",
       " Document(page_content='standard errors and Z-scores shown in Table 3.2. The Z-scores are deﬁned\\nin (3.12), and measure the eﬀect of dropping that variable fr om the model.\\nAZ-score greater than 2 in absolute value is approximately sig niﬁcant at\\nthe 5% level. (For our example, we have nine parameters, and t he 0.025 tail\\nquantiles of the t67−9distribution are±2.002!) The predictor lcavolshows\\nthe strongest eﬀect, with lweightandsvialso strong. Notice that lcpis', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 68}),\n",
       " Document(page_content='not signiﬁcant, once lcavolis in the model (when used in a model without\\nlcavol,lcpis strongly signiﬁcant). We can also test for the exclusion o f\\na number of terms at once, using the F-statistic (3.13). For example, we\\nconsider dropping all the non-signiﬁcant terms in Table 3.2 , namely age,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 68}),\n",
       " Document(page_content='3.2 Linear Regression Models and Least Squares 51\\nlcp,gleason, andpgg45. We get\\nF=(32.81−29.43)/(9−5)\\n29.43/(67−9)= 1.67, (3.16)\\nwhich has a p-value of 0.17 (Pr(F4,58>1.67) = 0.17), and hence is not\\nsigniﬁcant.\\nThe mean prediction error on the test data is 0 .521. In contrast, predic-\\ntion using the mean training value of lpsahas a test error of 1 .057, which\\nis called the “base error rate.” Hence the linear model reduc es the base', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 69}),\n",
       " Document(page_content='error rate by about 50%. We will return to this example later t o compare\\nvarious selection and shrinkage methods.\\n3.2.2 The Gauss–Markov Theorem\\nOne of the most famous results in statistics asserts that the least squares\\nestimates of the parameters βhave the smallest variance among all linear\\nunbiased estimates. We will make this precise here, and also make clear\\nthat the restriction to unbiased estimates is not necessari ly a wise one. This', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 69}),\n",
       " Document(page_content='observationwillleadustoconsiderbiasedestimatessucha sridgeregression\\nlater in the chapter. We focus on estimation of any linear com bination of\\nthe parameters θ=aTβ; for example, predictions f(x0) =xT\\n0βare of this\\nform. The least squares estimate of aTβis\\nˆθ=aTˆβ=aT(XTX)−1XTy. (3.17)\\nConsidering Xto be ﬁxed, this is a linear function cT\\n0yof the response\\nvectory. If we assume that the linear model is correct, aTˆβis unbiased\\nsince\\nE(aTˆβ) = E(aT(XTX)−1XTy)\\n=aT(XTX)−1XTXβ\\n=aTβ. (3.18)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 69}),\n",
       " Document(page_content='The Gauss–Markov theorem states that if we have any other lin ear estima-\\ntor˜θ=cTythat is unbiased for aTβ, that is, E( cTy) =aTβ, then\\nVar(aTˆβ)≤Var(cTy). (3.19)\\nThe proof (Exercise 3.3) uses the triangle inequality. For s implicity we have\\nstated the result in terms of estimation of a single paramete raTβ, but with\\na few more deﬁnitions one can state it in terms of the entire pa rameter\\nvectorβ(Exercise 3.3).\\nConsider the mean squared error of an estimator ˜θin estimating θ:', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 69}),\n",
       " Document(page_content='MSE(˜θ) = E( ˜θ−θ)2\\n= Var( ˜θ)+[E(˜θ)−θ]2. (3.20)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 69}),\n",
       " Document(page_content='52 3. Linear Methods for Regression\\nThe ﬁrst term is the variance, while the second term is the squ ared bias.\\nTheGauss-Markovtheoremimpliesthattheleastsquaresest imatorhasthe\\nsmallest mean squared error of all linear estimators with no bias. However,\\nthere may well exist a biased estimator with smaller mean squ ared error.\\nSuchanestimatorwouldtradealittlebiasforalargerreduc tioninvariance.\\nBiased estimates are commonly used. Any method that shrinks or sets to', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 70}),\n",
       " Document(page_content='zero some of the least squares coeﬃcients may result in a bias ed estimate.\\nWe discuss many examples, including variable subset select ion and ridge\\nregression, later in this chapter. From a more pragmatic poi nt of view, most\\nmodels are distortions of the truth, and hence are biased; pi cking the right\\nmodel amounts to creating the right balance between bias and variance.\\nWe go into these issues in more detail in Chapter 7.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 70}),\n",
       " Document(page_content='Mean squared error is intimately related to prediction accu racy, as dis-\\ncussed in Chapter 2. Consider the prediction of the new respo nse at input\\nx0,\\nY0=f(x0)+ε0. (3.21)\\nThen the expected prediction error of an estimate ˜f(x0) =xT\\n0˜βis\\nE(Y0−˜f(x0))2=σ2+E(xT\\n0˜β−f(x0))2\\n=σ2+MSE( ˜f(x0)). (3.22)\\nTherefore, expected prediction error and mean squared erro r diﬀer only by\\nthe constant σ2, representing the variance of the new observation y0.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 70}),\n",
       " Document(page_content='3.2.3 Multiple Regression from Simple Univariate Regressi on\\nThe linear model (3.1) with p >1 inputs is called the multiple linear\\nregression model . The least squares estimates (3.6) for this model are best\\nunderstood in terms of the estimates for the univariate (p= 1) linear\\nmodel, as we indicate in this section.\\nSuppose ﬁrst that we have a univariate model with no intercep t, that is,\\nY=Xβ+ε. (3.23)\\nThe least squares estimate and residuals are\\nˆβ=∑N\\n1xiyi∑N\\n1x2\\ni,\\nri=yi−xiˆβ.(3.24)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 70}),\n",
       " Document(page_content='In convenient vector notation, we let y= (y1,...,y N)T,x= (x1,...,x N)T\\nand deﬁne\\n⟨x,y⟩=N∑\\ni=1xiyi,\\n=xTy, (3.25)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 70}),\n",
       " Document(page_content='3.2 Linear Regression Models and Least Squares 53\\ntheinner product between xandy1. Then we can write\\nˆβ=⟨x,y⟩\\n⟨x,x⟩,\\nr=y−xˆβ.(3.26)\\nAs we will see, this simple univariate regression provides t he building block\\nfor multiple linear regression. Suppose next that the input sx1,x2,...,xp\\n(the columns of the data matrix X) are orthogonal; that is ⟨xj,xk⟩= 0\\nfor allj̸=k. Then it is easy to check that the multiple least squares esti -', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 71}),\n",
       " Document(page_content='matesˆβjare equal to⟨xj,y⟩/⟨xj,xj⟩—the univariate estimates. In other\\nwords, when the inputs are orthogonal, they have no eﬀect on e ach other’s\\nparameter estimates in the model.\\nOrthogonal inputs occur most often with balanced, designed experiments\\n(where orthogonality is enforced), but almost never with ob servational\\ndata. Hence we will have to orthogonalize them in order to car ry this idea\\nfurther. Suppose next that we have an intercept and a single i nputx. Then', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 71}),\n",
       " Document(page_content='the least squares coeﬃcient of xhas the form\\nˆβ1=⟨x−¯x1,y⟩\\n⟨x−¯x1,x−¯x1⟩, (3.27)\\nwhere ¯x=∑\\nixi/N, and1=x0, the vector of Nones. We can view the\\nestimate (3.27) as the result of two applications of the simp le regression\\n(3.26). The steps are:\\n1. regress xon1to produce the residual z=x−¯x1;\\n2. regress yon the residual zto give the coeﬃcient ˆβ1.\\nInthisprocedure,“regress bona”meansasimpleunivariateregressionof b\\nonawith no intercept, producing coeﬃcient ˆ γ=⟨a,b⟩/⟨a,a⟩and residual', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 71}),\n",
       " Document(page_content='vectorb−ˆγa. We say that bis adjusted for a, or is “orthogonalized” with\\nrespect to a.\\nStep 1 orthogonalizes xwith respect to x0=1. Step 2 is just a simple\\nunivariate regression, using the orthogonal predictors 1andz. Figure 3.4\\nshows this process for two general inputs x1andx2. The orthogonalization\\ndoes not change the subspace spanned by x1andx2, it simply produces an\\northogonal basis for representing it.\\nThis recipe generalizes to the case of pinputs, as shown in Algorithm 3.1.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 71}),\n",
       " Document(page_content='Note that the inputs z0,...,zj−1in step 2 are orthogonal, hence the simple\\nregression coeﬃcients computed there are in fact also the mu ltiple regres-\\nsion coeﬃcients.\\n1The inner-product notation is suggestive of generalizations of linea r regression to\\ndiﬀerent metric spaces, as well as to probability spaces.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 71}),\n",
       " Document(page_content='54 3. Linear Methods for Regression\\nx1x2y\\nˆ yz z z z z\\nFIGURE 3.4. Least squares regression by orthogonalization of the inputs . The\\nvectorx2is regressed on the vector x1, leaving the residual vector z. The regres-\\nsion ofyonzgives the multiple regression coeﬃcient of x2. Adding together the\\nprojections of yon each of x1andzgives the least squares ﬁt ˆy.\\nAlgorithm 3.1 Regression by Successive Orthogonalization.\\n1. Initialize z0=x0=1.\\n2. Forj= 1,2,...,p', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 72}),\n",
       " Document(page_content='Regress xjonz0,z1,...,,zj−1to produce coeﬃcients ˆ γℓj=\\n⟨zℓ,xj⟩/⟨zℓ,zℓ⟩,ℓ= 0,...,j−1 and residual vector zj=\\nxj−∑j−1\\nk=0ˆγkjzk.\\n3. Regress yon the residual zpto give the estimate ˆβp.\\nThe result of this algorithm is\\nˆβp=⟨zp,y⟩\\n⟨zp,zp⟩. (3.28)\\nRe-arranging the residual in step 2, we can see that each of th exjis a linear\\ncombination of the zk, k≤j. Since the zjare all orthogonal, they form\\na basis for the column space of X, and hence the least squares projection', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 72}),\n",
       " Document(page_content='onto this subspace is ˆy. Sincezpalone involves xp(with coeﬃcient 1), we\\nsee that the coeﬃcient (3.28) is indeed the multiple regress ion coeﬃcient of\\nyonxp. This key result exposes the eﬀect of correlated inputs in mu ltiple\\nregression. Note also that by rearranging the xj, any one of them could\\nbe in the last position, and a similar results holds. Hence st ated more\\ngenerally, we have shown that the jth multiple regression coeﬃcient is the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 72}),\n",
       " Document(page_content='univariate regression coeﬃcient of yonxj·012...(j−1)(j+1)...,p, the residual\\nafter regressing xjonx0,x1,...,xj−1,xj+1,...,xp:', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 72}),\n",
       " Document(page_content='3.2 Linear Regression Models and Least Squares 55\\nThe multiple regression coeﬃcient ˆβjrepresents the additional\\ncontribution of xjony, afterxjhas been adjusted for x0,x1,...,xj−1,\\nxj+1,...,xp.\\nIfxpis highly correlated with some of the other xk’s, the residual vector\\nzpwill be close to zero, and from (3.28) the coeﬃcient ˆβpwill be very\\nunstable. This will be true for all the variables in the corre lated set. In\\nsuch situations, we might have all the Z-scores (as in Table 3 .2) be small—', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 73}),\n",
       " Document(page_content='any one of the set can be deleted—yet we cannot delete them all. From\\n(3.28) we also obtain an alternate formula for the variance e stimates (3.8),\\nVar(ˆβp) =σ2\\n⟨zp,zp⟩=σ2\\n∥zp∥2. (3.29)\\nIn other words, the precision with which we can estimate ˆβpdepends on\\nthe length of the residual vector zp; this represents how much of xpis\\nunexplained by the other xk’s.\\nAlgorithm 3.1 is known as the Gram–Schmidt procedure for multiple', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 73}),\n",
       " Document(page_content='regression, and is also a useful numerical strategy for comp uting the esti-\\nmates. We can obtain from it not just ˆβp, but also the entire multiple least\\nsquares ﬁt, as shown in Exercise 3.4.\\nWe can represent step 2 of Algorithm 3.1 in matrix form:\\nX=ZΓ, (3.30)\\nwhereZhas as columns the zj(in order), and Γis the upper triangular ma-\\ntrix with entries ˆ γkj. Introducing the diagonal matrix Dwithjth diagonal\\nentryDjj=∥zj∥, we get\\nX=ZD−1DΓ\\n=QR, (3.31)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 73}),\n",
       " Document(page_content='the so-called QRdecomposition of X. HereQis anN×(p+1) orthogonal\\nmatrix,QTQ=I, andRis a (p+1)×(p+1) upper triangular matrix.\\nTheQRdecomposition represents a convenient orthogonal basis fo r the\\ncolumn space of X. It is easy to see, for example, that the least squares\\nsolution is given by\\nˆβ=R−1QTy, (3.32)\\nˆy=QQTy. (3.33)\\nEquation (3.32) is easy to solve because Ris upper triangular\\n(Exercise 3.4).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 73}),\n",
       " Document(page_content='56 3. Linear Methods for Regression\\n3.2.4 Multiple Outputs\\nSuppose we have multiple outputs Y1,Y2,...,Y Kthat we wish to predict\\nfrom our inputs X0,X1,X2,...,X p. We assume a linear model for each\\noutput\\nYk=β0k+p∑\\nj=1Xjβjk+εk (3.34)\\n=fk(X)+εk. (3.35)\\nWithNtraining cases we can write the model in matrix notation\\nY=XB+E. (3.36)\\nHereYis theN×Kresponse matrix, with ikentryyik,Xis theN×(p+1)\\ninput matrix, Bis the (p+ 1)×Kmatrix of parameters and Eis the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 74}),\n",
       " Document(page_content='N×Kmatrix of errors. A straightforward generalization of the u nivariate\\nloss function (3.2) is\\nRSS(B) =K∑\\nk=1N∑\\ni=1(yik−fk(xi))2(3.37)\\n= tr[(Y−XB)T(Y−XB)]. (3.38)\\nThe least squares estimates have exactly the same form as bef ore\\nˆB= (XTX)−1XTY. (3.39)\\nHence the coeﬃcients for the kth outcome are just the least squares es-\\ntimates in the regression of ykonx0,x1,...,xp. Multiple outputs do not\\naﬀect one another’s least squares estimates.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 74}),\n",
       " Document(page_content='If the errors ε= (ε1,...,ε K) in (3.34) are correlated, then it might seem\\nappropriate to modify (3.37) in favor of a multivariate vers ion. Speciﬁcally,\\nsuppose Cov( ε) =Σ, then the multivariate weighted criterion\\nRSS(B;Σ) =N∑\\ni=1(yi−f(xi))TΣ−1(yi−f(xi)) (3.40)\\narises naturally from multivariate Gaussian theory. Here f(x) is the vector\\nfunction (f1(x),...,f K(x))T, andyithe vector of Kresponses for obser-\\nvationi. However, it can be shown that again the solution is given by', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 74}),\n",
       " Document(page_content='(3.39);Kseparate regressions that ignore the correlations (Exerci se 3.11).\\nIf theΣivary among observations, then this is no longer the case, and the\\nsolution for Bno longer decouples.\\nIn Section 3.7 we pursue the multiple outcome problem, and co nsider\\nsituations where it does pay to combine the regressions.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 74}),\n",
       " Document(page_content='3.3 Subset Selection 57\\n3.3 Subset Selection\\nThere are two reasons why we are often not satisﬁed with the le ast squares\\nestimates (3.6).\\n•The ﬁrst is prediction accuracy : the least squares estimates often have\\nlow bias but large variance. Prediction accuracy can someti mes be\\nimproved by shrinking or setting some coeﬃcients to zero. By doing\\nsowesacriﬁcealittlebitofbiastoreducethevarianceofth epredicted\\nvalues, and hence may improve the overall prediction accura cy.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 75}),\n",
       " Document(page_content='•The second reason is interpretation . With a large number of predic-\\ntors, we often would like to determine a smaller subset that e xhibit\\nthe strongest eﬀects. In order to get the “big picture,” we ar e willing\\nto sacriﬁce some of the small details.\\nIn this section we describe a number of approaches to variabl e subset selec-\\ntionwithlinearregression.Inlatersectionswediscusssh rinkageandhybrid\\napproaches for controlling variance, as well as other dimen sion-reduction', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 75}),\n",
       " Document(page_content='strategies. These all fall under the general heading model selection . Model\\nselection is not restricted to linear models; Chapter 7 cove rs this topic in\\nsome detail.\\nWith subset selection we retain only a subset of the variable s, and elim-\\ninate the rest from the model. Least squares regression is us ed to estimate\\nthe coeﬃcients of the inputs that are retained. There are a nu mber of dif-\\nferent strategies for choosing the subset.\\n3.3.1 Best-Subset Selection', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 75}),\n",
       " Document(page_content='Best subset regression ﬁnds for each k∈{0,1,2,...,p}the subset of size k\\nthat gives smallest residual sum of squares (3.2). An eﬃcien t algorithm—\\ntheleaps and bounds procedure (Furnival and Wilson, 1974)—makes this\\nfeasible for pas large as 30 or 40. Figure 3.5 shows all the subset models\\nfor the prostate cancer example. The lower boundary represe nts the models\\nthat are eligible for selection by the best-subsets approac h. Note that the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 75}),\n",
       " Document(page_content='best subset of size 2, for example, need not include the varia ble that was\\nin the best subset of size 1 (for this example all the subsets a re nested).\\nThe best-subset curve (red lower boundary in Figure 3.5) is n ecessarily\\ndecreasing, so cannot be used to select the subset size k. The question of\\nhow to choose kinvolves the tradeoﬀ between bias and variance, along with\\nthe more subjective desire for parsimony. There are a number of criteria', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 75}),\n",
       " Document(page_content='that one may use; typically we choose the smallest model that minimizes\\nan estimate of the expected prediction error.\\nMany of the other approaches that we discuss in this chapter a re similar,\\nin that they use the training data to produce a sequence of mod els varying\\nin complexity and indexed by a single parameter. In the next s ection we use', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 75}),\n",
       " Document(page_content='58 3. Linear Methods for Regression\\nSubset Size kResidual Sum−of−Squares\\n0 20 40 60 80 100\\n0 1 2 3 4 5 6 7 8•\\n••••••••\\n••••••••••••••••••••••••••\\n•••••••••••••••••••••••••••••••••••••••••••••••••\\n••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\\n•••••••••••••••••••••••••••••••••••••••••••••\\n••••••••••••••••••••••••••\\n••••••••\\n••\\n•\\n•••••• •\\nFIGURE 3.5. All possible subset models for the prostate cancer example. At', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 76}),\n",
       " Document(page_content='each subset size is shown the residual sum-of-squares for ea ch model of that size.\\ncross-validation to estimate prediction error and select k; the AIC criterion\\nis a popular alternative. We defer more detailed discussion of these and\\nother approaches to Chapter 7.\\n3.3.2 Forward- and Backward-Stepwise Selection\\nRather than search through all possible subsets (which beco mes infeasible\\nforpmuchlargerthan40),wecanseekagoodpaththroughthem. Forward-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 76}),\n",
       " Document(page_content='stepwise selection starts with the intercept, and then sequentially adds into\\nthe model the predictor that most improves the ﬁt. With many c andidate\\npredictors, this might seem like a lot of computation; howev er, clever up-\\ndating algorithms can exploit the QR decomposition for the c urrent ﬁt to\\nrapidly establish the next candidate (Exercise 3.9). Like b est-subset re-\\ngression, forward stepwise produces a sequence of models in dexed byk, the\\nsubset size, which must be determined.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 76}),\n",
       " Document(page_content='Forward-stepwise selection is a greedy algorithm , producing a nested se-\\nquence of models. In this sense it might seem sub-optimal com pared to\\nbest-subset selection. However, there are several reasons why it might be\\npreferred:', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 76}),\n",
       " Document(page_content='3.3 Subset Selection 59\\n•Computational; for largepwe cannot compute the best subset se-\\nquence, but we can always compute the forward stepwise seque nce\\n(even when p≫N).\\n•Statistical; a price is paid in variance for selecting the best subset\\nof each size; forward stepwise is a more constrained search, and will\\nhave lower variance, but perhaps more bias.\\n0 5 10 15 20 25 300.65 0.70 0.75 0.80 0.85 0.90 0.95Best Subset\\nForward Stepwise\\nBackward Stepwise\\nForward StagewiseE||ˆβ(k)−β||2\\nSubset Size k', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 77}),\n",
       " Document(page_content='FIGURE 3.6. Comparison of four subset-selection techniques on a simulate d lin-\\near regression problem Y=XTβ+ε. There are N= 300observations on p= 31\\nstandard Gaussian variables, with pairwise correlations all e qual to0.85. For10of\\nthe variables, the coeﬃcients are drawn at random from a N(0,0.4)distribution;\\nthe rest are zero. The noise ε∼N(0,6.25), resulting in a signal-to-noise ratio of\\n0.64. Results are averaged over 50simulations. Shown is the mean-squared error', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 77}),\n",
       " Document(page_content='of the estimated coeﬃcient ˆβ(k)at each step from the true β.\\nBackward-stepwise selection starts with the full model, and sequentially\\ndeletes the predictor that has the least impact on the ﬁt. The candidate for\\ndroppingisthevariablewiththesmallestZ-score(Exercis e3.10).Backward\\nselection can only be used when N >p, while forward stepwise can always\\nbe used.\\nFigure 3.6 shows the results of a small simulation study to co mpare', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 77}),\n",
       " Document(page_content='best-subset regression with the simpler alternatives forw ard and backward\\nselection. Their performance is very similar, as is often th e case. Included in\\nthe ﬁgure is forward stagewise regression (next section), w hich takes longer\\nto reach minimum error.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 77}),\n",
       " Document(page_content='60 3. Linear Methods for Regression\\nOn the prostate cancer example, best-subset, forward and ba ckward se-\\nlection all gave exactly the same sequence of terms.\\nSome software packages implement hybrid stepwise-selecti on strategies\\nthat consider both forward and backward moves at each step, a nd select\\nthe “best” of the two. For example in the Rpackage the stepfunction uses\\nthe AIC criterion for weighing the choices, which takes prop er account of', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 78}),\n",
       " Document(page_content='the number of parameters ﬁt; at each step an add or drop will be performed\\nthat minimizes the AIC score.\\nOthermoretraditionalpackagesbasetheselectionon F-statistics,adding\\n“signiﬁcant” terms, and dropping “non-signiﬁcant” terms. These are out\\nof fashion, since they do not take proper account of the multi ple testing\\nissues. It is also tempting after a model search to print out a summary of\\nthe chosen model, such as in Table 3.2; however, the standard errors are', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 78}),\n",
       " Document(page_content='not valid, since they do not account for the search process. T he bootstrap\\n(Section 8.2) can be useful in such settings.\\nFinally, we note that often variables come in groups (such as the dummy\\nvariables that code a multi-level categorical predictor). Smart stepwise pro-\\ncedures (such as stepinR) will add or drop whole groups at a time, taking\\nproper account of their degrees-of-freedom.\\n3.3.3 Forward-Stagewise Regression\\nForward-stagewise regression (FS) is even more constraine d than forward-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 78}),\n",
       " Document(page_content='stepwise regression. It starts like forward-stepwise regr ession, with an in-\\ntercept equal to ¯ y, and centered predictors with coeﬃcients initially all 0.\\nAt each step the algorithm identiﬁes the variable most corre lated with the\\ncurrent residual. It then computes the simple linear regres sion coeﬃcient\\nof the residual on this chosen variable, and then adds it to th e current co-\\neﬃcient for that variable. This is continued till none of the variables have', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 78}),\n",
       " Document(page_content='correlation with the residuals—i.e. the least-squares ﬁt wh enN >p.\\nUnlike forward-stepwise regression, none of the other vari ables are ad-\\njusted when a term is added to the model. As a consequence, for ward\\nstagewise can take many more than psteps to reach the least squares ﬁt,\\nand historically has been dismissed as being ineﬃcient. It t urns out that\\nthis “slow ﬁtting” can pay dividends in high-dimensional pr oblems. We', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 78}),\n",
       " Document(page_content='see in Section 3.8.1 that both forward stagewise and a varian t which is\\nslowed down even further are quite competitive, especially in very high-\\ndimensional problems.\\nForward-stagewise regression is included in Figure 3.6. In this example it\\ntakes over 1000 steps to get all the correlations below 10−4. For subset size\\nk, we plotted the error for the last step for which there where knonzero\\ncoeﬃcients. Although it catches up with the best ﬁt, it takes longer to\\ndo so.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 78}),\n",
       " Document(page_content='3.4 Shrinkage Methods 61\\n3.3.4 Prostate Cancer Data Example (Continued)\\nTable 3.3 shows the coeﬃcients from a number of diﬀerent sele ction and\\nshrinkagemethods.Theyare best-subset selection usinganall-subsetssearch,\\nridge regression , thelasso,principal components regression andpartial least\\nsquares. Each method has a complexity parameter, and this was chosen to\\nminimize an estimate of prediction error based on tenfold cr oss-validation;', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 79}),\n",
       " Document(page_content='fulldetailsaregiveninSection7.10.Brieﬂy,cross-valid ationworksbydivid-\\ning the training data randomly into ten equal parts. The lear ning method\\nis ﬁt—for a range of values of the complexity parameter—to nine -tenths of\\nthe data, and the prediction error is computed on the remaini ng one-tenth.\\nThis is done in turn for each one-tenth of the data, and the ten prediction\\nerror estimates are averaged. From this we obtain an estimat ed prediction', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 79}),\n",
       " Document(page_content='error curve as a function of the complexity parameter.\\nNote that we have already divided these data into a training s et of size\\n67 and a test set of size 30. Cross-validation is applied to th e training set,\\nsince selecting the shrinkage parameter is part of the train ing process. The\\ntest set is there to judge the performance of the selected mod el.\\nThe estimated prediction error curves are shown in Figure 3. 7. Many of\\nthe curves are very ﬂat over large ranges near their minimum. Included', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 79}),\n",
       " Document(page_content='are estimated standard error bands for each estimated error rate, based on\\nthe ten error estimates computed by cross-validation. We ha ve used the\\n“one-standard-error” rule—we pick the most parsimonious mo del within\\none standard error of the minimum (Section 7.10, page 244). S uch a rule\\nacknowledges the fact that the tradeoﬀ curve is estimated wi th error, and\\nhence takes a conservative approach.\\nBest-subset selection chose to use the two predictors lcvolandlweight.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 79}),\n",
       " Document(page_content='The last two lines of the table give the average prediction er ror (and its\\nestimated standard error) over the test set.\\n3.4 Shrinkage Methods\\nBy retaining a subset of the predictors and discarding the re st, subset selec-\\ntion produces a model that is interpretable and has possibly lower predic-\\ntion error than the full model. However, because it is a discr ete process—\\nvariables are either retained or discarded—it often exhibit s high variance,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 79}),\n",
       " Document(page_content='and so doesn’t reduce the prediction error of the full model. Shrinkage\\nmethods are more continuous, and don’t suﬀer as much from hig h\\nvariability.\\n3.4.1 Ridge Regression\\nRidge regression shrinks the regression coeﬃcients by impo sing a penalty\\non their size. The ridge coeﬃcients minimize a penalized res idual sum of', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 79}),\n",
       " Document(page_content='62 3. Linear Methods for Regression\\nSubset SizeCV Error\\n0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•\\n•\\n•••••••All Subsets\\nDegrees of FreedomCV Error\\n0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•\\n•\\n•\\n••••••Ridge Regression\\nShrinkage Factor sCV Error\\n0.0 0.2 0.4 0.6 0.8 1.00.6 0.8 1.0 1.2 1.4 1.6 1.8•\\n•\\n•\\n••••••Lasso\\nNumber of DirectionsCV Error\\n0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•\\n••••••••Principal Components Regression\\nNumber of  DirectionsCV Error\\n0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 80}),\n",
       " Document(page_content='••••••• •Partial Least Squares\\nFIGURE 3.7. Estimated prediction error curves and their standard error s for\\nthe various selection and shrinkage methods. Each curve is plo tted as a function\\nof the corresponding complexity parameter for that method. T he horizontal axis\\nhas been chosen so that the model complexity increases as we mo ve from left to\\nright. The estimates of prediction error and their standard errors were obtained by', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 80}),\n",
       " Document(page_content='tenfold cross-validation; full details are given in Section 7. 10. The least complex\\nmodel within one standard error of the best is chosen, indica ted by the purple\\nvertical broken lines.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 80}),\n",
       " Document(page_content='3.4 Shrinkage Methods 63\\nTABLE 3.3. Estimated coeﬃcients and test error results, for diﬀerent su bset\\nand shrinkage methods applied to the prostate data. The blank e ntries correspond\\nto variables omitted.\\nTerm LS Best Subset Ridge Lasso PCR PLS\\nIntercept 2.465 2.477 2.452 2.468 2.497 2.452\\nlcavol 0.680 0.740 0.420 0.533 0.543 0.419\\nlweight 0.263 0.316 0.238 0.169 0.289 0.344\\nage−0.141 −0.046 −0.152−0.026\\nlbph 0.210 0.162 0.002 0.214 0.220\\nsvi0.305 0.227 0.094 0.315 0.243', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 81}),\n",
       " Document(page_content='lcp−0.288 0.000 −0.051 0.079\\ngleason −0.021 0.040 0.232 0.011\\npgg45 0.267 0.133 −0.056 0.084\\nTest Error 0.521 0.492 0.492 0.479 0.449 0.528\\nStd Error 0.179 0.143 0.165 0.164 0.105 0.152\\nsquares,\\nˆβridge= argmin\\nβ{N∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2+λp∑\\nj=1β2\\nj}\\n.(3.41)\\nHereλ≥0 is a complexity parameter that controls the amount of shrin k-\\nage: the larger the value of λ, the greater the amount of shrinkage. The\\ncoeﬃcients are shrunk toward zero (and each other). The idea of penaliz-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 81}),\n",
       " Document(page_content='ing by the sum-of-squares of the parameters is also used in ne ural networks,\\nwhere it is known as weight decay (Chapter 11).\\nAn equivalent way to write the ridge problem is\\nˆβridge= argmin\\nβN∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2\\n,\\nsubject top∑\\nj=1β2\\nj≤t,(3.42)\\nwhich makes explicit the size constraint on the parameters. There is a one-\\nto-one correspondence between the parameters λin (3.41) and tin (3.42).\\nWhen there are many correlated variables in a linear regress ion model,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 81}),\n",
       " Document(page_content='their coeﬃcients can become poorly determined and exhibit h igh variance.\\nA wildly large positive coeﬃcient on one variable can be canc eled by a\\nsimilarly large negative coeﬃcient on its correlated cousi n. By imposing a\\nsize constraint on the coeﬃcients, as in (3.42), this proble m is alleviated.\\nThe ridge solutions are not equivariant under scaling of the inputs, and\\nso one normally standardizes the inputs before solving (3.4 1). In addition,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 81}),\n",
       " Document(page_content='64 3. Linear Methods for Regression\\nnotice that the intercept β0has been left out of the penalty term. Penal-\\nization of the intercept would make the procedure depend on t he origin\\nchosen forY; that is, adding a constant cto each of the targets yiwould\\nnot simply result in a shift of the predictions by the same amo untc. It\\ncan be shown (Exercise 3.5) that the solution to (3.41) can be separated\\ninto two parts, after reparametrization using centered inputs: each xijgets', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 82}),\n",
       " Document(page_content='replaced by xij−¯xj. We estimate β0by ¯y=1\\nN∑N\\n1yi. The remaining co-\\neﬃcients get estimated by a ridge regression without interc ept, using the\\ncenteredxij. Henceforth we assume that this centering has been done, so\\nthat the input matrix Xhasp(rather than p+1) columns.\\nWriting the criterion in (3.41) in matrix form,\\nRSS(λ) = (y−Xβ)T(y−Xβ)+λβTβ, (3.43)\\nthe ridge regression solutions are easily seen to be\\nˆβridge= (XTX+λI)−1XTy, (3.44)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 82}),\n",
       " Document(page_content='whereIis thep×pidentity matrix. Notice that with the choice of quadratic\\npenaltyβTβ, the ridge regression solution is again a linear function of\\ny. The solution adds a positive constant to the diagonal of XTXbefore\\ninversion. This makes the problem nonsingular, even if XTXis not of full\\nrank, and was the main motivation for ridge regression when i t was ﬁrst\\nintroducedinstatistics(HoerlandKennard,1970).Tradit ionaldescriptions', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 82}),\n",
       " Document(page_content='of ridge regression start with deﬁnition (3.44). We choose t o motivate it via\\n(3.41) and (3.42), as these provide insight into how it works .\\nFigure 3.8 shows the ridge coeﬃcient estimates for the prost ate can-\\ncer example, plotted as functions of df( λ), theeﬀective degrees of freedom\\nimplied by the penalty λ(deﬁned in (3.50) on page 68). In the case of or-\\nthonormal inputs, the ridge estimates are just a scaled vers ion of the least\\nsquares estimates, that is, ˆβridge=ˆβ/(1+λ).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 82}),\n",
       " Document(page_content='Ridge regression can also be derived as the mean or mode of a po ste-\\nrior distribution, with a suitably chosen prior distributi on. In detail, sup-\\nposeyi∼N(β0+xT\\niβ,σ2), and the parameters βjare each distributed as\\nN(0,τ2), independently of one another. Then the (negative) log-po sterior\\ndensity ofβ, withτ2andσ2assumed known, is equal to the expression\\nin curly braces in (3.41), with λ=σ2/τ2(Exercise 3.6). Thus the ridge', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 82}),\n",
       " Document(page_content='estimate is the mode of the posterior distribution; since th e distribution is\\nGaussian, it is also the posterior mean.\\nThesingular value decomposition (SVD) of the centered input matrix X\\ngives us some additional insight into the nature of ridge reg ression. This de-\\ncomposition is extremely useful in the analysis of many stat istical methods.\\nThe SVD of the N×pmatrixXhas the form\\nX=UDVT. (3.45)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 82}),\n",
       " Document(page_content='3.4 Shrinkage Methods 65Coefficients\\n0 2 4 6 8−0.2 0.0 0.2 0.4 0.6•\\n•••••\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n••••••\\n•lcavol\\n••••••••••••••••••••••••\\n•lweight\\n•••••••••••••••••••••••••\\nage••••••••••••••••••••••••\\n•lbph••••••••••••••••••••••••\\n•svi\\n••••••••••••••••••••••••\\n•\\nlcp••••••••••••••••••••••••\\n•gleason•\\n•••••••••••••••••••••••\\n•pgg45\\ndf(λ)\\nFIGURE 3.8. Proﬁles of ridge coeﬃcients for the prostate cancer example, a s\\nthe tuning parameter λis varied. Coeﬃcients are plotted versus df(λ), the eﬀective', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 83}),\n",
       " Document(page_content='degrees of freedom. A vertical line is drawn at df = 5.0, the value chosen by\\ncross-validation.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 83}),\n",
       " Document(page_content='66 3. Linear Methods for Regression\\nHereUandVareN×pandp×porthogonal matrices, with the columns\\nofUspanning the column space of X, and the columns of Vspanning the\\nrow space. Dis ap×pdiagonal matrix, with diagonal entries d1≥d2≥\\n···≥dp≥0 called the singular values of X. If one or more values dj= 0,\\nXis singular.\\nUsing the singular value decomposition we can write the leas t squares\\nﬁtted vector as\\nXˆβls=X(XTX)−1XTy\\n=UUTy, (3.46)\\nafter some simpliﬁcation. Note that UTyare the coordinates of ywith', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 84}),\n",
       " Document(page_content='respect to the orthonormal basis U. Note also the similarity with (3.33);\\nQandUare generally diﬀerent orthogonal bases for the column spac e of\\nX(Exercise 3.8).\\nNow the ridge solutions are\\nXˆβridge=X(XTX+λI)−1XTy\\n=U D(D2+λI)−1D UTy\\n=p∑\\nj=1ujd2\\nj\\nd2\\nj+λuT\\njy, (3.47)\\nwhere the ujare the columns of U. Note that since λ≥0, we haved2\\nj/(d2\\nj+\\nλ)≤1. Like linear regression, ridge regression computes the co ordinates of\\nywithrespecttotheorthonormalbasis U.Itthenshrinksthesecoordinates\\nby the factors d2', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 84}),\n",
       " Document(page_content='j/(d2\\nj+λ). This means that a greater amount of shrinkage\\nis applied to the coordinates of basis vectors with smaller d2\\nj.\\nWhat does a small value of d2\\njmean? The SVD of the centered matrix\\nXis another way of expressing the principal components of the variables\\ninX. The sample covariance matrix is given by S=XTX/N, and from\\n(3.45) we have\\nXTX=VD2VT, (3.48)\\nwhich is the eigen decomposition ofXTX(and ofS, up to a factor N).\\nThe eigenvectors vj(columns of V) are also called the principal compo-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 84}),\n",
       " Document(page_content='nents(or Karhunen–Loeve) directions of X. The ﬁrst principal component\\ndirectionv1has the property that z1=Xv1has the largest sample vari-\\nance amongst all normalized linear combinations of the colu mns ofX. This\\nsample variance is easily seen to be\\nVar(z1) = Var(Xv1) =d2\\n1\\nN, (3.49)\\nand in fact z1=Xv1=u1d1. The derived variable z1is called the ﬁrst\\nprincipal component of X, and hence u1is the normalized ﬁrst principal', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 84}),\n",
       " Document(page_content='3.4 Shrinkage Methods 67\\n-4 -2 0 2 4-4 -2 0 2 4ooo\\nooooooo\\no\\no\\nooo\\noo\\noo\\noo\\nooo\\no\\no\\noooo\\noo\\no\\nooo\\nooo\\nooooo\\noo\\nooo\\nooo\\noo\\noooo\\noo\\no\\noo\\noo\\nooo\\nooo\\nooo\\nooo\\no\\noo\\nooooo\\no\\noooo\\nooo\\no\\noo\\no\\noo\\noo\\no\\noo\\noooooo\\nooo\\noo\\nooo\\noo\\no\\nooo\\noooo\\no\\nooo\\nooo\\no\\nooo\\nooo\\noo\\no\\noooo\\no\\noooo\\noo\\noo\\noo\\nooo\\no\\nooo\\noo\\noo\\no\\noo\\noo\\noooo\\no\\nooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooLargest Principal\\nComponent\\nSmallest Principal\\nComponent\\nX1X2\\nFIGURE 3.9. Principal components of some input data points. The largest p rin-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 85}),\n",
       " Document(page_content='cipal component is the direction that maximizes the varianc e of the projected data,\\nand the smallest principal component minimizes that varianc e. Ridge regression\\nprojects yonto these components, and then shrinks the coeﬃcients of th e low–\\nvariance components more than the high-variance component s.\\ncomponent. Subsequent principal components zjhave maximum variance\\nd2\\nj/N, subject to being orthogonal to the earlier ones. Conversel y the last', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 85}),\n",
       " Document(page_content='principal component has minimum variance. Hence the small singular val-\\nuesdjcorrespond to directions in the column space of Xhaving small\\nvariance, and ridge regression shrinks these directions th e most.\\nFigure 3.9 illustrates the principal components of some dat a points in\\ntwo dimensions. If we consider ﬁtting a linear surface over t his domain\\n(theY-axis is sticking out of the page), the conﬁguration of the da ta allow', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 85}),\n",
       " Document(page_content='us to determine its gradient more accurately in the long dire ction than\\nthe short. Ridge regression protects against the potential ly high variance\\nof gradients estimated in the short directions. The implici t assumption is\\nthat the response will tend to vary most in the directions of h igh variance\\nof the inputs. This is often a reasonable assumption, since p redictors are\\noften chosen for study because they vary with the response va riable, but\\nneed not hold in general.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 85}),\n",
       " Document(page_content='68 3. Linear Methods for Regression\\nIn Figure 3.7 we have plotted the estimated prediction error versus the\\nquantity\\ndf(λ) = tr[ X(XTX+λI)−1XT],\\n= tr(Hλ)\\n=p∑\\nj=1d2\\nj\\nd2\\nj+λ. (3.50)\\nThis monotone decreasing function of λis theeﬀective degrees of freedom\\nof the ridge regression ﬁt. Usually in a linear-regression ﬁ t withpvariables,\\nthe degrees-of-freedom of the ﬁt is p, the number of free parameters. The\\nidea is that although all pcoeﬃcients in a ridge ﬁt will be non-zero, they', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 86}),\n",
       " Document(page_content='are ﬁt in a restricted fashion controlled by λ. Note that df( λ) =pwhen\\nλ= 0 (no regularization) and df( λ)→0 asλ→ ∞. Of course there\\nis always an additional one degree of freedom for the interce pt, which was\\nremoved apriori. This deﬁnition is motivated in more detail in Section 3.4.4\\nand Sections 7.4–7.6. In Figure 3.7 the minimum occurs at df( λ) = 5.0.\\nTable 3.3 shows that ridge regression reduces the test error of the full least\\nsquares estimates by a small amount.\\n3.4.2 The Lasso', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 86}),\n",
       " Document(page_content='The lasso is a shrinkage method like ridge, with subtle but im portant dif-\\nferences. The lasso estimate is deﬁned by\\nˆβlasso= argmin\\nβN∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2\\nsubject top∑\\nj=1|βj|≤t. (3.51)\\nJust as in ridge regression, we can re-parametrize the const antβ0by stan-\\ndardizing the predictors; the solution for ˆβ0is ¯y, and thereafter we ﬁt a\\nmodel without an intercept (Exercise 3.5). In the signal pro cessing litera-\\nture, the lasso is also known as basis pursuit (Chen et al., 1998).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 86}),\n",
       " Document(page_content='We can also write the lasso problem in the equivalent Lagrangian form\\nˆβlasso= argmin\\nβ{1\\n2N∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2+λp∑\\nj=1|βj|}\\n.(3.52)\\nNotice the similarity to the ridge regression problem (3.42 ) or (3.41): the\\nL2ridge penalty∑p\\n1β2\\njis replaced by the L1lasso penalty∑p\\n1|βj|. This\\nlatter constraint makes the solutions nonlinear in the yi, and there is no\\nclosed form expression as in ridge regression. Computing th e lasso solution', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 86}),\n",
       " Document(page_content='3.4 Shrinkage Methods 69\\nis a quadratic programming problem, although we see in Secti on 3.4.4 that\\neﬃcient algorithms are available for computing the entire p ath of solutions\\nasλis varied, with the same computational cost as for ridge regr ession.\\nBecause of the nature of the constraint, making tsuﬃciently small will\\ncause some of the coeﬃcients to be exactly zero. Thus the lass o does a kind\\nof continuous subset selection. If tis chosen larger than t0=∑p\\n1|ˆβj|(where\\nˆβj=ˆβls', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 87}),\n",
       " Document(page_content='j, the least squares estimates), then the lasso estimates are theˆβj’s.\\nOn the other hand, for t=t0/2 say, then the least squares coeﬃcients are\\nshrunk by about 50% on average. However, the nature of the shr inkage\\nis not obvious, and we investigate it further in Section 3.4. 4 below. Like\\nthe subset size in variable subset selection, or the penalty parameter in\\nridge regression, tshould be adaptively chosen to minimize an estimate of\\nexpected prediction error.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 87}),\n",
       " Document(page_content='In Figure 3.7, for ease of interpretation, we have plotted th e lasso pre-\\ndiction error estimates versus the standardized parameter s=t/∑p\\n1|ˆβj|.\\nA value ˆs≈0.36 was chosen by 10-fold cross-validation; this caused four\\ncoeﬃcients to be set to zero (ﬁfth column of Table 3.3). The re sulting\\nmodel has the second lowest test error, slightly lower than t he full least\\nsquares model, but the standard errors of the test error esti mates (last line\\nof Table 3.3) are fairly large.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 87}),\n",
       " Document(page_content='Figure 3.10 shows the lasso coeﬃcients as the standardized t uning pa-\\nrameters=t/∑p\\n1|ˆβj|is varied. At s= 1.0 these are the least squares\\nestimates; they decrease to 0 as s→0. This decrease is not always strictly\\nmonotonic, although it is in this example. A vertical line is drawn at\\ns= 0.36, the value chosen by cross-validation.\\n3.4.3 Discussion: Subset Selection, Ridge Regression and t he\\nLasso\\nInthissectionwediscussandcomparethethreeapproachesd iscussedsofar', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 87}),\n",
       " Document(page_content='for restricting the linear regression model: subset select ion, ridge regression\\nand the lasso.\\nIn the case of an orthonormal input matrix Xthe three procedures have\\nexplicit solutions. Each method applies a simple transform ation to the least\\nsquares estimate ˆβj, as detailed in Table 3.4.\\nRidge regression does a proportional shrinkage. Lasso tran slates each\\ncoeﬃcient by a constant factor λ, truncating at zero. This is called “soft', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 87}),\n",
       " Document(page_content='thresholding,”andisusedinthecontextofwavelet-baseds moothinginSec-\\ntion 5.9. Best-subset selection drops all variables with co eﬃcients smaller\\nthan theMth largest; this is a form of “hard-thresholding.”\\nBack to the nonorthogonal case; some pictures help understa nd their re-\\nlationship. Figure 3.11 depicts the lasso (left) and ridge r egression (right)\\nwhen there are only two parameters. The residual sum of squar es has ellip-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 87}),\n",
       " Document(page_content='tical contours, centered at the full least squares estimate . The constraint', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 87}),\n",
       " Document(page_content='70 3. Linear Methods for Regression\\n0.0 0.2 0.4 0.6 0.8 1.0−0.2 0.0 0.2 0.4 0.6\\nShrinkage Factor sCoefficientslcavol\\nlweight\\nagelbphsvi\\nlcpgleasonpgg45\\nFIGURE 3.10. Proﬁles of lasso coeﬃcients, as the tuning parameter tis varied.\\nCoeﬃcients are plotted versus s=t/∑p\\n1|ˆβj|. A vertical line is drawn at s= 0.36,\\nthe value chosen by cross-validation. Compare Figure 3.8 on pa ge 65; the lasso\\nproﬁles hit zero, while those for ridge do not. The proﬁles are pi ece-wise linear,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 88}),\n",
       " Document(page_content='and so are computed only at the points displayed; see Section 3. 4.4 for details.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 88}),\n",
       " Document(page_content='3.4 Shrinkage Methods 71\\nTABLE 3.4. Estimators of βjin the case of orthonormal columns of X.Mandλ\\nare constants chosen by the corresponding techniques; signdenotes the sign of its\\nargument ( ±1), andx+denotes “positive part” of x. Below the table, estimators\\nare shown by broken red lines. The 45◦line in gray shows the unrestricted estimate\\nfor reference.\\nEstimator Formula\\nBest subset (size M)ˆβj·I(|ˆβj|≥|ˆβ(M)|)\\nRidge ˆβj/(1+λ)\\nLasso sign( ˆβj)(|ˆβj|−λ)+', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 89}),\n",
       " Document(page_content='(0,0) (0,0) (0,0)|ˆβ(M)|λBest Subset Ridge Lasso\\nβ^β^ 2. . β\\n1β2\\nβ1β\\nFIGURE 3.11. Estimation picture for the lasso (left) and ridge regression\\n(right). Shown are contours of the error and constraint func tions. The solid blue\\nareas are the constraint regions |β1|+|β2| ≤tandβ2\\n1+β2\\n2≤t2, respectively,\\nwhile the red ellipses are the contours of the least squares erro r function.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 89}),\n",
       " Document(page_content='72 3. Linear Methods for Regression\\nregion for ridge regression is the disk β2\\n1+β2\\n2≤t, while that for lasso is\\nthe diamond|β1|+|β2|≤t. Both methods ﬁnd the ﬁrst point where the\\nelliptical contours hit the constraint region. Unlike the d isk, the diamond\\nhas corners; if the solution occurs at a corner, then it has on e parameter\\nβjequal to zero. When p>2, the diamond becomes a rhomboid, and has\\nmany corners, ﬂat edges and faces; there are many more opport unities for', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 90}),\n",
       " Document(page_content='the estimated parameters to be zero.\\nWe can generalize ridge regression and the lasso, and view th em as Bayes\\nestimates. Consider the criterion\\n˜β= argmin\\nβ{N∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2+λp∑\\nj=1|βj|q}\\n(3.53)\\nforq≥0. The contours of constant value of∑\\nj|βj|qare shown in Fig-\\nure 3.12, for the case of two inputs.\\nThinking of|βj|qas the log-prior density for βj, these are also the equi-\\ncontours of the prior distribution of the parameters. The va lueq= 0 corre-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 90}),\n",
       " Document(page_content='spondstovariablesubsetselection,asthepenaltysimplyc ountsthenumber\\nof nonzero parameters; q= 1 corresponds to the lasso, while q= 2 to ridge\\nregression. Notice that for q≤1, the prior is not uniform in direction, but\\nconcentrates more mass in the coordinate directions. The pr ior correspond-\\ning to theq= 1 case is an independent double exponential (or Laplace)\\ndistribution for each input, with density (1 /2τ)exp(−|β|/τ) andτ= 1/λ.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 90}),\n",
       " Document(page_content='The caseq= 1 (lasso) is the smallest qsuch that the constraint region\\nis convex; non-convex constraint regions make the optimiza tion problem\\nmore diﬃcult.\\nIn this view, the lasso, ridge regression and best subset sel ection are\\nBayes estimates with diﬀerent priors. Note, however, that t hey are derived\\nas posterior modes, that is, maximizers of the posterior. It is more common\\nto use the mean of the posterior as the Bayes estimate. Ridge r egression is', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 90}),\n",
       " Document(page_content='also the posterior mean, but the lasso and best subset select ion are not.\\nLooking again at the criterion (3.53), we might try using oth er values\\nofqbesides 0, 1, or 2. Although one might consider estimating qfrom\\nthe data, our experience is that it is not worth the eﬀort for t he extra\\nvariance incurred. Values of q∈(1,2) suggest a compromise between the\\nlasso and ridge regression. Although this is the case, with q >1,|βj|qis', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 90}),\n",
       " Document(page_content='diﬀerentiable at 0, and so does not share the ability of lasso (q= 1) for\\nq= 4 q= 2 q= 1 q= 0.5 q= 0.1\\nFIGURE 3.12. Contours of constant value of∑\\nj|βj|qfor given values of q.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 90}),\n",
       " Document(page_content='3.4 Shrinkage Methods 73\\nq= 1.2 α= 0.2\\nLq Elastic Net\\nFIGURE 3.13. Contours of constant value of∑\\nj|βj|qforq= 1.2(left plot),\\nand the elastic-net penalty∑\\nj(αβ2\\nj+(1−α)|βj|)forα= 0.2(right plot). Although\\nvisually very similar, the elastic-net has sharp (non-diﬀeren tiable) corners, while\\ntheq= 1.2penalty does not.\\nsetting coeﬃcients exactly to zero. Partly for this reason a s well as for\\ncomputational tractability, Zou and Hastie (2005) introdu ced theelastic-\\nnetpenalty\\nλp∑\\nj=1(\\nαβ2', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 91}),\n",
       " Document(page_content='j+(1−α)|βj|)\\n, (3.54)\\na diﬀerent compromise between ridge and lasso. Figure 3.13 c ompares the\\nLqpenalty with q= 1.2 and the elastic-net penalty with α= 0.2; it is\\nhard to detect the diﬀerence by eye. The elastic-net selects variables like\\nthe lasso, and shrinks together the coeﬃcients of correlate d predictors like\\nridge. It also has considerable computational advantages o ver theLqpenal-\\nties. We discuss the elastic-net further in Section 18.4.\\n3.4.4 Least Angle Regression', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 91}),\n",
       " Document(page_content='Least angle regression (LAR) is a relative newcomer (Efron e t al., 2004),\\nand can be viewed as a kind of “democratic” version of forward stepwise\\nregression (Section 3.3.2). As we will see, LAR is intimatel y connected\\nwith the lasso, and in fact provides an extremely eﬃcient alg orithm for\\ncomputing the entire lasso path as in Figure 3.10.\\nForward stepwise regression builds a model sequentially, a dding one vari-\\nable at a time. At each step, it identiﬁes the best variable to include in the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 91}),\n",
       " Document(page_content='active set , and then updates the least squares ﬁt to include all the acti ve\\nvariables.\\nLeast angle regression uses a similar strategy, but only ent ers “as much”\\nof a predictor as it deserves. At the ﬁrst step it identiﬁes th e variable\\nmost correlated with the response. Rather than ﬁt this varia ble completely,\\nLAR moves the coeﬃcient of this variable continuously towar d its least-\\nsquares value (causing its correlation with the evolving re sidual to decrease', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 91}),\n",
       " Document(page_content='in absolute value). As soon as another variable “catches up” in terms of\\ncorrelation with the residual, the process is paused. The se cond variable\\nthen joins the active set, and their coeﬃcients are moved tog ether in a way\\nthat keeps their correlations tied and decreasing. This pro cess is continued', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 91}),\n",
       " Document(page_content='74 3. Linear Methods for Regression\\nuntil all the variables are in the model, and ends at the full l east-squares\\nﬁt. Algorithm 3.2 provides the details. The termination con dition in step 5\\nrequires some explanation. If p>N−1, the LAR algorithm reaches a zero\\nresidual solution after N−1 steps (the−1 is because we have centered the\\ndata).\\nAlgorithm 3.2 Least Angle Regression.\\n1. Standardize the predictors to have mean zero and unit norm . Start\\nwith the residual r=y−¯y,β1,β2,...,β p= 0.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 92}),\n",
       " Document(page_content='2. Find the predictor xjmost correlated with r.\\n3. Moveβjfrom 0 towards its least-squares coeﬃcient ⟨xj,r⟩, until some\\nother competitor xkhas as much correlation with the current residual\\nas doesxj.\\n4. Moveβjandβkin the direction deﬁned by their joint least squares\\ncoeﬃcient of the current residual on ( xj,xk), until some other com-\\npetitorxlhas as much correlation with the current residual.\\n5. Continue in this way until all ppredictors have been entered. After', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 92}),\n",
       " Document(page_content='min(N−1,p) steps, we arrive at the full least-squares solution.\\nSupposeAkis the active set of variables at the beginning of the kth\\nstep, and let βAkbe the coeﬃcient vector for these variables at this step;\\nthere will be k−1 nonzero values, and the one just entered will be zero. If\\nrk=y−XAkβAkis the current residual, then the direction for this step is\\nδk= (XT\\nAkXAk)−1XT\\nAkrk. (3.55)\\nThe coeﬃcient proﬁle then evolves as βAk(α) =βAk+α·δk. Exercise 3.23', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 92}),\n",
       " Document(page_content='veriﬁes that the directions chosen in this fashion do what is claimed: keep\\nthe correlations tied and decreasing. If the ﬁt vector at the beginning of\\nthis step is ˆfk, then it evolves as ˆfk(α) =ˆfk+α·uk, whereuk=XAkδk\\nis the new ﬁt direction. The name “least angle” arises from a g eometrical\\ninterpretation of this process; ukmakes the smallest (and equal) angle\\nwith each of the predictors in Ak(Exercise 3.24). Figure 3.14 shows the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 92}),\n",
       " Document(page_content='absolute correlations decreasing and joining ranks with ea ch step of the\\nLAR algorithm, using simulated data.\\nBy construction the coeﬃcients in LAR change in a piecewise l inear fash-\\nion. Figure 3.15 [left panel] shows the LAR coeﬃcient proﬁle evolving as a\\nfunction of their L1arc length2. Note that we do not need to take small\\n2TheL1arc-length of a diﬀerentiable curve β(s) fors∈[0,S] is given by TV( β,S) =∫S\\n0||˙β(s)||1ds,where˙β(s) =∂β(s)/∂s. For the piecewise-linear LAR coeﬃcient proﬁle,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 92}),\n",
       " Document(page_content='this amounts to summing the L1norms of the changes in coeﬃcients from step to step.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 92}),\n",
       " Document(page_content='3.4 Shrinkage Methods 75\\n0 5 10 150.0 0.1 0.2 0.3 0.4v2 v6 v4 v5 v3 v1\\nL1Arc LengthAbsolute Correlations\\nFIGURE 3.14. Progression of the absolute correlations during each step of t he\\nLAR procedure, using a simulated data set with six predictors . The labels at the\\ntop of the plot indicate which variables enter the active set at each step. The step\\nlength are measured in units of L1arc length.\\n0 5 10 15−1.5 −1.0 −0.5 0.0 0.5Least Angle Regression\\n0 5 10 15−1.5 −1.0 −0.5 0.0 0.5Lasso', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 93}),\n",
       " Document(page_content='L1Arc Length L1Arc Length\\nCoeﬃcientsCoeﬃcients\\nFIGURE 3.15. Left panel shows the LAR coeﬃcient proﬁles on the simulated\\ndata, as a function of the L1arc length. The right panel shows the Lasso proﬁle.\\nThey are identical until the dark-blue coeﬃcient crosses zer o at an arc length of\\nabout18.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 93}),\n",
       " Document(page_content='76 3. Linear Methods for Regression\\nsteps and recheck the correlations in step 3; using knowledg e of the covari-\\nance of the predictors and the piecewise linearity of the alg orithm, we can\\nworkouttheexactsteplengthatthebeginningofeachstep(E xercise3.25).\\nThe right panel of Figure 3.15 shows the lasso coeﬃcient proﬁ les on the\\nsame data. They are almost identical to those in the left pane l, and diﬀer\\nfor the ﬁrst time when the blue coeﬃcient passes back through zero. For the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 94}),\n",
       " Document(page_content='prostate data, the LAR coeﬃcient proﬁle turns out to be ident ical to the\\nlasso proﬁle in Figure 3.10, which never crosses zero. These observations\\nlead to a simple modiﬁcation of the LAR algorithm that gives t he entire\\nlasso path, which is also piecewise-linear.\\nAlgorithm 3.2a Least Angle Regression: Lasso Modiﬁcation .\\n4a. If a non-zero coeﬃcient hits zero, drop its variable from the active set\\nof variables and recompute the current joint least squares d irection.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 94}),\n",
       " Document(page_content='TheLAR(lasso)algorithmisextremelyeﬃcient,requiringt hesameorder\\nof computation as that of a single least squares ﬁt using the ppredictors.\\nLeast angle regression always takes psteps to get to the full least squares\\nestimates. The lasso path can have more than psteps, although the two\\nare often quite similar. Algorithm 3.2 with the lasso modiﬁc ation 3.2a is\\nan eﬃcient way of computing the solution to any lasso problem , especially', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 94}),\n",
       " Document(page_content='whenp≫N. Osborne et al. (2000a) also discovered a piecewise-linear path\\nfor computing the lasso, which they called a homotopy algorithm.\\nWenowgiveaheuristicargumentforwhytheseproceduresare sosimilar.\\nAlthough the LAR algorithm is stated in terms of correlation s, if the input\\nfeatures are standardized, it is equivalent and easier to wo rk with inner-\\nproducts. Suppose Ais the active set of variables at some stage in the\\nalgorithm, tied in their absolute inner-product with the cu rrent residuals', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 94}),\n",
       " Document(page_content='y−Xβ. We can express this as\\nxT\\nj(y−Xβ) =γ·sj,∀j∈A (3.56)\\nwheresj∈{−1,1}indicates the sign of the inner-product, and γis the\\ncommon value. Also |xT\\nk(y−Xβ)|≤γ∀k̸∈A. Now consider the lasso\\ncriterion (3.52), which we write in vector form\\nR(β) =1\\n2||y−Xβ||2\\n2+λ||β||1. (3.57)\\nLetBbe the active set of variables in the solution for a given valu e ofλ.\\nFor these variables R(β) is diﬀerentiable, and the stationarity conditions\\ngive\\nxT\\nj(y−Xβ) =λ·sign(βj),∀j∈B (3.58)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 94}),\n",
       " Document(page_content='Comparing (3.58) with (3.56), we see that they are identical only if the\\nsign ofβjmatches the sign of the inner product. That is why the LAR', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 94}),\n",
       " Document(page_content='3.4 Shrinkage Methods 77\\nalgorithm and lasso start to diﬀer when an active coeﬃcient p asses through\\nzero; condition (3.58) is violated for that variable, and it is kicked out of the\\nactive setB. Exercise 3.23 shows that these equations imply a piecewise -\\nlinear coeﬃcient proﬁle as λdecreases. The stationarity conditions for the\\nnon-active variables require that\\n|xT\\nk(y−Xβ)|≤λ,∀k̸∈B, (3.59)\\nwhich again agrees with the LAR algorithm.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 95}),\n",
       " Document(page_content='Figure 3.16 compares LAR and lasso to forward stepwise and st agewise\\nregression. The setup is the same as in Figure 3.6 on page 59, e xcept here\\nN= 100 here rather than 300, so the problem is more diﬃcult. We s ee\\nthat the more aggressive forward stepwise starts to overﬁt q uite early (well\\nbefore the 10 true variables can enter the model), and ultima tely performs\\nworse than the slower forward stagewise regression. The beh avior of LAR', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 95}),\n",
       " Document(page_content='and lasso is similar to that of forward stagewise regression . Incremental\\nforward stagewise is similar to LAR and lasso, and is describ ed in Sec-\\ntion 3.8.1.\\nDegrees-of-Freedom Formula for LAR and Lasso\\nSuppose that we ﬁt a linear model via the least angle regressi on procedure,\\nstoppingatsomenumberofsteps k<p,orequivalentlyusingalassobound\\ntthat produces a constrained version of the full least square s ﬁt. How many\\nparameters, or “degrees of freedom” have we used?', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 95}),\n",
       " Document(page_content='Considerﬁrstalinearregressionusingasubsetof kfeatures.Ifthissubset\\nis prespeciﬁed in advance without reference to the training data, then the\\ndegrees of freedom used in the ﬁtted model is deﬁned to be k. Indeed, in\\nclassical statistics, the number of linearly independent p arameters is what\\nis meant by “degrees of freedom.” Alternatively, suppose th at we carry out\\na best subset selection to determine the “optimal” set of kpredictors. Then', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 95}),\n",
       " Document(page_content='the resulting model has kparameters, but in some sense we have used up\\nmore thankdegrees of freedom.\\nWe need a more general deﬁnition for the eﬀective degrees of f reedom of\\nan adaptively ﬁtted model. We deﬁne the degrees of freedom of the ﬁtted\\nvectorˆy= (ˆy1,ˆy2,...,ˆyN) as\\ndf(ˆy) =1\\nσ2N∑\\ni=1Cov(ˆyi,yi). (3.60)\\nHere Cov(ˆyi,yi) refers to the sampling covariance between the predicted\\nvalue ˆyiand its corresponding outcome value yi. This makes intuitive sense:', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 95}),\n",
       " Document(page_content='the harder that we ﬁt to the data, the larger this covariance a nd hence\\ndf(ˆy). Expression (3.60) is a useful notion of degrees of freedom , one that\\ncan be applied to any model prediction ˆy. This includes models that are', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 95}),\n",
       " Document(page_content='78 3. Linear Methods for Regression\\n0.0 0.2 0.4 0.6 0.8 1.00.55 0.60 0.65Forward Stepwise\\nLAR\\nLasso\\nForward Stagewise\\nIncremental Forward StagewiseE||ˆβ(k)−β||2\\nFraction of L1arc-length\\nFIGURE 3.16. Comparison of LAR and lasso with forward stepwise, forward\\nstagewise (FS) and incremental forward stagewise (FS 0) regression. The setup\\nis the same as in Figure 3.6, except N= 100here rather than 300. Here the\\nslower FS regression ultimately outperforms forward stepwise . LAR and lasso', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 96}),\n",
       " Document(page_content='show similar behavior to FS and FS 0. Since the procedures take diﬀerent numbers\\nof steps (across simulation replicates and methods), we plot th e MSE as a function\\nof the fraction of total L1arc-length toward the least-squares ﬁt.\\nadaptively ﬁtted to the training data. This deﬁnition is mot ivated and\\ndiscussed further in Sections 7.4–7.6.\\nNow for a linear regression with kﬁxed predictors, it is easy to show\\nthat df(ˆy) =k. Likewise for ridge regression, this deﬁnition leads to the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 96}),\n",
       " Document(page_content='closed-form expression (3.50) on page 68: df( ˆy) = tr(Sλ). In both these\\ncases, (3.60) is simple to evaluate because the ﬁt ˆy=Hλyis linear in y.\\nIf we think about deﬁnition (3.60) in the context of a best sub set selection\\nof sizek, it seems clear that df( ˆy) will be larger than k, and this can be\\nveriﬁed by estimating Cov(ˆ yi,yi)/σ2directly by simulation. However there\\nis no closed form method for estimating df( ˆy) for best subset selection.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 96}),\n",
       " Document(page_content='For LAR and lasso, something magical happens. These techniq ues are\\nadaptiveinasmootherwaythanbestsubsetselection,andhe nceestimation\\nof degrees of freedom is more tractable. Speciﬁcally it can b e shown that\\nafter thekth step of the LAR procedure, the eﬀective degrees of freedom of\\nthe ﬁt vector is exactly k. Now for the lasso, the (modiﬁed) LAR procedure', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 96}),\n",
       " Document(page_content='3.5 Methods Using Derived Input Directions 79\\noften takes more than psteps, since predictors can drop out. Hence the\\ndeﬁnitionisalittlediﬀerent;forthelasso,atanystagedf (ˆy)approximately\\nequals the number of predictors in the model. While this appr oximation\\nworks reasonably well anywhere in the lasso path, for each kit works best\\nat thelastmodel in the sequence that contains kpredictors. A detailed\\nstudy of the degrees of freedom for the lasso may be found in Zo u et al.\\n(2007).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 97}),\n",
       " Document(page_content='3.5 Methods Using Derived Input Directions\\nIn many situations we have a large number of inputs, often ver y correlated.\\nThe methods in this section produce a small number of linear c ombinations\\nZm, m= 1,...,Mof the original inputs Xj, and theZmare then used in\\nplace of the Xjas inputs in the regression. The methods diﬀer in how the\\nlinear combinations are constructed.\\n3.5.1 Principal Components Regression\\nIn this approach the linear combinations Zmused are the principal com-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 97}),\n",
       " Document(page_content='ponents as deﬁned in Section 3.4.1 above.\\nPrincipal component regression forms the derived input col umnszm=\\nXvm, and then regresses yonz1,z2,...,zMfor someM≤p. Since the zm\\nare orthogonal, this regression is just a sum of univariate r egressions:\\nˆypcr\\n(M)= ¯y1+M∑\\nm=1ˆθmzm, (3.61)\\nwhereˆθm=⟨zm,y⟩/⟨zm,zm⟩. Since the zmare each linear combinations\\nof the original xj, we can express the solution (3.61) in terms of coeﬃcients\\nof thexj(Exercise 3.13):\\nˆβpcr(M) =M∑\\nm=1ˆθmvm. (3.62)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 97}),\n",
       " Document(page_content='As with ridge regression, principal components depend on th e scaling of\\nthe inputs, so typically we ﬁrst standardize them. Note that ifM=p, we\\nwould just get back the usual least squares estimates, since the columns of\\nZ=UDspan the column space of X. ForM <pwe get a reduced regres-\\nsion. We see that principal components regression is very si milar to ridge\\nregression: both operate via the principal components of th e input ma-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 97}),\n",
       " Document(page_content='trix. Ridge regression shrinks the coeﬃcients of the princi pal components\\n(Figure 3.17), shrinking more depending on the size of the co rresponding\\neigenvalue; principal components regression discards the p−Msmallest\\neigenvalue components. Figure 3.17 illustrates this.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 97}),\n",
       " Document(page_content='80 3. Linear Methods for Regression\\nIndexShrinkage Factor\\n2 4 6 80.0 0.2 0.4 0.6 0.8 1.0•\\n••\\n••••\\n•• • • • • • •\\n• •ridge\\npcr\\nFIGURE 3.17. Ridge regression shrinks the regression coeﬃcients of the p rin-\\ncipal components, using shrinkage factors d2\\nj/(d2\\nj+λ)as in (3.47). Principal\\ncomponent regression truncates them. Shown are the shrinka ge and truncation\\npatterns corresponding to Figure 3.7, as a function of the pr incipal component\\nindex.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 98}),\n",
       " Document(page_content='In Figure 3.7 we see that cross-validation suggests seven te rms; the re-\\nsulting model has the lowest test error in Table 3.3.\\n3.5.2 Partial Least Squares\\nThis technique also constructs a set of linear combinations of the inputs\\nfor regression, but unlike principal components regressio n it uses y(in ad-\\ndition to X) for this construction. Like principal component regressi on,\\npartial least squares (PLS) is not scale invariant, so we ass ume that each', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 98}),\n",
       " Document(page_content='xjis standardized to have mean 0 and variance 1. PLS begins by co m-\\nputing ˆϕ1j=⟨xj,y⟩for eachj. From this we construct the derived input\\nz1=∑\\njˆϕ1jxj, which is the ﬁrst partial least squares direction. Hence\\nin the construction of each zm, the inputs are weighted by the strength\\nof their univariate eﬀect on y3. The outcome yis regressed on z1giving\\ncoeﬃcient ˆθ1, and then we orthogonalize x1,...,xpwith respect to z1. We\\ncontinue this process, until M≤pdirections have been obtained. In this', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 98}),\n",
       " Document(page_content='manner, partial least squares produces a sequence of derive d, orthogonal\\ninputs or directions z1,z2,...,zM. As with principal-component regres-\\nsion, if we were to construct all M=pdirections, we would get back a\\nsolution equivalent to the usual least squares estimates; u singM < pdi-\\nrections produces a reduced regression. The procedure is de scribed fully in\\nAlgorithm 3.3.\\n3Since the xjare standardized, the ﬁrst directions ˆ ϕ1jare the univariate regression', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 98}),\n",
       " Document(page_content='coeﬃcients (up to an irrelevant constant); this is not the case for subsequen t directions.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 98}),\n",
       " Document(page_content='3.5 Methods Using Derived Input Directions 81\\nAlgorithm 3.3 Partial Least Squares.\\n1. Standardize each xjto have mean zero and variance one. Set ˆy(0)=\\n¯y1, andx(0)\\nj=xj, j= 1,...,p.\\n2. Form= 1,2,...,p\\n(a)zm=∑p\\nj=1ˆϕmjx(m−1)\\nj, where ˆϕmj=⟨x(m−1)\\nj,y⟩.\\n(b)ˆθm=⟨zm,y⟩/⟨zm,zm⟩.\\n(c)ˆy(m)=ˆy(m−1)+ˆθmzm.\\n(d) Orthogonalize each x(m−1)\\njwith respect to zm:x(m)\\nj=x(m−1)\\nj−\\n[⟨zm,x(m−1)\\nj⟩/⟨zm,zm⟩]zm,j= 1,2,...,p.\\n3. Output the sequence of ﬁtted vectors {ˆy(m)}p\\n1. Since the{zℓ}m\\n1are', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 99}),\n",
       " Document(page_content='linear in the original xj, so isˆy(m)=Xˆβpls(m). These linear coeﬃ-\\ncients can be recovered from the sequence of PLS transformat ions.\\nIn the prostate cancer example, cross-validation chose M= 2 PLS direc-\\ntions in Figure 3.7. This produced the model given in the righ tmost column\\nof Table 3.3.\\nWhat optimization problem is partial least squares solving ? Since it uses\\nthe response yto construct its directions, its solution path is a nonlinea r', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 99}),\n",
       " Document(page_content='function of y. It can be shown (Exercise 3.15) that partial least squares\\nseeks directions that have high variance andhave high correlation with the\\nresponse, in contrast to principal components regression w hich keys only\\non high variance (Stone and Brooks, 1990; Frank and Friedman , 1993). In\\nparticular, the mth principal component direction vmsolves:\\nmaxαVar(Xα) (3.63)\\nsubject to||α||= 1, αTSvℓ= 0, ℓ= 1,...,m−1,\\nwhereSis the sample covariance matrix of the xj. The conditions αTSvℓ=', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 99}),\n",
       " Document(page_content='0 ensures that zm=Xαis uncorrelated with all the previous linear com-\\nbinations zℓ=Xvℓ. Themth PLS direction ˆ ϕmsolves:\\nmaxαCorr2(y,Xα)Var(Xα) (3.64)\\nsubject to||α||= 1, αTSˆϕℓ= 0, ℓ= 1,...,m−1.\\nFurther analysis reveals that the variance aspect tends to d ominate, and\\nso partial least squares behaves much like ridge regression and principal\\ncomponents regression. We discuss this further in the next s ection.\\nIf the input matrix Xis orthogonal, then partial least squares ﬁnds the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 99}),\n",
       " Document(page_content='least squares estimates after m= 1 steps. Subsequent steps have no eﬀect', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 99}),\n",
       " Document(page_content='82 3. Linear Methods for Regression\\nsince the ˆϕmjare zero for m>1 (Exercise 3.14). It can also be shown that\\nthe sequence of PLS coeﬃcients for m= 1,2,...,prepresents the conjugate\\ngradient sequence for computing the least squares solution s (Exercise 3.18).\\n3.6 Discussion: A Comparison of the Selection and\\nShrinkage Methods\\nThere are some simple settings where we can understand bette r the rela-\\ntionship between the diﬀerent methods described above. Con sider an exam-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 100}),\n",
       " Document(page_content='ple with two correlated inputs X1andX2, with correlation ρ. We assume\\nthat the true regression coeﬃcients are β1= 4 andβ2= 2. Figure 3.18\\nshows the coeﬃcient proﬁles for the diﬀerent methods, as the ir tuning pa-\\nrameters are varied. The top panel has ρ= 0.5, the bottom panel ρ=−0.5.\\nThe tuning parameters for ridge and lasso vary over a continu ous range,\\nwhile best subset, PLS and PCR take just two discrete steps to the least', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 100}),\n",
       " Document(page_content='squares solution. In the top panel, starting at the origin, r idge regression\\nshrinks the coeﬃcients together until it ﬁnally converges t o least squares.\\nPLS and PCR show similar behavior to ridge, although are disc rete and\\nmore extreme. Best subset overshoots the solution and then b acktracks.\\nThe behavior of the lasso is intermediate to the other method s. When the\\ncorrelation is negative (lower panel), again PLS and PCR rou ghly track', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 100}),\n",
       " Document(page_content='the ridge path, while all of the methods are more similar to on e another.\\nIt is interesting to compare the shrinkage behavior of these diﬀerent\\nmethods. Recall that ridge regression shrinks all directio ns, but shrinks\\nlow-variance directions more. Principal components regre ssion leaves M\\nhigh-variance directions alone, and discards the rest. Int erestingly, it can\\nbe shown that partial least squares also tends to shrink the l ow-variance', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 100}),\n",
       " Document(page_content='directions, but can actually inﬂate some of the higher varia nce directions.\\nThis can make PLS a little unstable, and cause it to have sligh tly higher\\nprediction error compared to ridge regression. A full study is given in Frank\\nand Friedman (1993). These authors conclude that for minimi zing predic-\\ntion error, ridge regression is generally preferable to var iable subset selec-\\ntion, principal components regression and partial least sq uares. However', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 100}),\n",
       " Document(page_content='the improvement over the latter two methods was only slight.\\nTo summarize, PLS, PCR and ridge regression tend to behave si milarly.\\nRidge regression may be preferred because it shrinks smooth ly, rather than\\nin discrete steps. Lasso falls somewhere between ridge regr ession and best\\nsubset regression, and enjoys some of the properties of each .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 100}),\n",
       " Document(page_content='3.6 Discussion: A Comparison of the Selection and Shrinkage Methods 83\\n0 1 2 3 4 5 6-1 0 1 2 3Least Squares\\n0Ridge\\nLasso\\nBest SubsetPLS PCR\\n•\\n0 1 2 3 4 5 6-1 0 1 2 3Least Squares\\nRidge\\nBest Subset\\nPLS\\nPCRLasso•\\n0ρ= 0.5\\nρ=−0.5\\nβ1β1β2 β2\\nFIGURE 3.18. Coeﬃcient proﬁles from diﬀerent methods for a simple problem:\\ntwo inputs with correlation ±0.5, and the true regression coeﬃcients β= (4,2).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 101}),\n",
       " Document(page_content='84 3. Linear Methods for Regression\\n3.7 Multiple Outcome Shrinkage and Selection\\nAs noted in Section 3.2.4, the least squares estimates in a mu ltiple-output\\nlinear model are simply the individual least squares estima tes for each of\\nthe outputs.\\nTo apply selection and shrinkage methods in the multiple out put case,\\none could apply a univariate technique individually to each outcome or si-\\nmultaneously to all outcomes. With ridge regression, for ex ample, we could', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 102}),\n",
       " Document(page_content='apply formula (3.44) to each of the Kcolumns of the outcome matrix Y,\\nusing possibly diﬀerent parameters λ, or apply it to all columns using the\\nsame value of λ. The former strategy would allow diﬀerent amounts of\\nregularization to be applied to diﬀerent outcomes but requi re estimation\\nofkseparate regularization parameters λ1,...,λ k, while the latter would\\npermit allkoutputs to be used in estimating the sole regularization pa-\\nrameterλ.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 102}),\n",
       " Document(page_content='Other more sophisticated shrinkage and selection strategi es that exploit\\ncorrelations in the diﬀerent responses can be helpful in the multiple output\\ncase. Suppose for example that among the outputs we have\\nYk=f(X)+εk (3.65)\\nYℓ=f(X)+εℓ; (3.66)\\ni.e., (3.65) and (3.66) share the same structural part f(X) in their models.\\nIt is clear in this case that we should pool our observations o nYkandYl\\nto estimate the common f.\\nCombining responses is at the heart of canonical correlation analysis', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 102}),\n",
       " Document(page_content='(CCA), a data reduction technique developed for the multipl e output case.\\nSimilar to PCA, CCA ﬁnds a sequence of uncorrelated linear co mbina-\\ntionsXvm, m= 1,...,M of thexj, and a corresponding sequence of\\nuncorrelated linear combinations Yumof the responses yk, such that the\\ncorrelations\\nCorr2(Yum,Xvm) (3.67)\\nare successively maximized. Note that at most M= min(K,p) directions\\ncan be found. The leading canonical response variates are th ose linear com-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 102}),\n",
       " Document(page_content='binations (derived responses) best predicted by the xj; in contrast, the\\ntrailing canonical variates can be poorly predicted by the xj, and are can-\\ndidates for being dropped. The CCA solution is computed usin g a general-\\nized SVD of the sample cross-covariance matrix YTX/N(assuming Yand\\nXare centered; Exercise 3.20).\\nReduced-rank regression (Izenman,1975;vanderMerweandZidek,1980)\\nformalizes this approach in terms of a regression model that explicitly pools', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 102}),\n",
       " Document(page_content='information. Given an error covariance Cov( ε) =Σ, we solve the following', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 102}),\n",
       " Document(page_content='3.7 Multiple Outcome Shrinkage and Selection 85\\nrestricted multivariate regression problem:\\nˆBrr(m) = argmin\\nrank(B)=mN∑\\ni=1(yi−BTxi)TΣ−1(yi−BTxi).(3.68)\\nWithΣreplaced by the estimate YTY/N, one can show (Exercise 3.21)\\nthat the solution is given by a CCA of YandX:\\nˆBrr(m) =ˆBUmU−\\nm, (3.69)\\nwhereUmis theK×msub-matrix of Uconsisting of the ﬁrst mcolumns,\\nandUis theK×Mmatrix of leftcanonical vectors u1,u2,...,u M.U−\\nm\\nis its generalized inverse. Writing the solution as\\nˆBrr(M) = (XTX)−1XT(YUm)U−', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 103}),\n",
       " Document(page_content='m, (3.70)\\nwe see that reduced-rank regression performs a linear regre ssion on the\\npooled response matrix YUm, and then maps the coeﬃcients (and hence\\nthe ﬁts as well) back to the original response space. The redu ced-rank ﬁts\\nare given by\\nˆYrr(m) =X(XTX)−1XTYUmU−\\nm\\n=HYPm,(3.71)\\nwhereHis the usual linear regression projection operator, and Pmis the\\nrank-mCCA response projection operator. Although a better estima te of\\nΣwould be( Y−XˆB)T(Y−XˆB)/(N−pK), onecanshowthat thesolution', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 103}),\n",
       " Document(page_content='remains the same (Exercise 3.22).\\nReduced-rank regression borrows strength among responses by truncat-\\ning the CCA. Breiman and Friedman (1997) explored with some s uccess\\nshrinkage of the canonical variates between XandY, a smooth version of\\nreduced rank regression. Their proposal has the form (compare (3.69))\\nˆBc+w=ˆBUΛU−1, (3.72)\\nwhereΛis a diagonal shrinkage matrix (the “c+w” stands for “Curds\\nand Whey,” the name they gave to their procedure). Based on op timal', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 103}),\n",
       " Document(page_content='prediction in the population setting, they show that Λhas diagonal entries\\nλm=c2\\nm\\nc2m+p\\nN(1−c2m), m= 1,...,M, (3.73)\\nwherecmis themth canonical correlation coeﬃcient. Note that as the ratio\\nof the number of input variables to sample size p/Ngets small, the shrink-\\nage factors approach 1. Breiman and Friedman (1997) propose d modiﬁed\\nversions of Λbased on training data and cross-validation, but the genera l\\nform is the same. Here the ﬁtted response has the form\\nˆYc+w=HYSc+w, (3.74)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 103}),\n",
       " Document(page_content='86 3. Linear Methods for Regression\\nwhereSc+w=UΛU−1is the response shrinkage operator.\\nBreiman and Friedman (1997) also suggested shrinking in bot h theY\\nspace andXspace. This leads to hybrid shrinkage models of the form\\nˆYridge,c+w=AλYSc+w, (3.75)\\nwhereAλ=X(XTX+λI)−1XTis the ridge regression shrinkage operator,\\nas in (3.46) on page 66. Their paper and the discussions there of contain\\nmany more details.\\n3.8 More on the Lasso and Related Path\\nAlgorithms', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 104}),\n",
       " Document(page_content='Since the publication of the LAR algorithm (Efron et al., 200 4) there has\\nbeen a lot of activity in developing algorithms for ﬁtting re gularization\\npaths for a variety of diﬀerent problems. In addition, L1regularization has\\ntaken on a life of its own, leading to the development of the ﬁe ldcompressed\\nsensingin the signal-processing literature. (Donoho, 2006a; Cand es, 2006).\\nInthissectionwediscusssomerelatedproposalsandotherp athalgorithms,\\nstarting oﬀ with a precursor to the LAR algorithm.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 104}),\n",
       " Document(page_content='3.8.1 Incremental Forward Stagewise Regression\\nHere we present another LAR-like algorithm, this time focus ed on forward\\nstagewiseregression.Interestingly,eﬀortstounderstan daﬂexiblenonlinear\\nregression procedure (boosting) led to a new algorithm for l inear models\\n(LAR). In reading the ﬁrst edition of this book and the forwar d stagewise\\nAlgorithm 3.4 Incremental Forward Stagewise Regression—FS ǫ.\\n1. Start with the residual requal to yandβ1,β2,...,β p= 0. All the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 104}),\n",
       " Document(page_content='predictors are standardized to have mean zero and unit norm.\\n2. Find the predictor xjmost correlated with r\\n3. Updateβj←βj+δj, whereδj=ǫ·sign[⟨xj,r⟩] andǫ>0 is a small\\nstep size, and set r←r−δjxj.\\n4. Repeat steps 2 and 3 many times, until the residuals are unc orrelated\\nwith all the predictors.\\nAlgorithm 16.1 of Chapter 164, our colleague Brad Efron realized that with\\n4In the ﬁrst edition, this was Algorithm 10.4 in Chapter 10.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 104}),\n",
       " Document(page_content='3.8 More on the Lasso and Related Path Algorithms 87−0.2 0.0 0.2 0.4 0.6lcavol\\nlweight\\nagelbphsvi\\nlcpgleasonpgg45\\n0 50 100 150 200\\n−0.2 0.0 0.2 0.4 0.6lcavol\\nlweight\\nagelbphsvi\\nlcpgleasonpgg45\\n0.0 0.5 1.0 1.5 2.0FSǫ FS0\\nIteration\\nCoeﬃcientsCoeﬃcients\\nL1Arc-length of Coeﬃcients\\nFIGURE 3.19. Coeﬃcient proﬁles for the prostate data. The left panel shows\\nincremental forward stagewise regression with step size ǫ= 0.01. The right panel', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 105}),\n",
       " Document(page_content='shows the inﬁnitesimal version FS 0obtained letting ǫ→0. This proﬁle was ﬁt by\\nthe modiﬁcation 3.2b to the LAR Algorithm 3.2. In this example t he FS 0proﬁles\\nare monotone, and hence identical to those of lasso and LAR.\\nlinearmodels,onecouldexplicitlyconstructthepiecewis e-linearlassopaths\\nof Figure 3.10. This led him to propose the LAR procedure of Se ction 3.4.4,\\nas well as the incremental version of forward-stagewise reg ression presented\\nhere.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 105}),\n",
       " Document(page_content='Consider the linear-regression version of the forward-sta gewise boosting\\nalgorithm16.1proposedinSection16.1(page608).Itgener atesacoeﬃcient\\nproﬁle by repeatedly updating (by a small amount ǫ) the coeﬃcient of the\\nvariable most correlated with the current residuals. Algor ithm 3.4 gives\\nthe details. Figure 3.19 (left panel) shows the progress of t he algorithm on\\nthe prostate data with step size ǫ= 0.01. Ifδj=⟨xj,r⟩(the least-squares', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 105}),\n",
       " Document(page_content='coeﬃcient of the residual on jth predictor), then this is exactly the usual\\nforward stagewise procedure (FS) outlined in Section 3.3.3 .\\nHere we are mainly interested in small values of ǫ. Lettingǫ→0 gives\\nthe right panel of Figure 3.19, which in this case is identica l to the lasso\\npath in Figure 3.10. We call this limiting procedure inﬁnitesimal forward\\nstagewise regression or FS0. This procedure plays an important role in', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 105}),\n",
       " Document(page_content='non-linear, adaptive methods like boosting (Chapters 10 an d 16) and is the\\nversion of incremental forward stagewise regression that i s most amenable\\nto theoretical analysis. B¨ uhlmann and Hothorn (2007) refe r to the same\\nprocedure as “L2boost”, because of its connections to boost ing.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 105}),\n",
       " Document(page_content='88 3. Linear Methods for Regression\\nEfron originally thought that the LAR Algorithm 3.2 was an im plemen-\\ntation of FS 0, allowing each tied predictor a chance to update their coeﬃ-\\ncients in a balanced way, while remaining tied in correlatio n. However, he\\nthen realized that the LAR least-squares ﬁt amongst the tied predictors\\ncan result in coeﬃcients moving in the opposite direction to their correla-\\ntion, which cannot happen in Algorithm 3.4. The following mo diﬁcation of', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 106}),\n",
       " Document(page_content='the LAR algorithm implements FS 0:\\nAlgorithm 3.2b Least Angle Regression: FS 0Modiﬁcation .\\n4. Find the new direction by solving the constrained least sq uares prob-\\nlem\\nmin\\nb||r−XAb||2\\n2subject tobjsj≥0, j∈A,\\nwheresjis the sign of⟨xj,r⟩.\\nThe modiﬁcation amounts to a non-negative least squares ﬁt, keeping the\\nsigns of the coeﬃcients the same as those of the correlations . One can show\\nthat this achieves the optimal balancing of inﬁnitesimal “u pdate turns”', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 106}),\n",
       " Document(page_content='for the variables tied for maximal correlation (Hastie et al ., 2007). Like\\nlasso, the entire FS 0path can be computed very eﬃciently via the LAR\\nalgorithm.\\nAs a consequence of these results, if the LAR proﬁles are mono tone non-\\nincreasing or non-decreasing, as they are in Figure 3.19, th en all three\\nmethods—LAR, lasso, and FS 0—give identical proﬁles. If the proﬁles are\\nnot monotone but do not cross the zero axis, then LAR and lasso are\\nidentical.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 106}),\n",
       " Document(page_content='Since FS 0is diﬀerent from the lasso, it is natural to ask if it optimize s\\na criterion. The answer is more complex than for lasso; the FS 0coeﬃcient\\nproﬁle is the solution to a diﬀerential equation. While the l asso makes op-\\ntimal progress in terms of reducing the residual sum-of-squ ares per unit\\nincrease in L1-norm of the coeﬃcient vector β, FS0is optimal per unit\\nincrease in L1arc-length traveled along the coeﬃcient path. Hence its co-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 106}),\n",
       " Document(page_content='eﬃcient path is discouraged from changing directions too of ten.\\nFS0is more constrained than lasso, and in fact can be viewed as a m ono-\\ntone version of the lasso; see Figure 16.3 on page 614 for a dra matic exam-\\nple. FS 0may be useful in p≫Nsituations, where its coeﬃcient proﬁles\\nare much smoother and hence have less variance than those of l asso. More\\ndetails on FS 0are given in Section 16.2.3 and Hastie et al. (2007). Fig-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 106}),\n",
       " Document(page_content='ure 3.16 includes FS 0where its performance is very similar to that of the\\nlasso.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 106}),\n",
       " Document(page_content='3.8 More on the Lasso and Related Path Algorithms 89\\n3.8.2 Piecewise-Linear Path Algorithms\\nThe least angle regression procedure exploits the piecewis e linear nature of\\nthe lasso solution paths. It has led to similar “path algorit hms” for other\\nregularized problems. Suppose we solve\\nˆβ(λ) = argminβ[R(β)+λJ(β)], (3.76)\\nwith\\nR(β) =N∑\\ni=1L(yi,β0+p∑\\nj=1xijβj), (3.77)\\nwhere both the loss function Land the penalty function Jare convex.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 107}),\n",
       " Document(page_content='Then the following are suﬃcient conditions for the solution pathˆβ(λ) to\\nbe piecewise linear (Rosset and Zhu, 2007):\\n1.Ris quadratic or piecewise-quadratic as a function of β, and\\n2.Jis piecewise linear in β.\\nThis also implies (in principle) that the solution path can b e eﬃciently\\ncomputed. Examples include squared- and absolute-error lo ss, “Huberized”\\nlosses, and the L1,L∞penalties on β. Another example is the “hinge loss”', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 107}),\n",
       " Document(page_content='function used in the support vector machine. There the loss i s piecewise\\nlinear, and the penalty is quadratic. Interestingly, this l eads to a piecewise-\\nlinear path algorithm in the dual space ; more details are given in Sec-\\ntion 12.3.5.\\n3.8.3 The Dantzig Selector\\nCandes and Tao (2007) proposed the following criterion:\\nminβ||β||1subject to||XT(y−Xβ)||∞≤s. (3.78)\\nThey call the solution the Dantzig selector (DS). It can be written equiva-\\nlently as\\nminβ||XT(y−Xβ)||∞subject to||β||1≤t. (3.79)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 107}),\n",
       " Document(page_content='Here||·||∞denotes the L∞norm, the maximum absolute value of the\\ncomponents of the vector. In this form it resembles the lasso , replacing\\nsquared error loss by the maximum absolute value of its gradi ent. Note\\nthat astgets large, both procedures yield the least squares solutio n if\\nN <p. Ifp≥N, they both yield the least squares solution with minimum\\nL1norm. However for smaller values of t, the DS procedure produces a\\ndiﬀerent path of solutions than the lasso.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 107}),\n",
       " Document(page_content='Candes and Tao (2007) show that the solution to DS is a linear p ro-\\ngramming problem; hence the name Dantzig selector, in honor of the late', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 107}),\n",
       " Document(page_content='90 3. Linear Methods for Regression\\nGeorge Dantzig, the inventor of the simplex method for linea r program-\\nming. They also prove a number of interesting mathematical p roperties for\\nthe method, related to its ability to recover an underlying s parse coeﬃ-\\ncient vector. These same properties also hold for the lasso, as shown later\\nby Bickel et al. (2008).\\nUnfortunately the operating properties of the DS method are somewhat', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 108}),\n",
       " Document(page_content='unsatisfactory. The method seems similar in spirit to the la sso, especially\\nwhen we look at the lasso’s stationary conditions (3.58). Li ke the LAR al-\\ngorithm, the lasso maintains the same inner product (and cor relation) with\\nthe current residual for all variables in the active set, and moves their co-\\neﬃcients to optimally decrease the residual sum of squares. In the process,\\nthis common correlation is decreased monotonically (Exerc ise 3.23), and at', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 108}),\n",
       " Document(page_content='all times this correlation is larger than that for non-activ e variables. The\\nDantzig selector instead tries to minimize the maximum inne r product of\\nthe current residual with all the predictors. Hence it can ac hieve a smaller\\nmaximum than the lasso, but in the process a curious phenomen on can\\noccur. If the size of the active set is m, there will be mvariables tied with\\nmaximum correlation. However, these need not coincide with the active set!', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 108}),\n",
       " Document(page_content='Hence it can include a variable in the model that has smaller c orrelation\\nwith the current residual than some of the excluded variable s (Efron et\\nal., 2007). This seems unreasonable and may be responsible f or its some-\\ntimes inferior prediction accuracy. Efron et al. (2007) als o show that DS\\ncanyieldextremelyerraticcoeﬃcientpathsastheregulari zation parameter\\nsis varied.\\n3.8.4 The Grouped Lasso\\nIn some problems, the predictors belong to pre-deﬁned group s; for example', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 108}),\n",
       " Document(page_content='genes that belong to the same biological pathway, or collect ions of indicator\\n(dummy) variables for representing the levels of a categori cal predictor. In\\nthis situation it may be desirable to shrink and select the me mbers of a\\ngroup together. The grouped lasso is one way to achieve this. Suppose that\\ntheppredictors are divided into Lgroups, with pℓthe number in group\\nℓ. For ease of notation, we use a matrix Xℓto represent the predictors', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 108}),\n",
       " Document(page_content='corresponding to the ℓth group, with corresponding coeﬃcient vector βℓ.\\nThe grouped-lasso minimizes the convex criterion\\nmin\\nβ∈IRp(\\n||y−β01−L∑\\nℓ=1Xℓβℓ||2\\n2+λL∑\\nℓ=1√pℓ||βℓ||2)\\n, (3.80)\\nwhere the√pℓterms accounts for the varying group sizes, and ||·||2is\\nthe Euclidean norm (not squared). Since the Euclidean norm o f a vector\\nβℓis zero only if all of its components are zero, this procedure encourages\\nsparsity at both the group and individual levels. That is, fo r some values of', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 108}),\n",
       " Document(page_content='λ, an entire group of predictors may drop out of the model. This procedure', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 108}),\n",
       " Document(page_content='3.8 More on the Lasso and Related Path Algorithms 91\\nwas proposed by Bakin (1999) and Lin and Zhang (2006), and stu died and\\ngeneralized by Yuan and Lin (2007). Generalizations includ e more general\\nL2norms||η||K= (ηTKη)1/2, as well as allowing overlapping groups of\\npredictors (Zhao et al., 2008). There are also connections t o methods for\\nﬁtting sparse additive models (Lin and Zhang, 2006; Ravikum ar et al.,\\n2008).\\n3.8.5 Further Properties of the Lasso', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 109}),\n",
       " Document(page_content='A number of authors have studied the ability of the lasso and r elated pro-\\ncedures to recover the correct model, as Nandpgrow. Examples of this\\nwork include Knight and Fu (2000), Greenshtein and Ritov (20 04), Tropp\\n(2004), Donoho (2006b), Meinshausen (2007), Meinshausen a nd B¨ uhlmann\\n(2006), Tropp (2006), Zhao and Yu (2006), Wainwright (2006) , and Bunea\\net al. (2007). For example Donoho (2006b) focuses on the p>Ncase and', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 109}),\n",
       " Document(page_content='considers the lasso solution as the bound tgets large. In the limit this gives\\nthe solution with minimum L1norm among all models with zero training\\nerror. He shows that under certain assumptions on the model m atrixX, if\\nthe true model is sparse, this solution identiﬁes the correc t predictors with\\nhigh probability.\\nMany of the results in this area assume a condition on the mode l matrix\\nof the form\\nmax\\nj∈Sc||xT\\njXS(XSTXS)−1||1≤(1−ǫ) for someǫ∈(0,1].(3.81)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 109}),\n",
       " Document(page_content='HereSindexes the subset of features with non-zero coeﬃcients in t he true\\nunderlying model, and XSare the columns of Xcorresponding to those\\nfeatures. Similarly Scare the features with true coeﬃcients equal to zero,\\nandXScthe corresponding columns. This says that the least squares coef-\\nﬁcients for the columns of XSconXSare not too large, that is, the “good”\\nvariablesSare not too highly correlated with the nuisance variables Sc.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 109}),\n",
       " Document(page_content='Regarding the coeﬃcients themselves, the lasso shrinkage c auses the esti-\\nmates of the non-zero coeﬃcients to be biased towards zero, a nd in general\\nthey are not consistent5. One approach for reducing this bias is to run\\nthe lasso to identify the set of non-zero coeﬃcients, and the n ﬁt an un-\\nrestricted linear model to the selected set of features. Thi s is not always\\nfeasible, if the selected set is large. Alternatively, one c an use the lasso to', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 109}),\n",
       " Document(page_content='select the set of non-zero predictors, and then apply the las so again, but\\nusing only the selected predictors from the ﬁrst step. This i s known as the\\nrelaxed lasso (Meinshausen, 2007). The idea is to use cross-validation to\\nestimate the initial penalty parameter for the lasso, and th en again for a\\nsecond penalty parameter applied to the selected set of pred ictors. Since\\n5Statistical consistency means as the sample size grows, the estimates converge to\\nthe true values.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 109}),\n",
       " Document(page_content='92 3. Linear Methods for Regression\\nthe variables in the second step have less “competition” fro m noise vari-\\nables, cross-validation will tend to pick a smaller value fo rλ, and hence\\ntheir coeﬃcients will be shrunken less than those in the init ial estimate.\\nAlternatively,onecanmodifythelassopenaltyfunctionso thatlargerco-\\neﬃcients are shrunken less severely; the smoothly clipped absolute deviation\\n(SCAD) penalty of Fan and Li (2005) replaces λ|β|byJa(β,λ), where\\ndJa(β,λ)\\ndβ=λ·sign(β)[', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 110}),\n",
       " Document(page_content='I(|β|≤λ)+(aλ−|β|)+\\n(a−1)λI(|β|>λ)]\\n(3.82)\\nfor somea≥2. The second term in square-braces reduces the amount of\\nshrinkage in the lasso for larger values of β, with ultimately no shrinkage\\nasa→∞. Figure 3.20 shows the SCAD penalty, along with the lasso and\\n−4 −2 0 2 40 1 2 3 4 5\\n−4 −2 0 2 40.0 0.5 1.0 1.5 2.0 2.5\\n−4 −2 0 2 40.5 1.0 1.5 2.0|β| SCAD |β|1−ν\\nβ β β\\nFIGURE 3.20. The lasso and two alternative non-convex penalties designed to', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 110}),\n",
       " Document(page_content='penalize large coeﬃcients less. For SCAD we use λ= 1anda= 4, andν=1\\n2in\\nthe last panel.\\n|β|1−ν. However this criterion is non-convex, which is a drawback s ince it\\nmakes the computation much more diﬃcult. The adaptive lasso (Zou, 2006)\\nuses a weighted penalty of the form∑p\\nj=1wj|βj|wherewj= 1/|ˆβj|ν,ˆβjis\\nthe ordinary least squares estimate and ν >0. This is a practical approxi-\\nmation to the|β|qpenalties (q= 1−νhere) discussed in Section 3.4.3. The', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 110}),\n",
       " Document(page_content='adaptive lasso yields consistent estimates of the paramete rs while retaining\\nthe attractive convexity property of the lasso.\\n3.8.6 Pathwise Coordinate Optimization\\nAn alternate approach to the LARS algorithm for computing th e lasso\\nsolution is simple coordinate descent. This idea was propos ed by Fu (1998)\\nandDaubechiesetal.(2004),andlaterstudiedandgenerali zedbyFriedman\\netal.(2007),WuandLange(2008)andothers.Theideaistoﬁx thepenalty', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 110}),\n",
       " Document(page_content='parameterλin the Lagrangian form (3.52) and optimize successively ove r\\neach parameter, holding the other parameters ﬁxed at their c urrent values.\\nSuppose the predictors are all standardized to have mean zer o and unit\\nnorm. Denote by ˜βk(λ) the current estimate for βkat penalty parameter', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 110}),\n",
       " Document(page_content='3.9 Computational Considerations 93\\nλ. We can rearrange (3.52) to isolate βj,\\nR(˜β(λ),βj) =1\\n2N∑\\ni=1(\\nyi−∑\\nk̸=jxik˜βk(λ)−xijβj)2\\n+λ∑\\nk̸=j|˜βk(λ)|+λ|βj|,\\n(3.83)\\nwhere we have suppressed the intercept and introduced a fact or1\\n2for con-\\nvenience. This can be viewed as a univariate lasso problem wi th response\\nvariable the partial residual yi−˜y(j)\\ni=yi−∑\\nk̸=jxik˜βk(λ). This has an\\nexplicit solution, resulting in the update\\n˜βj(λ)←S(N∑\\ni=1xij(yi−˜y(j)\\ni),λ)\\n. (3.84)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 111}),\n",
       " Document(page_content='HereS(t,λ) = sign(t)(|t|−λ)+isthesoft-thresholdingoperatorinTable3.4\\non page 71. The ﬁrst argument to S(·) is the simple least-squares coeﬃcient\\nof the partial residual on the standardized variable xij. Repeated iteration\\nof (3.84)—cycling through each variable in turn until conver gence—yields\\nthe lasso estimate ˆβ(λ).\\nWe can also use this simple algorithm to eﬃciently compute th e lasso\\nsolutions at a grid of values of λ. We start with the smallest value λmax', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 111}),\n",
       " Document(page_content='for which ˆβ(λmax) = 0, decrease it a little and cycle through the variables\\nuntil convergence. Then λis decreased again and the process is repeated,\\nusing the previous solution as a “warm start” for the new valu e ofλ. This\\ncan be faster than the LARS algorithm, especially in large pr oblems. A\\nkey to its speed is the fact that the quantities in (3.84) can b e updated\\nquickly asjvaries, and often the update is to leave ˜βj= 0. On the other', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 111}),\n",
       " Document(page_content='hand, it delivers solutions over a grid of λvalues, rather than the entire\\nsolution path. The same kind of algorithm can be applied to th e elastic\\nnet, the grouped lasso and many other models in which the pena lty is a\\nsum of functions of the individual parameters (Friedman et a l., 2010). It\\ncan also be applied, with some substantial modiﬁcations, to the fused lasso\\n(Section 18.4.2); details are in Friedman et al. (2007).\\n3.9 Computational Considerations', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 111}),\n",
       " Document(page_content='Least squares ﬁtting is usually done via the Cholesky decomp osition of\\nthe matrix XTXor a QR decomposition of X. WithNobservations and p\\nfeatures, the Cholesky decomposition requires p3+Np2/2 operations, while\\nthe QR decomposition requires Np2operations. Depending on the relative\\nsize ofNandp, the Cholesky can sometimes be faster; on the other hand,\\nit can be less numerically stable (Lawson and Hansen, 1974). Computation\\nof the lasso via the LAR algorithm has the same order of comput ation as', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 111}),\n",
       " Document(page_content='a least squares ﬁt.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 111}),\n",
       " Document(page_content='94 3. Linear Methods for Regression\\nBibliographic Notes\\nLinear regression is discussed in many statistics books, fo r example, Seber\\n(1984), Weisberg (1980) and Mardia et al. (1979). Ridge regr ession was\\nintroduced by Hoerl and Kennard (1970), while the lasso was p roposed by\\nTibshirani (1996). Around the same time, lasso-type penalt ies were pro-\\nposed in the basis pursuit method for signal processing (Chen et al., 1998).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 112}),\n",
       " Document(page_content='The least angle regression procedure was proposed in Efron e t al. (2004);\\nrelated to this is the earlier homotopy procedure of Osborne et al. (2000a)\\nand Osborne et al. (2000b). Their algorithm also exploits th e piecewise\\nlinearity used in the LAR/lasso algorithm, but lacks its tra nsparency. The\\ncriterion for the forward stagewise criterion is discussed in Hastie et al.\\n(2007). Park and Hastie (2007) develop a path algorithm simi lar to least', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 112}),\n",
       " Document(page_content='angle regression for generalized regression models. Parti al least squares\\nwas introduced by Wold (1975). Comparisons of shrinkage met hods may\\nbe found in Copas (1983) and Frank and Friedman (1993).\\nExercises\\nEx. 3.1Show that the Fstatistic (3.13) for dropping a single coeﬃcient\\nfrom a model is equal to the square of the corresponding z-score (3.12).\\nEx. 3.2Given data on two variables XandY, consider ﬁtting a cubic\\npolynomial regression model f(X) =∑3\\nj=0βjXj. In addition to plotting', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 112}),\n",
       " Document(page_content='the ﬁtted curve, you would like a 95% conﬁdence band about the curve.\\nConsider the following two approaches:\\n1. At each point x0, form a 95% conﬁdence interval for the linear func-\\ntionaTβ=∑3\\nj=0βjxj\\n0.\\n2. Form a 95% conﬁdence set for βas in (3.15), which in turn generates\\nconﬁdence intervals for f(x0).\\nHow do these approaches diﬀer? Which band is likely to be wide r? Conduct\\na small simulation experiment to compare the two methods.\\nEx. 3.3Gauss–Markov theorem:', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 112}),\n",
       " Document(page_content='(a) Prove the Gauss–Markov theorem: the least squares estim ate of a\\nparameteraTβhas variance no bigger than that of any other linear\\nunbiased estimate of aTβ(Section 3.2.2).\\n(b) The matrix inequality B⪯Aholds ifA−Bis positive semideﬁnite.\\nShow that if ˆVis the variance-covariance matrix of the least squares\\nestimate of βand˜Vis the variance-covariance matrix of any other\\nlinear unbiased estimate, then ˆV⪯˜V.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 112}),\n",
       " Document(page_content='Exercises 95\\nEx. 3.4Show how the vector of least squares coeﬃcients can be obtain ed\\nfrom a single pass of the Gram–Schmidt procedure (Algorithm 3.1). Rep-\\nresent your solution in terms of the QR decomposition of X.\\nEx. 3.5Consider the ridge regression problem (3.41). Show that thi s prob-\\nlem is equivalent to the problem\\nˆβc= argmin\\nβc{N∑\\ni=1[\\nyi−βc\\n0−p∑\\nj=1(xij−¯xj)βc\\nj]2+λp∑\\nj=1βc\\nj2}\\n.(3.85)\\nGive the correspondence between βcand the original βin (3.41). Char-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 113}),\n",
       " Document(page_content='acterize the solution to this modiﬁed criterion. Show that a similar result\\nholds for the lasso.\\nEx. 3.6Show that the ridge regression estimate is the mean (and mode )\\nof the posterior distribution, under a Gaussian prior β∼N(0,τI), and\\nGaussian sampling model y∼N(Xβ,σ2I). Find the relationship between\\nthe regularization parameter λin the ridge formula, and the variances τ\\nandσ2.\\nEx. 3.7Assumeyi∼N(β0+xT\\niβ,σ2),i= 1,2,...,N, and the parameters', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 113}),\n",
       " Document(page_content='βj, j= 1,...,pare each distributed as N(0,τ2), independently of one\\nanother. Assuming σ2andτ2are known, and β0is not governed by a\\nprior (or has a ﬂat improper prior), show that the (minus) log -posterior\\ndensity ofβis proportional to∑N\\ni=1(yi−β0−∑\\njxijβj)2+λ∑p\\nj=1β2\\nj\\nwhereλ=σ2/τ2.\\nEx. 3.8Consider the QR decomposition of the uncentered N×(p+ 1)\\nmatrixX(whose ﬁrst column is all ones), and the SVD of the N×p\\ncentered matrix ˜X. Show that Q2andUspan the same subspace, where', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 113}),\n",
       " Document(page_content='Q2is the sub-matrix of Qwith the ﬁrst column removed. Under what\\ncircumstances will they be the same, up to sign ﬂips?\\nEx. 3.9Forward stepwise regression. Suppose we have the QR decomposi-\\ntion for the N×qmatrixX1in a multiple regression problem with response\\ny, and we have an additional p−qpredictors in the matrix X2. Denote the\\ncurrent residual by r. We wish to establish which one of these additional\\nvariables will reduce the residual-sum-of squares the most when included', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 113}),\n",
       " Document(page_content='with those in X1. Describe an eﬃcient procedure for doing this.\\nEx. 3.10 Backward stepwise regression. Suppose we have the multiple re-\\ngression ﬁt of yonX, along with the standard errors and Z-scores as in\\nTable 3.2. We wish to establish which variable, when dropped , will increase\\nthe residual sum-of-squares the least. How would you do this ?\\nEx. 3.11 Show that the solution to the multivariate linear regressio n prob-\\nlem (3.40) is given by (3.39). What happens if the covariance matrices Σi', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 113}),\n",
       " Document(page_content='are diﬀerent for each observation?', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 113}),\n",
       " Document(page_content='96 3. Linear Methods for Regression\\nEx. 3.12 Show that the ridge regression estimates can be obtained by\\nordinary least squares regression on an augmented data set. We augment\\nthe centered matrix Xwithpadditional rows√\\nλI, and augment ywithp\\nzeros. By introducing artiﬁcial data having response value zero, the ﬁtting\\nprocedure is forced to shrink the coeﬃcients toward zero. Th is is related to\\nthe idea of hintsdue to Abu-Mostafa (1995), where model constraints are', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 114}),\n",
       " Document(page_content='implemented by adding artiﬁcial data examples that satisfy them.\\nEx. 3.13 Derive the expression (3.62), and show that ˆβpcr(p) =ˆβls.\\nEx. 3.14 Show that in the orthogonal case, PLS stops after m= 1 steps,\\nbecause subsequent ˆ ϕmjin step 2 in Algorithm 3.3 are zero.\\nEx. 3.15 Verify expression (3.64), and hence show that the partial le ast\\nsquares directions are a compromise between the ordinary re gression coef-\\nﬁcient and the principal component directions.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 114}),\n",
       " Document(page_content='Ex. 3.16 Derive the entries in Table 3.4, the explicit forms for estim ators\\nin the orthogonal case.\\nEx. 3.17 Repeat the analysis of Table 3.3 on the spam data discussed in\\nChapter 1.\\nEx.3.18 Readaboutconjugategradientalgorithms(Murrayetal.,19 81,for\\nexample), and establish a connection between these algorit hms and partial\\nleast squares.\\nEx. 3.19 Show that∥ˆβridge∥increases as its tuning parameter λ→0. Does\\nthe same property hold for the lasso and partial least square s estimates?', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 114}),\n",
       " Document(page_content='For the latter, consider the “tuning parameter” to be the suc cessive steps\\nin the algorithm.\\nEx. 3.20 Consider the canonical-correlation problem (3.67). Show t hat the\\nleading pair of canonical variates u1andv1solve the problem\\nmax\\nuT(YTY)u=1\\nvT(XTX)v=1uT(YTX)v, (3.86)\\na generalized SVD problem. Show that the solution is given by u1=\\n(YTY)−1\\n2u∗\\n1, andv1= (XTX)−1\\n2v∗\\n1, whereu∗\\n1andv∗\\n1are the leading left\\nand right singular vectors in\\n(YTY)−1\\n2(YTX)(XTX)−1\\n2=U∗D∗V∗T. (3.87)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 114}),\n",
       " Document(page_content='Show that the entire sequence um, vm, m= 1,...,min(K,p) is also given\\nby (3.87).\\nEx. 3.21 Show that the solution to the reduced-rank regression probl em\\n(3.68), with Σestimated by YTY/N, is given by (3.69). Hint:Transform', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 114}),\n",
       " Document(page_content='Exercises 97\\nYtoY∗=YΣ−1\\n2, and solved in terms of the canonical vectors u∗\\nm. Show\\nthatUm=Σ−1\\n2U∗\\nm, and a generalized inverse is U−\\nm=U∗\\nmTΣ1\\n2.\\nEx. 3.22 Show that the solution in Exercise 3.21 does not change if Σis\\nestimated by the more natural quantity ( Y−XˆB)T(Y−XˆB)/(N−pK).\\nEx. 3.23 Consider a regression problem with all variables and respon se hav-\\ning mean zero and standard deviation one. Suppose also that e ach variable\\nhas identical absolute correlation with the response:\\n1', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 115}),\n",
       " Document(page_content='N|⟨xj,y⟩|=λ, j= 1,...,p.\\nLetˆβbe the least-squares coeﬃcient of yonX, and let u(α) =αXˆβfor\\nα∈[0,1] be the vector that moves a fraction αtoward the least squares ﬁt\\nu. LetRSSbe the residual sum-of-squares from the full least squares ﬁ t.\\n(a) Show that\\n1\\nN|⟨xj,y−u(α)⟩|= (1−α)λ, j= 1,...,p,\\nand hence the correlations of each xjwith the residuals remain equal\\nin magnitude as we progress toward u.\\n(b) Show that these correlations are all equal to\\nλ(α) =(1−α)√\\n(1−α)2+α(2−α)\\nN·RSS·λ,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 115}),\n",
       " Document(page_content='and hence they decrease monotonically to zero.\\n(c) Use these results to show that the LAR algorithm in Sectio n 3.4.4\\nkeeps the correlations tied and monotonically decreasing, as claimed\\nin (3.55).\\nEx. 3.24 LAR directions. Using the notation around equation (3.55) on\\npage 74, show that the LAR direction makes an equal angle with each of\\nthe predictors in Ak.\\nEx. 3.25 LAR look-ahead (Efron et al., 2004, Sec. 2). Starting at the be-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 115}),\n",
       " Document(page_content='ginning of the kth step of the LAR algorithm, derive expressions to identify\\nthe next variable to enter the active set at step k+1, and the value of αat\\nwhich this occurs (using the notation around equation (3.55 ) on page 74).\\nEx. 3.26 Forward stepwise regression enters the variable at each ste p that\\nmost reduces the residual sum-of-squares. LAR adjusts vari ables that have\\nthe most (absolute) correlation with the current residuals . Show that these', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 115}),\n",
       " Document(page_content='two entry criteria are not necessarily the same. [Hint: let xj.Abe thejth', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 115}),\n",
       " Document(page_content='98 3. Linear Methods for Regression\\nvariable, linearly adjusted for all the variables currentl y in the model. Show\\nthat the ﬁrst criterion amounts to identifying the jfor which Cor( xj.A,r)\\nis largest in magnitude.\\nEx. 3.27 Lasso and LAR : Consider thelassoprobleminLagrange multiplier\\nform: with L(β) =1\\n2∑\\ni(yi−∑\\njxijβj)2, we minimize\\nL(β)+λ∑\\nj|βj| (3.88)\\nfor ﬁxedλ>0.\\n(a) Setting βj=β+\\nj−β−\\njwithβ+\\nj,β−\\nj≥0, expression (3.88) becomes\\nL(β)+λ∑\\nj(β+\\nj+β−\\nj). Show that the Lagrange dual function is', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 116}),\n",
       " Document(page_content='L(β)+λ∑\\nj(β+\\nj+β−\\nj)−∑\\njλ+\\njβ+\\nj−∑\\njλ−\\njβ−\\nj(3.89)\\nand the Karush–Kuhn–Tucker optimality conditions are\\n∇L(β)j+λ−λ+\\nj= 0\\n−∇L(β)j+λ−λ−\\nj= 0\\nλ+\\njβ+\\nj= 0\\nλ−\\njβ−\\nj= 0,\\nalong with the non-negativity constraints on the parameter s and all\\nthe Lagrange multipliers.\\n(b) Show that|∇L(β)j|≤λ∀j,and that the KKT conditions imply one\\nof the following three scenarios:\\nλ= 0⇒ ∇L(β)j= 0∀j\\nβ+\\nj>0, λ>0⇒λ+\\nj= 0,∇L(β)j=−λ<0, β−\\nj= 0\\nβ−\\nj>0, λ>0⇒λ−\\nj= 0,∇L(β)j=λ>0, β+\\nj= 0.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 116}),\n",
       " Document(page_content='Hence show that for any “active” predictor having βj̸= 0, we must\\nhave∇L(β)j=−λifβj>0, and∇L(β)j=λifβj<0. Assuming\\nthe predictors are standardized, relate λto the correlation between\\nthejth predictor and the current residuals.\\n(c) Suppose that the set of active predictors is unchanged fo rλ0≥λ≥λ1.\\nShow that there is a vector γ0such that\\nˆβ(λ) =ˆβ(λ0)−(λ−λ0)γ0 (3.90)\\nThus the lasso solution path is linear as λranges from λ0toλ1(Efron\\net al., 2004; Rosset and Zhu, 2007).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 116}),\n",
       " Document(page_content='Exercises 99\\nEx. 3.28 Suppose for a given tin (3.51), the ﬁtted lasso coeﬃcient for\\nvariableXjisˆβj=a. Suppose we augment our set of variables with an\\nidentical copy X∗\\nj=Xj. Characterize the eﬀect of this exact collinearity\\nby describing the set of solutions for ˆβjandˆβ∗\\nj, using the same value of t.\\nEx. 3.29 Suppose we run a ridge regression with parameter λon a single\\nvariableX, and get coeﬃcient a. We now include an exact copy X∗=X,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 117}),\n",
       " Document(page_content='and reﬁt our ridge regression. Show that both coeﬃcients are identical, and\\nderive their value. Show in general that if mcopies of a variable Xjare\\nincluded in a ridge regression, their coeﬃcients are all the same.\\nEx. 3.30 Consider the elastic-net optimization problem:\\nmin\\nβ||y−Xβ||2+λ[\\nα||β||2\\n2+(1−α)||β||1]\\n. (3.91)\\nShow how one can turn this into a lasso problem, using an augme nted\\nversion of Xandy.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 117}),\n",
       " Document(page_content='100 3. Linear Methods for Regression', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 118}),\n",
       " Document(page_content='This is page 101\\nPrinter: Opaque this\\n4\\nLinear Methods for Classiﬁcation\\n4.1 Introduction\\nIn this chapter we revisit the classiﬁcation problem and foc us on linear\\nmethods for classiﬁcation. Since our predictor G(x) takes values in a dis-\\ncrete setG, we can always divide the input space into a collection of reg ions\\nlabeledaccordingtotheclassiﬁcation.WesawinChapter2t hatthebound-\\naries of these regions can be rough or smooth, depending on th e prediction', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 119}),\n",
       " Document(page_content='function. For an important class of procedures, these decision boundaries\\nare linear; this is what we will mean by linear methods for cla ssiﬁcation.\\nThere are several diﬀerent ways in which linear decision bou ndaries can\\nbe found. In Chapter 2 we ﬁt linear regression models to the cl ass indicator\\nvariables, and classify to the largest ﬁt. Suppose there are Kclasses, for\\nconvenience labeled 1 ,2,...,K, and the ﬁtted linear model for the kth\\nindicator response variable is ˆfk(x) =ˆβk0+ˆβT', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 119}),\n",
       " Document(page_content='kx. The decision boundary\\nbetween class kandℓis that set of points for which ˆfk(x) =ˆfℓ(x), that is,\\nthe set{x: (ˆβk0−ˆβℓ0)+(ˆβk−ˆβℓ)Tx= 0}, an aﬃne set or hyperplane.1\\nSince the same is true for any pair of classes, the input space is divided\\ninto regions of constant classiﬁcation, with piecewise hyp erplanar decision\\nboundaries. This regression approach is a member of a class o f methods\\nthat model discriminant functions δk(x) for each class, and then classify x', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 119}),\n",
       " Document(page_content='to the class with the largest value for its discriminant func tion. Methods\\n1Strictly speaking, a hyperplane passes through the origin, while an aﬃne set need\\nnot. We sometimes ignore the distinction and refer in general to hyperpl anes.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 119}),\n",
       " Document(page_content='102 4. Linear Methods for Classiﬁcation\\nthat model the posterior probabilities Pr( G=k|X=x) are also in this\\nclass. Clearly, if either the δk(x) or Pr(G=k|X=x) are linear in x, then\\nthe decision boundaries will be linear.\\nActually, all we require is that some monotone transformati on ofδkor\\nPr(G=k|X=x) be linear for the decision boundaries to be linear. For\\nexample, if there are two classes, a popular model for the pos terior proba-\\nbilities is\\nPr(G= 1|X=x) =exp(β0+βTx)\\n1+exp(β0+βTx),', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 120}),\n",
       " Document(page_content='Pr(G= 2|X=x) =1\\n1+exp(β0+βTx).(4.1)\\nHerethemonotonetransformationisthe logittransformation:log[ p/(1−p)],\\nand in fact we see that\\nlogPr(G= 1|X=x)\\nPr(G= 2|X=x)=β0+βTx. (4.2)\\nThe decision boundary is the set of points for which the log-odds are zero,\\nand this is a hyperplane deﬁned by{\\nx|β0+βTx= 0}\\n. We discuss two very\\npopular but diﬀerent methods that result in linear log-odds or logits: linear\\ndiscriminant analysis and linear logistic regression. Alt hough they diﬀer in', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 120}),\n",
       " Document(page_content='their derivation, the essential diﬀerence between them is i n the way the\\nlinear function is ﬁt to the training data.\\nA more direct approach is to explicitly model the boundaries between\\nthe classes as linear. For a two-class problem in a p-dimensional input\\nspace, this amounts to modeling the decision boundary as a hy perplane—in\\nother words, a normal vector and a cut-point. We will look at t wo methods\\nthat explicitly look for “separating hyperplanes.” The ﬁrs t is the well-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 120}),\n",
       " Document(page_content='knownperceptron model of Rosenblatt (1958), with an algorithm that ﬁnds\\na separating hyperplane in the training data, if one exists. The second\\nmethod, due to Vapnik (1996), ﬁnds an optimally separating hyperplane if\\none exists, else ﬁnds a hyperplane that minimizes some measu re of overlap\\nin the training data. We treat the separable case here, and de fer treatment\\nof the nonseparable case to Chapter 12.\\nWhilethisentirechapterisdevotedtolineardecisionboun daries,thereis', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 120}),\n",
       " Document(page_content='considerable scope for generalization. For example, we can expand our vari-\\nablesetX1,...,X pbyincludingtheirsquaresandcross-products X2\\n1,X2\\n2,...,\\nX1X2,..., thereby adding p(p+1)/2 additional variables. Linear functions\\nin the augmented space map down to quadratic functions in the original\\nspace—hence linear decision boundaries to quadratic decisi on boundaries.\\nFigure 4.1 illustrates the idea. The data are the same: the le ft plot uses', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 120}),\n",
       " Document(page_content='linear decision boundaries in the two-dimensional space sh own, while the\\nrightplotuseslineardecisionboundariesintheaugmented ﬁve-dimensional\\nspace described above. This approach can be used with any bas is transfor-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 120}),\n",
       " Document(page_content='4.2 Linear Regression of an Indicator Matrix 103\\n1\\n11\\n11\\n111\\n1\\n111\\n1\\n11\\n11\\n1111 1\\n111\\n1\\n11\\n111\\n1111\\n11\\n1\\n11\\n111\\n11\\n1111\\n1 1\\n111\\n11\\n1\\n1\\n1 1\\n11\\n1111\\n11111\\n1\\n11\\n111\\n111\\n1111\\n1\\n111\\n11\\n11\\n11\\n1\\n11\\n11\\n111\\n11\\n1\\n1\\n11\\n111\\n111\\n11\\n11\\n11\\n11 1\\n11\\n11 111\\n1 11\\n1\\n11\\n1\\n11\\n1\\n11\\n1\\n11 1\\n1\\n11\\n11\\n1111\\n1111\\n1\\n111\\n11\\n111\\n1\\n111\\n111\\n1111\\n11\\n1 11\\n111\\n11\\n1\\n11\\n11\\n11\\n111\\n12\\n22\\n22\\n222\\n2\\n2\\n22\\n2 2\\n2\\n22\\n22\\n22\\n2222\\n2\\n22\\n222\\n2\\n22\\n2 2\\n22\\n222\\n2\\n222\\n22222\\n22\\n2\\n222\\n22\\n2\\n22\\n22\\n22\\n22\\n22\\n2222\\n22\\n22\\n222\\n222\\n22222\\n22\\n222\\n22\\n22\\n22\\n2\\n22\\n2222', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 121}),\n",
       " Document(page_content='22\\n2\\n22\\n2\\n222\\n2\\n222\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n2222 2\\n222\\n22\\n2\\n22\\n22\\n222\\n222\\n2\\n22\\n22\\n22222\\n22\\n222\\n22\\n2\\n222\\n22\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n22222\\n22\\n3\\n33\\n33\\n33\\n3\\n3333\\n33\\n3\\n33\\n3\\n333\\n33\\n33\\n333\\n333\\n3\\n3\\n33\\n333\\n333\\n333\\n333\\n3333\\n333\\n3333\\n3\\n33333\\n33\\n3\\n33\\n3\\n33\\n3333\\n333\\n3\\n333\\n333\\n33\\n333\\n3\\n3333\\n33\\n333\\n33\\n3\\n3333\\n333\\n33333\\n3\\n3\\n3\\n333\\n33\\n3\\n33\\n3\\n33\\n33\\n333\\n33\\n3\\n3333\\n33\\n333\\n3\\n333\\n33333\\n33\\n333\\n33\\n33\\n3333\\n333\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n3\\n333\\n33\\n33\\n33\\n1\\n11\\n11\\n111\\n1\\n111\\n1\\n11\\n11\\n1111 1\\n111\\n1\\n11\\n111\\n1111\\n11\\n1\\n11\\n111\\n11\\n1111\\n1 1', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 121}),\n",
       " Document(page_content='111\\n11\\n1\\n1\\n1 1\\n11\\n1111\\n11111\\n1\\n11\\n111\\n111\\n1111\\n1\\n111\\n11\\n11\\n11\\n1\\n11\\n11\\n111\\n11\\n1\\n1\\n11\\n111\\n111\\n11\\n11\\n11\\n11 1\\n11\\n11 111\\n1 11\\n1\\n11\\n1\\n11\\n1\\n11\\n1\\n11 1\\n1\\n11\\n11\\n1111\\n1111\\n1\\n111\\n11\\n111\\n1\\n111\\n111\\n1111\\n11\\n1 11\\n111\\n11\\n1\\n11\\n11\\n11\\n111\\n12\\n22\\n22\\n222\\n2\\n2\\n22\\n2 2\\n2\\n22\\n22\\n22\\n2222\\n2\\n22\\n222\\n2\\n22\\n2 2\\n22\\n222\\n2\\n222\\n22222\\n22\\n2\\n222\\n22\\n2\\n22\\n22\\n22\\n22\\n22\\n2222\\n22\\n22\\n222\\n222\\n22222\\n22\\n222\\n22\\n22\\n22\\n2\\n22\\n2222\\n22\\n2\\n22\\n2\\n222\\n2\\n222\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n2222 2\\n222\\n22\\n2\\n22\\n22\\n222\\n222\\n2\\n22\\n22\\n22222\\n22\\n222\\n22\\n2\\n222\\n22\\n22\\n222\\n222\\n22\\n2\\n22', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 121}),\n",
       " Document(page_content='22\\n22222\\n22\\n3\\n33\\n33\\n33\\n3\\n3333\\n33\\n3\\n33\\n3\\n333\\n33\\n33\\n333\\n333\\n3\\n3\\n33\\n333\\n333\\n333\\n333\\n3333\\n333\\n3333\\n3\\n33333\\n33\\n3\\n33\\n3\\n33\\n3333\\n333\\n3\\n333\\n333\\n33\\n333\\n3\\n3333\\n33\\n333\\n33\\n3\\n3333\\n333\\n33333\\n3\\n3\\n3\\n333\\n33\\n3\\n33\\n3\\n33\\n33\\n333\\n33\\n3\\n3333\\n33\\n333\\n3\\n333\\n33333\\n33\\n333\\n33\\n33\\n3333\\n333\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n3\\n333\\n33\\n33\\n33\\nFIGURE 4.1. The left plot shows some data from three classes, with linear\\ndecision boundaries found by linear discriminant analysis. T he right plot shows', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 121}),\n",
       " Document(page_content='quadratic decision boundaries. These were obtained by ﬁndi ng linear boundaries\\nin the ﬁve-dimensional space X1,X2,X1X2,X2\\n1,X2\\n2. Linear inequalities in this\\nspace are quadratic inequalities in the original space.\\nmationh(X) whereh: IRp↦→IRqwithq>p, and will be explored in later\\nchapters.\\n4.2 Linear Regression of an Indicator Matrix\\nHere each of the response categories are coded via an indicat or variable.\\nThus ifGhasKclasses, there will be Ksuch indicators Yk, k= 1,...,K,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 121}),\n",
       " Document(page_content='withYk= 1 ifG=kelse 0. These are collected together in a vector\\nY= (Y1,...,Y K), and theNtraining instances of these form an N×K\\nindicator response matrix Y.Yis a matrix of 0’s and 1’s, with each row\\nhaving a single 1. We ﬁt a linear regression model to each of th e columns\\nofYsimultaneously, and the ﬁt is given by\\nˆY=X(XTX)−1XTY. (4.3)\\nChapter 3 has more details on linear regression. Note that we have a coeﬃ-\\ncient vector for each response column yk, and hence a ( p+1)×Kcoeﬃcient', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 121}),\n",
       " Document(page_content='matrixˆB= (XTX)−1XTY. HereXis the model matrix with p+1 columns\\ncorresponding to the pinputs, and a leading column of 1’s for the intercept.\\nA new observation with input xis classiﬁed as follows:\\n•compute the ﬁtted output ˆf(x)T= (1,xT)ˆB, aKvector;\\n•identify the largest component and classify accordingly:\\nˆG(x) = argmaxk∈Gˆfk(x). (4.4)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 121}),\n",
       " Document(page_content='104 4. Linear Methods for Classiﬁcation\\nWhat is the rationale for this approach? One rather formal ju stiﬁcation\\nis to view the regression as an estimate of conditional expec tation. For the\\nrandom variable Yk,E(Yk|X=x) = Pr(G=k|X=x), so conditional\\nexpectation of each of the Ykseems a sensible goal. The real issue is: how\\ngood an approximation to conditional expectation is the rat her rigid linear\\nregression model? Alternatively, are the ˆfk(x) reasonable estimates of the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 122}),\n",
       " Document(page_content='posterior probabilities Pr( G=k|X=x), and more importantly, does this\\nmatter?\\nIt is quite straightforward to verify that∑\\nk∈Gˆfk(x) = 1 for any x, as\\nlong as there is an intercept in the model (column of 1’s in X). However,\\ntheˆfk(x) can be negative or greater than 1, and typically some are. Th is\\nis a consequence of the rigid nature of linear regression, es pecially if we\\nmake predictions outside the hull of the training data. Thes e violations in', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 122}),\n",
       " Document(page_content='themselves do not guarantee that this approach will not work , and in fact\\non many problems it gives similar results to more standard li near meth-\\nods for classiﬁcation. If we allow linear regression onto ba sis expansions\\nh(X) of the inputs, this approach can lead to consistent estimat es of the\\nprobabilities. As the size of the training set Ngrows bigger, we adaptively\\ninclude more basis elements so that linear regression onto t hese basis func-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 122}),\n",
       " Document(page_content='tions approaches conditional expectation. We discuss such approaches in\\nChapter 5.\\nA more simplistic viewpoint is to construct targetstkfor each class,\\nwheretkis thekth column of the K×Kidentity matrix. Our prediction\\nproblem is to try and reproduce the appropriate target for an observation.\\nWith the same coding as before, the response vector yi(ith row of Y) for\\nobservation ihas the value yi=tkifgi=k. We might then ﬁt the linear\\nmodel by least squares:\\nmin\\nBN∑\\ni=1||yi−[(1,xT\\ni)B]T||2. (4.5)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 122}),\n",
       " Document(page_content='The criterion is a sum-of-squared Euclidean distances of th e ﬁtted vectors\\nfrom their targets. A new observation is classiﬁed by comput ing its ﬁtted\\nvectorˆf(x) and classifying to the closest target:\\nˆG(x) = argmin\\nk||ˆf(x)−tk||2. (4.6)\\nThis is exactly the same as the previous approach:\\n•The sum-of-squared-norm criterion is exactly the criterio n for multi-\\nple response linear regression, just viewed slightly diﬀer ently. Since', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 122}),\n",
       " Document(page_content='a squared norm is itself a sum of squares, the components deco uple\\nand can be rearranged as a separate linear model for each elem ent.\\nNote that this is only possible because there is nothing in th e model\\nthat binds the diﬀerent responses together.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 122}),\n",
       " Document(page_content='4.2 Linear Regression of an Indicator Matrix 105\\nLinear Regression\\n1\\n11\\n1\\n1\\n11111\\n11\\n1\\n11111\\n111\\n11 111\\n111\\n1111\\n111\\n1\\n11111\\n111\\n11\\n1111\\n11\\n11\\n11111\\n11\\n11\\n11\\n1\\n11\\n11\\n1\\n11\\n11\\n1\\n11\\n1\\n1\\n11\\n111\\n111111\\n11\\n111\\n11 1\\n111\\n1\\n1\\n11\\n1\\n11\\n11\\n11\\n111\\n111\\n11\\n11\\n1 1\\n1\\n11\\n1111\\n11\\n1\\n111\\n11111\\n1\\n1111\\n11\\n111\\n111\\n111\\n1\\n11\\n1111\\n11\\n1\\n111 11\\n11\\n11\\n11\\n11\\n1\\n1\\n11\\n11\\n1\\n111\\n1\\n11\\n11\\n1\\n111\\n111 111\\n11\\n11\\n11\\n111111\\n1\\n11\\n11\\n11\\n1111\\n1111\\n111\\n1\\n11 1\\n111\\n1111\\n11\\n111\\n111\\n11\\n1\\n11\\n111\\n111\\n111\\n11\\n11\\n11\\n111\\n11 1\\n111\\n11\\n11\\n11\\n1\\n11\\n11\\n1\\n11', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 123}),\n",
       " Document(page_content='11\\n1\\n111\\n1\\n11111\\n11111\\n1\\n11\\n111\\n1\\n11\\n111 1\\n11\\n1\\n11 11\\n1\\n111\\n111\\n11\\n1\\n111\\n1\\n1\\n1111\\n1111\\n11\\n11\\n1111\\n11\\n1\\n11\\n11\\n11\\n1 1\\n111\\n1 11\\n1\\n11\\n11\\n11 1\\n11\\n1\\n1111111\\n1\\n11\\n11\\n1\\n11\\n111\\n11\\n111\\n1111\\n11\\n11\\n11\\n1 1\\n11\\n11111\\n1\\n11\\n11\\n111\\n11\\n11\\n11\\n1\\n1\\n11\\n11\\n111\\n111\\n11\\n111\\n11\\n111\\n1\\n11\\n1\\n11\\n11\\n1\\n111\\n1111\\n11\\n1\\n11\\n1\\n11\\n1 12\\n22\\n2\\n222\\n22\\n222\\n2\\n22\\n222\\n222\\n222\\n2222\\n2\\n222\\n22\\n2\\n2\\n222\\n2\\n222\\n2\\n222\\n2222\\n2\\n22\\n22\\n22\\n22222\\n2222\\n2\\n222\\n222\\n22\\n2\\n22\\n22\\n222\\n2\\n2222222\\n22\\n22\\n222\\n222\\n222\\n22\\n2\\n222\\n22\\n2\\n22\\n22\\n2 2\\n22\\n2222\\n2\\n2\\n2\\n22\\n2222\\n22\\n222\\n2', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 123}),\n",
       " Document(page_content='22\\n222\\n222\\n22\\n222\\n2\\n2\\n222\\n222\\n2\\n22\\n2222\\n222\\n2222\\n22\\n222\\n2\\n2222\\n222\\n22\\n2\\n222\\n2222\\n222\\n22\\n2\\n222\\n22\\n2\\n22222\\n2222\\n22\\n222\\n22\\n22\\n22\\n2222\\n2222\\n2\\n22\\n22\\n22\\n222\\n22222\\n22\\n22\\n2\\n22\\n2\\n2\\n22\\n2\\n22\\n22\\n2\\n22\\n22\\n22\\n22\\n2\\n222\\n22\\n2222\\n2222\\n222\\n222\\n222\\n2222\\n2\\n222\\n2\\n22\\n222\\n222\\n22\\n222 22\\n222\\n222\\n22222\\n22 222\\n222\\n2\\n22\\n22 2\\n2\\n2222\\n2\\n22\\n22\\n2\\n22\\n22\\n22\\n2\\n22 22\\n2\\n222\\n2222\\n222\\n2\\n2 2222 2\\n22\\n22\\n2222\\n22\\n222\\n2\\n22\\n22\\n2\\n22\\n22\\n222\\n22\\n22\\n2222\\n22\\n2222\\n2222\\n2\\n222\\n22 2\\n22\\n2\\n22\\n22222\\n22\\n22\\n22\\n222\\n2\\n222\\n22\\n2\\n2 22 222\\n22\\n22\\n22 2\\n22 22\\n222', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 123}),\n",
       " Document(page_content='22\\n22\\n22 22\\n22\\n223\\n33\\n33\\n33\\n3\\n3\\n333\\n3\\n33\\n3\\n333\\n33\\n3\\n33\\n333\\n33\\n333\\n3\\n333\\n3333\\n3\\n33\\n33\\n333\\n33\\n33\\n33\\n3\\n33\\n33\\n33333\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n3\\n33\\n3\\n333\\n33\\n3333\\n33\\n33\\n3\\n33\\n33\\n3333\\n33\\n3333\\n3\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n333\\n33\\n33\\n3333\\n3\\n33\\n33\\n333\\n3\\n3\\n33\\n33\\n3\\n33333\\n33\\n333\\n333\\n3333\\n33\\n33\\n333\\n3\\n33\\n33\\n33\\n333\\n3\\n3\\n3333\\n333\\n333\\n3\\n333\\n3\\n3\\n33333333333\\n33\\n333\\n3\\n33\\n333\\n33\\n33\\n3\\n3\\n333\\n33\\n3\\n33\\n3\\n333\\n3\\n3333\\n333\\n33\\n33\\n33\\n333\\n33\\n3\\n33\\n33\\n33\\n33\\n33333\\n33\\n33\\n33\\n33\\n3 33\\n333\\n3\\n33 33\\n3\\n33\\n333\\n333333\\n3\\n3333\\n33\\n33\\n3333\\n33\\n33\\n33\\n333', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 123}),\n",
       " Document(page_content='333\\n3\\n3333\\n3\\n33\\n3\\n3\\n33333\\n333\\n3333\\n3\\n33\\n333\\n3\\n333\\n3\\n333\\n33\\n3\\n33\\n33\\n3333\\n333\\n33\\n33\\n33 3\\n3\\n3333\\n333\\n3\\n3\\n3\\n333\\n33\\n3\\n333\\n33\\n3\\n3333\\n33\\n3\\n33\\n3333\\n333\\n33\\n333\\n33\\n3\\n3\\n33\\n333\\n3\\n3\\n3333333\\n3333\\n33\\n3\\n3\\n33\\n3\\n3333\\n3\\n33\\n33 3\\n33\\n3\\n3333\\n333\\n33\\n3\\n3\\n333\\n33\\n3\\n3333\\n3\\n33Linear Discriminant Analysis\\n1\\n11\\n1\\n1\\n11111\\n11\\n1\\n11111\\n111\\n11 111\\n111\\n1111\\n111\\n1\\n11111\\n111\\n11\\n1111\\n11\\n11\\n11111\\n11\\n11\\n11\\n1\\n11\\n11\\n1\\n11\\n11\\n1\\n11\\n1\\n1\\n11\\n111\\n111111\\n11\\n111\\n11 1\\n111\\n1\\n1\\n11\\n1\\n11\\n11\\n11\\n111\\n111\\n11\\n11\\n1 1\\n1\\n11\\n1111\\n11\\n1\\n111\\n11111\\n1\\n1111\\n11\\n111', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 123}),\n",
       " Document(page_content='111\\n111\\n1\\n11\\n1111\\n11\\n1\\n111 11\\n11\\n11\\n11\\n11\\n1\\n1\\n11\\n11\\n1\\n111\\n1\\n11\\n11\\n1\\n111\\n111 111\\n11\\n11\\n11\\n111111\\n1\\n11\\n11\\n11\\n1111\\n1111\\n111\\n1\\n11 1\\n111\\n1111\\n11\\n111\\n111\\n11\\n1\\n11\\n111\\n111\\n111\\n11\\n11\\n11\\n111\\n11 1\\n111\\n11\\n11\\n11\\n1\\n11\\n11\\n1\\n11\\n11\\n1\\n111\\n1\\n11111\\n11111\\n1\\n11\\n111\\n1\\n11\\n111 1\\n11\\n1\\n11 11\\n1\\n111\\n111\\n11\\n1\\n111\\n1\\n1\\n1111\\n1111\\n11\\n11\\n1111\\n11\\n1\\n11\\n11\\n11\\n1 1\\n111\\n1 11\\n1\\n11\\n11\\n11 1\\n11\\n1\\n1111111\\n1\\n11\\n11\\n1\\n11\\n111\\n11\\n111\\n1111\\n11\\n11\\n11\\n1 1\\n11\\n11111\\n1\\n11\\n11\\n111\\n11\\n11\\n11\\n1\\n1\\n11\\n11\\n111\\n111\\n11\\n111\\n11\\n111\\n1\\n11\\n1\\n11\\n11\\n1\\n111\\n1111\\n11\\n1\\n11', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 123}),\n",
       " Document(page_content='1\\n11\\n1 12\\n22\\n2\\n222\\n22\\n222\\n2\\n22\\n222\\n222\\n222\\n2222\\n2\\n222\\n22\\n2\\n2\\n222\\n2\\n222\\n2\\n222\\n2222\\n2\\n22\\n22\\n22\\n22222\\n2222\\n2\\n222\\n222\\n22\\n2\\n22\\n22\\n222\\n2\\n2222222\\n22\\n22\\n222\\n222\\n222\\n22\\n2\\n222\\n22\\n2\\n22\\n22\\n2 2\\n22\\n2222\\n2\\n2\\n2\\n22\\n2222\\n22\\n222\\n2\\n22\\n222\\n222\\n22\\n222\\n2\\n2\\n222\\n222\\n2\\n22\\n2222\\n222\\n2222\\n22\\n222\\n2\\n2222\\n222\\n22\\n2\\n222\\n2222\\n222\\n22\\n2\\n222\\n22\\n2\\n22222\\n2222\\n22\\n222\\n22\\n22\\n22\\n2222\\n2222\\n2\\n22\\n22\\n22\\n222\\n22222\\n22\\n22\\n2\\n22\\n2\\n2\\n22\\n2\\n22\\n22\\n2\\n22\\n22\\n22\\n22\\n2\\n222\\n22\\n2222\\n2222\\n222\\n222\\n222\\n2222\\n2\\n222\\n2\\n22\\n222\\n222\\n22\\n222 22\\n222\\n222\\n22222\\n22 222\\n222', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 123}),\n",
       " Document(page_content='2\\n22\\n22 2\\n2\\n2222\\n2\\n22\\n22\\n2\\n22\\n22\\n22\\n2\\n22 22\\n2\\n222\\n2222\\n222\\n2\\n2 2222 2\\n22\\n22\\n2222\\n22\\n222\\n2\\n22\\n22\\n2\\n22\\n22\\n222\\n22\\n22\\n2222\\n22\\n2222\\n2222\\n2\\n222\\n22 2\\n22\\n2\\n22\\n22222\\n22\\n22\\n22\\n222\\n2\\n222\\n22\\n2\\n2 22 222\\n22\\n22\\n22 2\\n22 22\\n222\\n22\\n22\\n22 22\\n22\\n223\\n33\\n33\\n33\\n3\\n3\\n333\\n3\\n33\\n3\\n333\\n33\\n3\\n33\\n333\\n33\\n333\\n3\\n333\\n3333\\n3\\n33\\n33\\n333\\n33\\n33\\n33\\n3\\n33\\n33\\n33333\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n3\\n33\\n3\\n333\\n33\\n3333\\n33\\n33\\n3\\n33\\n33\\n3333\\n33\\n3333\\n3\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n333\\n33\\n33\\n3333\\n3\\n33\\n33\\n333\\n3\\n3\\n33\\n33\\n3\\n33333\\n33\\n333\\n333\\n3333\\n33\\n33\\n333\\n3\\n33\\n33\\n33', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 123}),\n",
       " Document(page_content='333\\n3\\n3\\n3333\\n333\\n333\\n3\\n333\\n3\\n3\\n33333333333\\n33\\n333\\n3\\n33\\n333\\n33\\n33\\n3\\n3\\n333\\n33\\n3\\n33\\n3\\n333\\n3\\n3333\\n333\\n33\\n33\\n33\\n333\\n33\\n3\\n33\\n33\\n33\\n33\\n33333\\n33\\n33\\n33\\n33\\n3 33\\n333\\n3\\n33 33\\n3\\n33\\n333\\n333333\\n3\\n3333\\n33\\n33\\n3333\\n33\\n33\\n33\\n333\\n333\\n3\\n3333\\n3\\n33\\n3\\n3\\n33333\\n333\\n3333\\n3\\n33\\n333\\n3\\n333\\n3\\n333\\n33\\n3\\n33\\n33\\n3333\\n333\\n33\\n33\\n33 3\\n3\\n3333\\n333\\n3\\n3\\n3\\n333\\n33\\n3\\n333\\n33\\n3\\n3333\\n33\\n3\\n33\\n3333\\n333\\n33\\n333\\n33\\n3\\n3\\n33\\n333\\n3\\n3\\n3333333\\n3333\\n33\\n3\\n3\\n33\\n3\\n3333\\n3\\n33\\n33 3\\n33\\n3\\n3333\\n333\\n33\\n3\\n3\\n333\\n33\\n3\\n3333\\n3\\n33\\nX1 X1\\nX2X2', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 123}),\n",
       " Document(page_content='FIGURE 4.2. The data come from three classes in IR2and are easily separated\\nby linear decision boundaries. The right plot shows the bounda ries found by linear\\ndiscriminant analysis. The left plot shows the boundaries foun d by linear regres-\\nsion of the indicator response variables. The middle class is co mpletely masked\\n(never dominates).\\n•The closest target classiﬁcation rule (4.6) is easily seen t o be exactly\\nthe same as the maximum ﬁtted component criterion (4.4).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 123}),\n",
       " Document(page_content='Thereisaseriousproblemwiththeregressionapproachwhen thenumber\\nof classesK≥3, especially prevalent when Kis large. Because of the rigid\\nnature of the regression model, classes can be maskedby others. Figure 4.2\\nillustrates anextremesituation when K= 3.Thethreeclasses areperfectly\\nseparated by linear decision boundaries, yet linear regres sion misses the\\nmiddle class completely.\\nIn Figure 4.3 we have projected the data onto the line joining the three', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 123}),\n",
       " Document(page_content='centroids (there is no information in the orthogonal direct ion in this case),\\nand we have included and coded the three response variables Y1,Y2and\\nY3. The three regression lines (left panel) are included, and w e see that\\nthe line corresponding to the middle class is horizontal and its ﬁtted values\\nare never dominant! Thus, observations from class 2 are clas siﬁed either\\nas class 1 or class 3. The right panel uses quadratic regressi on rather than', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 123}),\n",
       " Document(page_content='linear regression. For this simple example a quadratic rath er than linear\\nﬁt (for the middle class at least) would solve the problem. Ho wever, it\\ncan be seen that if there were four rather than three classes l ined up like\\nthis, a quadratic would not come down fast enough, and a cubic would\\nbe needed as well. A loose but general rule is that if K≥3 classes are\\nlined up, polynomial terms up to degree K−1 might be needed to resolve', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 123}),\n",
       " Document(page_content='them. Note also that these are polynomials along the derived direction\\npassing through the centroids, which can have arbitrary ori entation. So in', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 123}),\n",
       " Document(page_content='106 4. Linear Methods for Classiﬁcation\\n111\\n111\\n1\\n1\\n1111\\n11\\n11\\n1\\n11\\n11111\\n1\\n11111\\n11\\n1\\n11\\n11\\n11\\n11111\\n111\\n111\\n1\\n1111\\n111\\n111\\n111\\n111111\\n11\\n1\\n11\\n111\\n1111\\n11\\n111\\n111\\n111\\n111\\n111\\n1\\n11\\n11\\n111\\n111\\n11\\n11\\n1111\\n11\\n1\\n111\\n1\\n11\\n1\\n11\\n1\\n11\\n11\\n111\\n1\\n1111\\n111\\n111\\n12222 2222222 2 2 222 2 2 22 2222 222222 2 2 2222 22 22222 2222222222222 22 22222 222 2222222 2 22 222 2 22222 22 22 222 222222 22222 222 22 2 222222222 22222222 222 222 2 22222222222222 22\\n3\\n3\\n33\\n3\\n3333\\n33\\n33\\n33\\n333\\n333\\n3\\n3\\n33333\\n3\\n33\\n333\\n33\\n333333', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 124}),\n",
       " Document(page_content='333333\\n3\\n3333\\n3\\n33\\n3\\n33\\n3\\n33\\n333\\n33\\n33\\n33333\\n33\\n3\\n3333\\n33\\n333\\n33\\n3333\\n33333\\n3333\\n33333\\n33\\n3\\n33\\n33\\n33\\n33\\n33\\n333\\n3\\n333\\n333\\n333\\n3333\\n3\\n3333\\n333\\n3\\n33333\\n0.00.51.0\\n0.0 0.2 0.4 0.6 0.8 1.0111\\n111\\n1\\n1\\n1111\\n11\\n11\\n1\\n11\\n11111\\n1\\n11\\n111\\n11\\n1\\n11\\n11\\n11\\n11111\\n11\\n1\\n111\\n1\\n1111\\n111\\n111\\n111\\n111111\\n111\\n11111\\n111111\\n111\\n111\\n111\\n111\\n111\\n1\\n1111111 111\\n111111111111\\n1 111 1111111 11111111 111111112\\n2\\n22\\n2\\n2222\\n22\\n22\\n22\\n222\\n222\\n2\\n2\\n22222\\n2\\n22\\n222\\n22\\n222222\\n222222\\n2\\n22222\\n2\\n22 2\\n222222222 222222\\n22', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 124}),\n",
       " Document(page_content='22222222 222222222 22 22\\n22 22\\n22\\n22\\n222\\n222\\n22\\n22\\n2222\\n22\\n2\\n222\\n2\\n22\\n2\\n22\\n2\\n22\\n22\\n222\\n2\\n222\\n222\\n222\\n233 3333\\n333333\\n3333\\n33333333\\n333333\\n33\\n33333\\n3 333333333333 3333\\n33\\n3\\n33\\n3\\n33\\n3333 333\\n33333\\n33\\n3\\n333333333\\n33\\n3333\\n33333\\n3333\\n33333\\n33\\n3\\n33\\n33\\n33\\n33\\n33\\n333\\n3\\n333\\n333\\n333\\n3333\\n3\\n333\\n3\\n33\\n3\\n33333\\n0.00.51.0\\n0.0 0.2 0.4 0.6 0.8 1.0Degree = 1; Error = 0.33 Degree = 2; Error = 0.04\\nFIGURE 4.3. The eﬀects of masking on linear regression in IRfor a three-class', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 124}),\n",
       " Document(page_content='problem. The rug plot at the base indicates the positions and class membership of\\neach observation. The three curves in each panel are the ﬁtte d regressions to the\\nthree-class indicator variables; for example, for the blue class ,yblueis1for the\\nblue observations, and 0for the green and orange. The ﬁts are linear and quadratic\\npolynomials. Above each plot is the training error rate. The Bay es error rate is\\n0.025for this problem, as is the LDA error rate.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 124}),\n",
       " Document(page_content='p-dimensional input space, one would need general polynomia l terms and\\ncross-products of total degree K−1,O(pK−1) terms in all, to resolve such\\nworst-case scenarios.\\nThe example is extreme, but for large Kand smallpsuch maskings\\nnaturally occur. As a more realistic illustration, Figure 4 .4 is a projection\\nof the training data for a vowel recognition problem onto an i nformative\\ntwo-dimensional subspace. There are K= 11 classes in p= 10 dimensions.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 124}),\n",
       " Document(page_content='This is a diﬃcult classiﬁcation problem, and the best method s achieve\\naround 40% errors on the test data. The main point here is summ arized in\\nTable 4.1; linear regression has an error rate of 67%, while a close relative,\\nlineardiscriminantanalysis,hasanerrorrateof56%.Itse emsthatmasking\\nhas hurt in this case. While all the other methods in this chap ter are based\\non linear functions of xas well, they use them in such a way that avoids\\nthis masking problem.\\n4.3 Linear Discriminant Analysis', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 124}),\n",
       " Document(page_content='Decision theory for classiﬁcation (Section 2.4) tells us th at we need to know\\nthe class posteriors Pr( G|X) for optimal classiﬁcation. Suppose fk(x) is\\nthe class-conditional density of Xin classG=k, and letπkbe the prior\\nprobability of class k, with∑K\\nk=1πk= 1. A simple application of Bayes', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 124}),\n",
       " Document(page_content='4.3 Linear Discriminant Analysis 107\\nCoordinate 1 for Training DataCoordinate 2 for Training Data\\n-4 -2 0 2 4-6 -4 -2 0 2 4oooo oo\\nooooooo\\no\\noooo\\noo\\no\\no\\no\\no\\noooooooooooo\\no\\no\\no\\no\\no ooooooo\\nooooooo\\no\\noo\\no\\nooooo\\no\\nooooooo\\noo\\noooo\\no\\no\\nooooooooooooo\\no\\noooooo\\no\\nooo\\noooo\\noooo\\no\\nooooo\\noooooo o\\no\\no\\noooooooooo\\no\\noo\\no\\noo\\nooooooooooooo\\noooooooooooooooooo\\noooooooooooo\\no\\no\\nooooooo\\no\\no\\nooooooo\\nooooo\\nooooooo\\no\\no\\noooooooooooooooo\\no\\no\\no\\no\\noooooooo\\noooo\\noo\\noooooooooo\\no\\nooo\\no\\no\\no\\noooo\\no\\no\\nooooo\\no\\no o\\no\\no\\no\\no', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 125}),\n",
       " Document(page_content='ooooooooo oooo\\no\\no\\noooooo\\no\\noooooooo\\no\\noo\\no\\noooooooo\\no\\no\\no\\no\\nooo\\nooooooooooooooo\\nooo\\noooooooo\\no\\nooo\\nooooooooooooo\\no\\noooo\\nooooo\\nooooo\\no\\noo\\no\\no\\no\\no\\nooo\\nooo\\no\\nooooo\\no\\noooo\\no\\nooooooooooooo\\no ooooooooooooooooo\\no\\no\\nooooo\\nooooooooooo\\no\\no\\nooo\\nooo\\noo oo\\noooooo\\noooooo\\nooooooooo\\no\\no\\nooooooo\\noooooooooooo\\noo\\no\\no\\noo\\n••••••••••••••\\n••\\n••\\n••••Linear Discriminant Analysis\\nFIGURE 4.4. A two-dimensional plot of the vowel training data. There are', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 125}),\n",
       " Document(page_content='eleven classes with X∈IR10, and this is the best view in terms of a LDA model\\n(Section 4.3.3). The heavy circles are the projected mean vec tors for each class.\\nThe class overlap is considerable.\\nTABLE 4.1. Training and test error rates using a variety of linear techni ques\\non the vowel data. There are eleven classes in ten dimensions, o f which three\\naccount for 90%of the variance (via a principal components analysis). We see', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 125}),\n",
       " Document(page_content='that linear regression is hurt by masking, increasing the tes t and training error\\nby over10%.\\nTechnique Error Rates\\nTraining Test\\nLinear regression 0.48 0.67\\nLinear discriminant analysis 0.32 0.56\\nQuadratic discriminant analysis 0.01 0.53\\nLogistic regression 0.22 0.51', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 125}),\n",
       " Document(page_content='108 4. Linear Methods for Classiﬁcation\\ntheorem gives us\\nPr(G=k|X=x) =fk(x)πk∑K\\nℓ=1fℓ(x)πℓ. (4.7)\\nWe see that in terms of ability to classify, having the fk(x) is almost equiv-\\nalent to having the quantity Pr( G=k|X=x).\\nMany techniques are based on models for the class densities:\\n•linear and quadratic discriminant analysis use Gaussian de nsities;\\n•moreﬂexiblemixturesofGaussiansallowfornonlineardeci sionbound-\\naries (Section 6.8);', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 126}),\n",
       " Document(page_content='•general nonparametric density estimates for each class den sity allow\\nthe most ﬂexibility (Section 6.6.2);\\n•Naive Bayes models are a variant of the previous case, and assume\\nthat each of the class densities are products of marginal den sities;\\nthat is, they assume that the inputs are conditionally indep endent in\\neach class (Section 6.6.3).\\nSuppose that we model each class density as multivariate Gau ssian\\nfk(x) =1\\n(2π)p/2|Σk|1/2e−1\\n2(x−µk)TΣ−1\\nk(x−µk). (4.8)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 126}),\n",
       " Document(page_content='Linear discriminant analysis (LDA) arises in the special ca se when we\\nassume that the classes have a common covariance matrix Σk=Σ∀k. In\\ncomparing two classes kandℓ, it is suﬃcient to look at the log-ratio, and\\nwe see that\\nlogPr(G=k|X=x)\\nPr(G=ℓ|X=x)= logfk(x)\\nfℓ(x)+logπk\\nπℓ\\n= logπk\\nπℓ−1\\n2(µk+µℓ)TΣ−1(µk−µℓ)\\n+xTΣ−1(µk−µℓ),(4.9)\\nan equation linear in x. The equal covariance matrices cause the normal-\\nization factors to cancel, as well as the quadratic part in th e exponents.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 126}),\n",
       " Document(page_content='This linear log-odds function implies that the decision bou ndary between\\nclasseskandℓ—the set where Pr( G=k|X=x) = Pr(G=ℓ|X=x)—is\\nlinear inx; inpdimensions a hyperplane. This is of course true for any pair\\nof classes, so all the decision boundaries are linear. If we d ivide IRpinto\\nregions that are classiﬁed as class 1, class 2, etc., these re gions will be sep-\\narated by hyperplanes. Figure 4.5 (left panel) shows an idea lized example', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 126}),\n",
       " Document(page_content='with three classes and p= 2. Here the data do arise from three Gaus-\\nsian distributions with a common covariance matrix. We have included in', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 126}),\n",
       " Document(page_content='4.3 Linear Discriminant Analysis 109\\n+++\\n3\\n21\\n11\\n233\\n3\\n123\\n32\\n11211\\n33\\n12 1\\n23\\n23\\n3\\n12\\n211\\n1\\n13\\n222\\n21 3\\n2 23\\n13\\n13\\n32\\n13\\n3\\n23\\n133\\n2133\\n22\\n3\\n22\\n21\\n11\\n11\\n2\\n133\\n1\\n13\\n32\\n222 3\\n12\\nFIGURE 4.5. The left panel shows three Gaussian distributions, with the sa me\\ncovariance and diﬀerent means. Included are the contours of c onstant density\\nenclosing 95% of the probability in each case. The Bayes decision boundari es\\nbetween each pair of classes are shown (broken straight lines) , and the Bayes', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 127}),\n",
       " Document(page_content='decision boundaries separating all three classes are the thi cker solid lines (a subset\\nof the former). On the right we see a sample of 30drawn from each Gaussian\\ndistribution, and the ﬁtted LDA decision boundaries.\\nthe ﬁgure the contours corresponding to 95% highest probabi lity density,\\nas well as the class centroids. Notice that the decision boun daries are not\\nthe perpendicular bisectors of the line segments joining th e centroids. This', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 127}),\n",
       " Document(page_content='would be the case if the covariance Σwere spherical σ2I, and the class\\npriors were equal. From (4.9) we see that the linear discriminant functions\\nδk(x) =xTΣ−1µk−1\\n2µT\\nkΣ−1µk+logπk (4.10)\\nareanequivalentdescriptionofthedecisionrule,with G(x) = argmaxkδk(x).\\nIn practice we do not know the parameters of the Gaussian dist ributions,\\nand will need to estimate them using our training data:\\n•ˆπk=Nk/N, whereNkis the number of class- kobservations;\\n•ˆµk=∑\\ngi=kxi/Nk;\\n•ˆΣ=∑K\\nk=1∑', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 127}),\n",
       " Document(page_content='gi=k(xi−ˆµk)(xi−ˆµk)T/(N−K).\\nFigure 4.5 (right panel) shows the estimated decision bound aries based on\\na sample of size 30 each from three Gaussian distributions. F igure 4.1 on\\npage 103 is another example, but here the classes are not Gaus sian.\\nWith two classes there is a simple correspondence between li near dis-\\ncriminant analysis and classiﬁcation by linear regression , as in (4.5). The\\nLDA rule classiﬁes to class 2 if\\nxTˆΣ−1(ˆµ2−ˆµ1)>1\\n2(ˆµ2+ ˆµ1)TˆΣ−1(ˆµ2−ˆµ1)−log(N2/N1),(4.11)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 127}),\n",
       " Document(page_content='110 4. Linear Methods for Classiﬁcation\\nand class 1 otherwise. Suppose we code the targets in the two c lasses as +1\\nand−1, respectively. It is easy to show that the coeﬃcient vector from least\\nsquares is proportional to the LDA direction given in (4.11) (Exercise 4.2).\\n[In fact, this correspondence occurs for any (distinct) cod ing of the targets;\\nsee Exercise 4.2]. However unless N1=N2the intercepts are diﬀerent and\\nhence the resulting decision rules are diﬀerent.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 128}),\n",
       " Document(page_content='SincethisderivationoftheLDAdirectionvialeastsquares doesnotusea\\nGaussian assumption for the features, its applicability ex tends beyond the\\nrealm of Gaussian data. However the derivation of the partic ular intercept\\nor cut-point given in (4.11) doesrequire Gaussian data. Thus it makes\\nsense to instead choose the cut-point that empirically mini mizes training\\nerror for a given dataset. This is something we have found to w ork well in\\npractice, but have not seen it mentioned in the literature.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 128}),\n",
       " Document(page_content='With more than two classes, LDA is not the same as linear regre ssion of\\nthe class indicator matrix, and it avoids the masking proble ms associated\\nwith that approach (Hastie et al., 1994). A correspondence b etween regres-\\nsion and LDA can be established through the notion of optimal scoring ,\\ndiscussed in Section 12.5.\\nGetting back to the general discriminant problem (4.8), if t heΣkare\\nnot assumed to be equal, then the convenient cancellations i n (4.9) do not', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 128}),\n",
       " Document(page_content='occur; in particular the pieces quadratic in xremain. We then get quadratic\\ndiscriminant functions (QDA),\\nδk(x) =−1\\n2log|Σk|−1\\n2(x−µk)TΣ−1\\nk(x−µk)+logπk.(4.12)\\nThe decision boundary between each pair of classes kandℓis described by\\na quadratic equation {x:δk(x) =δℓ(x)}.\\nFigure 4.6 shows an example (from Figure 4.1 on page 103) wher e the\\nthree classes are Gaussian mixtures (Section 6.8) and the de cision bound-\\naries are approximated by quadratic equations in x. Here we illustrate', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 128}),\n",
       " Document(page_content='two popular ways of ﬁtting these quadratic boundaries. The r ight plot\\nuses QDA as described here, while the left plot uses LDA in the enlarged\\nﬁve-dimensional quadratic polynomial space. The diﬀerenc es are generally\\nsmall; QDA is the preferred approach, with the LDA method a co nvenient\\nsubstitute2.\\nTheestimatesforQDAaresimilartothoseforLDA,excepttha tseparate\\ncovariance matrices must be estimated for each class. When pis large this', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 128}),\n",
       " Document(page_content='can mean a dramatic increase in parameters. Since the decisi on boundaries\\nare functions of the parameters of the densities, counting t he number of\\nparameters must be done with care. For LDA, it seems there are (K−\\n1)×(p+1) parameters, since we only need the diﬀerences δk(x)−δK(x)\\n2For this ﬁgure and many similar ﬁgures in the book we compute the d ecision bound-\\naries by an exhaustive contouring method. We compute the decision rule on a ﬁne lattice', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 128}),\n",
       " Document(page_content='of points, and then use contouring algorithms to compute the bound aries.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 128}),\n",
       " Document(page_content='4.3 Linear Discriminant Analysis 111\\n1\\n11\\n11\\n111\\n1\\n111\\n1\\n11\\n11\\n1111 1\\n111\\n1\\n11\\n111\\n1111\\n11\\n1\\n11\\n111\\n11\\n1111\\n1 1\\n111\\n11\\n1\\n1\\n1 1\\n11\\n1111\\n11111\\n1\\n11\\n111\\n111\\n1111\\n1\\n111\\n11\\n11\\n11\\n1\\n11\\n11\\n111\\n11\\n1\\n1\\n11\\n111\\n111\\n11\\n11\\n11\\n11 1\\n11\\n11 111\\n1 11\\n1\\n11\\n1\\n11\\n1\\n11\\n1\\n11 1\\n1\\n11\\n11\\n1111\\n1111\\n1\\n111\\n11\\n111\\n1\\n111\\n111\\n1111\\n11\\n1 11\\n111\\n11\\n1\\n11\\n11\\n11\\n111\\n12\\n22\\n22\\n222\\n2\\n2\\n22\\n2 2\\n2\\n22\\n22\\n22\\n2222\\n2\\n22\\n222\\n2\\n22\\n2 2\\n22\\n222\\n2\\n222\\n22222\\n22\\n2\\n222\\n22\\n2\\n22\\n22\\n22\\n22\\n22\\n2222\\n22\\n22\\n222\\n222\\n22222\\n22\\n222\\n22\\n22\\n22\\n2\\n22\\n2222\\n22\\n2\\n22\\n2', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 129}),\n",
       " Document(page_content='222\\n2\\n222\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n2222 2\\n222\\n22\\n2\\n22\\n22\\n222\\n222\\n2\\n22\\n22\\n22222\\n22\\n222\\n22\\n2\\n222\\n22\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n22222\\n22\\n3\\n33\\n33\\n33\\n3\\n3333\\n33\\n3\\n33\\n3\\n333\\n33\\n33\\n333\\n333\\n3\\n3\\n33\\n333\\n333\\n333\\n333\\n3333\\n333\\n3333\\n3\\n33333\\n33\\n3\\n33\\n3\\n33\\n3333\\n333\\n3\\n333\\n333\\n33\\n333\\n3\\n3333\\n33\\n333\\n33\\n3\\n3333\\n333\\n33333\\n3\\n3\\n3\\n333\\n33\\n3\\n33\\n3\\n33\\n33\\n333\\n33\\n3\\n3333\\n33\\n333\\n3\\n333\\n33333\\n33\\n333\\n33\\n33\\n3333\\n333\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n3\\n333\\n33\\n33\\n33\\n1\\n11\\n11\\n111\\n1\\n111\\n1\\n11\\n11\\n1111 1\\n111\\n1\\n11\\n111\\n1111\\n11\\n1\\n11\\n111\\n11\\n1111\\n1 1\\n111\\n11\\n1\\n1', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 129}),\n",
       " Document(page_content='1 1\\n11\\n1111\\n11111\\n1\\n11\\n111\\n111\\n1111\\n1\\n111\\n11\\n11\\n11\\n1\\n11\\n11\\n111\\n11\\n1\\n1\\n11\\n111\\n111\\n11\\n11\\n11\\n11 1\\n11\\n11 111\\n1 11\\n1\\n11\\n1\\n11\\n1\\n11\\n1\\n11 1\\n1\\n11\\n11\\n1111\\n1111\\n1\\n111\\n11\\n111\\n1\\n111\\n111\\n1111\\n11\\n1 11\\n111\\n11\\n1\\n11\\n11\\n11\\n111\\n12\\n22\\n22\\n222\\n2\\n2\\n22\\n2 2\\n2\\n22\\n22\\n22\\n2222\\n2\\n22\\n222\\n2\\n22\\n2 2\\n22\\n222\\n2\\n222\\n22222\\n22\\n2\\n222\\n22\\n2\\n22\\n22\\n22\\n22\\n22\\n2222\\n22\\n22\\n222\\n222\\n22222\\n22\\n222\\n22\\n22\\n22\\n2\\n22\\n2222\\n22\\n2\\n22\\n2\\n222\\n2\\n222\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n2222 2\\n222\\n22\\n2\\n22\\n22\\n222\\n222\\n2\\n22\\n22\\n22222\\n22\\n222\\n22\\n2\\n222\\n22\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n22222', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 129}),\n",
       " Document(page_content='22\\n3\\n33\\n33\\n33\\n3\\n3333\\n33\\n3\\n33\\n3\\n333\\n33\\n33\\n333\\n333\\n3\\n3\\n33\\n333\\n333\\n333\\n333\\n3333\\n333\\n3333\\n3\\n33333\\n33\\n3\\n33\\n3\\n33\\n3333\\n333\\n3\\n333\\n333\\n33\\n333\\n3\\n3333\\n33\\n333\\n33\\n3\\n3333\\n333\\n33333\\n3\\n3\\n3\\n333\\n33\\n3\\n33\\n3\\n33\\n33\\n333\\n33\\n3\\n3333\\n33\\n333\\n3\\n333\\n33333\\n33\\n333\\n33\\n33\\n3333\\n333\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n3\\n333\\n33\\n33\\n33\\nFIGURE 4.6. Two methods for ﬁtting quadratic boundaries. The left plot sho ws\\nthe quadratic decision boundaries for the data in Figure 4.1 (obtained using LDA\\nin the ﬁve-dimensional space X1,X2,X1X2,X2\\n1,X2', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 129}),\n",
       " Document(page_content='2). The right plot shows the\\nquadratic decision boundaries found by QDA. The diﬀerences are small, as is\\nusually the case.\\nbetween the discriminant functions where Kis some pre-chosen class (here\\nwe have chosen the last), and each diﬀerence requires p+ 1 parameters3.\\nLikewise for QDA there will be ( K−1)×{p(p+ 3)/2 + 1}parameters.\\nBoth LDA and QDA perform well on an amazingly large and divers e set\\nof classiﬁcation tasks. For example, in the STATLOG project (Michie et', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 129}),\n",
       " Document(page_content='al., 1994) LDA was among the top three classiﬁers for 7 of the 2 2 datasets,\\nQDA among the top three for four datasets, and one of the pair w ere in the\\ntop threefor 10datasets. Both techniques arewidely used,a ndentirebooks\\nare devoted to LDA. It seems that whatever exotic tools are th e rage of the\\nday, we should always have available these two simple tools. The question\\narises why LDA and QDA have such a good track record. The reaso n is not', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 129}),\n",
       " Document(page_content='likely to be that the data are approximately Gaussian, and in addition for\\nLDA that the covariances are approximately equal. More like ly a reason is\\nthat the data can only support simple decision boundaries su ch as linear or\\nquadratic, and the estimates provided via the Gaussian mode ls are stable.\\nThis is a bias variance tradeoﬀ—we can put up with the bias of a l inear\\ndecision boundary because it can be estimated with much lowe r variance', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 129}),\n",
       " Document(page_content='than more exotic alternatives. This argument is less believ able for QDA,\\nsince it can have many parameters itself, although perhaps f ewer than the\\nnon-parametric alternatives.\\n3Althoughweﬁtthecovariancematrix ˆΣtocomputetheLDAdiscriminantfunctions,\\na much reduced function of it is all that is required to estimate the O(p) parameters\\nneeded to compute the decision boundaries.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 129}),\n",
       " Document(page_content='112 4. Linear Methods for Classiﬁcation\\nMisclassification Rate\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••Regularized Discriminant Analysis on the Vowel Data\\nTest Data\\nTrain Data\\nα\\nFIGURE 4.7. Test and training errors for the vowel data, using regularize d\\ndiscriminant analysis with a series of values of α∈[0,1]. The optimum for the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 130}),\n",
       " Document(page_content='test data occurs around α= 0.9, close to quadratic discriminant analysis.\\n4.3.1 Regularized Discriminant Analysis\\nFriedman (1989) proposed a compromise between LDA and QDA, w hich\\nallows one to shrink the separate covariances of QDA toward a common\\ncovariance as in LDA. These methods are very similar in ﬂavor to ridge\\nregression. The regularized covariance matrices have the f orm\\nˆΣk(α) =αˆΣk+(1−α)ˆΣ, (4.13)\\nwhereˆΣis the pooled covariance matrix as used in LDA. Here α∈[0,1]', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 130}),\n",
       " Document(page_content='allows a continuum of models between LDA and QDA, and needs to be\\nspeciﬁed. In practice αcan be chosen based on the performance of the\\nmodel on validation data, or by cross-validation.\\nFigure 4.7 shows the results of RDA applied to the vowel data. Both\\nthe training and test error improve with increasing α, although the test\\nerror increases sharply after α= 0.9. The large discrepancy between the\\ntraining and test error is partly due to the fact that there ar e many repeat', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 130}),\n",
       " Document(page_content='measurements on a small number of individuals, diﬀerent in t he training\\nand test set.\\nSimilar modiﬁcations allow ˆΣitself to be shrunk toward the scalar\\ncovariance,\\nˆΣ(γ) =γˆΣ+(1−γ)ˆσ2I (4.14)\\nforγ∈[0,1]. Replacing ˆΣin (4.13) by ˆΣ(γ) leads to a more general family\\nof covariances ˆΣ(α,γ) indexed by a pair of parameters.\\nIn Chapter 12, we discuss other regularized versions of LDA, which are\\nmore suitable when the data arise from digitized analog sign als and images.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 130}),\n",
       " Document(page_content='4.3 Linear Discriminant Analysis 113\\nInthesesituationsthefeaturesarehigh-dimensionalandc orrelated,andthe\\nLDA coeﬃcients can be regularized to be smooth or sparse in th e original\\ndomain of the signal. This leads to better generalization an d allows for\\neasier interpretation of the coeﬃcients. In Chapter 18 we al so deal with\\nvery high-dimensional problems, where for example the feat ures are gene-\\nexpression measurements in microarray studies. There the m ethods focus', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 131}),\n",
       " Document(page_content='on the case γ= 0 in (4.14), and other severely regularized versions of LDA .\\n4.3.2 Computations for LDA\\nAs a lead-in to the next topic, we brieﬂy digress on the comput ations\\nrequired for LDA and especially QDA. Their computations are simpliﬁed\\nby diagonalizing ˆΣorˆΣk. For the latter, suppose we compute the eigen-\\ndecomposition for each ˆΣk=UkDkUT\\nk, whereUkisp×porthonormal,\\nandDka diagonal matrix of positive eigenvalues dkℓ. Then the ingredients\\nforδk(x) (4.12) are\\n•(x−ˆµk)TˆΣ−1', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 131}),\n",
       " Document(page_content='k(x−ˆµk) = [UT\\nk(x−ˆµk)]TD−1\\nk[UT\\nk(x−ˆµk)];\\n•log|ˆΣk|=∑\\nℓlogdkℓ.\\nIn light of the computational steps outlined above, the LDA c lassiﬁer\\ncan be implemented by the following pair of steps:\\n•Spherethe data with respect to the common covariance estimate ˆΣ:\\nX∗←D−1\\n2UTX, whereˆΣ=UDUT. The common covariance esti-\\nmate ofX∗will now be the identity.\\n•Classifytotheclosestclass centroidinthetransformedsp ace,modulo\\nthe eﬀect of the class prior probabilities πk.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 131}),\n",
       " Document(page_content='4.3.3 Reduced-Rank Linear Discriminant Analysis\\nSo far we have discussed LDA as a restricted Gaussian classiﬁ er. Part of\\nits popularity is due to an additional restriction that allo ws us to view\\ninformative low-dimensional projections of the data.\\nTheKcentroids in p-dimensional input space lie in an aﬃne subspace\\nof dimension≤K−1, and ifpis much larger than K, this will be a con-\\nsiderable drop in dimension. Moreover, in locating the clos est centroid, we', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 131}),\n",
       " Document(page_content='can ignore distances orthogonal to this subspace, since the y will contribute\\nequally to each class. Thus we might just as well project the X∗onto this\\ncentroid-spanning subspace HK−1, and make distance comparisons there.\\nThus there is a fundamental dimension reduction in LDA, name ly, that we\\nneed only consider the data in a subspace of dimension at most K−1.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 131}),\n",
       " Document(page_content='114 4. Linear Methods for Classiﬁcation\\nIfK= 3, for instance, this could allow us to view the data in a two-\\ndimensional plot, color-coding the classes. In doing so we w ould not have\\nrelinquished any of the information needed for LDA classiﬁc ation.\\nWhat ifK >3? We might then ask for a L<K−1 dimensional subspace\\nHL⊆HK−1optimal for LDA in some sense. Fisher deﬁned optimal to\\nmean that the projected centroids were spread out as much as p ossible in', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 132}),\n",
       " Document(page_content='terms of variance. This amounts to ﬁnding principal compone nt subspaces\\nof the centroids themselves (principal components are desc ribed brieﬂy in\\nSection3.5.1,andinmoredetailinSection14.5.1).Figure 4.4showssuchan\\noptimal two-dimensional subspace for the vowel data. Here t here are eleven\\nclasses, each a diﬀerent vowel sound, in a ten-dimensional i nput space. The\\ncentroids require the full space in this case, since K−1 =p, but we have', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 132}),\n",
       " Document(page_content='shown an optimal two-dimensional subspace. The dimensions are ordered,\\nsowecancomputeadditionaldimensionsinsequence.Figure 4.8showsfour\\nadditional pairs of coordinates, also known as canonical ordiscriminant\\nvariables. In summary then, ﬁnding the sequences of optimal subspaces\\nfor LDA involves the following steps:\\n•compute the K×pmatrix of class centroids Mand the common\\ncovariance matrix W(forwithin-class covariance);\\n•compute M∗=MW−1\\n2using the eigen-decomposition of W;', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 132}),\n",
       " Document(page_content='•compute B∗,thecovariancematrixof M∗(Bforbetween-class covari-\\nance), and its eigen-decomposition B∗=V∗DBV∗T. The columns\\nv∗\\nℓofV∗in sequence from ﬁrst to last deﬁne the coordinates of the\\noptimal subspaces.\\nCombining all these operations the ℓthdiscriminant variable is given by\\nZℓ=vT\\nℓXwithvℓ=W−1\\n2v∗\\nℓ.\\nFisher arrived at this decomposition via a diﬀerent route, w ithout refer-\\nring to Gaussian distributions at all. He posed the problem:\\nFind the linear combination Z=aTXsuch that the between-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 132}),\n",
       " Document(page_content='class variance is maximized relative to the within-class va riance.\\nAgain, the between class variance is the variance of the clas s means of\\nZ, and the within class variance is the pooled variance about t he means.\\nFigure 4.9 shows why this criterion makes sense. Although th e direction\\njoining the centroids separates the means as much as possibl e (i.e., max-\\nimizes the between-class variance), there is considerable overlap between', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 132}),\n",
       " Document(page_content='the projected classes due to the nature of the covariances. B y taking the\\ncovariance into account as well, a direction with minimum ov erlap can be\\nfound.\\nThe between-class variance of ZisaTBaand the within-class variance\\naTWa, whereWis deﬁned earlier, and Bis the covariance matrix of the\\nclass centroid matrix M. Note that B+W=T, where Tis thetotal\\ncovariance matrix of X, ignoring class information.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 132}),\n",
       " Document(page_content='4.3 Linear Discriminant Analysis 115\\nCoordinate 1 Coordinate 3 \\n-4 -2 0 2 4-2 0 2o\\nooooo\\no\\no\\nooo\\nooo\\nooooooo\\no\\noo\\nooo\\nooo\\noo\\nooooooooo\\nooo\\no\\nooo\\nooooo\\noooooo ooooooo\\no\\no\\nooo oo\\no\\no\\nooo\\no\\no\\noooooooooooo\\noooo\\noooooo\\noo\\nooo\\noooooooo\\nooo\\noooo\\noo ooo\\noooo\\noooo\\noo\\noooo\\no\\noo\\noooooooo\\noooo\\noooooooooo\\noo\\nooo\\no\\nooo\\noooooo\\nooo\\noooooo\\no\\no\\noooooo\\noooooooooooo\\no\\nooo\\noo\\nooo\\nooooo\\noo\\nooo\\nooooo\\no\\nooooo\\no\\nooooooo\\noooo\\no\\noooo\\noo\\nooooo\\noooo\\no\\noo\\nooooo\\no\\no\\no\\nooo\\no\\nooooo\\nooooooo\\no\\noooo\\noooooo\\noooo\\no\\no\\noo ooo\\noo\\nooo\\no', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 133}),\n",
       " Document(page_content='oooo\\no\\nooooooo\\no\\noo\\noo\\no\\nooo\\nooooo\\no\\nooooo\\no\\noo\\no\\noo\\no\\noooooo\\no\\noo\\nooooooooooo\\noo\\noooooo\\noo\\no\\no\\nooo o\\no\\nooooo\\no\\no\\no\\nooo\\noooooooooo\\noooooo\\noooooo\\noooooo\\no\\no\\nooo\\nooooooooooo\\noo oo\\nooo\\noooooo\\no\\no\\noooooooo\\nooo\\noooooo\\no\\nooooo\\nooo\\no\\nooo\\no\\noooooooooo oooo ooooooo\\nooooooo\\n••••••••\\n••••••••••••••\\nCoordinate 2 Coordinate 3 \\n-6 -4 -2 0 2 4-2 0 2o\\nooooo\\no\\no\\nooo\\nooo\\nooooooo\\no\\noo\\nooo\\nooo\\noo\\nooooooooo\\nooo\\no\\nooo\\nooooo\\noooooo ooooooo\\no\\no\\nooo oo\\no\\no\\nooo\\no\\no\\noooooooooooo\\noooo\\noooooo\\noo\\nooo\\noooooooo\\nooo\\noooo\\nooooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 133}),\n",
       " Document(page_content='oooo\\noooo\\noo\\noooo\\no\\noo\\noooooooo\\noooo\\noooooooooo\\noo\\nooo\\no\\nooo\\noooooo\\nooo\\noooooo\\no\\no\\noooooo\\noooooooooooo\\no\\nooo\\noo\\nooo\\nooooo\\noo\\nooo\\nooooo\\no\\nooooo\\no\\nooooooo\\noooo\\no\\noooo\\noo\\nooooo\\noooo\\no\\noo\\nooooo\\no\\no\\no\\nooo\\no\\nooooo\\nooooooo\\no\\noooo\\noooooo\\noooo\\no\\no\\noo ooo\\noo\\nooo\\no\\noooo\\no\\nooooooo\\no\\noo\\noo\\no\\nooo\\nooooo\\no\\nooooo\\no\\noo\\no\\noo\\no\\noooooo\\no\\noo\\nooooooooooo\\noo\\noooooo\\noo\\no\\no\\nooo o\\no\\nooooo\\no\\no\\no\\nooo\\noooooooooo\\noooooo\\noooooo\\noooooo\\no\\no\\nooo\\nooooooooooo\\noo oo\\nooo\\noooooo\\no\\no\\noooooooo\\nooo\\noooooo\\no\\nooooo\\nooo\\no\\nooo\\no', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 133}),\n",
       " Document(page_content='oooooooooo ooooooooooo\\nooooooo\\n••••••••\\n••••••••••••••\\nCoordinate 1 Coordinate 7 \\n-4 -2 0 2 4-3 -2 -1 0 1 2 3ooo ooo\\no\\nooooooo\\noooo\\nooooooooooooooo\\nooo\\nooo\\no\\no\\nooooooooooooo\\noooo\\no\\nooooo\\no\\nooooooo\\no\\no\\nooooo\\nooooooooooo\\nooo\\no\\noooo\\noooo\\noooo\\noo\\noo\\no\\nooo\\noooooooooo\\nooo\\nooooo\\noooooooooooo\\noo oooo\\noooooo\\noooooo\\noooooooo\\no\\nooooooooo\\no\\nooooooooooo\\no\\nooooo\\noooooo\\noo\\noooo\\nooooooooooo\\nooo o\\nooo\\nooooooooo\\nooo\\noooooo\\noooo\\noooooooo\\no\\noooooo\\nooo\\nooo\\no\\noooo\\nooooooooooo\\no\\noooooo\\nooooo\\nooooooooooooo\\no\\nooooooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 133}),\n",
       " Document(page_content='oooo\\noooooooo\\no ooo\\noooooo\\no\\no\\noooo\\nooooo\\no\\noooooo\\noooooooooooo\\noooo o\\nooooooooooooo\\nooo\\no\\nooo\\noo\\no\\noooooooo\\nooooooo\\nooo\\noo\\noooooooooooo\\nooooooooo\\nooo\\noo\\nooooo\\no\\nooo\\no\\no\\nooo\\no\\noo\\no\\noooooo\\nooooo\\noo\\noo\\no\\noooooo\\nooo\\noo\\nooooooooooo\\no\\noooooo\\no\\nooooooooooo\\noooo\\no\\noo\\n••••••••••••••••••••••\\nCoordinate 9 Coordinate 10 \\n-2 -1 0 1 2 3-2 -1 0 1 2oo\\no\\no\\noooooooo\\nooooo\\no\\nooooooo\\nooooo\\nooooo\\no\\nooooooo\\no\\no\\no\\noo\\noo\\no\\nooo\\noooooooooo\\no\\no\\noooooooooooo o\\noo\\nooo\\nooooooo\\nooooo\\noo\\noooo\\nooooooo\\noo\\no\\nooo\\noooo\\nooooooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 133}),\n",
       " Document(page_content='oooooo\\noo\\no\\noooooo\\no\\noo\\no\\noooooooooooooooooo\\nooo\\nooo\\nooo\\noo\\nooooooo\\no\\noooo\\nooooo\\no\\nooooooooooo\\nooooo\\nooooooooooooo\\no\\noooo\\nooooo\\no\\no\\no\\no\\no\\noooo\\nooooo\\no\\nooo\\no\\noooo\\noo\\no\\noo\\noo\\noooo\\noooo\\no\\nooooooooooo\\no\\nooo\\noo\\no\\nooo\\no\\no oo\\noooooooo\\nooo\\nooooo\\nooooo\\no\\no\\nooo\\noooooooo\\nooo\\noo\\noo\\noooooooooooo\\no\\no\\no\\noo\\nooooooooo\\noooo\\no\\noooooooooo\\nooo\\no\\no\\no\\no\\no\\noo\\nooooooo\\no\\no\\noo\\no\\no\\noooooooooo\\noooo\\nooo\\noooooooo\\nooo\\noooo\\nooo\\no\\nooo\\nooooo\\nooooooooooooo\\nooooo\\no\\no\\nooo\\nooooooo\\nooo\\no\\noo\\noooo\\nooooo\\noo\\no\\no\\no\\noooooooooooooooo\\nooo\\no', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 133}),\n",
       " Document(page_content='o ooooo\\noo\\noo\\no\\no\\no\\no••••••••••••••••••••••Linear Discriminant Analysis\\nFIGURE 4.8. Four projections onto pairs of canonical variates. Notice t hat as\\nthe rank of the canonical variates increases, the centroids become less spread out.\\nIn the lower right panel they appear to be superimposed, and th e classes most\\nconfused.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 133}),\n",
       " Document(page_content='116 4. Linear Methods for Classiﬁcation\\n++\\n++\\nFIGURE 4.9. Although the line joining the centroids deﬁnes the direction o f\\ngreatest centroid spread, the projected data overlap becaus e of the covariance\\n(left panel). The discriminant direction minimizes this over lap for Gaussian data\\n(right panel).\\nFisher’s problem therefore amounts to maximizing the Rayleigh quotient ,\\nmax\\naaTBa\\naTWa, (4.15)\\nor equivalently\\nmax\\naaTBasubject toaTWa= 1. (4.16)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 134}),\n",
       " Document(page_content='This is a generalized eigenvalue problem, with agiven by the largest\\neigenvalue of W−1B. It is not hard to show (Exercise 4.1) that the optimal\\na1is identical to v1deﬁned above. Similarly one can ﬁnd the next direction\\na2, orthogonal in Wtoa1, such that aT\\n2Ba2/aT\\n2Wa2is maximized; the\\nsolution is a2=v2, and so on. The aℓare referred to as discriminant\\ncoordinates , not to be confused with discriminant functions. They are al so', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 134}),\n",
       " Document(page_content='referred to as canonical variates , since an alternative derivation of these\\nresults is through a canonical correlation analysis of the i ndicator response\\nmatrixYon the predictor matrix X. This line is pursued in Section 12.5.\\nTo summarize the developments so far:\\n•Gaussian classiﬁcation with common covariances leads to li near deci-\\nsion boundaries. Classiﬁcation can be achieved by sphering the data\\nwith respect to W, and classifying to the closest centroid (modulo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 134}),\n",
       " Document(page_content='logπk) in the sphered space.\\n•Since only the relative distances to the centroids count, on e can con-\\nﬁne the data to the subspace spanned by the centroids in the sp hered\\nspace.\\n•This subspace can be further decomposed into successively o ptimal\\nsubspaces in term of centroid separation. This decompositi on is iden-\\ntical to the decomposition due to Fisher.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 134}),\n",
       " Document(page_content='4.3 Linear Discriminant Analysis 117\\nDimensionMisclassification Rate\\n2 4 6 8 100.3 0.4 0.5 0.6 0.7LDA and Dimension Reduction on the Vowel Data\\n•\\n••••• • ••••\\n•\\n• •\\n•\\n•••••Test Data\\nTrain Data\\nFIGURE 4.10. Training and test error rates for the vowel data, as a functio n\\nof the dimension of the discriminant subspace. In this case t he best error rate is\\nfor dimension 2. Figure 4.11 shows the decision boundaries in this space.\\nThe reduced subspaces have been motivated as a data reductio n (for', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 135}),\n",
       " Document(page_content='viewing) tool. Can they also be used for classiﬁcation, and w hat is the\\nrationale? Clearly they can, as in our original derivation; we simply limit\\nthe distance-to-centroid calculations to the chosen subsp ace. One can show\\nthat this is a Gaussian classiﬁcation rule with the addition al restriction\\nthat the centroids of the Gaussians lie in a L-dimensional subspace of IRp.\\nFitting such a model by maximum likelihood, and then constru cting the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 135}),\n",
       " Document(page_content='posterior probabilities using Bayes’ theorem amounts to th e classiﬁcation\\nrule described above (Exercise 4.8).\\nGaussian classiﬁcation dictates the log πkcorrection factor in the dis-\\ntance calculation. The reason for this correction can be see n in Figure 4.9.\\nThe misclassiﬁcation rate is based on the area of overlap bet ween the two\\ndensities. If the πkare equal (implicit in that ﬁgure), then the optimal\\ncut-point is midway between the projected means. If the πkare not equal,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 135}),\n",
       " Document(page_content='moving the cut-point toward the smallerclass will improve the error rate.\\nAs mentioned earlier for two classes, one can derive the line ar rule using\\nLDA (or any other method), and then choose the cut-point to mi nimize\\nmisclassiﬁcation error over the training data.\\nAs an example of the beneﬁt of the reduced-rank restriction, we return\\nto the vowel data. There are 11 classes and 10 variables, and h ence 10\\npossible dimensions for the classiﬁer. We can compute the tr aining and', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 135}),\n",
       " Document(page_content='test error in each of these hierarchical subspaces; Figure 4 .10 shows the\\nresults. Figure 4.11 shows the decision boundaries for the c lassiﬁer based\\non the two-dimensional LDA solution.\\nThere is a close connection between Fisher’s reduced rank di scriminant\\nanalysis and regression of an indicator response matrix. It turns out that', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 135}),\n",
       " Document(page_content='118 4. Linear Methods for Classiﬁcation\\noooo\\noo o\\noo\\noo\\no\\noo o\\noo\\noooo\\no\\noo\\nooooo\\noo\\nooo\\no\\nooo\\noooo\\noo\\noooo\\no\\nooo\\no\\noo\\nooo\\no\\no\\no\\nooo\\no\\noo\\noo oo\\noo\\nooo\\no\\noo\\noo\\noo\\no\\noo\\no\\noooo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\no\\no\\no\\noo\\no\\noo\\noo\\no\\no\\nooo\\no\\nooo\\nooo\\noo\\no\\nooo\\noo\\noooo\\noo\\noo\\no\\nooo\\no\\no\\nooo\\no\\nooo\\noo\\noo\\noooo\\noo\\noo\\no\\noo\\no\\nooo\\no\\noooo\\noooo\\noo\\no o\\nooo\\no\\no\\no\\noo\\noooo\\noo\\noo\\no\\noo\\nooo\\no\\noo\\noo\\noo\\no oo\\nooo\\no\\noo\\noo\\noooo\\noo\\noo\\noo\\no\\noo\\no\\noo\\noooo\\no\\nooo\\noooooo\\nooo\\noo\\noo\\nooooo\\nooo\\no\\nooo\\no oo\\noo\\nooo\\no\\noooo\\noo\\no\\nooo\\no\\nooo\\nooooo\\no\\noo\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 136}),\n",
       " Document(page_content='o oo\\noo\\noo\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\noooo\\no\\no\\noooo\\no\\noo\\noo\\no\\noo\\noo\\no\\noo\\nooo\\noooo\\noo\\noo\\nooo\\noo\\noo\\no\\noo\\noo\\nooo\\no\\noo\\noo\\no\\noooo\\noooo\\no\\noo\\noo\\no\\nooo\\nooooo\\nooo\\noo\\noo\\noo\\noo\\no\\no\\no\\no\\no\\noo\\noo\\no\\noo\\nooo\\no\\no o\\noo\\no o\\noo\\no\\noo\\no\\nooo\\noo\\nooo\\no\\noo\\no\\noo o\\nooo\\noo\\noo\\no\\noo\\noo\\noo\\noo\\nooo\\nooo\\no\\noo\\no\\noo\\noo\\noo\\noo\\no\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\nooo\\noo\\noo\\no\\noo\\no\\no\\no\\nCanonical Coordinate 1Canonical Coordinate 2Classification in Reduced Subspace\\n••••••••••••••\\n••\\n••\\n••••', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 136}),\n",
       " Document(page_content='FIGURE 4.11. Decision boundaries for the vowel training data, in the two- di-\\nmensional subspace spanned by the ﬁrst two canonical variat es. Note that in\\nany higher-dimensional subspace, the decision boundaries are higher-dimensional\\naﬃne planes, and could not be represented as lines.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 136}),\n",
       " Document(page_content='4.4 Logistic Regression 119\\nLDA amounts to the regression followed by an eigen-decompos ition of\\nˆYTY. In the case of two classes, there is a single discriminant va riable\\nthat is identical up to a scalar multiplication to either of t he columns of ˆY.\\nThese connections are developed in Chapter 12. A related fac t is that if one\\ntransforms the original predictors XtoˆY, then LDA using ˆYis identical\\nto LDA in the original space (Exercise 4.3).\\n4.4 Logistic Regression', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 137}),\n",
       " Document(page_content='The logistic regression model arises from the desire to mode l the posterior\\nprobabilities of the Kclasses via linear functions in x, while at the same\\ntime ensuring that they sum to one and remain in [0 ,1]. The model has\\nthe form\\nlogPr(G= 1|X=x)\\nPr(G=K|X=x)=β10+βT\\n1x\\nlogPr(G= 2|X=x)\\nPr(G=K|X=x)=β20+βT\\n2x\\n...\\nlogPr(G=K−1|X=x)\\nPr(G=K|X=x)=β(K−1)0+βT\\nK−1x.(4.17)\\nThe model is speciﬁed in terms of K−1 log-odds or logit transformations', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 137}),\n",
       " Document(page_content='(reﬂecting the constraint that the probabilities sum to one ). Although the\\nmodel uses the last class as the denominator in the odds-rati os, the choice\\nof denominator is arbitrary in that the estimates are equiva riant under this\\nchoice. A simple calculation shows that\\nPr(G=k|X=x) =exp(βk0+βT\\nkx)\\n1+∑K−1\\nℓ=1exp(βℓ0+βT\\nℓx), k= 1,...,K−1,\\nPr(G=K|X=x) =1\\n1+∑K−1\\nℓ=1exp(βℓ0+βT\\nℓx), (4.18)\\nand they clearly sum toone. Toemphasize the dependenceon th eentire pa-\\nrameter set θ={β10,βT\\n1,...,β (K−1)0,βT', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 137}),\n",
       " Document(page_content='K−1}, we denote the probabilities\\nPr(G=k|X=x) =pk(x;θ).\\nWhenK= 2, this model is especially simple, since there is only a sin gle\\nlinear function. It is widely used in biostatistical applic ations where binary\\nresponses(twoclasses)occurquitefrequently.Forexampl e,patientssurvive\\nor die, have heart disease or not, or a condition is present or absent.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 137}),\n",
       " Document(page_content='120 4. Linear Methods for Classiﬁcation\\n4.4.1 Fitting Logistic Regression Models\\nLogistic regression models are usually ﬁt by maximum likeli hood, using the\\nconditional likelihood of GgivenX. Since Pr(G|X) completely speciﬁes the\\nconditional distribution, the multinomial distribution is appropriate. The\\nlog-likelihood for Nobservations is\\nℓ(θ) =N∑\\ni=1logpgi(xi;θ), (4.19)\\nwherepk(xi;θ) = Pr(G=k|X=xi;θ).\\nWe discuss in detail the two-class case, since the algorithm s simplify', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 138}),\n",
       " Document(page_content='considerably. It is convenient to code the two-class givia a 0/1 responseyi,\\nwhereyi= 1 whengi= 1, andyi= 0 whengi= 2. Letp1(x;θ) =p(x;θ),\\nandp2(x;θ) = 1−p(x;θ). The log-likelihood can be written\\nℓ(β) =N∑\\ni=1{\\nyilogp(xi;β)+(1−yi)log(1−p(xi;β))}\\n=N∑\\ni=1{\\nyiβTxi−log(1+eβTxi)}\\n. (4.20)\\nHereβ={β10,β1}, and we assume that the vector of inputs xiincludes\\nthe constant term 1 to accommodate the intercept.\\nTo maximize the log-likelihood, we set its derivatives to ze ro. These score\\nequations are\\n∂ℓ(β)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 138}),\n",
       " Document(page_content='∂β=N∑\\ni=1xi(yi−p(xi;β)) = 0, (4.21)\\nwhich arep+1 equations nonlinear inβ. Notice that since the ﬁrst compo-\\nnentofxiis1,theﬁrstscoreequationspeciﬁesthat∑N\\ni=1yi=∑N\\ni=1p(xi;β);\\ntheexpected number of class ones matches the observed number (and hence\\nalso class twos.)\\nTo solve the score equations (4.21), we use the Newton–Raphs on algo-\\nrithm, which requires the second-derivative or Hessian mat rix\\n∂2ℓ(β)\\n∂β∂βT=−N∑\\ni=1xixiTp(xi;β)(1−p(xi;β)). (4.22)\\nStarting with βold, a single Newton update is', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 138}),\n",
       " Document(page_content='βnew=βold−(∂2ℓ(β)\\n∂β∂βT)−1∂ℓ(β)\\n∂β, (4.23)\\nwhere the derivatives are evaluated at βold.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 138}),\n",
       " Document(page_content='4.4 Logistic Regression 121\\nIt is convenient to write the score and Hessian in matrix nota tion. Let\\nydenote the vector of yivalues,XtheN×(p+ 1) matrix of xivalues,\\npthe vector of ﬁtted probabilities with ith element p(xi;βold) andWa\\nN×Ndiagonal matrix of weights with ith diagonal element p(xi;βold)(1−\\np(xi;βold)). Then we have\\n∂ℓ(β)\\n∂β=XT(y−p) (4.24)\\n∂2ℓ(β)\\n∂β∂βT=−XTWX (4.25)\\nThe Newton step is thus\\nβnew=βold+(XTWX)−1XT(y−p)\\n= (XTWX)−1XTW(\\nXβold+W−1(y−p))\\n= (XTWX)−1XTWz. (4.26)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 139}),\n",
       " Document(page_content='In the second and third line we have re-expressed the Newton s tep as a\\nweighted least squares step, with the response\\nz=Xβold+W−1(y−p), (4.27)\\nsometimes known as the adjusted response . These equations get solved re-\\npeatedly, since at each iteration pchanges, and hence so does Wandz.\\nThis algorithm is referred to as iteratively reweighted least squares or IRLS,\\nsince each iteration solves the weighted least squares prob lem:\\nβnew←argmin\\nβ(z−Xβ)TW(z−Xβ). (4.28)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 139}),\n",
       " Document(page_content='It seems that β= 0 is a good starting value for the iterative procedure,\\nalthough convergence is never guaranteed. Typically the al gorithm does\\nconverge, since the log-likelihood is concave, but oversho oting can occur.\\nIn the rare cases that the log-likelihood decreases, step si ze halving will\\nguarantee convergence.\\nFor the multiclass case ( K≥3) the Newton algorithm can also be ex-\\npressed as an iteratively reweighted least squares algorit hm, but with a', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 139}),\n",
       " Document(page_content='vectorofK−1 responses and a nondiagonal weight matrix per observation .\\nThe latter precludes any simpliﬁed algorithms, and in this c ase it is numer-\\nically more convenient to work with the expanded vector θdirectly (Ex-\\nercise 4.4). Alternatively coordinate-descent methods (S ection 3.8.6) can\\nbe used to maximize the log-likelihood eﬃciently. The Rpackageglmnet\\n(Friedman et al., 2010) can ﬁt very large logistic regressio n problems ef-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 139}),\n",
       " Document(page_content='ﬁciently, both in Nandp. Although designed to ﬁt regularized models,\\noptions allow for unregularized ﬁts.\\nLogistic regression models are used mostly as a data analysi s and infer-\\nence tool, where the goal is to understand the role of the inpu t variables', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 139}),\n",
       " Document(page_content='122 4. Linear Methods for Classiﬁcation\\nTABLE 4.2. Results from a logistic regression ﬁt to the South African hear t\\ndisease data.\\nCoeﬃcient Std. Error ZScore\\n(Intercept) −4.130 0 .964−4.285\\nsbp 0.006 0 .006 1.023\\ntobacco 0.080 0 .026 3.034\\nldl 0.185 0 .057 3.219\\nfamhist 0.939 0 .225 4.178\\nobesity -0.035 0 .029−1.187\\nalcohol 0.001 0 .004 0.136\\nage 0.043 0 .010 4.184\\ninexplaining the outcome. Typically many models are ﬁt in a search for a', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 140}),\n",
       " Document(page_content='parsimonious model involving a subset of the variables, pos sibly with some\\ninteractions terms. The following example illustrates som e of the issues\\ninvolved.\\n4.4.2 Example: South African Heart Disease\\nHere we present an analysis of binary data to illustrate the t raditional\\nstatistical use of the logistic regression model. The data i n Figure 4.12 are a\\nsubset of the Coronary Risk-Factor Study (CORIS) baseline s urvey, carried', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 140}),\n",
       " Document(page_content='out in three rural areas of the Western Cape, South Africa (Ro usseauw et\\nal., 1983). The aim of the study was to establish the intensit y of ischemic\\nheart disease risk factors in that high-incidence region. T he data represent\\nwhite males between 15 and 64, and the response variable is th e presence or\\nabsence of myocardial infarction (MI) at the time of the surv ey (the overall\\nprevalence of MI was 5.1% in this region). There are 160 cases in our data', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 140}),\n",
       " Document(page_content='set, and a sample of 302 controls. These data are described in more detail\\nin Hastie and Tibshirani (1987).\\nWe ﬁt a logistic-regression model by maximum likelihood, gi ving the\\nresults shown in Table 4.2. This summary includes Zscores for each of the\\ncoeﬃcients in the model (coeﬃcients divided by their standa rd errors); a\\nnonsigniﬁcant Zscoresuggestsacoeﬃcientcanbedroppedfromthemodel.\\nEach of these correspond formally to a test of the null hypoth esis that the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 140}),\n",
       " Document(page_content='coeﬃcient in question is zero, while all the others are not (a lso known as\\nthe Wald test). A Zscore greater than approximately 2 in absolute value\\nis signiﬁcant at the 5% level.\\nThere are some surprises in this table of coeﬃcients, which m ust be in-\\nterpreted with caution. Systolic blood pressure ( sbp) is not signiﬁcant! Nor\\nisobesity, and its sign is negative. This confusion is a result of the co rre-\\nlation between the set of predictors. On their own, both sbpandobesity', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 140}),\n",
       " Document(page_content='are signiﬁcant, and with positive sign. However, in the pres ence of many', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 140}),\n",
       " Document(page_content='4.4 Logistic Regression 123\\nsbp0 10 20 30\\no\\no\\noo\\nooo\\noooo\\no\\noo\\nooooo\\no\\nooo\\noo\\nooooooo\\nooo\\noo\\nooo\\noo\\noo\\nooo\\noooooo\\nooo\\noo\\nooooooooo\\noooooooooooo\\nooooo\\nooooooooo\\noo\\nooooo\\noo\\noooo\\noo\\noooo\\noo\\nooooo\\noooooo\\noooooo\\noooo\\noooooo\\nooo\\noo\\nooo\\noo\\noo\\nooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noo\\noo\\no\\nooo\\noooo\\no\\noooooooo\\nooooooo\\noooooooooo\\nooo\\noo\\noooo\\noooo\\noooooooo\\nooooo\\no\\nooooo\\noo\\noo\\nooo\\noooo\\nooooo\\nooo\\noooooo\\nooo\\nooo\\nooo\\noo\\nooo\\noooooo\\noooooo\\nooo\\noooo\\noooo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\nooooo\\nooo\\noooo\\no oo\\no\\nooo\\nooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooooooo\\nooooo\\noo\\noooo\\nooooooooo\\noooooo\\noooo\\noooo\\noooo\\noooooo\\no\\nooo\\noooooo\\no\\nooo\\noooo\\noo\\no\\nooo\\noooooooooooo\\nooo\\noooo\\noooo\\noo\\noo\\nooo\\no\\noooo\\no\\noo\\nooo\\noooo\\no\\noo\\noo ooo\\no\\nooooo\\nooooooo\\nooo\\noo\\nooo\\noo\\noo\\nooo\\noooooo\\nooo\\noo\\nooooo\\nooo\\nooooooooooooo\\nooooo\\nooooooooo\\noo\\nooooo\\noo\\noooo\\noo\\noooo\\noo\\nooooo\\noooooo\\noooooo\\noooooooooo\\nooo\\noo\\nooo\\noo\\noo\\nooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noo\\noo\\no\\nooo\\nooooo\\noooooooo\\nooooooo\\noooooooooo\\nooo\\noo\\noooo\\noooo\\noooooooo\\nooooo\\no\\nooooo\\noo\\noo\\nooo\\noooo\\noooooo oo\\noooooo\\nooo\\noooooo\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oooo\\nooooo\\noooooo\\nooo\\noooo\\noooo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\nooooo\\nooo\\noooo\\nooo\\no\\nooo\\nooo\\noooo ooo\\nooooo\\noo\\noooo\\noooo\\nooooo\\noooooo\\noooo\\noooo\\noooo\\noooooo\\no\\nooo\\noooo oo\\noooo\\noooo\\noo\\noooo\\nooooo\\nooooooo\\nooo\\noooooooo\\noo\\noo\\nooo\\no\\nooo0.0 0.4 0.8\\no\\no\\noo\\nooo\\noooo\\no\\noo\\noo ooo\\no\\nooo\\noo\\nooo o ooo\\nooo\\noo\\nooo\\noo\\noo\\nooo\\nooo ooo\\nooo\\noo\\nooo oo\\noooo\\noooooo o ooo oo\\noooo o\\no ooo\\no oooo\\noo\\no oo\\noo\\noo\\noooo\\noo\\noooo\\noo\\nooooo\\noooooo\\noo\\no ooo\\noooooooo oo\\no oo\\noo\\noo ooo\\noo\\nooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noo\\nooo\\nooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oooo\\no\\nooo o\\noo oo\\noo ooooo\\noo o ooooooo\\nooo\\noo\\noo oo\\noo oo\\noooooooo\\nooooo\\no\\no ooo o\\noo\\noo\\nooo\\noo oo\\noooooooo\\noooo oo\\nooo\\nooo\\nooo\\noo\\nooooooooo\\noooooo\\no oo\\noooo\\noo\\noo\\noooo\\no\\nooo\\noooo\\no oo\\noo\\nooo\\nooooo\\nooo\\noo oo\\nooo\\no\\no oo\\nooo\\nooo oo oo\\no oooo\\noo\\nooo o\\no o oo\\no\\no o oo\\nooo\\nooo\\no ooo\\noooo\\noooo\\no oo ooo\\no\\nooo\\noooooo\\noooo\\no ooo\\noo\\noooo\\nooooo\\nooo o ooo\\nooo\\noo o ooooo\\noo\\noo\\nooo\\no\\noooo\\no\\noo\\nooo\\noooo\\no\\noo\\noo ooo\\no\\nooooo\\nooooooo\\nooo\\noo\\nooo\\noo\\noo\\nooo\\noooooo\\nooo\\noo\\nooooooooooooooooooooo\\nooooo\\nooooooooo\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooo\\noo\\noo\\noooo\\noo\\noooo\\noo\\nooooo\\noooooo\\noo\\noooo\\noooo\\noooooo\\nooo\\noo\\nooooo\\nooooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noo\\noo\\no\\nooo\\noooo\\no\\noooooooo\\nooooooo\\noooo\\noooooo\\nooo\\noo\\noooo\\noooo\\noooooooo\\nooooo\\no\\nooooo\\noo\\noo\\nooo\\noooo\\noooooooo\\noooooo\\nooo\\noooooo\\noo\\nooo\\noooooo\\noooooo\\nooo\\noooo\\noooo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\nooooo\\nooo\\noooo\\nooo\\no\\nooo\\nooo\\nooooooo\\nooooo\\noo\\noooo\\noooo\\nooooo\\noooooo\\noooo\\noooo\\noooo\\noooooo\\no\\nooo\\noooooo\\noooo\\noooo\\noo\\no\\nooo\\nooooo\\nooooooo\\nooo\\noooooooo\\noo\\noo\\nooo\\no\\nooo0 50 100\\no\\no\\noo\\nooo\\noooo\\no\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooooo\\no\\nooooo\\nooooooo\\nooo\\noo\\nooo\\noo\\noo\\nooo\\noooooo\\nooo\\noo\\noooooooo\\nooooooooooooo\\nooooo\\noooo\\nooooo\\noo\\nooooo\\noo\\noooo\\noo\\noo\\noo\\noo\\nooooo\\nooo ooo\\noo\\noooo\\noooooooooo\\nooo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noooo\\no\\nooo\\nooooo\\noooooooo\\nooooooo\\noooooooooo\\nooo\\noo\\noooo\\noooo\\noooooooo\\nooo\\noo\\no\\nooooo\\noo\\noo\\nooo\\noooo\\noooooooo\\noooooo\\nooo\\noooooo\\noo\\noooo\\nooooo\\noooooo\\nooo\\noooo\\noo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\nooooo\\nooo\\noooo\\nooo\\no\\nooo\\nooo\\nooooooo\\nooooo\\noo\\noooooooo\\nooooo\\noooooo\\noooo\\noooo\\noooo\\noooooo\\no\\nooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oooooo\\noooo\\noooo\\noo\\noooo\\nooooo\\nooooooo\\nooo\\noooooooo\\noo\\noo\\nooo\\no\\nooo\\n100 160 220o\\no\\noo\\nooo\\no ooo\\no\\noo\\nooooo\\no\\nooooo\\nooooooo\\nooo\\noo\\nooo\\noo\\noo\\nooo\\noooooo\\nooo\\noo\\nooooooo o\\noooooooooo ooo\\nooooo\\noooo\\nooooo\\noo\\nooo\\noo\\noo\\noooo\\noo\\noooo\\noo\\nooooo\\no ooooo\\noooooo\\noooo\\noooo oo\\nooo\\noo\\nooooo\\nooooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noo\\noo\\no\\nooo\\nooooo\\nooooooo\\no\\nooo oooo\\noo ooooo ooo\\nooo\\noo\\noooo\\noooo\\noooooooo\\nooo\\noo\\no\\nooooo\\noo\\noo\\nooo\\noooo\\noo ooo\\nooo\\noooooo\\nooo\\noooooo\\noo\\noooooo\\nooo\\noooo oo\\nooo\\noooo\\noo\\noo\\noooo\\no\\nooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oooo\\nooo\\no o\\nooo\\nooooo\\nooo\\noooo\\nooo\\no\\nooo\\nooo\\nooo oooo\\nooooo\\noooooo\\noo oo\\nooooo\\nooo\\nooo\\noooo\\noooo\\noooo\\noooooo\\no\\nooo\\nooo\\nooo\\noooo\\noooo\\noo\\noooo\\nooooo\\nooooooo\\nooo\\noooooooo\\noo\\noo\\nooo\\no\\nooo0 10 20 30o\\noooo\\nooooooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\no\\nooooo\\noooo\\noo\\nooooooo\\nooo\\noooo\\noo\\noo\\nooooo\\noooo\\noo\\noooo\\noooooo\\nooo\\noo\\nooo\\nooooooooooooo\\no\\nooooooo\\noooo\\nooo\\noo\\nooooooooo\\nooooo\\noooooooo oo\\nooooooooo\\no o\\noooo\\noo\\noo\\noo\\nooooo\\nooooooooooooo\\nooooooo\\noo\\no oooooooooooooooo\\noooo\\noooooo\\noooo\\noooooooo\\noooo\\noooo\\noo\\nooooo\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oooooo\\nooooo\\nooo\\noooooo\\nooo\\noo\\noooo\\noo\\noo\\nooooooo\\noooooooooooooo\\no\\nooooooo\\no\\noooo\\nooo\\noooo\\no\\noooo\\nooo oo\\nooooo\\noooo\\nooooo\\nooooo\\nooo\\nooo\\nooooooo\\noo\\noooooo\\nooo\\noo\\noooooooo\\nooo\\no\\noooo\\nooooo\\noooo\\noo\\noo o\\nooo\\no\\no\\noo\\noo\\noo\\noo\\noooo\\noo\\no\\nooooo\\nooo oooooooo\\nooo\\noooooooooooooooo oo\\notobaccoo\\noooo\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noooooo\\noooo\\noo\\nooooooo\\nooo\\noooo\\noo\\noo\\nooooo\\noooo\\noo\\noooo\\noooooo\\nooooo\\nooo\\nooo oooooooooo\\no\\nooooooo\\noooo\\nooo\\noo\\noo\\nooooooo\\nooooo\\noooooooo oo\\nooo\\noooooo\\noo\\noooo\\noo\\noo\\noo\\nooooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooooooooooooo\\nooooooo\\noo\\nooooo oooooooooooo\\noooo\\noooooo\\noooo\\noooooooo\\noooo\\noooo\\noo\\nooooo\\noo\\nooo\\nooo\\nooooo\\nooo\\noooooo\\nooo\\noo\\noooooo\\noo\\noooo\\nooo\\nooooooo ooooooo\\no\\nooooooo\\no\\noooo\\nooo\\noooo\\no\\noooo\\nooooo\\nooooo\\noooo\\nooooo\\noo ooo\\nooo\\nooo\\nooooooo\\noo\\nooooooooo\\noo\\noooo oo\\noo\\nooo\\no\\noo\\noo\\nooooo\\noooo\\noo\\nooo\\nooo\\no\\no\\noo\\noo\\noo\\noo\\noooo\\noo\\no\\nooooo\\nooooooooooo\\no oo\\noooooooooo\\noooooooo\\noo\\no ooo\\noo o\\noooo\\nooo\\nooo\\noo\\noo\\no oo\\nooo\\no\\nooooooooo\\noo\\no o ooooo\\nooo\\noooo\\noo\\noo\\no o o oo\\noooo\\noo\\noooo\\no ooo oo\\nooooo\\no oo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oo ooooooo oooo\\no\\no o ooooo\\noooo\\nooo\\noooo\\noooooo o\\no oooo\\nooooooo o oo\\no oooo oooo\\noo\\noooo\\noo\\noo\\noo\\nooooo\\noooooo o oooooo\\no oooo\\noo\\noo\\no o oo oo o oo oooo ooo ooooo\\noooooo\\noooo\\no ooo oo oo\\noooo\\noooo\\no o\\nooo oooo\\no ooooo\\nooooo\\nooo\\noo oooo\\nooo\\noo\\noo oo\\noo\\noo\\nooooooo\\noooooooooooooo\\no\\nooo o o oo\\no\\noooo\\nooo\\noooo\\no\\noo oo\\noo o oo\\nooooo\\nooo o\\nooooo\\noo oo o\\nooo\\nooo\\nooooooo\\noooo oo oo\\nooo\\noo\\no ooo oooo\\no oo\\no\\noo\\noo\\nooooo\\noooo\\noo\\nooo\\nooo\\no\\nooo\\noo\\noo\\noo\\noooo\\noo\\noooo oo\\nooooo o o o ooo\\no oo\\noo oo oooooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='o ooo oooo\\noo\\noooo\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\no\\nooooooooo\\noo\\nooooooo\\nooo\\noooo\\noo\\noo\\nooooo\\noooo\\noo\\noooo\\noooooo\\nooo\\noo\\nooo\\nooooo\\noooooooo\\no\\nooooooo\\noooo\\nooo\\noo\\nooooooooo\\nooooo\\noooooooooo\\nooooooooo\\noo\\noooo\\noo\\noo\\noo\\nooooo\\nooooooooooooo\\nooooo\\noo\\noo\\nooooooooooooooooo\\noooo\\noooooo\\noooo\\noooooooo\\noooo\\noooo\\noo\\nooooooo\\noooooo\\nooooo\\nooo\\nooo ooo\\nooo\\noo\\noooooo\\noo\\nooooooo\\noooooooooooooo\\no\\nooooooo\\no\\noooo\\nooooooo\\no\\noooo\\nooo oo\\nooooo\\noooo\\nooooo\\nooooo\\nooo\\nooo\\nooooooo\\noooooooo\\nooo\\noo\\noooooooo\\nooo\\no\\noo\\noo\\nooooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oooo\\noo\\nooo\\nooo\\no\\no\\noo\\noo\\noo\\noo\\noooo\\noo\\no\\nooooo\\nooooooo oooo\\nooo\\noooooooooo\\noooo oooo\\noo\\noooo\\nooo\\noooo\\no oo\\nooo\\noo\\noo\\nooo\\nooo\\no\\nooooooooo\\noo\\nooooooo\\nooooooo\\noo\\noo\\nooooo\\noooo\\noo\\noooo\\nooo ooo\\nooooo\\nooo\\nooooooooooooo\\no\\nooooooo\\noooo\\nooo\\noo\\nooooooooo\\nooooo\\nooooooo ooo\\nooo\\noooooo\\noo\\noooo\\noo\\noo\\noo\\no oooo\\nooooooooooooo\\nooooo\\noo\\noo\\noooooooooooooooo o\\noooo\\noooooo\\noooo\\noooooooo\\noooo\\noo oo\\noo\\nooooooo\\nooooo o\\nooooo\\nooo\\noooooo\\nooo\\noo\\noooo\\noo\\noo\\noooo\\nooo\\noooooooooooooo\\noooooooo\\no\\noooo\\nooo\\noooo\\no\\noooo\\nooooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooooo\\noooo\\nooooo\\nooooo\\nooo\\nooo\\nooooooo\\nooooooooooo\\noo\\no ooo oo\\noo\\nooo\\no\\noo\\noo\\nooooo\\noooo\\noo\\nooo\\nooo\\no\\no\\noo\\noo\\noo\\noo\\noooo\\noo\\no\\nooooo\\no ooooooo ooo\\nooo\\noooooooooooooooooo\\noo\\noooo\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\no\\nooooo\\noooo\\noo\\nooooooo\\nooo\\noooo\\noo\\noo\\no o ooo\\noooo\\noo\\noooo\\noo oooo\\nooooo\\no oo\\nooo oooooo oooo\\no\\nooooooo\\noooo\\nooo\\noo\\noooo ooooo\\nooooo\\no ooooooo oo\\nooo\\noooooo\\noo\\noooo\\noo\\noo\\noo\\nooooo\\nooooooooooooo\\nooooo\\noo\\noo\\nooooooooooooooooooooo\\noooo oo\\noooo\\no ooooooo\\noooo\\noooo\\noo\\nooooo\\noo\\noooooo\\nooooo\\nooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooo ooo\\nooo\\noo\\noooo\\noo\\noo\\noooo\\nooo\\nooooooooo\\nooooo\\nooooo ooo\\no\\noooo\\nooo\\noooo\\nooooo\\nooo oo\\no oooo\\noooo\\nooooo\\nooooo\\nooo\\nooo\\nooooooo\\nooooooooooo\\noo\\noooooooo\\nooo\\nooo\\noo\\nooooo\\noooo\\noo\\nooo\\nooo\\no\\no\\noo\\noo\\noo\\noo\\noooo\\noo\\no\\nooooo\\noooooooooooooo\\noooooooooo\\nooo o oooo\\no\\noooo\\noo\\noooo\\noo\\nooooo\\nooo\\nooo\\nooo\\no\\nooo\\nooooo\\noo\\no\\nooo\\no\\noooooo\\nooo\\noo\\nooo\\noooo\\noooo\\nooooo\\noooooo\\noooooo\\noo\\no\\nooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noooo\\nooo\\noo\\noo\\nooo\\noooo\\nooo\\nooo\\noo\\nooo\\noo\\nooooooooo\\noooooo\\noooo\\nooo\\nooo\\nooooooo\\nooo\\noo\\nooo\\nooo\\noooo\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooooo\\noooo\\nooooo\\no\\nooooooooo\\noo\\no\\no\\nooo\\noooo\\noooo\\nooo\\nooo\\noo\\nooooooo\\nooooo\\no\\nooooooo\\noo\\nooo\\nooo\\nooooo\\nooooo\\noooo\\noooo\\noooooooooooo\\noo\\noo\\noooooooo\\noo\\nooooo oo\\noooo\\noooooooooooo\\nooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noooooo\\noo\\noooooo\\noooooooooo\\nooo\\noooooo\\nooo\\nooo\\noo\\nooooooooo\\noo\\noooooo\\no\\nooooo\\noo\\noo\\noo\\noooooooooo\\noooo\\nooooo ooooo\\noo\\noooo\\nooo\\noooo\\no\\noooo\\noo\\noo\\nooo\\noo\\noooo\\noo\\nooooo\\nooo\\nooo\\no oo\\no\\nooo\\nooooo\\noo\\no\\nooo\\no\\noooooo\\nooo\\noo\\nooo\\noooo\\noooo\\nooooo\\noooooo\\noooooo\\noo\\noooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noooo\\nooo\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oo\\nooo\\noooo\\nooo\\nooo\\nooooo\\noo\\nooooooooo\\noooooo\\noooo\\nooo\\nooo\\no oooooo\\nooo\\noo\\nooo\\nooo\\nooo\\no\\noo\\nooooo\\noo\\noo\\nooooo\\no\\nooo\\noooooo\\noo\\no\\no\\nooo\\noooo\\noooo\\nooo\\nooo\\nooo\\noooooo\\nooooo\\no\\noooo\\noo\\no\\noo\\nooo\\nooo\\nooooo\\nooooo\\noooo\\noooo\\noooooooo oooo\\noo\\noo\\noooooooo\\noo\\nooooooo\\noooo\\noooooooooooo\\nooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\nooo\\nooo\\noo\\noooooo\\noooooo\\noooo\\nooo\\noooooo\\nooo\\nooo\\noo\\no\\noooooooo\\noo\\noooooo\\no\\nooooo\\noo\\noo\\noo\\noooo oooooo\\noooo\\noooo\\noooooo\\noo\\no\\nooo\\nooo\\noooo\\no\\nooo\\no\\noo\\noldl\\noo oo\\noo\\noooo\\noo\\noo o oo\\nooo\\nooo\\nooo\\no\\no oo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oo ooo\\noo\\noooo\\no\\noo oooo\\nooo\\noo\\no oo\\noooo\\nooo o\\no oo\\noo\\noooooo\\noo oo oo\\noo\\noooo oo\\nooo\\nooo\\noooo\\noo\\noo oo\\nooo o\\noo o\\noo\\noo\\nooo\\noooo\\nooo\\nooo\\noo\\nooo\\noo\\no o ooooooo\\nooo ooo\\nooooo ooooo\\nooooooo\\nooo\\noo\\no oo\\no oo\\nooo\\no\\noo\\nooooo\\noooo\\nooooo\\no\\nooo\\no oo ooo\\noo\\no\\no\\nooo\\noooo\\noooo\\nooo\\nooo\\noo\\no\\noooooo\\noo ooo\\no\\noooooo\\no\\noo\\nooo\\nooo\\no oooo\\nooooo\\no ooo\\noooo\\noooooooooooo\\noo\\noo\\nooooo o oo\\noo\\no oo\\noo oo\\no ooo\\nooooo ooooooo\\nooo\\no oo\\nooooo\\no\\noo\\noo\\nooo\\no\\nooo\\nooo\\noo\\noooo\\noo\\noooooo\\noooo\\no o o\\nooo ooo\\nooo\\nooo\\noo\\no', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oooooooo\\noo\\nooo ooo\\no\\nooooo\\noo\\noo\\noo\\noo oooooooo\\noo oo\\noooo\\no ooooo\\noo\\noooo\\nooo\\noo oo\\no\\nooo\\no\\noo\\nooooo\\noo\\noooo\\noo\\nooooo\\nooo\\nooo\\nooo\\no\\nooo\\nooooo\\noo\\no\\nooo\\no\\noo o ooo\\nooo\\noo\\nooo\\noooo\\noooo\\nooooo\\noooooo\\noooooo\\noo\\nooooooooo\\nooo\\noooo\\noo\\noooo\\noooo\\nooo\\noo\\noo\\nooo\\noooo\\nooo\\nooo\\noo\\nooo\\noo\\noo ooooooo\\noooooo\\noo\\noo\\nooo\\nooo\\nooooooo\\nooo\\noo\\nooo\\nooo\\noooo\\noo\\nooooo\\noooo\\nooo oo\\no\\nooooooooooo\\no\\no\\nooo\\noooo\\noooo\\nooo\\nooo\\nooooooooo\\nooooo\\no\\noooooo\\no\\noo\\nooo\\nooo\\nooooo\\nooooo\\noooo\\noooo\\noooooooooooo\\noo\\noo\\noooooooo\\noo\\nooooooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oooo\\noooooooooooo\\nooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noooooo\\noo\\noooooo\\noooooooooo\\nooo\\noooooo\\nooo\\nooo\\noo\\no\\noooooooo\\noo\\noooooo\\no\\nooooo\\noo\\noo\\noo\\noooooooooo\\nooo o\\noooooooooo\\noo\\noooo\\nooo\\noooo\\no\\nooo\\no\\noo\\nooooo\\noo\\noooo\\noo\\nooo oo\\nooo\\nooo\\nooo\\no\\nooo\\nooooo\\noo\\noooo\\no\\noooooo\\nooo\\noo\\nooo\\noooo\\noooo\\nooooo\\noooooo\\noooooo\\noo\\nooooooo\\noo\\nooo\\noooo\\noo\\noooo\\noooo\\nooo\\noo\\noo\\nooo\\noooo\\nooo\\nooo\\noo\\nooo\\noo\\nooooooooo\\noooooo\\noooo\\no oo\\nooo\\nooooooo\\nooo\\noo\\nooo\\nooo\\nooo\\no\\noo\\nooooo\\noooo\\nooooo\\no\\nooo\\noooooo\\noo\\no\\no\\nooo\\noo\\noo\\noooo\\nooo\\nooo\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oo ooooo\\nooooo\\no\\noooooo\\no\\noo\\nooo\\nooo\\nooooo\\nooooo\\noooo\\noooo\\noooooooooooo\\noo\\noo\\noooooooo\\noo\\nooooooo\\noooo\\noooooooooooo\\no oo\\nooo\\noo\\nooo\\no\\nooo\\no\\nooo\\no\\nooo\\nooo\\noo\\noooo\\noo\\noooooo\\nooooo oo\\noooooo\\nooo\\nooo\\noo\\no\\noooooooo\\noo\\noooooo\\no\\nooooo\\noo\\noo\\noo\\noooo oooooo\\noooo\\noooo\\noooooo\\noo\\noooo\\nooooooo\\no\\noooo\\noo\\no\\n2 6 10 14oooo\\noo\\noooo\\noo\\nooooo\\nooo\\nooo\\nooo\\no\\nooo\\nooooo\\noo\\no\\nooo\\no\\noo o ooo\\nooo\\noo\\nooo\\noooo\\noooo\\no oooo\\noooooo\\noooooo\\noo\\noooooo\\noooooo\\no ooo\\noo\\noooo\\noooo\\nooo\\noo\\noo\\nooo\\noo oo\\nooo\\nooo\\noo\\no oo\\noo\\nooo ooooo o', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oooooo\\no oooo oo\\nooo\\no ooo ooo\\nooo\\noo\\no oo\\nooo\\noooo\\noo\\nooooo\\noo\\noo\\nooooo\\no\\nooooooooo\\noo\\no\\no\\nooo\\noooo\\noooo\\nooo\\nooo\\noo\\nooooooo\\no oooo\\no\\noooooo\\no\\noo\\nooo\\nooo\\noo ooo\\nooooo\\noooo\\noooo\\noooooooooooo\\noo\\noo\\noooooooo\\noo\\nooo\\noooo\\noooo\\nooo ooooooooo\\nooo\\nooo\\nooooo\\no\\noo\\noo\\nooo\\no\\nooo\\no oo\\noo\\noooo\\noo\\noooooooooo\\noo o\\noooooo\\nooo\\nooo\\noo\\no\\noooooooo\\noo\\noooooo\\no\\nooooo\\noo\\noo\\noo\\noooooooooo\\noooo\\noooo\\noooooo\\noo\\noooo\\nooooooo\\no\\noooo\\noo\\no0.0 0.4 0.8o\\nooooo\\noooo\\noo\\noooo\\noooo\\nooo\\no oo\\nooo\\noooo\\noooooooo\\nooo\\noooo\\nooo\\nooo\\noooooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooo\\noooo\\noo\\nooooo\\noooo\\nooo\\noo\\noo\\noooo\\nooooo ooooo\\noo\\noo\\nooooo\\noooo\\noo\\noo\\nooo\\nooooooo\\noo\\noo\\noooooo\\noo\\noo\\noo\\noooooo\\noooooooo\\nooooo o ooooooooo\\noooooo\\nooooooooo\\noooo\\nooo\\nooo\\nooo\\nooo\\nooo\\nooo\\noo\\noo\\nooooooo o oo\\noo\\nooo\\nooo o\\nooooooo\\noooo\\nooo\\nooo\\noo\\noooooooo\\noo\\nooooo\\nooooo o\\noo\\noooo\\noooooo oo\\nooooooooo\\nooooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noo\\nooo\\noo\\nooo\\noooooo\\noooo\\noo\\noooo\\nooo o\\noo\\nooooooo\\nooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\nooooo\\noooo\\nooooo\\nooooo\\noo\\nooooo\\noooo\\noooooo oo\\nooo\\no oooo\\nooo\\noo\\noooooo\\noo\\no oooo\\noooo\\noooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oooo\\nooo\\noo ooo o\\nooooo\\noooo\\noo\\noooo\\nooo o\\noo o\\no oo\\nooo\\noooo\\noooooooo\\nooo\\noooo\\nooo\\nooo\\noooooo\\nooo\\noooo\\noo\\nooooo\\noooo\\nooo\\noo\\noo\\noooo\\noooooooooo\\noo\\noo\\no oooo\\noooo\\noo\\noo\\nooo\\noo ooooo\\noo\\noo\\noooooo\\noo\\noo\\noo\\noo oooo\\noooooooo\\nooooo o ooooooooo\\noo oooo\\nooooooooo\\no ooo\\nooo\\nooo\\nooo\\nooo\\nooo\\nooo\\noo\\noo\\noooooooooo\\noo\\nooo\\noooo\\noooo ooo\\noooo\\nooo\\nooo\\noo\\noooooooo\\noo\\nooooo\\nooooo o\\noo\\noooo\\noooooooo\\nooooooooo\\nooooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noo\\nooo\\noo\\nooo\\noooooo\\no ooo\\noo\\no ooo\\noooo\\noo\\nooooooo\\nooo\\noo\\noooo\\noooo\\nooo\\nooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooo\\nooooo\\noooo\\nooooo\\nooo oo\\noo\\nooooo\\noo oo\\noo o ooooo\\nooo\\nooooo\\nooo\\noo\\noooooo\\noo\\nooooo\\no ooo\\noooo\\noooo\\nooo\\nooooo o\\nooooo\\noooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\noooo\\noooooooo\\nooo\\noooo\\nooo\\nooo\\noooooo\\nooo\\noooo\\noo\\nooooo\\noooo\\nooo\\noo\\noo\\noooo\\noooooo oooo\\noo\\noo\\nooooo\\noooo\\noo\\noo\\nooo\\nooooooo\\noo\\noo\\noooooo\\noo\\noo\\noo\\noooooo\\nooooooo o\\nooooooooooooooo\\noooooo\\nooooooooo\\noooo\\nooo\\nooo\\nooo\\nooo\\nooo\\nooo\\noo\\noo\\no ooo oooooo\\noo\\nooo\\noooo\\nooo oooo\\noooo\\nooo\\nooo\\noo\\noooooooo\\noo\\nooooo\\noooooo\\noo\\noooo\\noooooooo\\nooooooo oo\\nooooooo\\nooo\\nooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oooo\\noo\\noooo\\noo\\nooo\\noo\\nooo\\noooooo\\noooo\\noo\\noooo\\noooo\\noo\\noo ooooo\\nooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\nooooo\\noooo\\nooooo\\nooooo\\noo\\nooooo\\noooo\\noooo oooo\\nooo\\nooooo\\nooo\\noo\\noooooo\\noo\\nooooo\\noooo\\noooo\\noooo\\nooo\\nooo oo\\nfamhisto\\nooooo\\noooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\noooo\\noooooooo\\nooo\\no ooo\\nooo\\nooo\\noo oooo\\noo o\\noooo\\noo\\noo ooo\\noooo\\nooo\\noo\\noo\\noooo\\noooooooooo\\noo\\noo\\nooooo\\noooo\\noo\\noo\\nooo\\nooo oooo\\noo\\noo\\noooooo\\noo\\noo\\noo\\noooooo\\noooooooo\\noo ooooooooooooo\\noooooo\\nooooooooo\\noooo\\nooo\\no oo\\nooo\\nooo\\nooo\\nooo\\noo\\noo\\noooooooooo\\noo\\nooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooo o\\nooooooo\\noooo\\nooo\\nooo\\noo\\noooooooo\\noo\\nooooo\\noooooo\\noo\\noooo\\noooooooo\\noooooo ooo\\nooooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noo\\noo o\\noo\\nooo\\noooooo\\noooo\\noo\\noooo\\noooo\\noo\\nooooooo\\nooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\nooooo\\noooo\\nooooo\\nooooo\\noo\\nooooo\\noooo\\noooooooo\\noo o\\nooooo\\nooo\\noo\\noooooo\\noo\\nooooo\\noooo\\noooo\\noooo\\no oo\\nooooo o\\nooooo\\noooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\noooo\\nooooooo o\\nooo\\noooo\\nooo\\nooo\\noo oooo\\nooo\\noooo\\noo\\nooooo\\noooo\\nooo\\noo\\noo\\noooo\\noooooooooo\\noo\\noo\\noo o oo\\noooo\\noo\\noo\\nooo\\nooooooo\\noo\\noo\\noooooo\\noo\\noo\\noo\\noooooo\\noooooooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='o ooooooo ooooooo\\noooooo\\nooooooooo\\noooo\\nooo\\nooo\\nooo\\nooo\\nooo\\nooo\\noo\\noo\\noooooooooo\\noo\\nooo\\nooo o\\nooo o ooo\\noooo\\nooo\\nooo\\noo\\no ooooooo\\noo\\nooooo\\no ooooo\\noo\\noooo\\noooooooo\\nooooooooo\\nooooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noo\\nooo\\noo\\nooo\\noooooo\\noooo\\noo\\nooo o\\noooo\\noo\\nooooooo\\nooo\\noo\\noooo\\noooo\\noo o\\noo o\\nooo\\nooooo\\noooo\\nooooo\\nooooo\\noo\\nooooo\\noooo\\no ooooooo\\nooo\\noo ooo\\nooo\\noo\\noooooo\\noo\\nooooo\\noooo\\nooo o\\noooo\\nooo\\nooooo o\\nooooo\\noo oo\\noo\\no ooo\\noooo\\nooo\\nooo\\nooo\\noooo\\noooooo oo\\nooo\\no ooo\\no oo\\nooo\\noooooo\\nooo\\noooo\\noo\\noo ooo\\noooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooo\\noo\\noo\\noooo\\no o o ooooooo\\noo\\noo\\noo ooo\\noooo\\noo\\noo\\nooo\\no o ooooo\\noo\\noo\\noooooo\\noo\\noo\\noo\\noooooo\\noooooooo\\no o ooo o ooo oooooo\\noo oooo\\noo o oooooo\\noooo\\nooo\\nooo\\nooo\\no oo\\no oo\\noo o\\noo\\noo\\no oo oooo o oo\\noo\\nooo\\noooo\\noo ooooo\\noo oo\\nooo\\nooo\\noo\\no o oooooo\\noo\\noo ooo\\noo oooo\\noo\\noooo\\noooooooo\\nooooooo oo\\noooo ooo\\nooo\\noo o\\noooo\\noo\\no ooo\\noo\\nooo\\noo\\nooo\\nooo ooo\\noooo\\noo\\noooo\\no oo o\\noo\\nooooooo\\nooo\\noo\\noooo\\noooo\\nooo\\noo o\\nooo\\nooooo\\noooo\\nooooo\\nooooo\\noo\\nooooo\\noooo\\noo oooooo\\nooo\\noo ooo\\nooo\\noo\\noooooo\\noo\\nooooo\\noooo\\noooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='o ooo\\nooo\\nooooo\\noooo\\noo\\nooooo\\nooooooo\\noo\\nooooo\\noo\\nooo\\nooooo\\nooo\\nooo\\noooo\\nooo\\nooooo\\nooooooooooo\\nooo\\noo\\noooo\\noo\\nooooooo\\noo\\noo\\noo\\noo\\nooo\\noo\\nooooooooooo\\noo\\noooo\\noo\\noo\\noooo\\noooooo\\noo\\noo\\noo\\nooooooooo\\nooo\\nooooo\\nooooooooo\\noooo\\noooooooo\\no\\nooo\\nooo\\nooooo\\no\\noo\\noo\\nooooo\\noo\\nooooo\\no ooo\\nooooo\\noo\\noo\\noo\\noo\\noooo\\noo\\noo\\nooooooooo\\nooooo\\noooooo\\no\\noooooo\\noooooo\\nooo\\noooo\\noooo\\noooo\\noooooo\\nooooooo\\nooooo\\nooooo\\nooooo\\nooo\\no\\nooo\\nooo\\noooo\\no\\nooooooooo\\nooo\\noooooo\\nooooooo\\noooooo\\no\\noooooo\\nooooo\\nooooooooo\\no\\nooooo\\nooooooo\\nooo\\nooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oooooo\\nooo\\noo\\nooooo\\nooo\\nooo\\noooo\\no\\noo\\nooo\\nooo oo\\nooooo\\noo\\noooo\\nooooooo\\noooo\\noo\\nooo\\nooooo\\noo\\noo\\noo\\nooooo\\noo\\noooo\\no\\nooooooooo\\nooooooo\\nooo\\nooooo\\nooo\\nooo\\noooo\\nooo\\nooooo\\nooooooooooo\\nooo\\noo\\noooo\\noo\\nooooooo\\noo\\noo\\noo\\noo\\nooo\\noo\\noooo\\nooooooo\\noo\\noooo\\noo\\noo\\noooo\\noooooo\\noo\\noo\\noo\\nooooooooo\\nooo\\nooooo\\nooooooooo\\noooo\\noooooooo\\no\\nooo\\nooo\\nooooo\\no\\noo\\nooooooo\\noo\\nooooooooo\\nooooooooo\\noo\\noo\\noooo\\noo\\noo\\noooooo\\nooo\\nooooo\\noooooo\\no\\noooooo\\noooooo\\nooo\\noooo\\noo oo\\noooo\\noooooo\\nooooooo\\nooooo\\nooooo\\nooooo\\nooo\\no\\nooo\\nooo\\noooo\\no', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooooooooo\\nooo\\noooooo\\nooooooo\\nooooooo\\noooooo\\nooooo\\nooooooooo\\no\\noooooooo\\noooo\\nooo\\nooo\\noooooo\\nooo\\noo\\nooooo\\nooo\\nooo\\noooo\\no\\noo\\nooo\\nooooo\\nooooo\\noo\\nooooooooooo\\noooo\\noo\\nooo\\nooooo\\noo\\noo\\noo\\nooooo\\noo\\noooooooooo oo\\noo\\nooooooo\\nooo\\nooooo\\nooo\\nooo\\noooo\\nooo\\nooooo\\nooooooooooo\\nooo\\noo\\noooo\\noo\\nooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\noo\\nooooo oooooo\\noo\\noooo\\noo\\noo\\noooo\\noooooo\\noo\\noo\\noo\\noo\\noo\\nooooo\\nooo\\nooooo\\nooooooooo\\noooo\\noooooooo\\no\\nooo\\nooo\\nooooo\\no\\noo\\noo\\nooooo\\noo\\noooo\\nooooo\\nooooooo\\noo\\noooo\\noooo\\noo\\noo\\nooooooooo\\nooooo\\noooooo\\no\\noooooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oooooo\\nooo\\noooo\\noooo\\noooo\\noooooo\\nooooooo\\nooooo\\nooooo\\nooooo\\nooo\\no\\nooo\\nooo\\noooo\\no\\nooooooooo\\nooo\\nooo\\nooo\\nooooooo\\nooooooo\\noooooo\\nooooo\\noooo\\nooooo\\no\\noooooooo\\noooo\\nooo\\nooo\\noooooo\\nooo\\noo\\nooooo\\nooo\\nooo\\noooo\\no\\noo\\nooo\\nooooo\\nooooo\\noo\\nooo oooooooo\\noooo\\noo\\nooo\\nooooo\\noo\\noo\\noo\\nooo oo\\noo\\nooooo\\nooo oo oooo\\nooooooo\\noo o\\nooooo\\nooo\\nooo\\noooo\\nooo\\noo ooo\\noooooo o ooo o\\nooo\\noo\\noooo\\noo\\nooooo\\noo\\no o\\noo\\noo\\noo\\nooo\\noo\\noooooooo ooo\\noo\\nooo o\\noo\\noo\\noooo\\noooooo\\noo\\noo\\noo\\nooooo o ooo\\nooo\\noo oo o\\nooooooo oo\\noooo\\noooooooo\\no\\nooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooo\\nooooo\\no\\noo\\nooo oooo\\noo\\no ooo\\nooo oo\\no oooooo\\noo\\noo\\noo\\noooo\\noo\\noo\\nooo o ooooo\\nooooo\\noooo oo\\no\\no oo oo o\\noooooo\\no oo\\noooo\\no\\nooo\\noooo\\no o\\noooo\\nooooooo\\nooooo\\nooooo\\no\\noooo\\nooo\\no\\nooo\\nooo\\noooo\\no\\no oooo oo oo\\nooo\\nooo\\nooo\\noo ooooo\\nooooooo\\noooooo\\nooooo\\no oooooo oo\\no\\noo\\noooooooooo\\nooo\\nooo\\noooooo\\nooo\\noo\\noo ooo\\nooo\\nooo\\noooo\\no\\noo\\no oo\\nooooo\\nooooo\\noo\\nooo oooooooo\\noooo\\noo\\nooo\\nooooo\\noo\\noo\\noo\\noobesity\\noooo\\noo\\noooo\\noooooooooo\\nooooo\\noo\\nooo\\nooooo\\nooo\\nooo\\noooo\\nooo\\nooooo\\noooooooooo o\\nooo\\noo\\noooo\\noo\\nooooooo\\noo\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oo\\noo\\nooooo\\noooo\\nooooooooo\\noooo\\noo\\noo\\noooo\\noooooo\\noo\\noo\\noo\\noo\\nooooooo\\nooo\\nooooo\\nooooooooo\\noooo\\noooooooo\\no\\nooo\\nooo\\nooooo\\no\\noo\\nooooooo\\noo\\noooo\\nooooo\\nooooooo\\noo\\noo\\noo\\noooo\\noo\\noo\\nooooooooo\\nooooo\\noooooo\\no\\noooooo\\noooooo\\nooo\\noooo\\no\\nooo\\noooo\\nooooo\\no\\nooooooo\\nooooo\\nooooo\\nooooo\\nooo\\no\\nooo\\nooo\\noooo\\no\\nooooooooo\\nooo\\nooo\\nooo\\nooooooo\\nooooooo\\noooooo\\nooooo\\nooooooooo\\no\\noo\\nooooo\\nooooo\\nooo\\nooo\\noooooo\\nooo\\noo\\nooooo\\noooooo\\noooo\\no\\noo\\nooo\\nooo oo\\nooooo\\noo\\nooooooooo oo\\noooo\\noo\\nooo\\noooo\\no\\noo\\noo\\noo\\no\\n15 25 35 45oooo\\noo\\nooooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooo oooooo\\nooooooo\\nooo\\nooooo\\nooo\\nooo\\noooo\\nooo\\nooooo\\noooooo ooooo\\nooo\\noo\\noooo\\noo\\nooooooo\\noo\\noo\\noo\\noo\\nooo\\noo\\nooooooooooo\\noo\\noooo\\noo\\noo\\noooo\\noooooo\\noo\\noo\\noo\\noooo\\nooooo\\nooo\\nooooo\\noooo\\nooo oo\\noooo\\nooo ooooo\\no\\nooo\\nooo\\nooooo\\no\\noo\\noo\\nooooo\\noo\\nooooo\\noooo\\nooooooo\\noo\\noo\\noo\\noooo\\noo\\noo\\nooooooooo\\nooooo\\noooooo\\no\\noooooo\\noooooo\\nooo\\noooo\\no\\nooo\\noooo\\noo\\nooo\\no\\nooooooo\\nooooo\\nooooo\\no\\noooo\\nooo\\no\\nooo\\nooo\\noooo\\no\\nooooooooo\\nooo\\nooo\\nooo\\nooooooo\\nooooooo\\noooooo\\nooooo\\nooooooooo\\no\\noo\\noooooo\\noooo\\nooo\\nooo\\noooooo\\nooo\\noo\\nooooo\\nooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooo\\noooo\\no\\noo\\nooo\\nooooo\\nooooo\\noo\\noooo\\nooooooo\\noooo\\noo\\nooo\\nooooo\\noo\\noo\\noo\\no0 50 100o\\noooo\\noooooo\\nooo ooo\\noo\\nooo\\noooooo\\nooooooooo\\nooo\\nooooooooo\\nooo\\noooo\\noooooooooo\\nooooooo\\nooooooo\\noooooo\\nooooooooooo\\nooooooooo\\no\\noooooooo\\noo\\noo\\noooooo\\noooo\\nooo\\nooo\\nooo\\noooo\\noo\\nooooooooooo\\no\\noo\\noo\\noo\\nooooooooo oooooooooooooo\\nooooooooo\\nooooooo\\no\\nooo\\nooooo\\noooooooooooooooo\\noo\\nooo\\noooo\\noooooo\\noo\\noooooooo\\noooooo\\nooo\\noooo\\noooooo\\noooooooooooo\\nooo\\nooooo\\noooo\\noo\\nooo\\nooooooo\\nooo\\no\\noo\\nooo\\no\\noo\\nooo\\noo\\noooo oooooooooo\\nooo\\no\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooo\\noooooo\\noo\\nooo\\nooooooo\\noo\\noooooo\\no\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\no\\nooooooo\\noo\\nooooo\\no ooo\\noooo\\noo\\noooo\\no\\noooooo ooooooooo\\noo\\nooooo\\no\\noo\\nooo\\nooooo\\noo\\noooo\\noooooo\\noo oooo\\noo\\nooo\\no ooooo\\nooooooooo\\nooo\\noooo\\nooooo\\nooo\\noooo\\noooooooooo\\nooooooo\\nooooooo\\noooooo\\nooooooooooo\\nooo\\noooooo\\nooooooooo\\noo\\noo\\noooooo\\noooo\\nooo\\nooo\\nooo\\noooo\\noo\\nooooooooooo\\no\\noo\\noo\\noo\\nooooooooooooooooooooooo\\noooooooooooooooo\\no\\nooo\\no oo\\noo\\noo\\noooooooooooooooo\\nooo\\noooo\\noooooo\\noo\\noooooooo\\noooooo\\nooo\\noooo\\noooooo\\noooooooooooo\\nooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooooo\\noooo\\noo\\nooo\\noooooooooo\\no\\noo\\nooo\\no\\noo\\nooo\\noo\\nooooooooo\\nooooo\\nooo\\no\\noo\\nooo\\noooooooo\\nooo\\noo\\nooooo\\noo\\noooooo\\no\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\no\\nooooooooo\\noo ooo\\noooo\\noooo\\noo\\noooo\\no\\noo ooooo oooooooo\\noo\\nooooo\\no\\noo\\nooo\\nooooo\\noo\\noooo\\noooooo\\noooooo\\noo\\nooo\\noooooo\\nooooooooo\\nooo\\nooooo oooo\\nooo\\noo\\noo\\noooooooooo\\nooooooo\\nooooooo\\noooooo\\nooo oooooooo\\nooo\\noooooo\\nooooooooo\\noo\\noo\\noooooo\\noooo\\nooo\\nooo\\nooo\\noooooo\\nooooo\\noooooo\\no\\noo\\noo\\noo\\nooooooooooooooooooooooo\\noooooooooooooooo\\no\\nooo\\nooo\\noo\\noo\\noooooooooooooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oo\\nooo\\noooo\\noooooo\\noo\\no ooooooo\\noooooo\\nooo\\noooo\\noooooo\\noooooooooooo\\nooo\\nooooo\\noooo\\noo\\nooo\\noooooooooo\\no\\noo\\nooo\\no\\noo\\nooo\\noo\\noooooooooooooo\\nooo\\no\\noo\\nooo\\noooooo\\noo\\nooo\\nooooooo\\noo\\noooooo\\no\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\no\\nooooooooo\\nooooooooo\\noooo\\noo\\noooo\\no\\noooooooooo ooo oo\\noo\\nooooo\\no\\noo\\nooo\\noooo o\\noo\\no ooo\\noo oooo\\no oo oo o\\noo\\nooo\\no o o o oo\\no oooooooo\\nooo\\no ooooo ooo\\no oo\\noooo\\nooo o o ooooo\\nooooooo\\no ooo ooo\\no o oooo\\no oo oooooooo\\noooo o oooo\\noo ooo o o oo\\noo\\noo\\noooooo\\no ooo\\nooo\\nooo\\no oo\\noooo\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooooo\\noooooo\\no\\noo\\noo\\noo\\nooooooooo ooo ooooooooooo\\no oooo o ooooooo ooo\\no\\nooo\\no oo\\noo\\nooooooooooo ooo oo\\noo\\nooo\\noooo\\noooooo\\noo\\no oo oo ooo\\noo oooo\\nooo\\noooo\\no ooooo\\no o ooo ooooooo\\nooo\\noo ooo\\noooo\\noo\\nooo\\noo oo o oo\\noooo\\noo\\nooo\\no\\noo\\nooo\\noo\\no oo o oooooooooo\\nooo\\no\\noo\\no oo\\noo o ooooo\\nooo\\noo\\noo ooo\\noo\\nooo o oo\\no\\noo\\noo\\noo\\nooo\\no oo\\noo ooo\\noooo\\noo oo\\no\\nooooo o ooo\\no oooooooo\\noooo\\noo\\no ooo\\no\\no ooooo o oooooo oo\\noo\\no oooo\\no\\noo\\nooo\\nooooo\\noo\\noooo\\noooooo\\noooooo\\noo\\nooo\\nooooooooooooooo\\nooo\\nooooo oooo\\nooo\\noooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooooooo\\nooo\\nooooooo\\nooooooo\\noooooo\\nooooooooooo\\nooooooooo\\no\\noooooooo\\noo\\noo\\noooooo\\noooo\\nooo\\nooo\\nooo\\noooooo\\nooooo\\noo oooo\\no\\noo\\noo\\noo\\nooooooooo oooooooooooooo\\noooooo oooooooooo\\no\\nooo\\nooo\\noo\\nooooooooooo ooooo\\noo\\nooo\\noooo\\noooooo\\noo\\noooooooo\\noooooo\\nooo\\noooo\\noooooo\\noooooooooooo\\nooo\\nooooo\\noooo\\noo\\nooo\\nooooooo\\nooo\\no\\noo\\nooo\\no\\noo\\nooo\\noo\\noooooooooooooo\\nooo\\no\\noo\\nooo\\noooooooo\\nooo\\noo\\nooooo\\noo\\noooooo\\no\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\no\\nooooooooo\\nooooooooo\\noooo\\noo\\noooo\\no\\nooooooooooooooo\\noo\\nooooo\\no\\noo\\nooo\\nooooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oalcoholo\\noooo\\noooooo\\nooo ooo\\noo\\nooo\\noo oooo\\nooooooooo\\nooo\\nooooo oooo\\nooo\\noooo\\nooo o ooo\\nooo\\nooooooo\\nooooooo\\noooooo\\noooo o oooooo\\nooooooooo\\nooooo oooo\\noo\\noo\\noo oooo\\no ooo\\nooo\\nooo\\nooo\\noooo\\noo\\nooooo\\noooooo\\no\\noo\\noo\\noo\\nooooooooooooooooo o ooooo\\nooooooooo\\nooooooo\\no\\nooo\\nooooo\\noooo o oooooo ooooo\\noo\\nooo\\noo oo\\nooo o\\noo\\noo\\noooooooo\\no oooo\\no\\nooo\\noooo\\no oooo o\\noooooooooooo\\nooo\\nooooo\\noooo\\noo\\nooo\\nooooo oo\\noooo\\noo\\nooo\\no\\noo\\nooo\\noo\\noooo ooooo\\nooooo\\nooo\\nooo\\nooo\\noooooooo\\nooo\\noooo ooo\\noo\\noooooo\\no\\noo\\noo\\noo\\nooo\\nooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooooo\\noooo\\noooo\\no\\nooooooooo\\noo ooo\\noooo\\noooo\\noo\\noooo\\no\\no ooooo ooooooo oo\\noo\\nooooo\\no\\noo\\nooo\\no ooo o\\no\\n100 160 220oo\\noo\\noo\\noo\\nooo\\no\\nooo\\nooooo\\noooo\\no\\nooo\\noo\\nooo\\noo\\nooo\\noo\\no\\no\\noo\\nooo\\no\\noooo\\no\\nooo\\noo\\noo\\nooo\\no\\noo\\noooo\\nooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\nooo\\no\\nooo\\noo\\nooo\\noo\\noooooo\\noo\\noo\\noo\\nooo\\nooo\\no o\\noo\\noo\\noo\\noooo\\noo\\noo\\noo\\nooooo\\no\\noo\\no\\nooooo\\no\\noooooo\\noo\\noo\\nooo\\noo\\no\\noooooo\\no\\noooo\\no\\no\\nooooo\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\nooo\\noooo\\nooooo\\no\\noooooooooo\\no\\no\\noo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\no\\nooooo\\no', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooooo\\noo\\nooooo\\noooo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\nooo\\nooooo\\nooo\\nooooo\\noo\\nooo\\noo\\no\\nooo\\noo\\nooo\\noo\\nooo\\no\\noooooo\\no\\noo\\noo\\noo\\noo\\no\\no\\no\\noooo\\no\\noo\\noooo\\no\\noo\\noo\\noooooo\\nooo\\no\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\noo\\noooooooooo\\no\\noo\\no\\nooo\\nooo\\noooo\\no\\nooooo\\noooo\\noo\\noo\\noo\\nooo\\no\\nooo\\nooooo\\noooo\\no\\nooo\\noo\\nooo\\noo\\nooo\\noo\\no\\no\\noo\\nooo\\no\\nooooo\\nooo\\noo\\noo\\nooo\\no\\noo\\noooo\\nooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\noo o\\no\\nooo\\noo\\nooo\\noo\\nooooo\\no\\noo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo\\noo\\no\\nooo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\no', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooooo\\no\\noooo\\noo\\noo\\noo\\nooo\\noo\\no\\no\\nooooo\\nooooo\\no\\no\\nooooo\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\nooo\\noooo\\nooooo\\no\\noooooooooo\\no\\no\\noo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\no\\nooooo\\no\\nooooo\\noo\\nooooo\\noooo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\nooo\\noooo o\\nooo\\nooooo\\noo\\nooo\\noo\\no\\nooo\\noo\\nooo\\noo\\nooo\\no\\noooooo\\no\\noo\\noo\\noo\\noo\\no\\no\\no\\noooo\\no\\noo\\noooo\\no\\noo\\noo\\noooooo\\nooo\\no\\nooooo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\noo\\noooooooooo\\no\\noo\\no\\nooo\\nooo\\noooo\\no\\nooo\\noo\\noo\\n2 6 10 14oo\\noo\\noo\\noo\\nooo\\no\\nooo\\nooooo\\noooo\\no\\nooooo\\nooo\\noo\\nooo\\noo\\no\\no\\noo\\nooo\\no\\noooo\\no\\noo\\no\\noo\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooo\\no\\noo\\noooo\\nooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\nooo\\no\\nooo\\noo\\nooo\\noo\\noooooo\\noo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo\\noo\\noooo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\no\\nooooo\\no\\noooooo\\noo\\noo\\nooo\\noo\\no\\noooooo\\nooooo\\no\\no\\noooo o\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\nooo\\nooo\\no\\nooooo\\no\\nooo\\noooooooo\\no\\noo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\no\\nooooo\\no\\nooooo\\noo\\nooooo\\noooo\\noo\\noooo\\no\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\nooo\\nooooo\\nooo\\nooooo\\noo\\nooo\\noo\\no\\nooo\\noo\\nooo\\noo\\nooo\\no\\noooooo\\no\\noo\\noo\\noo\\noo\\no\\no\\no\\noooo\\no\\noo\\noooo\\no\\noo\\noo\\noooooo\\nooo\\no\\nooooo\\no\\noo\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooo\\noo\\no\\nooo\\noo\\noooooooooo\\no\\noo\\no\\nooo\\nooo\\no\\nooo\\no\\nooo\\noo\\noooo\\noo\\noo\\noo\\nooo\\no\\nooo\\nooooo\\noooo\\no\\nooooo\\nooo\\noo\\nooo\\noo\\no\\no\\noo\\nooo\\no\\noooo\\no\\nooo\\noo\\noo\\nooo\\no\\noo\\no oo o\\nooo\\noooo o o\\noooo o\\nooo\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\noo o\\no\\nooo\\noo\\nooo\\noo\\noooo oo\\noo\\noo\\noo\\noo o\\nooo\\noo\\noo\\noo\\noo\\noooo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\no\\nooooo\\no\\noooooo\\noo\\noo\\nooo\\noo\\no\\noooooo\\nooo oo\\no\\no\\noo o o o\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\nooo\\noooo\\nooooo\\no\\nooo\\noooo oooo\\no\\noo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\no\\nooooo\\no\\nooooo\\noo\\nooooo\\noooo\\noo\\noooo\\no\\nooo\\noooo\\nooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\nooo\\nooooo\\noo o\\noooo o\\noo\\nooo\\noo\\no\\nooo\\noo\\noo o\\noo\\nooo\\no\\noooooo\\no\\noo\\noo\\noo\\noo\\no\\noo\\noooo\\no\\noo\\noooo\\no\\noo\\noo\\nooooo o\\noo o\\no\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\no oo\\no o\\nooooo ooooo\\no\\noo\\no\\nooo\\nooo\\no\\nooo\\no\\noo o\\noo\\noo\\n15 25 35 45oo\\noo\\no\\no\\noo\\nooo\\no\\nooo\\nooooo\\noooo\\no\\nooo\\noo\\nooo\\noo\\nooo\\noo\\no\\no\\noo\\nooo\\no\\noooo\\no\\nooo\\noo\\noo\\nooo\\no\\noo\\noooo\\nooo\\noooooo\\nooo oo\\nooo\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\nooo\\no\\nooo\\noo\\nooo\\noo\\noooooo\\noo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noooo\\noooo\\noo\\noo\\noo\\nooooo\\no\\noo\\no\\nooooo\\no\\noooooo\\noo\\noo\\nooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oo\\no\\noooooo\\noooooo\\no\\nooooo\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\nooo\\noooo\\nooooo\\no\\nooo\\nooooooo\\no\\no\\noo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\no\\nooooo\\no\\nooooo\\noo\\nooooo\\noooo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\nooo\\nooooo\\nooo\\nooooo\\noo\\nooo\\noo\\no\\nooo\\noo\\nooo\\noo\\nooo\\no\\noooooo\\no\\noo\\noo\\noo\\noo\\no\\noo\\noooo\\no\\noo\\noooo\\no\\noo\\noo\\nooooooooo\\no\\nooooo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\noo\\noooooooooo\\no\\noo\\no\\nooo\\nooo\\noooo\\no\\noo ooo\\noooo\\noo\\noo\\noo\\nooo\\no\\nooo\\nooooo\\noooo\\no\\nooooo\\nooo\\noo\\nooo\\noo\\no\\no\\noo\\nooo\\no\\noooo\\no\\noo\\no\\noo\\noo\\nooo\\no\\noo\\noooo\\nooo\\noooooo\\nooooo\\nooo\\nooo\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\nooo\\no\\nooo\\noo\\nooo\\noo\\noooooo\\noo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo\\noo\\noooo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\no\\nooooo\\no\\noooooo\\noo\\noo\\nooo\\noo\\no\\noooooo\\nooooo\\no\\no\\nooooo\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\nooo\\noooo\\no oooo\\no\\nooo\\nooooooo\\no\\no\\noo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\no\\nooooo\\no\\nooooo\\noo\\nooooo\\noooo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\nooo\\nooooo\\nooo\\nooooo\\noo\\nooo\\noo\\no\\nooo\\noo\\nooo\\noo\\nooo\\no\\noooooo\\no\\noo\\noo\\noo\\noo\\no\\noo\\noooo\\no\\noo\\noooo\\no\\noo\\noo\\noooooo\\nooo\\no\\nooooo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\noo\\noooooooooo\\no\\noo\\no\\nooo\\nooo\\no', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooo\\no\\nooo\\noo\\noo\\n20 40 60\\n20 40 60age\\nFIGURE 4.12. A scatterplot matrix of the South African heart disease data.\\nEach plot shows a pair of risk factors, and the cases and contro ls are color coded\\n(red is a case). The variable family history of heart disease ( famhist)is binary\\n(yes or no).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='124 4. Linear Methods for Classiﬁcation\\nTABLE 4.3. Results from stepwise logistic regression ﬁt to South African heart\\ndisease data.\\nCoeﬃcient Std. Error Zscore\\n(Intercept) −4.204 0 .498−8.45\\ntobacco 0.081 0 .026 3.16\\nldl 0.168 0 .054 3.09\\nfamhist 0.924 0 .223 4.14\\nage 0.044 0 .010 4.52\\nother correlated variables, they are no longer needed (and c an even get a\\nnegative sign).\\nAt this stage the analyst might do some model selection; ﬁnd a subset', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 142}),\n",
       " Document(page_content='of the variables that are suﬃcient for explaining their join t eﬀect on the\\nprevalence of chd. One way to proceed by is to drop the least signiﬁcant co-\\neﬃcient, and reﬁt the model. This is done repeatedly until no further terms\\ncan be dropped from the model. This gave the model shown in Tab le 4.3.\\nA better but more time-consuming strategy is to reﬁt each of t he models\\nwith one variable removed, and then perform an analysis of deviance to', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 142}),\n",
       " Document(page_content='decide which variable to exclude. The residual deviance of a ﬁtted model\\nis minus twice its log-likelihood, and the deviance between two models is\\nthe diﬀerence of their individual residual deviances (in an alogy to sums-of-\\nsquares). This strategy gave the same ﬁnal model as above.\\nHow does one interpret a coeﬃcient of 0 .081 (Std. Error = 0 .026) for\\ntobacco, for example? Tobacco is measured in total lifetime usage in kilo-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 142}),\n",
       " Document(page_content='grams, with a median of 1 .0kg for the controls and 4 .1kg for the cases. Thus\\nan increase of 1kg in lifetime tobacco usage accounts for an i ncrease in the\\nodds of coronary heart disease of exp(0 .081) = 1.084 or 8.4%. Incorporat-\\ning the standard error we get an approximate 95% conﬁdence in terval of\\nexp(0.081±2×0.026) = (1.03,1.14).\\nWe return to these data in Chapter 5, where we see that some of t he\\nvariables have nonlinear eﬀects, and when modeled appropri ately, are not', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 142}),\n",
       " Document(page_content='excluded from the model.\\n4.4.3 Quadratic Approximations and Inference\\nThe maximum-likelihood parameter estimates ˆβsatisfy a self-consistency\\nrelationship: they are the coeﬃcients of a weighted least sq uares ﬁt, where\\nthe responses are\\nzi=xT\\niˆβ+(yi−ˆpi)\\nˆpi(1−ˆpi), (4.29)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 142}),\n",
       " Document(page_content='4.4 Logistic Regression 125\\nand the weights are wi= ˆpi(1−ˆpi), both depending on ˆβitself. Apart from\\nproviding a convenient algorithm, this connection with lea st squares has\\nmore to oﬀer:\\n•The weighted residual sum-of-squares is the familiar Pears on chi-\\nsquare statistic\\nN∑\\ni=1(yi−ˆpi)2\\nˆpi(1−ˆpi), (4.30)\\na quadratic approximation to the deviance.\\n•Asymptotic likelihood theory says that if the model is corre ct, then\\nˆβis consistent (i.e., converges to the trueβ).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 143}),\n",
       " Document(page_content='•A central limit theorem then shows that the distribution of ˆβcon-\\nverges toN(β,(XTWX)−1). This and other asymptotics can be de-\\nriveddirectlyfromtheweightedleastsquaresﬁtbymimicki ngnormal\\ntheory inference.\\n•Model building can be costly for logistic regression models , because\\neach model ﬁtted requires iteration. Popular shortcuts are theRao\\nscore test which tests for inclusion of a term, and the Wald test which\\ncan be used to test for exclusion of a term. Neither of these re quire', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 143}),\n",
       " Document(page_content='iterative ﬁtting, and are based on the maximum-likelihood ﬁ t of the\\ncurrent model. It turns out that both of these amount to addin g\\nor dropping a term from the weighted least squares ﬁt, using t he\\nsameweights. Such computations can be done eﬃciently, without\\nrecomputing the entire weighted least squares ﬁt.\\nSoftware implementations can take advantage of these conne ctions. For\\nexample, the generalized linear modeling software in R (whi ch includes lo-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 143}),\n",
       " Document(page_content='gistic regression as part of the binomial family of models) e xploits them\\nfully.GLM(generalizedlinearmodel)objectscanbetreate daslinearmodel\\nobjects, and all the tools available for linear models can be applied auto-\\nmatically.\\n4.4.4L1Regularized Logistic Regression\\nTheL1penalty used in the lasso (Section 3.4.2) can be used for vari able\\nselection and shrinkage with any linear regression model. F or logistic re-\\ngression, we would maximize a penalized version of (4.20):\\nmax\\nβ0,β\\uf8f1\\n\\uf8f2\\n\\uf8f3N∑\\ni=1[', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 143}),\n",
       " Document(page_content='yi(β0+βTxi)−log(1+eβ0+βTxi)]\\n−λp∑\\nj=1|βj|\\uf8fc\\n\\uf8fd\\n\\uf8fe.(4.31)\\nAs with the lasso, we typically do not penalize the intercept term, and stan-\\ndardize the predictors for the penalty to be meaningful. Cri terion (4.31) is', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 143}),\n",
       " Document(page_content='126 4. Linear Methods for Classiﬁcation\\nconcave, and a solution can be found using nonlinear program ming meth-\\nods (Koh et al., 2007, for example). Alternatively, using th e same quadratic\\napproximations that were used in the Newton algorithm in Sec tion 4.4.1,\\nwe can solve (4.31) by repeated application of a weighted las so algorithm.\\nInterestingly, the score equations [see (4.24)] for the var iables with non-zero\\ncoeﬃcients have the form\\nxT\\nj(y−p) =λ·sign(βj), (4.32)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 144}),\n",
       " Document(page_content='which generalizes (3.58) in Section 3.4.4; the active varia bles are tied in\\ntheirgeneralized correlation with the residuals.\\nPath algorithms such as LAR for lasso are more diﬃcult, becau se the\\ncoeﬃcient proﬁles are piecewise smooth rather than linear. Nevertheless,\\nprogress can be made using quadratic approximations.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 144}),\n",
       " Document(page_content='******************************* ************** ****** *************************************************** ****************************************** **************************************************************************** *****************', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 144}),\n",
       " Document(page_content='0.0 0.5 1.0 1.5 2.00.0 0.2 0.4 0.6******************************* * ******************************************************************************************************************************************************************************************** *****************', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 144}),\n",
       " Document(page_content='******************************* ************** * ****************************************************************************************************************************************************************************** *****************\\n****************************** ********************************************************************************************************************************************************************************************** *****************', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 144}),\n",
       " Document(page_content='******************************* ************** ****** *************************************************** *************************************************** ************************ ******************************************* ************************************************ ************** ****** *************************************************** *************************************************** ************************ *************************** *************** *', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 144}),\n",
       " Document(page_content='********************************************************************************************************************************************************************************************************************************************* *****************', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 144}),\n",
       " Document(page_content='obesityalcoholsbptobaccoldlfamhistage1 2 4 5 6 7Coeﬃcients βj(λ)\\n||β(λ)||1\\nFIGURE 4.13. L1regularized logistic regression coeﬃcients for the South\\nAfrican heart disease data, plotted as a function of the L1norm. The variables\\nwere all standardized to have unit variance. The proﬁles are c omputed exactly at\\neach of the plotted points.\\nFigure 4.13 shows the L1regularization path for the South African\\nheart disease data of Section 4.4.2. This was produced using theRpackage', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 144}),\n",
       " Document(page_content='glmpath (Park and Hastie, 2007), which uses predictor–corrector methods\\nof convex optimization to identify the exact values of λat which the active\\nset of non-zero coeﬃcients changes (vertical lines in the ﬁg ure). Here the\\nproﬁles look almost linear; in other examples the curvature will be more\\nvisible.\\nCoordinate descent methods (Section 3.8.6) are very eﬃcien t for comput-\\ning the coeﬃcient proﬁles on a grid of values for λ. TheRpackageglmnet', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 144}),\n",
       " Document(page_content='4.4 Logistic Regression 127\\n(Friedman et al., 2010) can ﬁt coeﬃcient paths for very large logistic re-\\ngression problems eﬃciently (large in Norp). Their algorithms can exploit\\nsparsity in the predictor matrix X, which allows for even larger problems.\\nSee Section 18.4 for more details, and a discussion of L1-regularized multi-\\nnomial models.\\n4.4.5 Logistic Regression or LDA?\\nIn Section 4.3 we ﬁnd that the log-posterior odds between cla sskandK\\nare linear functions of x(4.9):\\nlogPr(G=k|X=x)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 145}),\n",
       " Document(page_content='Pr(G=K|X=x)= logπk\\nπK−1\\n2(µk+µK)TΣ−1(µk−µK)\\n+xTΣ−1(µk−µK)\\n=αk0+αT\\nkx. (4.33)\\nThis linearity is a consequence of the Gaussian assumption f or the class\\ndensities, as well as the assumption of a common covariance m atrix. The\\nlinear logistic model (4.17) by construction has linear log its:\\nlogPr(G=k|X=x)\\nPr(G=K|X=x)=βk0+βT\\nkx. (4.34)\\nItseemsthatthemodelsarethesame.Althoughtheyhaveexac tlythesame\\nform, the diﬀerence lies in the way the linear coeﬃcients are estimated. The', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 145}),\n",
       " Document(page_content='logistic regression model is more general, in that it makes l ess assumptions.\\nWe can write the joint density ofXandGas\\nPr(X,G=k) = Pr(X)Pr(G=k|X), (4.35)\\nwhere Pr(X) denotes the marginal density of the inputs X. For both LDA\\nand logistic regression, the second term on the right has the logit-linear\\nform\\nPr(G=k|X=x) =eβk0+βT\\nkx\\n1+∑K−1\\nℓ=1eβℓ0+βT\\nℓx, (4.36)\\nwhere we have again arbitrarily chosen the last class as the r eference.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 145}),\n",
       " Document(page_content='The logistic regression model leaves the marginal density o fXas an arbi-\\ntrary density function Pr( X), and ﬁts the parameters of Pr( G|X) by max-\\nimizing the conditional likelihood —the multinomial likelihood with proba-\\nbilities the Pr( G=k|X). Although Pr( X) is totally ignored, we can think\\nof this marginal density as being estimated in a fully nonpar ametric and\\nunrestricted fashion, using the empirical distribution fu nction which places\\nmass 1/Nat each observation.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 145}),\n",
       " Document(page_content='With LDA we ﬁt the parameters by maximizing the full log-like lihood,\\nbased on the joint density\\nPr(X,G=k) =φ(X;µk,Σ)πk, (4.37)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 145}),\n",
       " Document(page_content='128 4. Linear Methods for Classiﬁcation\\nwhereφis the Gaussian density function. Standard normal theory le ads\\neasily to the estimates ˆ µk,ˆΣ, and ˆπkgiven in Section 4.3. Since the linear\\nparameters of the logistic form (4.33) are functions of the G aussian param-\\neters, we get their maximum-likelihood estimates by pluggi ng in the corre-\\nsponding estimates. However, unlike in the conditional cas e, the marginal\\ndensity Pr( X) does play a role here. It is a mixture density\\nPr(X) =K∑', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 146}),\n",
       " Document(page_content='k=1πkφ(X;µk,Σ), (4.38)\\nwhich also involves the parameters.\\nWhat role can this additional component/restriction play? By relying\\non the additional model assumptions, we have more informati on about the\\nparameters, and hence can estimate them more eﬃciently (low er variance).\\nIf in fact the true fk(x) are Gaussian, then in the worst case ignoring this\\nmarginal part of the likelihood constitutes a loss of eﬃcien cy of about 30%', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 146}),\n",
       " Document(page_content='asymptotically in the error rate (Efron, 1975). Paraphrasi ng: with 30%\\nmore data, the conditional likelihood will do as well.\\nFor example, observations far from the decision boundary (w hich are\\ndown-weighted by logistic regression) play a role in estima ting the common\\ncovariance matrix. This is not all good news, because it also means that\\nLDA is not robust to gross outliers.\\nFrom the mixture formulation, it is clear that even observat ions without', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 146}),\n",
       " Document(page_content='class labels have information about the parameters. Often i t is expensive\\nto generate class labels, but unclassiﬁed observations com e cheaply. By\\nrelying on strong model assumptions, such as here, we can use both types\\nof information.\\nThe marginal likelihood can be thought of as a regularizer, r equiring\\nin some sense that class densities be visiblefrom this marginal view. For\\nexample, if the data in a two-class logistic regression mode l can be per-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 146}),\n",
       " Document(page_content='fectly separated by a hyperplane, the maximum likelihood es timates of the\\nparameters are undeﬁned (i.e., inﬁnite; see Exercise 4.5). The LDA coeﬃ-\\ncients for the same data will be well deﬁned, since the margin al likelihood\\nwill not permit these degeneracies.\\nIn practice these assumptions are never correct, and often s ome of the\\ncomponents of Xare qualitative variables. It is generally felt that logist ic\\nregression is a safer, more robust bet than the LDA model, rel ying on fewer', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 146}),\n",
       " Document(page_content='assumptions. It is our experience that the models give very s imilar results,\\neven when LDA is used inappropriately, such as with qualitat ive predictors.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 146}),\n",
       " Document(page_content='4.5 Separating Hyperplanes 129\\nFIGURE 4.14. A toy example with two classes separable by a hyperplane. The\\norange line is the least squares solution, which misclassiﬁes on e of the training\\npoints. Also shown are two blue separating hyperplanes found by theperceptron\\nlearning algorithm with diﬀerent random starts.\\n4.5 Separating Hyperplanes\\nWe have seen that linear discriminant analysis and logistic regression both\\nestimate linear decision boundaries in similar but slightl y diﬀerent ways.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 147}),\n",
       " Document(page_content='For the rest of this chapter we describe separating hyperpla ne classiﬁers.\\nThese procedures construct linear decision boundaries tha t explicitly try\\nto separate the data into diﬀerent classes as well as possibl e. They provide\\nthe basis for support vector classiﬁers, discussed in Chapt er 12. The math-\\nematical level of this section is somewhat higher than that o f the previous\\nsections.\\nFigure 4.14 shows 20 data points in two classes in IR2. These data can be', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 147}),\n",
       " Document(page_content='separated by a linear boundary. Included in the ﬁgure (blue l ines) are two\\nof the inﬁnitely many possible separating hyperplanes . The orange line is\\nthe least squares solution to the problem, obtained by regre ssing the−1/1\\nresponseYonX(with intercept); the line is given by\\n{x:ˆβ0+ˆβ1x1+ˆβ2x2= 0}. (4.39)\\nThis least squares solution does not do a perfect job in separ ating the\\npoints, and makes one error. This is the same boundary found b y LDA,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 147}),\n",
       " Document(page_content='in light of its equivalence with linear regression in the two -class case (Sec-\\ntion 4.3 and Exercise 4.2).\\nClassiﬁers such as (4.39), that compute a linear combinatio n of the input\\nfeaturesandreturnthesign,werecalled perceptrons intheengineeringliter-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 147}),\n",
       " Document(page_content='130 4. Linear Methods for Classiﬁcation\\nx0x\\nβ∗β0+βTx= 0\\nFIGURE 4.15. The linear algebra of a hyperplane (aﬃne set).\\nature in the late 1950s (Rosenblatt, 1958). Perceptrons set the foundations\\nfor the neural network models of the 1980s and 1990s.\\nBeforewecontinue,letusdigressslightlyandreviewsomev ectoralgebra.\\nFigure 4.15 depicts a hyperplane or aﬃne setLdeﬁned by the equation\\nf(x) =β0+βTx= 0; since we are in IR2this is a line.\\nHere we list some properties:', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 148}),\n",
       " Document(page_content='1. For any two points x1andx2lying inL,βT(x1−x2) = 0, and hence\\nβ∗=β/||β||is the vector normal to the surface of L.\\n2. For any point x0inL,βTx0=−β0.\\n3. The signed distance of any point xtoLis given by\\nβ∗T(x−x0) =1\\n∥β∥(βTx+β0)\\n=1\\n||f′(x)||f(x). (4.40)\\nHencef(x) is proportional to the signed distance from xto the hyperplane\\ndeﬁned byf(x) = 0.\\n4.5.1 Rosenblatt’s Perceptron Learning Algorithm\\nTheperceptron learning algorithm tries to ﬁnd a separating hyperplane by', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 148}),\n",
       " Document(page_content='minimizing the distance of misclassiﬁed points to the decis ion boundary. If', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 148}),\n",
       " Document(page_content='4.5 Separating Hyperplanes 131\\na responseyi= 1 is misclassiﬁed, then xT\\niβ+β0<0, and the opposite for\\na misclassiﬁed response with yi=−1. The goal is to minimize\\nD(β,β0) =−∑\\ni∈Myi(xT\\niβ+β0), (4.41)\\nwhereMindexes the set of misclassiﬁed points. The quantity is non-\\nnegative and proportional to the distance of the misclassiﬁ ed points to\\nthe decision boundary deﬁned by βTx+β0= 0. The gradient (assuming\\nMis ﬁxed) is given by\\n∂D(β,β0)\\n∂β=−∑\\ni∈Myixi, (4.42)\\n∂D(β,β0)\\n∂β0=−∑\\ni∈Myi. (4.43)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 149}),\n",
       " Document(page_content='The algorithm in fact uses stochastic gradient descent to minimize this\\npiecewise linear criterion. This means that rather than com puting the sum\\nof the gradient contributions of each observation followed by a step in the\\nnegative gradient direction, a step is taken after each obse rvation is visited.\\nHence the misclassiﬁed observations are visited in some seq uence, and the\\nparameters βare updated via\\n(\\nβ\\nβ0)\\n←(\\nβ\\nβ0)\\n+ρ(\\nyixi\\nyi)\\n. (4.44)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 149}),\n",
       " Document(page_content='Hereρis the learning rate, which in this case can be taken to be 1 wit hout\\nloss in generality. If the classes are linearly separable, i t can be shown that\\nthe algorithm converges to a separating hyperplane in a ﬁnit e number of\\nsteps (Exercise 4.6). Figure 4.14 shows two solutions to a to y problem, each\\nstarted at a diﬀerent random guess.\\nThere are a number of problems with this algorithm, summariz ed in\\nRipley (1996):\\n•When the data are separable, there are many solutions, and wh ich', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 149}),\n",
       " Document(page_content='one is found depends on the starting values.\\n•The “ﬁnite” number of steps can be very large. The smaller the gap,\\nthe longer the time to ﬁnd it.\\n•When the data are not separable, the algorithm will not conve rge,\\nand cycles develop. The cycles can be long and therefore hard to\\ndetect.\\nThe second problem can often be eliminated by seeking a hyper plane not\\nin the original space, but in a much enlarged space obtained b y creating', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 149}),\n",
       " Document(page_content='132 4. Linear Methods for Classiﬁcation\\nmany basis-function transformations of the original varia bles. This is anal-\\nogous to driving the residuals in a polynomial regression pr oblem down\\nto zero by making the degree suﬃciently large. Perfect separ ation cannot\\nalways be achieved: for example, if observations from two di ﬀerent classes\\nshare the same input. It may not be desirable either, since th e resulting\\nmodel is likely to be overﬁt and will not generalize well. We r eturn to this', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 150}),\n",
       " Document(page_content='point at the end of the next section.\\nA rather elegant solution to the ﬁrst problem is to add additi onal con-\\nstraints to the separating hyperplane.\\n4.5.2 Optimal Separating Hyperplanes\\nTheoptimal separating hyperplane separates the two classes and maximizes\\nthe distance to the closest point from either class (Vapnik, 1996). Not only\\ndoes this provide a unique solution to the separating hyperp lane problem,\\nbutbymaximizingthemarginbetweenthetwoclassesonthetr ainingdata,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 150}),\n",
       " Document(page_content='this leads to better classiﬁcation performance on test data .\\nWeneedtogeneralizecriterion(4.41).Considertheoptimi zationproblem\\nmax\\nβ,β0,||β||=1M\\nsubject toyi(xT\\niβ+β0)≥M, i= 1,...,N.(4.45)\\nThe set of conditions ensure that all the points are at least a signed\\ndistanceMfrom the decision boundary deﬁned by βandβ0, and we seek\\nthe largest such Mand associated parameters. We can get rid of the ||β||=\\n1 constraint by replacing the conditions with\\n1\\n||β||yi(xT\\niβ+β0)≥M, (4.46)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 150}),\n",
       " Document(page_content='(which redeﬁnes β0) or equivalently\\nyi(xT\\niβ+β0)≥M||β||. (4.47)\\nSince for any βandβ0satisfying these inequalities, any positively scaled\\nmultiple satisﬁes them too, we can arbitrarily set ||β||= 1/M. Thus (4.45)\\nis equivalent to\\nmin\\nβ,β01\\n2||β||2\\nsubject toyi(xT\\niβ+β0)≥1, i= 1,...,N.(4.48)\\nIn light of (4.40), the constraints deﬁne an empty slab or mar gin around the\\nlinear decision boundary of thickness 1 /||β||. Hence we choose βandβ0to', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 150}),\n",
       " Document(page_content='maximize its thickness. This is a convex optimization probl em (quadratic', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 150}),\n",
       " Document(page_content='4.5 Separating Hyperplanes 133\\ncriterion with linear inequality constraints). The Lagran ge (primal) func-\\ntion, to be minimized w.r.t. βandβ0, is\\nLP=1\\n2||β||2−N∑\\ni=1αi[yi(xT\\niβ+β0)−1]. (4.49)\\nSetting the derivatives to zero, we obtain:\\nβ=N∑\\ni=1αiyixi, (4.50)\\n0 =N∑\\ni=1αiyi, (4.51)\\nand substituting these in (4.49) we obtain the so-called Wol fe dual\\nLD=N∑\\ni=1αi−1\\n2N∑\\ni=1N∑\\nk=1αiαkyiykxT\\nixk\\nsubject toαi≥0 andN∑\\ni=1αiyi= 0. (4.52)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 151}),\n",
       " Document(page_content='The solution is obtained by maximizing LDin the positive orthant, a sim-\\npler convex optimization problem, for which standard softw are can be used.\\nIn addition the solution must satisfy the Karush–Kuhn–Tuck er conditions,\\nwhich include (4.50), (4.51), (4.52) and\\nαi[yi(xT\\niβ+β0)−1] = 0∀i. (4.53)\\nFrom these we can see that\\n•ifαi>0, thenyi(xT\\niβ+β0) = 1, or in other words, xiis on the\\nboundary of the slab;\\n•ifyi(xT\\niβ+β0)>1,xiis not on the boundary of the slab, and αi= 0.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 151}),\n",
       " Document(page_content='From (4.50) we see that the solution vector βis deﬁned in terms of a linear\\ncombination of the support points xi—those points deﬁned to be on the\\nboundary of the slab via αi>0. Figure 4.16 shows the optimal separating\\nhyperplane for our toy example; there are three support poin ts. Likewise,\\nβ0is obtained by solving (4.53) for any of the support points.\\nThe optimal separating hyperplane produces a function ˆf(x) =xTˆβ+ˆβ0\\nfor classifying new observations:\\nˆG(x) = signˆf(x). (4.54)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 151}),\n",
       " Document(page_content='Although none of the training observations fall in the margi n (by con-\\nstruction), this will not necessarily be the case for test ob servations. The', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 151}),\n",
       " Document(page_content='134 4. Linear Methods for Classiﬁcation\\nFIGURE 4.16. The same data as in Figure 4.14. The shaded region delineates\\nthe maximum margin separating the two classes. There are thre e support points\\nindicated, which lie on the boundary of the margin, and the opt imal separating\\nhyperplane (blue line) bisects the slab. Included in the ﬁgure is t he boundary found\\nusing logistic regression (red line), which is very close to the optimal separating\\nhyperplane (see Section 12.3.3).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 152}),\n",
       " Document(page_content='intuition is that a large margin on the training data will lea d to good\\nseparation on the test data.\\nThe description of the solution in terms of support points se ems to sug-\\ngest that the optimal hyperplane focuses more on the points t hat count,\\nand is more robust to model misspeciﬁcation. The LDA solutio n, on the\\nother hand, depends on all of the data, even points far away fr om the de-\\ncision boundary. Note, however, that the identiﬁcation of t hese support', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 152}),\n",
       " Document(page_content='points required the use of all the data. Of course, if the clas ses are really\\nGaussian, then LDA is optimal, and separating hyperplanes w ill pay a price\\nfor focusing on the (noisier) data at the boundaries of the cl asses.\\nIncluded in Figure 4.16 is the logistic regression solution to this prob-\\nlem, ﬁt by maximum likelihood. Both solutions are similar in this case.\\nWhen a separating hyperplane exists, logistic regression w ill always ﬁnd', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 152}),\n",
       " Document(page_content='it, since the log-likelihood can be driven to 0 in this case (E xercise 4.5).\\nThe logistic regression solution shares some other qualita tive features with\\nthe separating hyperplane solution. The coeﬃcient vector i s deﬁned by a\\nweighted least squares ﬁt of a zero-mean linearized respons e on the input\\nfeatures, and the weights are larger for points near the deci sion boundary\\nthan for those further away.\\nWhen the data are not separable, there will be no feasible sol ution to', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 152}),\n",
       " Document(page_content='this problem, and an alternative formulation is needed. Aga in one can en-\\nlarge the space using basis transformations, but this can le ad to artiﬁcial', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 152}),\n",
       " Document(page_content='Exercises 135\\nseparation through over-ﬁtting. In Chapter 12 we discuss a m ore attractive\\nalternative known as the support vector machine , which allows for overlap,\\nbut minimizes a measure of the extent of this overlap.\\nBibliographic Notes\\nGood general texts on classiﬁcation include Duda et al. (200 0), Hand\\n(1981), McLachlan (1992) and Ripley (1996). Mardia et al. (1 979) have\\na concise discussion of linear discriminant analysis. Mich ie et al. (1994)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 153}),\n",
       " Document(page_content='compare a large number of popular classiﬁers on benchmark da tasets. Lin-\\near separating hyperplanes are discussed in Vapnik (1996). Our account of\\nthe perceptron learning algorithm follows Ripley (1996).\\nExercises\\nEx. 4.1Show how to solve the generalized eigenvalue problem max aTBa\\nsubject toaTWa= 1 by transforming to a standard eigenvalue problem.\\nEx. 4.2Suppose we have features x∈IRp, a two-class response, with class\\nsizesN1,N2, and the target coded as −N/N1,N/N2.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 153}),\n",
       " Document(page_content='(a) Show that the LDA rule classiﬁes to class 2 if\\nxTˆΣ−1(ˆµ2−ˆµ1)>1\\n2(ˆµ2+ ˆµ1)TˆΣ−1(ˆµ2−ˆµ1)−log(N2/N1),\\nand class 1 otherwise.\\n(b) Consider minimization of the least squares criterion\\nN∑\\ni=1(yi−β0−xT\\niβ)2. (4.55)\\nShow that the solution ˆβsatisﬁes\\n[\\n(N−2)ˆΣ+NˆΣB]\\nβ=N(ˆµ2−ˆµ1) (4.56)\\n(after simpliﬁcation), where ˆΣB=N1N2\\nN2(ˆµ2−ˆµ1)(ˆµ2−ˆµ1)T.\\n(c) Hence show that ˆΣBβis in the direction (ˆ µ2−ˆµ1) and thus\\nˆβ∝ˆΣ−1(ˆµ2−ˆµ1). (4.57)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 153}),\n",
       " Document(page_content='Therefore the least-squares regression coeﬃcient is ident ical to the\\nLDA coeﬃcient, up to a scalar multiple.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 153}),\n",
       " Document(page_content='136 4. Linear Methods for Classiﬁcation\\n(d) Show that this result holds for any (distinct) coding of t he two classes.\\n(e) Find the solution ˆβ0(up to the same scalar multiple as in (c), and\\nhence the predicted value ˆf(x) =ˆβ0+xTˆβ. Consider the following\\nrule: classify to class 2 if ˆf(x)>0 and class 1 otherwise. Show this is\\nnot the same as the LDA rule unless the classes have equal numb ers\\nof observations.\\n(Fisher, 1936; Ripley, 1996)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 154}),\n",
       " Document(page_content='Ex. 4.3Suppose we transform the original predictors XtoˆYvia linear\\nregression. In detail, let ˆY=X(XTX)−1XTY=XˆB, where Yis the\\nindicator response matrix. Similarly for any input x∈IRp, we get a trans-\\nformed vector ˆ y=ˆBTx∈IRK. Show that LDA using ˆYis identical to\\nLDA in the original space.\\nEx. 4.4Consider the multilogit model with Kclasses (4.17). Let βbe the\\n(p+ 1)(K−1)-vector consisting of all the coeﬃcients. Deﬁne a suitabl y', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 154}),\n",
       " Document(page_content='enlarged version of the input vector xto accommodate this vectorized co-\\neﬃcient matrix. Derive the Newton-Raphson algorithm for ma ximizing the\\nmultinomial log-likelihood, and describe how you would imp lement this\\nalgorithm.\\nEx. 4.5Consider a two-class logistic regression problem with x∈IR. Char-\\nacterize the maximum-likelihood estimates of the slope and intercept pa-\\nrameterifthesample xiforthetwoclassesareseparatedbyapoint x0∈IR.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 154}),\n",
       " Document(page_content='Generalize this result to (a) x∈IRp(see Figure 4.16), and (b) more than\\ntwo classes.\\nEx. 4.6Suppose we have Npointsxiin IRpin general position, with class\\nlabelsyi∈{−1,1}. Prove that the perceptron learning algorithm converges\\nto a separating hyperplane in a ﬁnite number of steps:\\n(a) Denote a hyperplane by f(x) =βT\\n1x+β0= 0, or in more compact\\nnotationβTx∗= 0, where x∗= (x,1) andβ= (β1,β0). Letzi=\\nx∗\\ni/||x∗\\ni||. Show that separability implies the existence of a βsepsuch\\nthatyiβT\\nsepzi≥1∀i', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 154}),\n",
       " Document(page_content='(b)Givenacurrent βold,theperceptronalgorithmidentiﬁesapoint zithat\\nis misclassiﬁed, and produces the update βnew←βold+yizi. Show\\nthat||βnew−βsep||2≤||βold−βsep||2−1, and hence that the algorithm\\nconverges to a separating hyperplane in no more than ||βstart−βsep||2\\nsteps (Ripley, 1996).\\nEx. 4.7Consider the criterion\\nD∗(β,β0) =−N∑\\ni=1yi(xT\\niβ+β0), (4.58)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 154}),\n",
       " Document(page_content='Exercises 137\\na generalization of (4.41) where we sum over all the observat ions. Consider\\nminimizing D∗subject to||β||= 1. Describe this criterion in words. Does\\nit solve the optimal separating hyperplane problem?\\nEx. 4.8Consider the multivariate Gaussian model X|G=k∼N(µk,Σ),\\nwith the additional restriction that rank {µk}K\\n1=L <max(K−1,p).\\nDerive the constrained MLEs for the µkandΣ. Show that the Bayes clas-\\nsiﬁcation rule is equivalent to classifying in the reduced s ubspace computed', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 155}),\n",
       " Document(page_content='by LDA (Hastie and Tibshirani, 1996b).\\nEx. 4.9Write a computer program to perform a quadratic discriminan t\\nanalysis by ﬁtting a separate Gaussian model per class. Try i t out on the\\nvowel data, and compute the misclassiﬁcation error for the t est data. The\\ndatacanbefoundinthebookwebsite www-stat.stanford.edu/ElemStatLearn .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 155}),\n",
       " Document(page_content='138 4. Linear Methods for Classiﬁcation', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 156}),\n",
       " Document(page_content='This is page 139\\nPrinter: Opaque this\\n5\\nBasis Expansions and Regularization\\n5.1 Introduction\\nWe have already made use of models linear in the input feature s, both for\\nregression and classiﬁcation. Linear regression, linear d iscriminant analysis,\\nlogistic regression and separating hyperplanes all rely on a linear model.\\nIt is extremely unlikely that the true function f(X) is actually linear in\\nX. In regression problems, f(X) = E(Y|X) will typically be nonlinear and', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 157}),\n",
       " Document(page_content='nonadditive in X, and representing f(X) by a linear model is usually a con-\\nvenient, and sometimes a necessary, approximation. Conven ient because a\\nlinear model is easy to interpret, and is the ﬁrst-order Tayl or approxima-\\ntion tof(X). Sometimes necessary, because with Nsmall and/or plarge,\\na linear model might be all we are able to ﬁt to the data without overﬁt-\\nting. Likewise in classiﬁcation, a linear, Bayes-optimal d ecision boundary', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 157}),\n",
       " Document(page_content='implies that some monotone transformation of Pr( Y= 1|X) is linear in X.\\nThis is inevitably an approximation.\\nIn this chapter and the next we discuss popular methods for mo ving\\nbeyond linearity. The core idea in this chapter is to augment /replace the\\nvector of inputs Xwith additional variables, which are transformations of\\nX, and then use linear models in this new space of derived input features.\\nDenote by hm(X) : IRp↦→IR themth transformation of X,m=\\n1,...,M. We then model\\nf(X) =M∑', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 157}),\n",
       " Document(page_content='m=1βmhm(X), (5.1)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 157}),\n",
       " Document(page_content='140 5. Basis Expansions and Regularization\\nalinear basis expansion inX. The beauty of this approach is that once the\\nbasis functions hmhave been determined, the models are linear in these\\nnew variables, and the ﬁtting proceeds as before.\\nSome simple and widely used examples of the hmare the following:\\n•hm(X) =Xm, m= 1,...,precovers the original linear model.\\n•hm(X) =X2\\njorhm(X) =XjXkallowsustoaugmenttheinputswith\\npolynomial terms to achieve higher-order Taylor expansion s. Note,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 158}),\n",
       " Document(page_content='however, that the number of variables grows exponentially i n the de-\\ngree of the polynomial. A full quadratic model in pvariables requires\\nO(p2) square and cross-product terms, or more generally O(pd) for a\\ndegree-dpolynomial.\\n•hm(X) = log(Xj),√\\nXj,...permits other nonlinear transformations\\nof single inputs. More generally one can use similar functio ns involv-\\ning several inputs, such as hm(X) =||X||.\\n•hm(X) =I(Lm≤Xk<Um), an indicator for a region of Xk. By', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 158}),\n",
       " Document(page_content='breaking the range of Xkup intoMksuch nonoverlapping regions\\nresults in a model with a piecewise constant contribution fo rXk.\\nSometimestheproblemathandwillcallforparticularbasis functionshm,\\nsuchaslogarithmsorpowerfunctions.Moreoften,however, weusethebasis\\nexpansions as a device to achieve more ﬂexible representati ons forf(X).\\nPolynomials are an example of the latter, although they are l imited by\\ntheir global nature—tweaking the coeﬃcients to achieve a fun ctional form', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 158}),\n",
       " Document(page_content='in one region can cause the function to ﬂap about madly in remo te regions.\\nIn this chapter we consider more useful families of piecewise-polynomials\\nandsplinesthat allow for local polynomial representations. We also di scuss\\nthewaveletbases, especially useful for modeling signals and images. T hese\\nmethods produce a dictionaryDconsisting of typically a very large number\\n|D|of basis functions, far more than we can aﬀord to ﬁt to our data . Along', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 158}),\n",
       " Document(page_content='with the dictionary we require a method for controlling the c omplexity\\nof our model, using basis functions from the dictionary. The re are three\\ncommon approaches:\\n•Restriction methods, where we decide before-hand to limit t he class\\nof functions. Additivity is an example, where we assume that our\\nmodel has the form\\nf(X) =p∑\\nj=1fj(Xj)\\n=p∑\\nj=1Mj∑\\nm=1βjmhjm(Xj). (5.2)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 158}),\n",
       " Document(page_content='5.2 Piecewise Polynomials and Splines 141\\nThe size of the model is limited by the number of basis functio nsMj\\nused for each component function fj.\\n•Selection methods, which adaptively scan the dictionary an d include\\nonly those basis functions hmthat contribute signiﬁcantly to the ﬁt of\\nthe model. Here the variable selection techniques discusse d in Chap-\\nter 3 are useful. The stagewise greedy approaches such as CAR T,\\nMARS and boosting fall into this category as well.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 159}),\n",
       " Document(page_content='•Regularization methods where we use the entire dictionary b ut re-\\nstrict the coeﬃcients. Ridge regression is a simple example of a regu-\\nlarization approach, while the lasso is both a regularizati on and selec-\\ntion method. Here we discuss these and more sophisticated me thods\\nfor regularization.\\n5.2 Piecewise Polynomials and Splines\\nWe assume until Section 5.7 that Xis one-dimensional. A piecewise poly-\\nnomial function f(X) is obtained by dividing the domain of Xinto contigu-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 159}),\n",
       " Document(page_content='ous intervals, and representing fby a separate polynomial in each interval.\\nFigure 5.1 shows two simple piecewise polynomials. The ﬁrst is piecewise\\nconstant, with three basis functions:\\nh1(X) =I(X <ξ 1), h2(X) =I(ξ1≤X <ξ 2), h3(X) =I(ξ2≤X).\\nSince these are positive over disjoint regions, the least sq uares estimate of\\nthe modelf(X) =∑3\\nm=1βmhm(X) amounts to ˆβm=¯Ym, the mean of Y\\nin themth region.\\nThe top right panel shows a piecewise linear ﬁt. Three additi onal basis', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 159}),\n",
       " Document(page_content='functions are needed: hm+3=hm(X)X, m= 1,...,3. Except in special\\ncases, we would typically prefer the third panel, which is al so piecewise\\nlinear, but restricted to be continuous at the two knots. The se continu-\\nity restrictions lead to linear constraints on the paramete rs; for example,\\nf(ξ−\\n1) =f(ξ+\\n1) implies that β1+ξ1β4=β2+ξ1β5. In this case, since there\\nare two restrictions, we expect to get backtwo parameters, leaving four free\\nparameters.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 159}),\n",
       " Document(page_content='A more direct way to proceed in this case is to use a basis that i ncorpo-\\nrates the constraints:\\nh1(X) = 1, h2(X) =X, h 3(X) = (X−ξ1)+, h4(X) = (X−ξ2)+,\\nwheret+denotes the positive part. The function h3is shown in the lower\\nright panel of Figure 5.1. We often prefer smoother function s, and these\\ncan be achieved by increasing the order of the local polynomi al. Figure 5.2\\nshows a series of piecewise-cubic polynomials ﬁt to the same data, with', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 159}),\n",
       " Document(page_content='142 5. Basis Expansions and Regularization\\nOO\\nOOO\\nO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOO\\nO\\nO\\nOO\\nO\\nOOPiecewise Constant\\nOO\\nOOO\\nO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOO\\nO\\nO\\nOO\\nO\\nOOPiecewise Linear\\nOO\\nOOO\\nO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOO\\nO\\nO\\nOO\\nO\\nOOContinuous Piecewise Linear Piecewise-linear Basis Function\\n•\\n•••\\n••••\\n••\\n•• • ••\\n••••\\n••\\n•••\\n•\\n•••\\n•\\n•••\\n••\\n•••\\n• ••••\\n• ••\\n•\\n••\\n••\\nξ1 ξ1ξ1 ξ1\\nξ2 ξ2ξ2 ξ2\\n(X−ξ1)+', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 160}),\n",
       " Document(page_content='FIGURE 5.1. The top left panel shows a piecewise constant function ﬁt to so me\\nartiﬁcial data. The broken vertical lines indicate the posit ions of the two knots\\nξ1andξ2. The blue curve represents the true function, from which the d ata were\\ngenerated with Gaussian noise. The remaining two panels show p iecewise lin-\\near functions ﬁt to the same data—the top right unrestricted , and the lower left\\nrestricted to be continuous at the knots. The lower right pane l shows a piecewise–', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 160}),\n",
       " Document(page_content='linear basis function, h3(X) = (X−ξ1)+, continuous at ξ1. The black points\\nindicate the sample evaluations h3(xi), i= 1,...,N.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 160}),\n",
       " Document(page_content='5.2 Piecewise Polynomials and Splines 143\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOOO\\nO\\nOO\\nOOODiscontinuous\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOOO\\nO\\nOO\\nOOOContinuous\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOOO\\nO\\nOO\\nOOOContinuous First Derivative\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOOO\\nO\\nOO\\nOOOContinuous Second DerivativePiecewise Cubic Polynomials\\nξ1 ξ1ξ1 ξ1\\nξ2 ξ2ξ2 ξ2', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 161}),\n",
       " Document(page_content='FIGURE 5.2. A series of piecewise-cubic polynomials, with increasing ord ers of\\ncontinuity.\\nincreasing orders of continuity at the knots. The function i n the lower\\nright panel is continuous, and has continuous ﬁrst and secon d derivatives\\nat the knots. It is known as a cubic spline . Enforcing one more order of\\ncontinuity would lead to a global cubic polynomial. It is not hard to show\\n(Exercise 5.1) that the following basis represents a cubic s pline with knots\\natξ1andξ2:', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 161}),\n",
       " Document(page_content='h1(X) = 1, h3(X) =X2, h5(X) = (X−ξ1)3\\n+,\\nh2(X) =X, h 4(X) =X3, h6(X) = (X−ξ2)3\\n+.(5.3)\\nTherearesixbasisfunctionscorrespondingtoasix-dimens ionallinearspace\\nof functions. A quick check conﬁrms the parameter count: (3 r egions)×(4\\nparameters per region) −(2 knots)×(3 constraints per knot)= 6.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 161}),\n",
       " Document(page_content='144 5. Basis Expansions and Regularization\\nMore generally, an order- Mspline with knots ξj, j= 1,...,Kis a\\npiecewise-polynomial of order M, and has continuous derivatives up to\\norderM−2. A cubic spline has M= 4. In fact the piecewise-constant\\nfunction in Figure 5.1 is an order-1 spline, while the contin uous piece-\\nwise linear function is an order-2 spline. Likewise the gene ral form for the\\ntruncated-power basis set would be\\nhj(X) =Xj−1, j= 1,...,M,\\nhM+ℓ(X) = (X−ξℓ)M−1\\n+, ℓ= 1,...,K.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 162}),\n",
       " Document(page_content='It is claimed that cubic splines are the lowest-order spline for which the\\nknot-discontinuity is not visible to the human eye. There is seldom any\\ngood reason to go beyond cubic-splines, unless one is intere sted in smooth\\nderivatives. In practice the most widely used orders are M= 1,2 and 4.\\nThese ﬁxed-knot splines are also known as regression splines . One needs\\nto select the order of the spline, the number of knots and thei r placement.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 162}),\n",
       " Document(page_content='One simple approach is to parameterize a family of splines by the number\\nof basis functions or degrees of freedom, and have the observ ationsxide-\\ntermine the positions of the knots. For example, the express ionbs(x,df=7)\\ninRgenerates a basis matrix of cubic-spline functions evaluat ed at theN\\nobservations in x, with the 7−3 = 41interior knots at the appropriate per-\\ncentilesof x(20,40,60and80th.)Onecanbemoreexplicit,however; bs(x,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 162}),\n",
       " Document(page_content='degree=1, knots = c(0.2, 0.4, 0.6)) generates a basis for linear splines,\\nwith three interior knots, and returns an N×4 matrix.\\nSincethespaceofsplinefunctionsofaparticularorderand knotsequence\\nis a vector space, there are many equivalent bases for repres enting them\\n(just as there are for ordinary polynomials.) While the trun cated power\\nbasis is conceptually simple, it is not too attractive numer ically: powers of\\nlarge numbers can lead to severe rounding problems. The B-spline basis,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 162}),\n",
       " Document(page_content='described in the Appendix to this chapter, allows for eﬃcien t computations\\neven when the number of knots Kis large.\\n5.2.1 Natural Cubic Splines\\nWe know that the behavior of polynomials ﬁt to data tends to be erratic\\nnear the boundaries, and extrapolation can be dangerous. Th ese problems\\nare exacerbated with splines. The polynomials ﬁt beyond the boundary\\nknots behave even more wildly than the corresponding global polynomials', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 162}),\n",
       " Document(page_content='in that region. This can be conveniently summarized in terms of the point-\\nwise variance of spline functions ﬁt by least squares (see th e example in the\\nnext section for details on these variance calculations). F igure 5.3 compares\\n1A cubic spline with four knots is eight-dimensional. The bs()function omits by\\ndefault the constant term in the basis, since terms like this are typical ly included with\\nother terms in the model.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 162}),\n",
       " Document(page_content='5.2 Piecewise Polynomials and Splines 145\\nXPointwise Variances\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5 0.6•\\n••\\n•\\n••\\n•••••••••••••• •••• ••••••••••••••••••••••••••\\n•••••\\n•••••••••••••••••••••••••••••••••••••••••••\\n•\\n••\\n•\\n••••••••••••••••••••••••••\\n•\\n••••••••••••••••••\\n•••••••••••••••••••• ••••••••••••••••••••••••• ••••Global Linear\\nGlobal Cubic Polynomial\\nCubic Spline - 2 knots\\nNatural Cubic Spline - 6 knots\\nFIGURE 5.3. Pointwise variance curves for four diﬀerent models, with Xcon-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 163}),\n",
       " Document(page_content='sisting of 50points drawn at random from U[0,1], and an assumed error model\\nwith constant variance. The linear and cubic polynomial ﬁts ha ve two and four\\ndegrees of freedom, respectively, while the cubic spline and na tural cubic spline\\neach have six degrees of freedom. The cubic spline has two knot s at0.33and0.66,\\nwhile the natural spline has boundary knots at 0.1and0.9, and four interior knots\\nuniformly spaced between them.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 163}),\n",
       " Document(page_content='the pointwise variances for a variety of diﬀerent models. Th e explosion of\\nthe variance near the boundaries is clear, and inevitably is worst for cubic\\nsplines.\\nAnatural cubic spline adds additional constraints, namely that the func-\\ntion is linear beyond the boundary knots. This frees up four d egrees of\\nfreedom (two constraints each in both boundary regions), wh ich can be\\nspent more proﬁtably by sprinkling more knots in the interio r region. This', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 163}),\n",
       " Document(page_content='tradeoﬀ is illustrated in terms of variance in Figure 5.3. Th ere will be a\\nprice paid in bias near the boundaries, but assuming the func tion is lin-\\near near the boundaries (where we have less information anyw ay) is often\\nconsidered reasonable.\\nA natural cubic spline with Kknots is represented by Kbasis functions.\\nOne can start from a basis for cubic splines, and derive the re duced ba-\\nsis by imposing the boundary constraints. For example, star ting from the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 163}),\n",
       " Document(page_content='truncated power series basis described in Section 5.2, we ar rive at (Exer-\\ncise 5.4):\\nN1(X) = 1, N2(X) =X, N k+2(X) =dk(X)−dK−1(X),(5.4)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 163}),\n",
       " Document(page_content='146 5. Basis Expansions and Regularization\\nwhere\\ndk(X) =(X−ξk)3\\n+−(X−ξK)3\\n+\\nξK−ξk. (5.5)\\nEach of these basis functions can be seen to have zero second a nd third\\nderivative for X≥ξK.\\n5.2.2 Example: South African Heart Disease (Continued)\\nIn Section 4.4.2 we ﬁt linear logistic regression models to t he South African\\nheart disease data. Here we explore nonlinearities in the fu nctions using\\nnatural splines. The functional form of the model is', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 164}),\n",
       " Document(page_content='logit[Pr(chd|X)] =θ0+h1(X1)Tθ1+h2(X2)Tθ2+···+hp(Xp)Tθp,(5.6)\\nwhere each of the θjare vectors of coeﬃcients multiplying their associated\\nvector of natural spline basis functions hj.\\nWe use four natural spline bases for each term in the model. Fo r example,\\nwithX1representing sbp,h1(X1) is a basis consisting of four basis func-\\ntions. This actually implies three rather than two interior knots (chosen at\\nuniform quantiles of sbp), plus two boundary knots at the extremes of the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 164}),\n",
       " Document(page_content='data, since we exclude the constant term from each of the hj.\\nSincefamhist is a two-level factor, it is coded by a simple binary or\\ndummy variable, and is associated with a single coeﬃcient in the ﬁt of the\\nmodel.\\nMore compactly we can combine all pvectors of basis functions (and\\nthe constant term) into one big vector h(X), and then the model is simply\\nh(X)Tθ, with total number of parameters df = 1 +∑p\\nj=1dfj, the sum of\\nthe parameters in each component term. Each basis function i s evaluated', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 164}),\n",
       " Document(page_content='at each of the Nsamples, resulting in a N×df basis matrix H. At this\\npoint the model is like any other linear logistic model, and t he algorithms\\ndescribed in Section 4.4.1 apply.\\nWe carried out a backward stepwise deletion process, droppi ng terms\\nfrom this model while preserving the group structure of each term, rather\\nthan dropping one coeﬃcient at a time. The AIC statistic (Sec tion 7.5) was\\nused to drop terms, and all the terms remaining in the ﬁnal mod el would', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 164}),\n",
       " Document(page_content='cause AIC to increase if deleted from the model (see Table 5.1 ). Figure 5.4\\nshows a plot of the ﬁnal model selected by the stepwise regres sion. The\\nfunctions displayed are ˆfj(Xj) =hj(Xj)Tˆθjfor each variable Xj. The\\ncovariance matrix Cov( ˆθ) =Σis estimated by ˆΣ= (HTWH)−1, whereW\\nis the diagonal weight matrix from the logistic regression. Hencevj(Xj) =\\nVar[ˆfj(Xj)] =hj(Xj)TˆΣjjhj(Xj) is the pointwise variance function of ˆfj,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 164}),\n",
       " Document(page_content='whereCov( ˆθj) =ˆΣjjistheappropriatesub-matrixof ˆΣ.Theshadedregion\\nin each panel is deﬁned by ˆfj(Xj)±2√\\nvj(Xj).\\nThe AIC statistic is slightly more generous than the likelih ood-ratio test\\n(deviance test). Both sbpandobesityare included in this model, while', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 164}),\n",
       " Document(page_content='5.2 Piecewise Polynomials and Splines 147\\n100 120 140 160 180 200 220-2 0 2 4\\n0 5 10 15 20 25 300 2 4 6 8\\n2 4 6 8 10 12 14-4 -2 0 2 4\\n-4 -2 0 2 4\\nAbsent Present\\n15 20 25 30 35 40 45-2 0 2 4 6\\n20 30 40 50 60-6 -4 -2 0 2ˆf(sbp)\\nsbp\\nˆf(tobacco)\\ntobaccoˆf(ldl)\\nldlˆf(obesity)\\nobesity\\nˆf(age)\\nageˆf(famhist)\\nfamhist\\nFIGURE 5.4. Fitted natural-spline functions for each of the terms in the ﬁn al\\nmodel selected by the stepwise procedure. Included are pointw ise standard-error', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 165}),\n",
       " Document(page_content='bands. The rug plot at the base of each ﬁgure indicates the location of each of the\\nsample values for that variable (jittered to break ties).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 165}),\n",
       " Document(page_content='148 5. Basis Expansions and Regularization\\nTABLE 5.1. Final logistic regression model, after stepwise deletion of na tural\\nsplines terms. The column labeled “LRT” is the likelihood-ratio te st statistic when\\nthat term is deleted from the model, and is the change in devianc e from the full\\nmodel (labeled “none”).\\nTerms Df Deviance AIC LRT P-value\\nnone 458.09 502.09\\nsbp4 467.16 503.16 9.076 0.059\\ntobacco 4 470.48 506.48 12.387 0.015\\nldl4 472.39 508.39 14.307 0.006', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 166}),\n",
       " Document(page_content='famhist 1 479.44 521.44 21.356 0.000\\nobesity 4 466.24 502.24 8.147 0.086\\nage4 481.86 517.86 23.768 0.000\\nthey were not in the linear model. The ﬁgure explains why, sin ce their\\ncontributions are inherently nonlinear. These eﬀects at ﬁr st may come as\\na surprise, but an explanation lies in the nature of the retro spective data.\\nThese measurements were made sometime after the patients su ﬀered a\\nheart attack, and in many cases they had already beneﬁted fro m a healthier', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 166}),\n",
       " Document(page_content='diet and lifestyle, hence the apparent increase in risk at low values for\\nobesityandsbp. Table 5.1 shows a summary of the selected model.\\n5.2.3 Example: Phoneme Recognition\\nIn this example we use splines to reduce ﬂexibility rather th an increase it;\\nthe application comes under the general heading of functional modeling. In\\nthe top panel of Figure 5.5 are displayed a sample of 15 log-pe riodograms\\nfor each of the two phonemes “aa” and “ao” measured at 256 freq uencies.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 166}),\n",
       " Document(page_content='The goal is to use such data to classify a spoken phoneme. Thes e two\\nphonemes were chosen because they are diﬃcult to separate.\\nThe input feature is a vector xof length 256, which we can think of as\\na vector of evaluations of a function X(f) over a grid of frequencies f. In\\nreality there is a continuous analog signal which is a functi on of frequency,\\nand we have a sampled version of it.\\nThe gray lines in the lower panel of Figure 5.5 show the coeﬃci ents of', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 166}),\n",
       " Document(page_content='a linear logistic regression model ﬁt by maximum likelihood to a training\\nsample of 1000 drawn from the total of 695 “aa”s and 1022 “ao”s . The\\ncoeﬃcients are also plotted as a function of frequency, and i n fact we can\\nthink of the model in terms of its continuous counterpart\\nlogPr(aa|X)\\nPr(ao|X)=∫\\nX(f)β(f)df, (5.7)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 166}),\n",
       " Document(page_content='5.2 Piecewise Polynomials and Splines 149\\nFrequencyLog-periodogram\\n0 50 100 150 200 2500 5 10 15 20 25Phoneme Examples\\naa\\nao\\nFrequencyLogistic Regression Coefficients\\n0 50 100 150 200 250-0.4 -0.2 0.0 0.2 0.4Phoneme Classification: Raw and Restricted Logistic Regression\\nFIGURE 5.5. The top panel displays the log-periodogram as a function of fre -\\nquency for 15examples each of the phonemes “aa” and “ao” sampled from a total', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 167}),\n",
       " Document(page_content='of695“aa”s and 1022“ao”s. Each log-periodogram is measured at 256uniformly\\nspaced frequencies. The lower panel shows the coeﬃcients (as a function of fre-\\nquency) of a logistic regression ﬁt to the data by maximum likeli hood, using the\\n256log-periodogram values as inputs. The coeﬃcients are restric ted to be smooth\\nin the red curve, and are unrestricted in the jagged gray curv e.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 167}),\n",
       " Document(page_content='150 5. Basis Expansions and Regularization\\nwhich we approximate by\\n256∑\\nj=1X(fj)β(fj) =256∑\\nj=1xjβj. (5.8)\\nThe coeﬃcients compute a contrast functional, and will have appreciable\\nvalues in regions of frequency where the log-periodograms d iﬀer between\\nthe two classes.\\nThe gray curves are very rough. Since the input signals have f airly strong\\npositive autocorrelation, this results in negative autoco rrelation in the co-\\neﬃcients. In addition the sample size eﬀectively provides o nly four obser-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 168}),\n",
       " Document(page_content='vations per coeﬃcient.\\nApplications such as this permit a natural regularization. We force the\\ncoeﬃcientstovarysmoothlyasafunctionoffrequency.Ther edcurveinthe\\nlower panel of Figure 5.5 shows such a smooth coeﬃcient curve ﬁt to these\\ndata.Weseethatthelowerfrequenciesoﬀerthemostdiscrim inatorypower.\\nNot only does the smoothing allow easier interpretation of t he contrast, it\\nalso produces a more accurate classiﬁer:\\nRawRegularized\\nTraining error 0.080 0.185\\nTest error 0.255 0.158', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 168}),\n",
       " Document(page_content='The smooth red curve was obtained through a very simple use of natural\\ncubic splines. We can represent the coeﬃcient function as an expansion of\\nsplinesβ(f) =∑M\\nm=1hm(f)θm. In practice this means that β=Hθwhere,\\nHis ap×Mbasis matrix of natural cubic splines, deﬁned on the set of\\nfrequencies. Here we used M= 12 basis functions, with knots uniformly\\nplaced over the integers 1 ,2,...,256 representing the frequencies. Since\\nxTβ=xTHθ, we can simply replace the input features xby their ﬁltered', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 168}),\n",
       " Document(page_content='versionsx∗=HTx, and ﬁtθby linear logistic regression on the x∗. The\\nred curve is thus ˆβ(f) =h(f)Tˆθ.\\n5.3 Filtering and Feature Extraction\\nIn the previous example, we constructed a p×Mbasis matrix H, and then\\ntransformed our features xinto new features x∗=HTx. These ﬁltered\\nversions of the features were then used as inputs into a learn ing procedure:\\nin the previous example, this was linear logistic regressio n.\\nPreprocessing of high-dimensional features is a very gener al and pow-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 168}),\n",
       " Document(page_content='erful method for improving the performance of a learning alg orithm. The\\npreprocessing need not be linear as it was above, but can be a g eneral', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 168}),\n",
       " Document(page_content='5.4 Smoothing Splines 151\\n(nonlinear) function of the form x∗=g(x). The derived features x∗can\\nthen be used as inputs into any (linear or nonlinear) learnin g procedure.\\nForexample,forsignalorimagerecognitionapopularappro achistoﬁrst\\ntransform the raw features via a wavelet transform x∗=HTx(Section 5.9)\\nand then use the features x∗as inputs into a neural network (Chapter 11).\\nWavelets are eﬀective in capturing discrete jumps or edges, and the neural', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 169}),\n",
       " Document(page_content='network is a powerful tool for constructing nonlinear funct ions of these\\nfeatures for predicting the target variable. By using domai n knowledge\\nto construct appropriate features, one can often improve up on a learning\\nmethod that has only the raw features xat its disposal.\\n5.4 Smoothing Splines\\nHere we discuss a spline basis method that avoids the knot sel ection prob-\\nlem completely by using a maximal set of knots. The complexit y of the ﬁt', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 169}),\n",
       " Document(page_content='is controlled by regularization. Consider the following pr oblem: among all\\nfunctionsf(x) with two continuous derivatives, ﬁnd one that minimizes th e\\npenalized residual sum of squares\\nRSS(f,λ) =N∑\\ni=1{yi−f(xi)}2+λ∫\\n{f′′(t)}2dt, (5.9)\\nwhereλis a ﬁxed smoothing parameter . The ﬁrst term measures closeness\\nto the data, while the second term penalizes curvature in the function, and\\nλestablishes a tradeoﬀ between the two. Two special cases are :', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 169}),\n",
       " Document(page_content='λ= 0 :fcan be any function that interpolates the data.\\nλ=∞:the simple least squares line ﬁt, since no second derivative can\\nbe tolerated.\\nThesevaryfromveryroughtoverysmooth,andthehopeisthat λ∈(0,∞)\\nindexes an interesting class of functions in between.\\nThe criterion (5.9) is deﬁned on an inﬁnite-dimensional fun ction space—\\nin fact, a Sobolev space of functions for which the second ter m is deﬁned.\\nRemarkably, it can be shown that (5.9) has an explicit, ﬁnite -dimensional,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 169}),\n",
       " Document(page_content='unique minimizer which is a natural cubic spline with knots a t the unique\\nvalues of the xi, i= 1,...,N(Exercise 5.7). At face value it seems that\\nthe family is still over-parametrized, since there are as ma ny asNknots,\\nwhich implies Ndegrees of freedom. However, the penalty term translates\\nto a penalty on the spline coeﬃcients, which are shrunk some o f the way\\ntoward the linear ﬁt.\\nSince the solution is a natural spline, we can write it as\\nf(x) =N∑\\nj=1Nj(x)θj, (5.10)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 169}),\n",
       " Document(page_content='152 5. Basis Expansions and Regularization\\nAgeRelative Change in Spinal BMD\\n10 15 20 25-0.05 0.0 0.05 0.10 0.15 0.20••\\n••\\n••\\n••\\n•\\n••\\n••\\n••\\n••\\n•\\n•\\n•\\n••••\\n••\\n•\\n••\\n•\\n•\\n••\\n•••\\n••\\n•••\\n•••\\n•\\n•\\n•\\n••\\n•\\n••••\\n•••••\\n••\\n•••••\\n•\\n•\\n••\\n•••\\n••\\n•\\n••\\n•••\\n•\\n••\\n••••\\n•\\n•••\\n••\\n•••••\\n•\\n••\\n••\\n•\\n•\\n••••\\n• ••• ••••\\n••\\n••\\n•••\\n•\\n••\\n•••••\\n•\\n•••••\\n••\\n•••\\n•\\n• •••\\n•••\\n••••\\n••\\n••••\\n• •\\n••••\\n••• •\\n••••\\n•\\n•••\\n•\\n••••••\\n••\\n•\\n••\\n•\\n•\\n••\\n••\\n•\\n••\\n•\\n• •••\\n•\\n•\\n••••••\\n•\\n••\\n••\\n••••\\n•\\n••\\n••\\n•\\n••••\\n••\\n•\\n•••\\n••\\n•\\n•••\\n•\\n• •••\\n• ••\\n•••••\\n•\\n••', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 170}),\n",
       " Document(page_content='•••\\n•\\n••••\\n••\\n•\\n••\\n•••\\n•\\n•••\\n•\\n••\\n•••••\\n••\\n•\\n••••••\\n•\\n•\\n•••\\n• ••\\n•••••••\\n•\\n•••\\n••\\n•\\n•••\\n•\\n•••\\n••\\n••\\n••\\n••\\n•• ••\\n•\\n••••\\n•\\n•\\n•••\\n••\\n• •\\n•••\\n•\\n•••\\n• •\\n••\\n•\\n•••\\n••••\\n•\\n••••\\n••\\n•\\n••\\n••••\\n••••\\n•\\n••\\n•\\n•\\n••••\\n•\\n••\\n•\\n••••\\n• ••\\n•\\n••••\\n•\\n••\\n•••\\n•\\n••\\n•••\\n•\\n•\\n••\\n••\\n•\\n•\\n••\\n••\\n••\\n•••\\n•••\\n•\\n••\\n•\\n•••\\n•\\n•\\n•••\\n••\\n••\\n••\\n•\\n•••\\n•••\\n• ••••\\n••\\n••\\n••\\n•\\n•Male\\nFemale\\nFIGURE 5.6. The response is the relative change in bone mineral density me a-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 170}),\n",
       " Document(page_content='sured at the spine in adolescents, as a function of age. A separ ate smoothing spline\\nwas ﬁt to the males and females, with λ≈0.00022. This choice corresponds to\\nabout12degrees of freedom.\\nwhere the Nj(x) are anN-dimensional set of basis functions for repre-\\nsenting this family of natural splines (Section 5.2.1 and Ex ercise 5.4). The\\ncriterion thus reduces to\\nRSS(θ,λ) = (y−Nθ)T(y−Nθ)+λθTΩNθ, (5.11)\\nwhere{N}ij=Nj(xi) and{ΩN}jk=∫\\nN′′\\nj(t)N′′\\nk(t)dt. The solution is\\neasily seen to be', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 170}),\n",
       " Document(page_content='ˆθ= (NTN+λΩN)−1NTy, (5.12)\\na generalized ridge regression. The ﬁtted smoothing spline is given by\\nˆf(x) =N∑\\nj=1Nj(x)ˆθj. (5.13)\\nEﬃcient computational techniques for smoothing splines ar e discussed in\\nthe Appendix to this chapter.\\nFigure 5.6 shows a smoothing spline ﬁt to some data on bone min eral\\ndensity (BMD) in adolescents. The response is relative chan ge in spinal\\nBMD over two consecutive visits, typically about one year ap art. The data', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 170}),\n",
       " Document(page_content='are color coded by gender, and two separate curves were ﬁt. Th is simple', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 170}),\n",
       " Document(page_content='5.4 Smoothing Splines 153\\nsummary reinforces the evidence in the data that the growth s purt for\\nfemales precedes that for males by about two years. In both ca ses the\\nsmoothing parameter λwas approximately 0 .00022; this choice is discussed\\nin the next section.\\n5.4.1 Degrees of Freedom and Smoother Matrices\\nWe have not yet indicated how λis chosen for the smoothing spline. Later\\nin this chapter we describe automatic methods using techniq ues such as', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 171}),\n",
       " Document(page_content='cross-validation. In this section we discuss intuitive way s of prespecifying\\nthe amount of smoothing.\\nA smoothing spline with prechosen λis an example of a linear smoother\\n(as in linear operator). This is because the estimated param eters in (5.12)\\nare a linear combination of the yi. Denote by ˆftheN-vector of ﬁtted values\\nˆf(xi) at the training predictors xi. Then\\nˆf=N(NTN+λΩN)−1NTy\\n=Sλy. (5.14)\\nAgain the ﬁt is linear in y, and the ﬁnite linear operator Sλis known as', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 171}),\n",
       " Document(page_content='thesmoother matrix . One consequence of this linearity is that the recipe\\nfor producing ˆffromydoes not depend on yitself;Sλdepends only on\\nthexiandλ.\\nLinear operators are familiar in more traditional least squ ares ﬁtting as\\nwell. Suppose Bξis aN×Mmatrix ofMcubic-spline basis functions\\nevaluated at the Ntraining points xi, with knot sequence ξ, andM≪N.\\nThen the vector of ﬁtted spline values is given by\\nˆf=Bξ(BT\\nξBξ)−1BT\\nξy\\n=Hξy. (5.15)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 171}),\n",
       " Document(page_content='Here the linear operator Hξis a projection operator, also known as the hat\\nmatrix in statistics. There are some important similaritie s and diﬀerences\\nbetween HξandSλ:\\n•Both are symmetric, positive semideﬁnite matrices.\\n•HξHξ=Hξ(idempotent), while SλSλ⪯Sλ, meaning that the right-\\nhand side exceeds the left-hand side by a positive semideﬁni te matrix.\\nThis is a consequence of the shrinking nature of Sλ, which we discuss\\nfurther below.\\n•Hξhas rankM, whileSλhas rankN.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 171}),\n",
       " Document(page_content='The expression M= trace(Hξ) gives the dimension of the projection space,\\nwhich is also the number of basis functions, and hence the num ber of pa-\\nrameters involved in the ﬁt. By analogy we deﬁne the eﬀective degrees of', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 171}),\n",
       " Document(page_content='154 5. Basis Expansions and Regularization\\nfreedomof a smoothing spline to be\\ndfλ= trace(Sλ), (5.16)\\nthe sum of the diagonal elements of Sλ. This very useful deﬁnition allows\\nus a more intuitive way to parameterize the smoothing spline , and indeed\\nmany other smoothers as well, in a consistent fashion. For ex ample, in Fig-\\nure 5.6 we speciﬁed df λ= 12 for each of the curves, and the corresponding\\nλ≈0.00022 was derived numerically by solving trace( Sλ) = 12. There are', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 172}),\n",
       " Document(page_content='many arguments supporting this deﬁnition of degrees of free dom, and we\\ncover some of them here.\\nSinceSλis symmetric (and positive semideﬁnite), it has a real eigen -\\ndecomposition. Before we proceed, it is convenient to rewri teSλin the\\nReinschform\\nSλ= (I+λK)−1, (5.17)\\nwhereKdoes not depend on λ(Exercise 5.9). Since ˆf=Sλysolves\\nmin\\nf(y−f)T(y−f)+λfTKf, (5.18)\\nKis known as the penalty matrix , and indeed a quadratic form in Khas', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 172}),\n",
       " Document(page_content='a representation in terms of a weighted sum of squared (divid ed) second\\ndiﬀerences. The eigen-decomposition of Sλis\\nSλ=N∑\\nk=1ρk(λ)ukuT\\nk (5.19)\\nwith\\nρk(λ) =1\\n1+λdk, (5.20)\\nanddkthe corresponding eigenvalue of K. Figure 5.7 (top) shows the re-\\nsults of applying a cubic smoothing spline to some air pollut ion data (128\\nobservations). Two ﬁts are given: a smoother ﬁt corresponding to a larger\\npenaltyλand arougherﬁt for a smaller penalty. The lower panels repre-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 172}),\n",
       " Document(page_content='sent the eigenvalues (lower left) and some eigenvectors (lo wer right) of the\\ncorresponding smoother matrices. Some of the highlights of the eigenrep-\\nresentation are the following:\\n•Theeigenvectorsarenotaﬀectedbychangesin λ,andhencethewhole\\nfamily of smoothing splines (for a particular sequence x) indexed by\\nλhave the same eigenvectors.\\n•Sλy=∑N\\nk=1ukρk(λ)⟨uk,y⟩, and hence the smoothing spline oper-\\nates by decomposing yw.r.t. the (complete) basis {uk}, and diﬀer-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 172}),\n",
       " Document(page_content='entially shrinking the contributions using ρk(λ). This is to be con-\\ntrasted with a basis-regression method, where the componen ts are', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 172}),\n",
       " Document(page_content='5.4 Smoothing Splines 155\\nDaggot Pressure GradientOzone Concentration\\n-50 0 50 1000 10 20 30•••\\n••••••\\n••\\n••••••\\n•\\n•••\\n•\\n•••\\n••\\n•••\\n•\\n••\\n•\\n••\\n•••\\n•\\n••\\n••\\n••\\n••\\n••••\\n•••••\\n•\\n•••\\n•••\\n••\\n•\\n••\\n•••\\n••\\n••\\n•••\\n••••\\n••\\n••\\n•••\\n•\\n••••\\n••\\n••\\n••\\n•••\\n•••\\n•••\\n••\\n•\\n••\\n••••\\n••\\n•••\\n•\\n•\\n•\\n•\\nOrderEigenvalues\\n5 10 15 20 25-0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2•••\\n•\\n•\\n•\\n••••••••••• •• • • •• • ••••••••\\n•\\n•\\n•\\n•\\n•\\n•••••••••••••df=5\\ndf=11\\n-50 0 50 100 -50 0 50 100', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 173}),\n",
       " Document(page_content='FIGURE 5.7. (Top:) Smoothing spline ﬁt of ozone concentration versus Dag got\\npressure gradient. The two ﬁts correspond to diﬀerent values of the smoothing\\nparameter, chosen to achieve ﬁve and eleven eﬀective degrees of freedom, deﬁned\\nby dfλ=trace(Sλ). (Lower left:) First 25eigenvalues for the two smoothing-spline\\nmatrices. The ﬁrst two are exactly 1, and all are ≥0. (Lower right:) Third to\\nsixth eigenvectors of the spline smoother matrices. In each c ase,ukis plotted', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 173}),\n",
       " Document(page_content='againstx, and as such is viewed as a function of x. Therugat the base of the\\nplots indicate the occurrence of data points. The damped func tions represent the\\nsmoothed versions of these functions (using the 5df smoother).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 173}),\n",
       " Document(page_content='156 5. Basis Expansions and Regularization\\neither left alone, or shrunk to zero—that is, a projection mat rix such\\nasHξabove hasMeigenvalues equal to 1, and the rest are 0. For\\nthis reason smoothing splines are referred to as shrinking smoothers,\\nwhile regression splines are projection smoothers (see Figure 3.17 on\\npage 80).\\n•The sequence of uk, ordered by decreasing ρk(λ), appear to increase\\nincomplexity. Indeed,theyhave thezero-crossingbehavio r ofpolyno-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 174}),\n",
       " Document(page_content='mials of increasing degree. Since Sλuk=ρk(λ)uk, we see how each of\\nthe eigenvectors themselves are shrunk by the smoothing spl ine: the\\nhigher the complexity, the more they are shrunk. If the domai n ofX\\nis periodic, then the ukare sines and cosines at diﬀerent frequencies.\\n•The ﬁrst two eigenvalues are alwaysone, and they correspond to the\\ntwo-dimensional eigenspace of functions linear in x(Exercise 5.11),\\nwhich are never shrunk.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 174}),\n",
       " Document(page_content='•The eigenvalues ρk(λ) = 1/(1 +λdk) are an inverse function of the\\neigenvalues dkof the penalty matrix K, moderated by λ;λcontrols\\nthe rate at which the ρk(λ) decrease to zero. d1=d2= 0 and again\\nlinear functions are not penalized.\\n•One can reparametrize the smoothing spline using the basis v ectors\\nuk(theDemmler–Reinsch basis). In this case the smoothing spline\\nsolves\\nmin\\nθ∥y−Uθ∥2+λθTDθ, (5.21)\\nwhereUhas columns ukandDis a diagonal matrix with elements\\ndk.\\n•dfλ= trace( Sλ) =∑N', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 174}),\n",
       " Document(page_content='k=1ρk(λ). For projection smoothers, all the\\neigenvalues are 1, each one corresponding to a dimension of t he pro-\\njection subspace.\\nFigure 5.8 depicts a smoothing spline matrix, with the rows o rdered with\\nx. The banded nature of this representation suggests that a sm oothing\\nspline is a local ﬁtting method, much like the locally weight ed regression\\nprocedures in Chapter 6. The right panel shows in detail sele cted rows of\\nS, which we call the equivalent kernels . Asλ→0, dfλ→N, andSλ→I,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 174}),\n",
       " Document(page_content='theN-dimensional identity matrix. As λ→∞, dfλ→2, andSλ→H, the\\nhat matrix for linear regression on x.\\n5.5 Automatic Selection of the Smoothing\\nParameters\\nThe smoothing parameters for regression splines encompass the degree of\\nthe splines, and the number and placement of the knots. For sm oothing', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 174}),\n",
       " Document(page_content='5.5 Automatic Selection of the Smoothing Parameters 157\\n11510075502512Smoother Matrix\\n••••• • •••••• •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\\n•Row 115••••• ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 175}),\n",
       " Document(page_content='•Row 100••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••Row 75•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• • •Row 50•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• • •Row 25••••••', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 175}),\n",
       " Document(page_content='•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• • •Row 12Equivalent Kernels', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 175}),\n",
       " Document(page_content='FIGURE 5.8. The smoother matrix for a smoothing spline is nearly banded,\\nindicating an equivalent kernel with local support. The left pa nel represents the\\nelements of Sas an image. The right panel shows the equivalent kernel or wei ght-\\ning function in detail for the indicated rows.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 175}),\n",
       " Document(page_content='158 5. Basis Expansions and Regularization\\nsplines, we have only the penalty parameter λto select, since the knots are\\nat all the unique training X’s, and cubic degree is almost always used in\\npractice.\\nSelectingtheplacementandnumberofknotsforregressions plinescanbe\\na combinatorially complex task, unless some simpliﬁcation s are enforced.\\nThe MARS procedure in Chapter 9 uses a greedy algorithm with s ome\\nadditional approximations to achieve a practical compromi se. We will not', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 176}),\n",
       " Document(page_content='discuss this further here.\\n5.5.1 Fixing the Degrees of Freedom\\nSince df λ= trace(Sλ) is monotone in λfor smoothing splines, we can in-\\nvert the relationship and specify λby ﬁxing df. In practice this can be\\nachieved by simple numerical methods. So, for example, in Rone can use\\nsmooth.spline(x,y,df=6) to specify the amount of smoothing. This encour-\\nages a more traditional mode of model selection, where we mig ht try a cou-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 176}),\n",
       " Document(page_content='ple of diﬀerent values of df, and select one based on approxim ateF-tests,\\nresidual plots and other more subjective criteria. Using df in this way pro-\\nvides a uniform approach to compare many diﬀerent smoothing methods.\\nIt is particularly useful in generalized additive models (Chapter 9), where\\nseveral smoothing methods can be simultaneously used in one model.\\n5.5.2 The Bias–Variance Tradeoﬀ\\nFigure 5.9 shows the eﬀect of the choice of df λwhen using a smoothing\\nspline on a simple example:', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 176}),\n",
       " Document(page_content='Y=f(X)+ε,\\nf(X) =sin(12(X+0.2))\\nX+0.2,(5.22)\\nwithX∼U[0,1] andε∼N(0,1). Our training sample consists of N= 100\\npairsxi,yidrawn independently from this model.\\nThe ﬁtted splines for three diﬀerent values of df λare shown. The yellow\\nshaded region in the ﬁgure represents the pointwise standar d error of ˆfλ,\\nthat is, we have shaded the region between ˆfλ(x)±2·se(ˆfλ(x)). Since\\nˆf=Sλy,\\nCov(ˆf) =SλCov(y)ST\\nλ\\n=SλST\\nλ. (5.23)\\nThe diagonal contains the pointwise variances at the traini ngxi. The bias', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 176}),\n",
       " Document(page_content='is given by\\nBias(ˆf) =f−E(ˆf)\\n=f−Sλf, (5.24)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 176}),\n",
       " Document(page_content='5.5 Automatic Selection of the Smoothing Parameters 159\\n6 8 10 12 141.0 1.1 1.2 1.3 1.4 1.5\\n0.0 0.2 0.4 0.6 0.8 1.0−4 −2 0 2yO\\nO\\nO\\nOO\\nOO\\nO\\nOOO\\nOO\\nO\\nOO\\nOOOO\\nOO\\nOOOOO\\nOOO\\nO\\nOO\\nOO\\nOO\\nOO\\nO\\nO\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOOO\\nOOOO\\nOO\\nO\\nOO\\nOOOOOO\\nO\\nOOO\\nOOO\\nOO\\nOO\\nOOOOO\\nOOOO\\nOO\\n0.0 0.2 0.4 0.6 0.8 1.0−4 −2 0 2yO\\nO\\nO\\nOO\\nOO\\nO\\nOOO\\nOO\\nO\\nOO\\nOOOO\\nOO\\nOOOOO\\nOOO\\nO\\nOO\\nOO\\nOO\\nOO\\nO\\nO\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOOO\\nOOOO\\nOO\\nO\\nOO\\nOOOOOO\\nO\\nOOO\\nOOO\\nOO\\nOO\\nOOOOO\\nOOOO\\nOO\\n0.0 0.2 0.4 0.6 0.8 1.0−4 −2 0 2yO\\nO\\nO\\nOO\\nOO\\nO\\nOOO\\nOO', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 177}),\n",
       " Document(page_content='O\\nOO\\nOOOO\\nOO\\nOOOOO\\nOOO\\nO\\nOO\\nOO\\nOO\\nOO\\nO\\nO\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOOO\\nOOOO\\nOO\\nO\\nOO\\nOOOOOO\\nO\\nOOO\\nOOO\\nOO\\nOO\\nOOOOO\\nOOOO\\nOOEPECV\\nX XXdfλ= 5\\ndfλ= 9 dfλ= 15dfλCross-ValidationEPE(λ) and CV( λ)\\nFIGURE 5.9. The top left panel shows the EPE(λ)andCV(λ)curves for a\\nrealization from a nonlinear additive error model (5.22). The remaining panels\\nshow the data, the true functions (in purple), and the ﬁtted cu rves (in green) with\\nyellow shaded ±2×standard error bands, for three diﬀerent values of dfλ.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 177}),\n",
       " Document(page_content='160 5. Basis Expansions and Regularization\\nwherefis the (unknown) vector of evaluations of the true fat the training\\nX’s. The expectations and variances are with respect to repea ted draws\\nof samples of size N= 100 from the model (5.22). In a similar fashion\\nVar(ˆfλ(x0)) and Bias( ˆfλ(x0)) can be computed at any point x0(Exer-\\ncise 5.10). The three ﬁts displayed in the ﬁgure give a visual demonstration\\nof the bias-variance tradeoﬀ associated with selecting the smoothing\\nparameter.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 178}),\n",
       " Document(page_content='dfλ= 5:The spline under ﬁts, and clearly trims down the hills and ﬁlls in\\nthe valleys . This leads to a bias that is most dramatic in regions of\\nhigh curvature. The standard error band is very narrow, so we esti-\\nmate a badly biased version of the true function with great re liability!\\ndfλ= 9:Here the ﬁtted function is close to the true function, althou gh a\\nslight amount of bias seems evident. The variance has not inc reased\\nappreciably.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 178}),\n",
       " Document(page_content='dfλ= 15:The ﬁtted function is somewhat wiggly, but close to the true\\nfunction. The wiggliness also accounts for the increased wi dth of the\\nstandard error bands—the curve is starting to follow some ind ividual\\npoints too closely.\\nNote that in these ﬁgures we are seeing a single realization o f data and\\nhence ﬁtted spline ˆfin each case, while the bias involves an expectation\\nE(ˆf). We leave it as an exercise (5.10) to compute similar ﬁgures where the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 178}),\n",
       " Document(page_content='bias is shown as well. The middle curve seems “just right,” in that it has\\nachieved a good compromise between bias and variance.\\nThe integrated squared prediction error (EPE) combines bot h bias and\\nvariance in a single summary:\\nEPE(ˆfλ) = E(Y−ˆfλ(X))2\\n= Var(Y)+E[\\nBias2(ˆfλ(X))+Var( ˆfλ(X))]\\n=σ2+MSE( ˆfλ). (5.25)\\nNote that this is averaged both over the training sample (giv ing rise to ˆfλ),\\nand the values of the (independently chosen) prediction poi nts (X,Y). EPE', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 178}),\n",
       " Document(page_content='is a natural quantity of interest, and does create a tradeoﬀ b etween bias\\nand variance. The blue points in the top left panel of Figure 5 .9 suggest\\nthat df λ= 9 is spot on!\\nSincewedon’tknowthetruefunction,wedonothaveaccessto EPE,and\\nneed an estimate. This topic is discussed in some detail in Ch apter 7, and\\ntechniques such as K-fold cross-validation, GCV and Cpare all in common\\nuse. In Figure 5.9 we include the N-fold (leave-one-out) cross-validation\\ncurve:', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 178}),\n",
       " Document(page_content='5.6 Nonparametric Logistic Regression 161\\nCV(ˆfλ) =1\\nNN∑\\ni=1(yi−ˆf(−i)\\nλ(xi))2(5.26)\\n=1\\nNN∑\\ni=1(\\nyi−ˆfλ(xi)\\n1−Sλ(i,i))2\\n, (5.27)\\nwhich can (remarkably) be computed for each value of λfrom the original\\nﬁtted values and the diagonal elements Sλ(i,i) ofSλ(Exercise 5.13).\\nThe EPE and CV curves have a similar shape, but the entire CV cu rve\\nis above the EPE curve. For some realizations this is reverse d, and overall\\nthe CV curve is approximately unbiased as an estimate of the E PE curve.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 179}),\n",
       " Document(page_content='5.6 Nonparametric Logistic Regression\\nThe smoothing spline problem (5.9) in Section 5.4 is posed in a regression\\nsetting. It is typically straightforward to transfer this t echnology to other\\ndomains. Here we consider logistic regression with a single quantitative\\ninputX. The model is\\nlogPr(Y= 1|X=x)\\nPr(Y= 0|X=x)=f(x), (5.28)\\nwhich implies\\nPr(Y= 1|X=x) =ef(x)\\n1+ef(x). (5.29)\\nFittingf(x) in a smooth fashion leads to a smooth estimate of the condi-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 179}),\n",
       " Document(page_content='tional probability Pr( Y= 1|x), which can be used for classiﬁcation or risk\\nscoring.\\nWe construct the penalized log-likelihood criterion\\nℓ(f;λ) =N∑\\ni=1[yilogp(xi)+(1−yi)log(1−p(xi))]−1\\n2λ∫\\n{f′′(t)}2dt\\n=N∑\\ni=1[\\nyif(xi)−log(1+ef(xi))]\\n−1\\n2λ∫\\n{f′′(t)}2dt,(5.30)\\nwhere we have abbreviated p(x) = Pr(Y= 1|x). The ﬁrst term in this ex-\\npression is the log-likelihood based on the binomial distri bution (c.f. Chap-\\nter 4, page 120). Arguments similar to those used in Section 5 .4 show that', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 179}),\n",
       " Document(page_content='the optimal fis aﬁnite-dimensional natural spline with knots at the uniq ue', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 179}),\n",
       " Document(page_content='162 5. Basis Expansions and Regularization\\nvalues ofx. This means that we can represent f(x) =∑N\\nj=1Nj(x)θj. We\\ncompute the ﬁrst and second derivatives\\n∂ℓ(θ)\\n∂θ=NT(y−p)−λΩθ, (5.31)\\n∂2ℓ(θ)\\n∂θ∂θT=−NTWN−λΩ, (5.32)\\nwherepis theN-vector with elements p(xi), andWis a diagonal matrix\\nof weightsp(xi)(1−p(xi)). The ﬁrst derivative (5.31) is nonlinear in θ, so\\nwe need to use an iterative algorithm as in Section 4.4.1. Usi ng Newton–', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 180}),\n",
       " Document(page_content='Raphson as in (4.23) and (4.26) for linear logistic regressi on, the update\\nequation can be written\\nθnew= (NTWN+λΩ)−1NTW(\\nNθold+W−1(y−p))\\n= (NTWN+λΩ)−1NTWz. (5.33)\\nWe can also express this update in terms of the ﬁtted values\\nfnew=N(NTWN+λΩ)−1NTW(\\nfold+W−1(y−p))\\n=Sλ,wz. (5.34)\\nReferring back to (5.12) and (5.14), we see that the update ﬁt s a weighted\\nsmoothing spline to the working response z(Exercise 5.12).\\nThe form of (5.34) is suggestive. It is tempting to replace Sλ,wby any', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 180}),\n",
       " Document(page_content='nonparametric (weighted) regression operator, and obtain general fami-\\nlies of nonparametric logistic regression models. Althoug h herexis one-\\ndimensional, this procedure generalizes naturally to high er-dimensional x.\\nThese extensions are at the heart of generalized additive models , which we\\npursue in Chapter 9.\\n5.7 Multidimensional Splines\\nSo far we have focused on one-dimensional spline models. Eac h of the ap-\\nproaches have multidimensional analogs. Suppose X∈IR2, and we have', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 180}),\n",
       " Document(page_content='a basis of functions h1k(X1), k= 1,...,M 1for representing functions of\\ncoordinate X1, and likewise a set of M2functionsh2k(X2) for coordinate\\nX2. Then the M1×M2dimensional tensor product basis deﬁned by\\ngjk(X) =h1j(X1)h2k(X2), j= 1,...,M 1, k= 1,...,M 2(5.35)\\ncan be used for representing a two-dimensional function:\\ng(X) =M1∑\\nj=1M2∑\\nk=1θjkgjk(X). (5.36)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 180}),\n",
       " Document(page_content='5.7 Multidimensional Splines 163\\nFIGURE 5.10. A tensor product basis of B-splines, showing some selected pai rs.\\nEach two-dimensional function is the tensor product of the c orresponding one\\ndimensional marginals.\\nFigure 5.10 illustrates a tensor product basis using B-spli nes. The coeﬃ-\\ncients can be ﬁt by least squares, as before. This can be gener alized tod\\ndimensions, but note that the dimension of the basis grows ex ponentially', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 181}),\n",
       " Document(page_content='fast—yet another manifestation of the curse of dimensionali ty. The MARS\\nprocedure discussed in Chapter 9 is a greedy forward algorit hm for includ-\\ning only those tensor products that are deemed necessary by l east squares.\\nFigure5.11 illustrates thediﬀerencebetween additive and tensor product\\n(natural) splines on the simulated classiﬁcation example f rom Chapter 2.\\nA logistic regression model logit[Pr( T|x)] =h(x)Tθis ﬁt to the binary re-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 181}),\n",
       " Document(page_content='sponse, and the estimated decision boundary is the contour h(x)Tˆθ= 0.\\nThe tensor product basis can achieve more ﬂexibility at the d ecision bound-\\nary, but introduces some spurious structure along the way.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 181}),\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "file_path = '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf'\n",
    "\n",
    "loader = PyPDFLoader(file_path=file_path)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "data = loader.load_and_split(text_splitter=text_splitter)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4362f102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Springer Series in Statistics\\nTrevor Hastie\\nRobert TibshiraniJerome FriedmanSpringer Series in Statistics\\nThe Elements of\\nStatistical Learning\\nData Mining, Inference, and Prediction\\nThe Elements of Statistical LearningDuring the past decade there has been an explosion in computation and information tech-'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ae6ecb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (1.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c582b114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6be26f23f446dd882cde73f4001cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(show_progress_bar=True)\n",
    "\n",
    "vector1 = embeddings.embed_query('How are you?')\n",
    "\n",
    "len(vector1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e3d6e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4bb91b674e41428319e93bdc267d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1059f8b26243dcbf3c1c72f67df9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8819345646753848"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "    return np.dot(vec1,vec2)/(norm(vec1)*norm(vec2))\n",
    "    \n",
    "vector1 = embeddings.embed_query('machine learning')\n",
    "vector2 = embeddings.embed_query('artificial intelligence')\n",
    "cosine = get_cosine(vector1, vector2)\n",
    "cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d15940a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71e86724c9946219ab3c46192c60fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7401281950537992"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector3 = embeddings.embed_query('peperoni pizza')\n",
    "cosine = get_cosine(vector2, vector3)\n",
    "cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b142511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b81b29524b2420dbe0b8b9020f8dc20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "index = FAISS.from_documents(data, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6fa1f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d4c237d4e94b1581d3c68ccc1fc502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='This is page 1\\nPrinter: Opaque this\\n1\\nIntroduction\\nStatistical learning plays a key role in many areas of science, ﬁnance and\\nindustry. Here are some examples of learning problems:\\n•Predict whether a patient, hospitalized due to a heart attac k, will\\nhave a second heart attack. The prediction is to be based on de mo-\\ngraphic, diet and clinical measurements for that patient.\\n•Predict the price of a stock in 6 months from now, on the basis o f\\ncompany performance measures and economic data.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 19}),\n",
       "  0.7547787193298542),\n",
       " (Document(page_content='This is page 389\\nPrinter: Opaque this\\n11\\nNeural Networks\\n11.1 Introduction\\nIn this chapter we describe a class of learning methods that w as developed\\nseparately in diﬀerent ﬁelds—statistics and artiﬁcial inte lligence—based\\non essentially identical models. The central idea is to extr act linear com-\\nbinations of the inputs as derived features, and then model t he target as\\na nonlinear function of these features. The result is a power ful learning', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 407}),\n",
       "  0.7512151611414096),\n",
       " (Document(page_content='While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples are given, with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book’s coverage is broad, from supervised learning(prediction) to unsupervised learning. The many topics include neural networks, supportvector machines, classification trees and boosting—the first comprehensive treatment of thistopic', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 0}),\n",
       "  0.750451607760362),\n",
       " (Document(page_content='The challenges in learning from data have led to a revolution in the sta-\\ntisticalsciences.Sincecomputationplayssuchakeyrole, itisnotsurprising\\nthat much of this new development has been done by researcher s in other\\nﬁelds such as computer science and engineering.\\nThe learning problems that we consider can be roughly catego rized as\\neithersupervised orunsupervised . In supervised learning, the goal is to pre-\\ndict the value of an outcome measure based on a number of input measures;', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 7}),\n",
       "  0.7482908863511782)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.similarity_search_with_relevance_scores(\n",
    "    \"What is machine learning?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "972168c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4407a957d4a24559b7b43ba9235b88d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "This is page 1\n",
      "Printer: Opaque this\n",
      "1\n",
      "Introduction\n",
      "Statistical learning plays a key role in many areas of science, ﬁnance and\n",
      "industry. Here are some examples of learning problems:\n",
      "•Predict whether a patient, hospitalized due to a heart attac k, will\n",
      "have a second heart attack. The prediction is to be based on de mo-\n",
      "graphic, diet and clinical measurements for that patient.\n",
      "•Predict the price of a stock in 6 months from now, on the basis o f\n",
      "company performance measures and economic data.\n",
      "\n",
      "This is page 389\n",
      "Printer: Opaque this\n",
      "11\n",
      "Neural Networks\n",
      "11.1 Introduction\n",
      "In this chapter we describe a class of learning methods that w as developed\n",
      "separately in diﬀerent ﬁelds—statistics and artiﬁcial inte lligence—based\n",
      "on essentially identical models. The central idea is to extr act linear com-\n",
      "binations of the inputs as derived features, and then model t he target as\n",
      "a nonlinear function of these features. The result is a power ful learning\n",
      "\n",
      "While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples are given, with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book’s coverage is broad, from supervised learning(prediction) to unsupervised learning. The many topics include neural networks, supportvector machines, classification trees and boosting—the first comprehensive treatment of thistopic\n",
      "\n",
      "The challenges in learning from data have led to a revolution in the sta-\n",
      "tisticalsciences.Sincecomputationplayssuchakeyrole, itisnotsurprising\n",
      "that much of this new development has been done by researcher s in other\n",
      "ﬁelds such as computer science and engineering.\n",
      "The learning problems that we consider can be roughly catego rized as\n",
      "eithersupervised orunsupervised . In supervised learning, the goal is to pre-\n",
      "dict the value of an outcome measure based on a number of input measures;\n",
      "\n",
      "nology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining,machine learning, and bioinformatics. Many of these tools have common underpinnings butare often expressed with different terminology. This book describes the important ideas inthese areas in a common conceptual framework.\n",
      "\n",
      "a prediction model, or learner, which will enable us to predict the outcome\n",
      "for new unseen objects. A good learner is one that accurately predicts such\n",
      "an outcome.\n",
      "The examples above describe what is called the supervised learning prob-\n",
      "lem. It is called “supervised” because of the presence of the outcome vari-\n",
      "able to guide the learning process. In the unsupervised learning problem ,\n",
      "we observe only the features and have no measurements of the o utcome.\n",
      "\n",
      "Bishop, C. (2006). Pattern Recognition and Machine Learning , Springer,\n",
      "New York.\n",
      "Bishop, Y., Fienberg, S. and Holland, P. (1975). Discrete Multivariate\n",
      "Analysis, MIT Press, Cambridge, MA.\n",
      "Boyd, S. and Vandenberghe, L. (2004). Convex Optimization , Cambridge\n",
      "University Press.\n",
      "Breiman, L. (1992). The little bootstrap and other methods f or dimension-\n",
      "ality selection in regression: X-ﬁxed prediction error, Journal of the\n",
      "American Statistical Association 87: 738–754.\n",
      "\n",
      "This book is about learning from data. In a typical scenario, we have\n",
      "an outcome measurement, usually quantitative (such as a sto ck price) or\n",
      "categorical (such as heart attack/no heart attack), that we wish to predict\n",
      "based on a set of features (such as diet and clinical measurements). We\n",
      "have atraining set of data, in which we observe the outcome and feature\n",
      "\n",
      "With supervised learning there is a clear measure of success , or lack\n",
      "thereof, that can be used to judge adequacy in particular sit uations and\n",
      "to compare the eﬀectiveness of diﬀerent methods over variou s situations.\n",
      "\n",
      "10.7 “Oﬀ-the-Shelf” Procedures for Data Mining 351\n",
      "TABLE 10.1. Some characteristics of diﬀerent learning methods. Key: ▲= good,\n",
      "◆=fair, and ▼=poor.\n",
      "Characteristic Neural SVM Trees MARS k-NN,\n",
      "Nets Kernels\n",
      "Natural handling of data\n",
      "of “mixed” type▼ ▼ ▲ ▲ ▼\n",
      "Handlingofmissingvalues ▼ ▼ ▲ ▲ ▲\n",
      "Robustness to outliers in\n",
      "input space▼ ▼ ▲ ▼ ▲\n",
      "Insensitive to monotone\n",
      "transformations of inputs▼ ▼ ▲ ▼ ▼\n",
      "Computational scalability\n",
      "(largeN)▼ ▼ ▲ ▲ ▼\n",
      "Ability to deal with irrel-\n",
      "evant inputs▼ ▼ ▲ ▲ ▼\n",
      "Human: What is machine learning?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Machine learning is a field of study that involves the development of algorithms and models that can learn from data and make predictions or decisions without being explicitly programmed. It focuses on creating computer systems that can automatically learn and improve from experience, rather than being explicitly programmed for specific tasks. Machine learning algorithms analyze large amounts of data to identify patterns, make predictions, or learn from examples and feedback. It is widely used in various fields such as science, finance, and industry for tasks like predicting stock prices, medical diagnoses, and customer behavior analysis.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "retriever = index.as_retriever()\n",
    "retriever.search_kwargs['fetch_k'] = 20\n",
    "retriever.search_kwargs['maximal_marginal_relevance'] = True\n",
    "retriever.search_kwargs['k'] = 10\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=retriever,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "handler = StdOutCallbackHandler()\n",
    "\n",
    "chain.run(\n",
    "    'What is machine learning?',\n",
    "    callbacks=[handler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b127210e",
   "metadata": {},
   "source": [
    "# Loading data into a Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d2379c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pinecone-client in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from pinecone-client) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.4 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from pinecone-client) (6.0)\n",
      "Requirement already satisfied: loguru>=0.5.0 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from pinecone-client) (0.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from pinecone-client) (4.7.1)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from pinecone-client) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from pinecone-client) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from pinecone-client) (1.26.16)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from pinecone-client) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from pinecone-client) (1.25.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from requests>=2.19.0->pinecone-client) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from requests>=2.19.0->pinecone-client) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from requests>=2.19.0->pinecone-client) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52bfda89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5052a7a5f93547c1b064fd601b7133ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0714bca0d314cc59ff156db77aef623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374131bada7f497b86426eb4f4f67d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e3e658f101b41d783bc0ba3e3212016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d605940c88fd40439898b1d75c87b804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pinecone \n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "    environment=PINECONE_ENV  # next to api key in console\n",
    ")\n",
    "\n",
    "index_name = \"langchain-demo\"\n",
    "db = Pinecone.from_documents(\n",
    "    data, \n",
    "    embeddings, \n",
    "    index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1edfee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c35c903ad4948a79e14bc4722560048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "This is page 1\n",
      "Printer: Opaque this\n",
      "1\n",
      "Introduction\n",
      "Statistical learning plays a key role in many areas of science, ﬁnance and\n",
      "industry. Here are some examples of learning problems:\n",
      "•Predict whether a patient, hospitalized due to a heart attac k, will\n",
      "have a second heart attack. The prediction is to be based on de mo-\n",
      "graphic, diet and clinical measurements for that patient.\n",
      "•Predict the price of a stock in 6 months from now, on the basis o f\n",
      "company performance measures and economic data.\n",
      "\n",
      "This is page 389\n",
      "Printer: Opaque this\n",
      "11\n",
      "Neural Networks\n",
      "11.1 Introduction\n",
      "In this chapter we describe a class of learning methods that w as developed\n",
      "separately in diﬀerent ﬁelds—statistics and artiﬁcial inte lligence—based\n",
      "on essentially identical models. The central idea is to extr act linear com-\n",
      "binations of the inputs as derived features, and then model t he target as\n",
      "a nonlinear function of these features. The result is a power ful learning\n",
      "\n",
      "While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples are given, with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book’s coverage is broad, from supervised learning(prediction) to unsupervised learning. The many topics include neural networks, supportvector machines, classification trees and boosting—the first comprehensive treatment of thistopic\n",
      "\n",
      "The challenges in learning from data have led to a revolution in the sta-\n",
      "tisticalsciences.Sincecomputationplayssuchakeyrole, itisnotsurprising\n",
      "that much of this new development has been done by researcher s in other\n",
      "ﬁelds such as computer science and engineering.\n",
      "The learning problems that we consider can be roughly catego rized as\n",
      "eithersupervised orunsupervised . In supervised learning, the goal is to pre-\n",
      "dict the value of an outcome measure based on a number of input measures;\n",
      "Human: What is machine learning?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Machine learning is a field of study that focuses on developing algorithms and models that enable computers to learn and make predictions or decisions without being explicitly programmed. It involves training a computer system on a large amount of data and allowing it to learn patterns and relationships within the data, which it can then use to make predictions or take actions in new, unseen situations. In essence, machine learning enables computers to learn from experience and improve their performance over time.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=db.as_retriever(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(\n",
    "    'What is machine learning?',\n",
    "    callbacks=[handler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07781599",
   "metadata": {},
   "source": [
    "# Providing sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d20e7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: newsapi-python in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (0.2.7)\n",
      "Requirement already satisfied: requests<3.0.0 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from newsapi-python) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from requests<3.0.0->newsapi-python) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from requests<3.0.0->newsapi-python) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from requests<3.0.0->newsapi-python) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from requests<3.0.0->newsapi-python) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install newsapi-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea95be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "from newsapi import NewsApiClient\n",
    "\n",
    "newsapi = NewsApiClient(api_key=NEWS_API_KEY)\n",
    "\n",
    "today = date.today()\n",
    "last_week = today - timedelta(days=7)\n",
    "\n",
    "latest_news = newsapi.get_everything(\n",
    "    q='artificial intelligence',\n",
    "    from_param=last_week.strftime(\"%Y-%m-%d\"),\n",
    "    to=today.strftime(\"%Y-%m-%d\"),\n",
    "    sort_by='relevancy',\n",
    "    language='en'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2f8e50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': {'id': None, 'name': 'Lifehacker.com'},\n",
       "  'author': 'Stephen Johnson',\n",
       "  'title': 'An AI Moderator Is Coming With ‘Call of Duty: Modern Warfare 3’',\n",
       "  'description': 'When Call of Duty: Modern Warfare 3 comes out on Nov. 10, all players’ voice chats will be silently monitored by artificial intelligence. The AI-powered moderation technology, Toxmod, is designed to identify toxic speech in real time in multiplayer games to c…',\n",
       "  'url': 'https://lifehacker.com/an-ai-moderator-is-coming-with-call-of-duty-modern-wa-1850793420',\n",
       "  'urlToImage': 'https://i.kinja-img.com/gawker-media/image/upload/c_fill,f_auto,fl_progressive,g_center,h_675,pg_1,q_80,w_1200/1c72c41d607fc3ffe4fb1c5268fd3449.jpg',\n",
       "  'publishedAt': '2023-08-31T22:30:00Z',\n",
       "  'content': 'When Call of Duty: Modern Warfare 3 comes out on Nov. 10, all players voice chats will be silently monitored by artificial intelligence. The AI-powered moderation technology, Toxmod, is designed to i… [+2695 chars]'},\n",
       " {'source': {'id': None, 'name': '[Removed]'},\n",
       "  'author': None,\n",
       "  'title': '[Removed]',\n",
       "  'description': '[Removed]',\n",
       "  'url': 'https://removed.com',\n",
       "  'urlToImage': None,\n",
       "  'publishedAt': '1970-01-01T00:00:00Z',\n",
       "  'content': '[Removed]'},\n",
       " {'source': {'id': None, 'name': 'ReadWrite'},\n",
       "  'author': 'Brad Anderson',\n",
       "  'title': 'Automated Threat Hunting: How AI Can Help Businesses Spot Shady Network Activity',\n",
       "  'description': 'The global artificial intelligence market is growing by leaps and bounds. It is expected to increase twentyfold by the end […]\\nThe post Automated Threat Hunting: How AI Can Help Businesses Spot Shady Network Activity\\xa0\\xa0 appeared first on ReadWrite.',\n",
       "  'url': 'https://readwrite.com/automated-threat-hunting-how-ai-can-help-businesses-spot-shady-network-activity/',\n",
       "  'urlToImage': 'https://readwrite.com/wp-content/uploads/2023/08/woman-on-two-computers-network-activity-AI-solutions-.jpeg',\n",
       "  'publishedAt': '2023-08-31T17:34:51Z',\n",
       "  'content': 'The global artificial intelligence market is growing by leaps and bounds. It is expected to increase twentyfold by the end of this decade,\\xa0valuing it at just under two trillion U.S. dollars, up from … [+6473 chars]'},\n",
       " {'source': {'id': None, 'name': 'ReadWrite'},\n",
       "  'author': 'Nate Nead',\n",
       "  'title': 'AI-Powered Legal Research: Optimizing Strategies for Law Firms',\n",
       "  'description': 'As demands increase and technology advances, Artificial Intelligence (AI) powered strategies can enable effective legal research and understanding processes that […]\\nThe post AI-Powered Legal Research: Optimizing Strategies for Law Firms appeared first on Rea…',\n",
       "  'url': 'https://readwrite.com/legal-research/',\n",
       "  'urlToImage': 'https://readwrite.com/wp-content/uploads/2022/06/AI-Writers.jpg',\n",
       "  'publishedAt': '2023-09-01T13:19:18Z',\n",
       "  'content': 'As demands increase and technology advances, Artificial Intelligence (AI) powered strategies can enable effective legal research and understanding processes that employ accuracy, speed, and efficienc… [+9097 chars]'},\n",
       " {'source': {'id': None, 'name': '[Removed]'},\n",
       "  'author': None,\n",
       "  'title': '[Removed]',\n",
       "  'description': '[Removed]',\n",
       "  'url': 'https://removed.com',\n",
       "  'urlToImage': None,\n",
       "  'publishedAt': '1970-01-01T00:00:00Z',\n",
       "  'content': '[Removed]'},\n",
       " {'source': {'id': None, 'name': 'Slashdot.org'},\n",
       "  'author': 'msmash',\n",
       "  'title': 'US Says It Has Not Blocked Chip Sales To Middle East',\n",
       "  'description': 'A U.S. Department of Commerce spokesperson on Thursday said the Biden administration \"has not blocked chip sales to the Middle East.\" From a report: The comments come after artificial intelligence chip firms Nvidia and Advanced Micro Devices received notifica…',\n",
       "  'url': 'https://news.slashdot.org/story/23/08/31/167230/us-says-it-has-not-blocked-chip-sales-to-middle-east',\n",
       "  'urlToImage': 'https://a.fsdn.com/sd/topics/usa_64.png',\n",
       "  'publishedAt': '2023-08-31T16:07:00Z',\n",
       "  'content': 'Sign up for the Slashdot newsletter! OR check out the new Slashdot job board to browse remote jobs or jobs in your areaDo you develop on GitHub? You can keep using GitHub but automatically sync your … [+268 chars]'},\n",
       " {'source': {'id': None, 'name': 'Gizmodo.com'},\n",
       "  'author': 'Thomas Germain',\n",
       "  'title': 'A New Facebook Setting Tells Meta Not to Use Your Data for AI',\n",
       "  'description': 'Meta, the maker of Facebook and Instagram, introduced a new privacy setting Thursday that lets you ask, pretty please, for the company not to use your data to train its AI models. Read more...',\n",
       "  'url': 'https://gizmodo.com/new-facebook-do-not-use-my-data-for-generative-ai-llama-1850792025',\n",
       "  'urlToImage': 'https://i.kinja-img.com/gawker-media/image/upload/c_fill,f_auto,fl_progressive,g_center,h_675,pg_1,q_80,w_1200/c048ad4abe4cfb2558e1894d2ccde06f.jpg',\n",
       "  'publishedAt': '2023-08-31T16:14:36Z',\n",
       "  'content': 'Meta, the maker of Facebook and Instagram, introduced a new privacy setting Thursday that lets you ask, pretty please, for the company not to use your data to train its AI models. \\r\\nBuried in the net… [+3765 chars]'},\n",
       " {'source': {'id': None, 'name': 'Slashdot.org'},\n",
       "  'author': 'msmash',\n",
       "  'title': \"Samsung Unveils Industry's First 32Gbit DDR5 Memory Die\",\n",
       "  'description': \"Samsung today revealed the world's first 32 Gb DDR5 DRAM die. From a report: The new memory die is made on the company's 12 nm-class DRAM fabrication process and not only offers increased density, but also lowers power consumption. The chip will allow Samsung…\",\n",
       "  'url': 'https://it.slashdot.org/story/23/09/01/1955206/samsung-unveils-industrys-first-32gbit-ddr5-memory-die',\n",
       "  'urlToImage': 'https://a.fsdn.com/sd/topics/it_64.png',\n",
       "  'publishedAt': '2023-09-01T21:20:00Z',\n",
       "  'content': \"The new memory die is made on the company's 12 nm-class DRAM fabrication process and not only offers increased density, but also lowers power consumption. The chip will allow Samsung to build record … [+1072 chars]\"},\n",
       " {'source': {'id': 'fox-news', 'name': 'Fox News'},\n",
       "  'author': 'Emma Colton',\n",
       "  'title': \"Tech expert says 'existential' fears from AI are overblown, but sees 'very disturbing' workplace threats\",\n",
       "  'description': \"University of Oxford professor of computer science Michael Wooldridge speculates artificial intelligence could one day be a hellish boss that monitors employees' emails.\",\n",
       "  'url': 'https://www.foxnews.com/tech/tech-expert-existential-fears-ai-are-overblown-sees-very-disturbing-workplace-threats',\n",
       "  'urlToImage': 'https://static.foxnews.com/foxnews.com/content/uploads/2023/08/aiboss.png',\n",
       "  'publishedAt': '2023-08-29T14:51:23Z',\n",
       "  'content': 'A U.K.-based tech expert said he is not losing sleep at night over the recent growth of artificial intelligence but argued he does have concerns over AI potentially becoming a hellish boss that overs… [+5460 chars]'},\n",
       " {'source': {'id': 'business-insider', 'name': 'Business Insider'},\n",
       "  'author': 'Tom Carter',\n",
       "  'title': \"Elon Musk's X tells users that it could use their posts to train AI models\",\n",
       "  'description': \"Elon Musk's X has changed its policies to allow information posted by users to be used to train its AI models.\",\n",
       "  'url': 'https://www.businessinsider.com/elon-musk-x-to-use-public-information-trains-ai-models-2023-9',\n",
       "  'urlToImage': 'https://i.insider.com/64f1c8d55114270019af00b7?width=1200&format=jpeg',\n",
       "  'publishedAt': '2023-09-01T12:52:05Z',\n",
       "  'content': 'Elon Musk has previously warned that AI could lead to \"civilization destruction.\"Chesnot/Getty Images\\r\\n<ul>\\n<li>Elon Musk\\'s X tells users it could use their posts to train artificial intelligence mod… [+3172 chars]'},\n",
       " {'source': {'id': None, 'name': 'The Atlantic'},\n",
       "  'author': 'Gal Beckerman',\n",
       "  'title': 'This Week in Books: Could AI Ever Write Like Stephen King and Margaret Atwood?',\n",
       "  'description': 'Two authors respond to the revelation that their work is being used to train artificial intelligence.',\n",
       "  'url': 'https://www.theatlantic.com/newsletters/archive/2023/09/books-briefing-ai-stephen-king-margaret-atwood/675213/?utm_source=feed',\n",
       "  'urlToImage': None,\n",
       "  'publishedAt': '2023-09-01T16:30:00Z',\n",
       "  'content': 'This is an edition of the revamped Books Briefing, our editors’ weekly guide to the best in books. Sign up for it here.The precipitous arrival of artificial intelligence into our lives over the past … [+7133 chars]'},\n",
       " {'source': {'id': None, 'name': '[Removed]'},\n",
       "  'author': None,\n",
       "  'title': '[Removed]',\n",
       "  'description': '[Removed]',\n",
       "  'url': 'https://removed.com',\n",
       "  'urlToImage': None,\n",
       "  'publishedAt': '1970-01-01T00:00:00Z',\n",
       "  'content': '[Removed]'},\n",
       " {'source': {'id': None, 'name': 'CNET'},\n",
       "  'author': 'Imad Khan',\n",
       "  'title': 'ChatGPT Glossary: 41 AI Terms that Everyone Should Know - CNET',\n",
       "  'description': 'With Google, Microsoft and just about every other company getting into AI, it can be hard to keep up with the latest terminology. This glossary helps.',\n",
       "  'url': 'https://www.cnet.com/tech/computing/chatgpt-glossary-41-ai-terms-that-everyone-should-know/',\n",
       "  'urlToImage': 'https://www.cnet.com/a/img/resize/9a13e1e92a7b66cbff9db2934b3f66bf01a4afb6/hub/2023/08/24/821b0d86-e29b-4028-ac71-ef63ca020de8/gettyimages-1472123000.jpg?auto=webp&fit=crop&height=675&width=1200',\n",
       "  'publishedAt': '2023-09-02T19:00:09Z',\n",
       "  'content': 'ChatGPT, the AI-chatbot from OpenAI, which has an uncanny ability to answer any question, was likely your first introduction to AI. From writing poems, resumes and fusion recipes, the power of ChatGP… [+8388 chars]'},\n",
       " {'source': {'id': None, 'name': 'Institutional Investor'},\n",
       "  'author': 'Michelle Celarier',\n",
       "  'title': 'Money Is Pouring into AI. Skeptics Say It’s a ‘Grift Shift.’',\n",
       "  'description': 'The move from crypto to artificial intelligence has fueled the markets this year, but some are questioning how much of it is real.',\n",
       "  'url': 'https://www.institutionalinvestor.com/article/2c4fad0w6irk838pca3gg/portfolio/money-is-pouring-into-ai-skeptics-say-its-a-grift-shift',\n",
       "  'urlToImage': 'https://cdn.assetmg.info/dims4/default/e0f778a/2147483647/strip/true/crop/1200x675+0+13/resize/1440x810!/quality/90/?url=https%3A%2F%2Fk2-prod-in-investor-prod.s3.amazonaws.com%2Fbrightspot%2F1e%2F9c%2Fcf470f664a3cab27db4b61b7db27%2Fart-grift-shift-feature.jpg',\n",
       "  'publishedAt': '2023-08-31T05:55:10Z',\n",
       "  'content': 'By the time a dormant penny stock company known as Applied Sciences managed to wrangle a listing on the Nasdaq in April 2022, it had reinvented itself as a cloud hosting service for bitcoin miners an… [+24758 chars]'},\n",
       " {'source': {'id': None, 'name': 'Boing Boing'},\n",
       "  'author': 'David Pescovitz',\n",
       "  'title': 'AI can now describe how something will smell by analyzing its chemical structure',\n",
       "  'description': 'A new artificial intelligence system is capable of analyzing the molecular make-up of a compound and then predicting how it will smell. According to the researchers, the descriptions the AI generates are similar in language and often more precise than humans.…',\n",
       "  'url': 'https://boingboing.net/2023/09/01/ai-can-now-describe-how-something-will-smell-by-analyzing-its-chemical-structure.html',\n",
       "  'urlToImage': 'https://i0.wp.com/boingboing.net/wp-content/uploads/2022/10/shutterstock_1935705550-scaled.jpg?fit=1200%2C800&ssl=1',\n",
       "  'publishedAt': '2023-09-01T17:50:47Z',\n",
       "  'content': 'A new artificial intelligence system is capable of analyzing the molecular make-up of a compound and then predicting how it will smell. According to the researchers, the descriptions the AI generates… [+1329 chars]'},\n",
       " {'source': {'id': None, 'name': 'Slashdot.org'},\n",
       "  'author': 'msmash',\n",
       "  'title': 'UK Government Seeks Expanded Use of AI-based Facial Recognition By Police',\n",
       "  'description': \"UK's Home Office is looking to increase its use of controversial facial recognition technologies to track and find criminals within policing and other security agencies. From a report: In a document released on Wednesday, the government outlined its ambitions…\",\n",
       "  'url': 'https://news.slashdot.org/story/23/08/31/1818201/uk-government-seeks-expanded-use-of-ai-based-facial-recognition-by-police',\n",
       "  'urlToImage': 'https://a.fsdn.com/sd/topics/uk_64.png',\n",
       "  'publishedAt': '2023-08-31T21:20:00Z',\n",
       "  'content': 'In a document released on Wednesday, the government outlined its ambitions to potentially deploy new biometric systems nationally over the next 12 to 18 months. The move comes after privacy campaigne… [+1100 chars]'},\n",
       " {'source': {'id': None, 'name': 'NPR'},\n",
       "  'author': 'Deepa Shivaram',\n",
       "  'title': \"Robots are pouring drinks in Vegas. As AI grows, the city's workers brace for change\",\n",
       "  'description': 'Workers in Las Vegas have been watching automation and technology inch into their workplace. Now with AI, the city is preparing to adapt its service-heavy tourism economy.',\n",
       "  'url': 'https://www.npr.org/2023/09/04/1197138244/vegas-ai-workers-brace-for-change',\n",
       "  'urlToImage': 'https://media.npr.org/assets/img/2023/09/01/img_2227_wide-ae547467d6e3a028878f4ea683286bcd43b94ce9-s1400-c100.jpg',\n",
       "  'publishedAt': '2023-09-04T09:00:54Z',\n",
       "  'content': 'This bar inside Planet Hollywood on the Las Vegas strip has two robots that serve customers drinks. The Tipsy Robot opened a second location on the strip this year.\\r\\nDeepa Shivaram/NPR\\r\\nWalk any dire… [+5272 chars]'},\n",
       " {'source': {'id': None, 'name': 'Marginalrevolution.com'},\n",
       "  'author': 'Tyler Cowen',\n",
       "  'title': 'An aggregate Bayesian approach to more (artificial) intelligence?',\n",
       "  'description': 'It is not disputed that current AI is bringing more intelligence into the world, with more to follow yet.\\xa0 Of course not everyone believes that augmentation is a good thing, or will be a good thing if we remain on our current path. To continue in aggregative …',\n",
       "  'url': 'https://marginalrevolution.com/marginalrevolution/2023/08/an-aggregate-bayesian-approach-to-more-artificial-intelligence.html',\n",
       "  'urlToImage': 'https://marginalrevolution.com/wp-content/uploads/2016/10/MR-logo-thumbnail.png',\n",
       "  'publishedAt': '2023-08-31T19:18:22Z',\n",
       "  'content': 'It is not disputed that current AI is bringing more intelligence into the world, with more to follow yet.\\xa0 Of course not everyone believes that augmentation is a good thing, or will be a good thing i… [+2224 chars]'},\n",
       " {'source': {'id': None, 'name': 'AppleInsider'},\n",
       "  'author': 'news@appleinsider.com (Evan Selleck)',\n",
       "  'title': 'How Apple is already using machine learning and AI in iOS',\n",
       "  'description': 'Apple may not be as flashy as other companies in adopting artificial intelligence features. Still, the already has a lot of smarts scattered throughout iOS.Apple\\'s SiriApple does not go out of its way to specifically name-drop \"artificial intelligence\" or AI …',\n",
       "  'url': 'https://appleinsider.com/articles/23/09/02/how-apple-is-already-using-machine-learning-and-ai-in-ios',\n",
       "  'urlToImage': 'https://photos5.appleinsider.com/gallery/56062-113816-Apple-Siri-xl.jpg',\n",
       "  'publishedAt': '2023-09-02T17:22:20Z',\n",
       "  'content': \"Apple's Siri\\r\\nApple may not be as flashy as other companies in adopting artificial intelligence features. Still, the already has a lot of smarts scattered throughout iOS.\\r\\nApple does not go out of it… [+7135 chars]\"},\n",
       " {'source': {'id': None, 'name': 'Slashdot.org'},\n",
       "  'author': 'BeauHD',\n",
       "  'title': \"Meet Aleph Alpha, Europe's Answer To OpenAI\",\n",
       "  'description': \"An anonymous reader quotes a report from Wired: Europe wants its own Open AI. The bloc's politicians are sick of regulating American tech giants from afar. They want Europe to build its own generative AI, which is why so many people are rooting for Jonas Andr…\",\n",
       "  'url': 'https://slashdot.org/story/23/08/30/0653223/meet-aleph-alpha-europes-answer-to-openai',\n",
       "  'urlToImage': 'https://a.fsdn.com/sd/topics/ai_64.png',\n",
       "  'publishedAt': '2023-08-30T13:00:00Z',\n",
       "  'content': \"Europe wants its own Open AI. The bloc's politicians are sick of regulating American tech giants from afar. They want Europe to build its own generative AI, which is why so many people are rooting fo… [+2953 chars]\"},\n",
       " {'source': {'id': None, 'name': 'VentureBeat'},\n",
       "  'author': 'Cliff Jurkiewicz, Phenom',\n",
       "  'title': '3 things businesses need to know as NYC begins enforcing its AI hiring law',\n",
       "  'description': \"New York's new AI regulations for hiring have many companies jittery. Here's how to get a handle on complying now and in the future.\",\n",
       "  'url': 'https://venturebeat.com/ai/3-things-businesses-need-to-know-as-nyc-begins-enforcing-its-ai-hiring-law/',\n",
       "  'urlToImage': 'https://venturebeat.com/wp-content/uploads/2023/08/annevb_technology_artificial_intelligence_and_law._colorful_fu_ce8f16b1-af12-449c-8d7a-af72cd4c62c5-1.png?w=1200&strip=all',\n",
       "  'publishedAt': '2023-09-03T17:10:00Z',\n",
       "  'content': 'Head over to our on-demand library to view sessions from VB Transform 2023. Register Here\\r\\nIn July, New York City officially began cracking down on companies that run afoul of its first-in-the-nation… [+1324 chars]'},\n",
       " {'source': {'id': None, 'name': 'VentureBeat'},\n",
       "  'author': 'Sharon Goldman',\n",
       "  'title': 'AI21 Labs raises $155M to accelerate GenAI for enterprises',\n",
       "  'description': 'Israel-based large language model leader AI21 Labs confirmed with VentureBeat that is has closed $155 million in Series C funding.',\n",
       "  'url': 'https://venturebeat.com/ai/ai21-labs-raises-155m-to-accelerate-genai-for-enterprises/',\n",
       "  'urlToImage': 'https://venturebeat.com/wp-content/uploads/2023/08/6101c79796100f175dfd4dad_image-9.png?w=1200&strip=all',\n",
       "  'publishedAt': '2023-08-30T20:21:44Z',\n",
       "  'content': 'Head over to our on-demand library to view sessions from VB Transform 2023. Register Here\\r\\nTel Aviv, Israel-based large language model leader AI21 Labs confirmed with VentureBeat that is has closed $… [+1157 chars]'},\n",
       " {'source': {'id': None, 'name': 'MakeUseOf'},\n",
       "  'author': 'Tomisin Olujinmi',\n",
       "  'title': '6 Amazing Raspberry Pi AI Projects',\n",
       "  'description': 'Bring your artificial intelligence ideas to life with a Raspberry Pi single-board computer.',\n",
       "  'url': 'https://www.makeuseof.com/raspberry-pi-artificial-intelligence-projects/',\n",
       "  'urlToImage': 'https://static1.makeuseofimages.com/wordpress/wp-content/uploads/2023/04/excel-1.jpg',\n",
       "  'publishedAt': '2023-09-04T16:30:24Z',\n",
       "  'content': 'Artificial intelligence, precisely the generative kind, has recently seen a sudden surge in popularity as people explore the possibilities of creating visual and textual content with these tools. Suc… [+5922 chars]'},\n",
       " {'source': {'id': 'new-scientist', 'name': 'New Scientist'},\n",
       "  'author': 'Matthew Sparkes',\n",
       "  'title': 'AI shows no sign of consciousness yet, but we know what to look for',\n",
       "  'description': 'The latest generations of artificial intelligence models show little to no trace of 14 signs of self-awareness predicted by prominent theories of human consciousness',\n",
       "  'url': 'https://www.newscientist.com/article/2388344-ai-shows-no-sign-of-consciousness-yet-but-we-know-what-to-look-for/',\n",
       "  'urlToImage': 'https://images.newscientist.com/wp-content/uploads/2023/08/30103521/SEI_168670348.jpg',\n",
       "  'publishedAt': '2023-08-30T11:00:00Z',\n",
       "  'content': 'Can machines think? Not yet, according to a review\\r\\nYuichiro Chino/Getty Images\\r\\nAre artificial intelligences conscious? No, is the conclusion of the most thorough and rigorous investigation of the q… [+3106 chars]'},\n",
       " {'source': {'id': None, 'name': 'NPR'},\n",
       "  'author': 'Rachel Treisman',\n",
       "  'title': 'Up First briefing: A Labor Day look at union fights, wins and close calls',\n",
       "  'description': \"It's Labor Day in the U.S. Striking writers and actors have brought Hollywood to a standstill. There's widespread support for unions, especially among Gen Z, yet membership is at an all-time low.\",\n",
       "  'url': 'https://www.npr.org/2023/09/04/1197238803/up-first-briefing-labor-day-hollywood-strikes-union-boom-busted',\n",
       "  'urlToImage': 'https://media.npr.org/assets/img/2023/09/01/untitled-design-10-_wide-1cb9a625b6b9564f6eb3e09ff5ef4723dbf71214-s1400-c100.jpg',\n",
       "  'publishedAt': '2023-09-04T11:01:24Z',\n",
       "  'content': \"Good morning. You're reading the Up First newsletter. Subscribe here to get it delivered to your inbox, and listen to the Up First podcast for all the news you need to start your day.\\r\\n The state of … [+4638 chars]\"},\n",
       " {'source': {'id': None, 'name': 'Slashdot.org'},\n",
       "  'author': 'EditorDavid',\n",
       "  'title': 'Why Self-Driving Cars Slowed Down in High-Tech Boston',\n",
       "  'description': 'The city of Boston also allows testing of self-driving cars. But the Boston Globe reports that \"There are far fewer complaints about self-driving cars because you barely see them.\"\\n\\n[F]ollowing a string of high-profile crashes and the disruption of the COVID …',\n",
       "  'url': 'https://tech.slashdot.org/story/23/09/04/0420246/why-self-driving-cars-slowed-down-in-high-tech-boston',\n",
       "  'urlToImage': 'https://a.fsdn.com/sd/topics/transportation_64.png',\n",
       "  'publishedAt': '2023-09-04T08:34:00Z',\n",
       "  'content': '[F]ollowing a string of high-profile crashes and the disruption of the COVID pandemic, the state Transportation Department — now under Governor Maura Healey — has seemingly lost its enthusiasm for AV… [+2154 chars]'},\n",
       " {'source': {'id': 'business-insider', 'name': 'Business Insider'},\n",
       "  'author': 'Joseph Wilkins',\n",
       "  'title': \"The S&P 500 could rally another 11% by year-end, Morgan Stanley Investment Management's Andrew Slimmon says\",\n",
       "  'description': 'Morgan Stanley Investment Management\\'s Andrew Slimmon thinks the stock market still has room to run and predicts the index could reach \"closer\" to 5,000 points by year-end.',\n",
       "  'url': 'https://markets.businessinsider.com/news/stocks/stock-market-sp-strong-rally-by-year-end-morgan-stanley-2023-8',\n",
       "  'urlToImage': 'https://i.insider.com/64ef068796b7920019ca3799?width=1200&format=jpeg',\n",
       "  'publishedAt': '2023-08-30T10:18:37Z',\n",
       "  'content': \"Andrew Slimmon is a senior portfolio manager at Morgan Stanley's $665 billion investment management businessCNBC\\r\\n<ul>\\n<li>Despite a downbeat August, the S&P 500 index is still boasting impressive ye… [+2332 chars]\"},\n",
       " {'source': {'id': None, 'name': '9to5Mac'},\n",
       "  'author': 'Filipe Espósito',\n",
       "  'title': 'Snapchat introduces new ‘Dreams’ feature to generate selfies with AI',\n",
       "  'description': 'Earlier this year, Snapchat added integration with OpenAI’s ChatGPT to let users get insights from an AI directly from the app. In another AI-related move, Snapchat is now rolling out a new “Dreams” feature that lets users generate photos using artificial int…',\n",
       "  'url': 'https://9to5mac.com/2023/08/29/snapchat-dreams-photos-ai/',\n",
       "  'urlToImage': 'https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2023/08/Snapchat-Dreams-AI.jpg?resize=1200%2C628&quality=82&strip=all&ssl=1',\n",
       "  'publishedAt': '2023-08-29T13:00:00Z',\n",
       "  'content': 'Earlier this year, Snapchat added integration with OpenAI’s ChatGPT to let users get insights from an AI directly from the app. In another AI-related move, Snapchat is now rolling out a new “Dreams” … [+1499 chars]'},\n",
       " {'source': {'id': 'al-jazeera-english', 'name': 'Al Jazeera English'},\n",
       "  'author': 'Al Jazeera',\n",
       "  'title': 'Jaron Lanier: Artificial intelligence is not a threat to humans',\n",
       "  'description': \"The man who coined 'virtual reality' says AI poses no threat, and resisting technological progress is futile anyway.\",\n",
       "  'url': 'https://www.aljazeera.com/program/the-bottom-line/2023/8/31/jaron-lanier-artificial-intelligence-is-not-a-threat-to-humans',\n",
       "  'urlToImage': 'https://www.aljazeera.com/wp-content/uploads/2023/08/image-88.jpg?resize=1920%2C1080&quality=80',\n",
       "  'publishedAt': '2023-08-31T11:07:41Z',\n",
       "  'content': 'The man who coined virtual reality says AI poses no threat, and resisting technological progress is futile anyway.What if the fears of artificial intelligence are unfounded? Is AI nothing more than a… [+547 chars]'},\n",
       " {'source': {'id': None, 'name': 'Science Daily'},\n",
       "  'author': None,\n",
       "  'title': 'AI enabled soft robotic implant monitors scar tissue to self-adapt for personalized drug treatment',\n",
       "  'description': 'Research teams have detailed a pioneering breakthrough in medical device technology that could lead to intelligent, long-lasting, tailored treatment for patients, thanks to soft robotics and artificial intelligence.',\n",
       "  'url': 'https://www.sciencedaily.com/releases/2023/08/230830151740.htm',\n",
       "  'urlToImage': 'https://www.sciencedaily.com/images/scidaily-icon.png',\n",
       "  'publishedAt': '2023-08-30T19:17:40Z',\n",
       "  'content': 'Research teams at University of Galway and Massachusetts Institute of Technology (MIT) have detailed a breakthrough in medical device technology that could lead to intelligent, long-lasting, tailored… [+5239 chars]'},\n",
       " {'source': {'id': None, 'name': 'Windows Central'},\n",
       "  'author': 'sendicott47@outlook.com (Sean Endicott)',\n",
       "  'title': 'Forget smartwatches, Microsoft may make a backpack with an AI assistant',\n",
       "  'description': \"A recent Microsoft patent describes a backpack with AI built in. The device could scan a person's surroundings and provide assistance in response to questions.\",\n",
       "  'url': 'https://www.windowscentral.com/software-apps/forget-smartwatches-microsoft-may-make-a-backpack-with-an-ai-assistant',\n",
       "  'urlToImage': 'https://cdn.mos.cms.futurecdn.net/9FnXVicPU9DcyJMa6WxQpk-1200-80.jpg',\n",
       "  'publishedAt': '2023-08-31T14:49:34Z',\n",
       "  'content': 'What you need to know\\r\\n<ul><li>A recent Microsoft patent describes AI-assisted wearables, including a backpack.</li><li>The patent illustrates how a backpack with sensors could relay information from… [+2931 chars]'},\n",
       " {'source': {'id': 'new-scientist', 'name': 'New Scientist'},\n",
       "  'author': 'Matthew Sparkes',\n",
       "  'title': 'AI beats champion human pilots in head-to-head drone races',\n",
       "  'description': 'The Swift AI has beaten expert drone racers in high-speed races using an on-board computer that fuses artificial intelligence and classical algorithms – a method that could speed up delivery drones',\n",
       "  'url': 'https://www.newscientist.com/article/2389071-ai-beats-champion-human-pilots-in-head-to-head-drone-races/',\n",
       "  'urlToImage': 'https://images.newscientist.com/wp-content/uploads/2023/08/30112509/SEI_169381544.jpg',\n",
       "  'publishedAt': '2023-08-30T16:00:32Z',\n",
       "  'content': 'Swift (blue) races against the drone (red) of Alex Vanover, the 2019 Drone Racing League world champion\\r\\nLeonard Bauersfeld\\r\\nAn artificial intelligence has consistently beaten champion drone pilots i… [+355 chars]'},\n",
       " {'source': {'id': 'the-next-web', 'name': 'The Next Web'},\n",
       "  'author': 'Thomas Macaulay',\n",
       "  'title': 'Google DeepMind unveils AI watermarking tool as political pressure mounts',\n",
       "  'description': 'As governments increase demands for labels on AI-generated content, Google DeepMind has released a potential solution: a watermarking tool. Named SynthID, the system harnesses deep learning to embed digital stamps into the pixels of images. According to DeepM…',\n",
       "  'url': 'https://thenextweb.com/news/google-deepmind-ai-watermarking-synthid',\n",
       "  'urlToImage': 'https://img-cdn.tnwcdn.com/image/tnw-blurple?filter_last=1&fit=1280%2C640&url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2023%2F08%2FUntitled-design-1-4.jpg&signature=f5d0e747fd93332c435e17b49afb3e74',\n",
       "  'publishedAt': '2023-08-30T14:17:01Z',\n",
       "  'content': 'As governments increase demands for labels on AI-generated content, Google DeepMind has released a potential solution: a watermarking tool.\\r\\nNamed SynthID, the system harnesses deep learning to embed… [+2529 chars]'},\n",
       " {'source': {'id': 'business-insider', 'name': 'Business Insider'},\n",
       "  'author': 'Peter Csathy',\n",
       "  'title': 'Elon Musk has unprecedented influence that spans multiple industries — and our fate is in his hands',\n",
       "  'description': 'He may be erratic and amoral, but he controls the most sensitive aspects of our lives (much to our peril).',\n",
       "  'url': 'https://www.businessinsider.com/elon-musk-unprecedented-influence-our-fate-in-his-hands-2023-9',\n",
       "  'urlToImage': 'https://i.insider.com/64d505da10f730001ad36f7b?width=1200&format=jpeg',\n",
       "  'publishedAt': '2023-09-02T09:25:01Z',\n",
       "  'content': \"Peter Csathy writes that Musk's fingerprints are all over virtually every major aspect of our lives and no other captain of industry has ever come close to this kind of power, not even the great Stev… [+6577 chars]\"},\n",
       " {'source': {'id': 'bbc-news', 'name': 'BBC News'},\n",
       "  'author': 'https://www.facebook.com/bbcnews',\n",
       "  'title': 'New tech boosts Dutch drive for sustainable farming',\n",
       "  'description': 'In the Netherlands, experiments are underway to ensure future food supply and cut carbon emissions.',\n",
       "  'url': 'https://www.bbc.co.uk/news/business-66461769',\n",
       "  'urlToImage': 'https://ichef.bbci.co.uk/news/1024/branded_news/A025/production/_130879904_49093602-98e6-4169-8667-4f15e57f85c9.jpg',\n",
       "  'publishedAt': '2023-09-03T23:01:58Z',\n",
       "  'content': \"I'm at the Wageningen University's Farm of the Future, where I had been expecting to see robots and perhaps, drones flying overhead. \\r\\nBut on a bright blue day project manager Wijnand Sukkel stands i… [+6826 chars]\"},\n",
       " {'source': {'id': 'business-insider', 'name': 'Business Insider'},\n",
       "  'author': 'Theron Mohamed',\n",
       "  'title': 'Stocks face a laundry list of dangers and could slump within months, warns one elite investor',\n",
       "  'description': \"The many risks to stocks include AI disappointment, an economic slowdown, and more banking woes, says Tudor Investments' Ulrike Hoffmann-Burchardi.\",\n",
       "  'url': 'https://markets.businessinsider.com/news/stocks/stock-market-outlook-risks-tudor-ai-rates-inflation-banking-shutdown-2023-8',\n",
       "  'urlToImage': 'https://i.insider.com/64f069cdf8c86200196132fe?width=1200&format=jpeg',\n",
       "  'publishedAt': '2023-08-31T11:52:51Z',\n",
       "  'content': \"Stocks could drop within months as headwinds mount, Ulrike Hoffmann-Burchardi says.Paulo Whitaker/Reuters\\r\\n<ul>\\n<li>Stocks could slump within the next few months, says Tudor Investments' Ulrike Hoffm… [+2892 chars]\"},\n",
       " {'source': {'id': 'business-insider', 'name': 'Business Insider'},\n",
       "  'author': 'Theron Mohamed',\n",
       "  'title': 'A pretzel-shop worker committed a $1 million fraud to buy Tesla, GameStop, and Nvidia shares - and ended up making $7,000 for his broker, SEC says',\n",
       "  'description': 'The fast-food worker fraudulently obtained $200,000 of advance credit, and plowed it into several trendy stocks including Apple and AMC, the SEC said.',\n",
       "  'url': 'https://markets.businessinsider.com/news/stocks/tesla-nvidia-gamestop-amc-meme-stocks-sec-fraud-retail-trader-2023-8',\n",
       "  'urlToImage': 'https://i.insider.com/626161f0db73840018a55fe7?width=1200&format=jpeg',\n",
       "  'publishedAt': '2023-08-29T12:16:47Z',\n",
       "  'content': \"Tesla CEO Elon Musk.HANNIBAL HANSCHKE /Getty Images\\r\\n<ul>\\n<li>A fast-food worker obtained $200,000 of advance credit by making bogus deposits, the SEC says.</li>\\n<li>The Auntie Anne's employee piled … [+3344 chars]\"},\n",
       " {'source': {'id': 'business-insider', 'name': 'Business Insider'},\n",
       "  'author': 'Madeline Renbarger',\n",
       "  'title': 'A VC firm has built an AI-powered pitch deck generator for startup founders — and is giving it away for free',\n",
       "  'description': \"Promise Phelon's venture fund Growth Warrior Capital is launching Elevo, a free AI tool for crafting founder pitch decks.\",\n",
       "  'url': 'https://www.businessinsider.com/growth-warrior-capital-elevo-pitch-deck-builder-generative-ai-venturecapital-2023-8',\n",
       "  'urlToImage': 'https://i.insider.com/64ef70926f301e00193e3d2c?width=1200&format=jpeg',\n",
       "  'publishedAt': '2023-08-31T14:00:01Z',\n",
       "  'content': 'Promise Phelon, the founder and managing partner of the VC firm Growth Warrior Capital.Growth Warrior Capital\\r\\n<ul><li>Growth Warrior Capital is launching a new tool Elevo, an AI-powered pitch deck g… [+3670 chars]'},\n",
       " {'source': {'id': None, 'name': 'Ted.com'},\n",
       "  'author': 'Karen Willcox, contact@ted.com (TED)',\n",
       "  'title': 'How \"digital twins\" could help us predict the future | Karen Willcox',\n",
       "  'description': 'From health-tracking wearables to smartphones and beyond, data collection and computer modeling have become a ubiquitous part of everyday life. Advancements in these areas have given birth to \"digital twins,\" or virtual models that evolve alongside real-world…',\n",
       "  'url': 'https://www.ted.com/talks/karen_willcox_how_digital_twins_could_help_us_predict_the_future',\n",
       "  'urlToImage': 'https://pi.tedcdn.com/r/talkstar-photos.s3.amazonaws.com/uploads/a1794d59-a605-4756-b818-401ad6adb540/KarenWillcox_2022X-embed.jpg?u%5Br%5D=2&u%5Bs%5D=0.5&u%5Ba%5D=0.8&u%5Bt%5D=0.03&quality=82c=1050%2C550&w=1050',\n",
       "  'publishedAt': '2023-09-01T14:59:05Z',\n",
       "  'content': 'Read transcript\\r\\nFrom health-tracking wearables to smartphones and beyond, data collection and computer modeling have become a ubiquitous part of everyday life. Advancements in these areas have given… [+3647 chars]'},\n",
       " {'source': {'id': None, 'name': 'Neatorama.com'},\n",
       "  'author': 'Miss Cellania',\n",
       "  'title': 'The Unexpected Way Artificial Intelligence Could Start Killing Us',\n",
       "  'description': \"There has been plenty of speculative fiction about artificial intelligence taking over the world and eliminating superfluous humans, whether by design or by accident, by messing with our national defense systems, infrastructure, or governments. But it's possi…\",\n",
       "  'url': 'https://www.neatorama.com/2023/08/31/The-Unexpected-Way-Artificial-Intelligence-Could-Start-Killing-Us/',\n",
       "  'urlToImage': 'https://uploads.neatorama.com/images/posts/252/123/123252/1693485830-0.jpg',\n",
       "  'publishedAt': '2023-08-31T12:43:50Z',\n",
       "  'content': 'There has been plenty of speculative fiction about artificial intelligence taking over the world and eliminating superfluous humans, whether by design or by accident, by messing with our national def… [+1445 chars]'},\n",
       " {'source': {'id': None, 'name': 'Slashdot.org'},\n",
       "  'author': 'msmash',\n",
       "  'title': 'Google Says Over Half of Generative AI Startups Use Its Cloud',\n",
       "  'description': 'When employees leave Google to join the artificial intelligence startup race, the search giant still has a way to benefit -- by keeping those former workers as cloud customers. From a report: More than half of venture-backed generative AI startups pay for Goo…',\n",
       "  'url': 'https://tech.slashdot.org/story/23/08/29/1711253/google-says-over-half-of-generative-ai-startups-use-its-cloud',\n",
       "  'urlToImage': 'https://a.fsdn.com/sd/topics/topicgoogle_fb.gif',\n",
       "  'publishedAt': '2023-08-29T17:20:00Z',\n",
       "  'content': \"More than half of venture-backed generative AI startups pay for Google's cloud computing platform, Alphabet, Google's parent company, said Tuesday. Of the startups valued at over $1 billion, 70% are … [+1020 chars]\"},\n",
       " {'source': {'id': None, 'name': 'Forbes'},\n",
       "  'author': 'Tax Notes Staff, Contributor, \\n Tax Notes Staff, Contributor\\n https://www.forbes.com/sites/taxnotes/people/taxanalysts/',\n",
       "  'title': 'Tax And The Future Of Artificial Intelligence',\n",
       "  'description': 'John McGowan of HubSync discusses artificial intelligence and its role in the tax field.',\n",
       "  'url': 'https://www.forbes.com/sites/taxnotes/2023/08/29/tax-and-the-future-of-artificial-intelligence/',\n",
       "  'urlToImage': 'https://imageio.forbes.com/specials-images/imageserve/64ee492b71e0e620574af6bd/0x0.jpg?format=jpg&width=1200',\n",
       "  'publishedAt': '2023-08-29T19:43:31Z',\n",
       "  'content': 'SHANGHAI, CHINA - JUNE 18: Cutting edge applications of Artificial Intelligence are seen on display ... [+] at the Artificial Intelligence Pavilion of Zhangjiang Future Park during a state organized … [+24590 chars]'},\n",
       " {'source': {'id': None, 'name': 'Rolling Stone'},\n",
       "  'author': 'Sean Woods',\n",
       "  'title': 'How A.I. Could Reincarnate Your Dead Grandparents — or Wipe Out Your Kids',\n",
       "  'description': 'Two experts on artificial intelligence, Ray Kurzweil and Eliezer Yudkowsky see wildly different outcomes from the revolutionary technology — a chance for immortality or doomsday',\n",
       "  'url': 'https://www.rollingstone.com/culture/culture-features/ai-bots-destroy-humanity-immortality-1234816682/',\n",
       "  'urlToImage': 'https://www.rollingstone.com/wp-content/uploads/2023/09/ai.jpg?w=1600&h=900&crop=1',\n",
       "  'publishedAt': '2023-09-04T13:34:01Z',\n",
       "  'content': 'WHEN, NOT IF, ROBOTS DESTROY humanity in the next few decades, they won’t resemble the Terminator, says Eliezer Yudkowsky, the leading pioneer of artificial intelligence who now believes we’re doomed… [+16351 chars]'},\n",
       " {'source': {'id': 'bbc-news', 'name': 'BBC News'},\n",
       "  'author': 'https://www.facebook.com/bbcnews',\n",
       "  'title': 'Google tests watermark to identify AI images',\n",
       "  'description': \"The tech giant's artificial intelligence firm DeepMind unveils measures to counter disinformation.\",\n",
       "  'url': 'https://www.bbc.co.uk/news/technology-66618852',\n",
       "  'urlToImage': 'https://ichef.bbci.co.uk/news/1024/branded_news/E7EC/production/_130927395_capture.jpg',\n",
       "  'publishedAt': '2023-08-29T12:17:39Z',\n",
       "  'content': \"Google is trialling a digital watermark to spot images made by artificial intelligence (AI) in a bid to fight disinformation.\\r\\nDeveloped by Deepmind, Google's AI arm, SynthID will identify images gen… [+3895 chars]\"},\n",
       " {'source': {'id': None, 'name': 'PetaPixel'},\n",
       "  'author': 'Lisa Marie Segarra',\n",
       "  'title': 'Google is First Tech Giant to Test Watermarks to Label AI Images',\n",
       "  'description': 'As images made by artificial intelligence (AI) become more advanced, companies are racing to find ways to combat misinformation. Tuesday, Google announced it is testing a watermark to detect whether an image was made using AI.\\n[Read More]',\n",
       "  'url': 'https://petapixel.com/2023/08/29/google-is-first-tech-giant-to-test-watermarks-to-label-ai-images/',\n",
       "  'urlToImage': 'https://petapixel.com/assets/uploads/2023/08/google-ai-image-watermark-detector-deepmind-imagen.jpg',\n",
       "  'publishedAt': '2023-08-29T18:17:48Z',\n",
       "  'content': 'As images made by artificial intelligence (AI) become more advanced, companies are racing to find ways to combat misinformation. Tuesday, Google announced it is testing a watermark to detect whether … [+2895 chars]'},\n",
       " {'source': {'id': None, 'name': 'Boing Boing'},\n",
       "  'author': \"Boing Boing's Shop\",\n",
       "  'title': 'Master the power of ChatGPT to grow your business, improve your marketing or to get self-published for only $29.99',\n",
       "  'description': 'We thank our sponsor for making this content possible; it is not written by the editorial staff nor does it necessarily reflect its views.\\n\\n\\n\\nTL;DR:\\xa0Harness the power of ChatGPT with\\xa0The 2023 ChatGPT for Business Mastery Bundle\\xa0and improve your financial futu…',\n",
       "  'url': 'https://boingboing.net/2023/09/03/master-the-power-of-chatgpt-to-grow-your-business-improve-your-marketing-or-to-get-self-published-for-only-29-99.html',\n",
       "  'urlToImage': 'https://i0.wp.com/boingboing.net/wp-content/uploads/2023/08/Boing-Boing-The-2023-ChatGPT-for-Business-Mastery-Bundle.jpeg?fit=1200%2C800&ssl=1',\n",
       "  'publishedAt': '2023-09-03T21:00:00Z',\n",
       "  'content': 'We thank our sponsor for making this content possible; it is not written by the editorial staff nor does it necessarily reflect its views.\\r\\nTL;DR:\\xa0Harness the power of ChatGPT with\\xa0The 2023 ChatGPT f… [+2360 chars]'},\n",
       " {'source': {'id': 'al-jazeera-english', 'name': 'Al Jazeera English'},\n",
       "  'author': 'Al Jazeera',\n",
       "  'title': 'China’s Baidu rolls out ChatGPT rival ERNIE to public',\n",
       "  'description': \"Public launch of the chatbot comes as China's tech sector aims to cash in on the artificial intelligence gold rush.\",\n",
       "  'url': 'https://www.aljazeera.com/economy/2023/8/31/chinas-baidu-rolls-out-chatgpt-rival-ernie-to-public',\n",
       "  'urlToImage': 'https://www.aljazeera.com/wp-content/uploads/2023/08/2023-08-30T170734Z_1730319418_RC27S1AFCLJA_RTRMADP_3_CHINA-AI-1693456592.jpg?resize=1920%2C1440',\n",
       "  'publishedAt': '2023-08-31T05:10:43Z',\n",
       "  'content': 'Chinas Baidu has rolled out its ChatGPT rival ERNIE Bot to the public, in a major leap for the countrys tech sector as it aims to cash in on the artificial intelligence gold rush.\\r\\nThe Chinese govern… [+3325 chars]'},\n",
       " {'source': {'id': None, 'name': 'Phys.Org'},\n",
       "  'author': 'Jack McGovan',\n",
       "  'title': 'To tackle wildfires, researchers in Europe team up with frontline forces',\n",
       "  'description': 'The EU is seeking to limit growing threats from blazes through the use of satellites, artificial intelligence and unmanned aerial vehicles.',\n",
       "  'url': 'https://phys.org/news/2023-08-tackle-wildfires-europe-team-frontline.html',\n",
       "  'urlToImage': 'https://scx2.b-cdn.net/gfx/news/2023/to-tackle-wildfires-re.jpg',\n",
       "  'publishedAt': '2023-08-29T19:04:03Z',\n",
       "  'content': 'The EU is seeking to limit growing threats from blazes through the use of satellites, artificial intelligence and unmanned aerial vehicles.\\r\\nPicture the following scene on the French island of Corsic… [+6957 chars]'},\n",
       " {'source': {'id': None, 'name': 'TMZ'},\n",
       "  'author': 'TMZ Staff',\n",
       "  'title': 'AI Model Labeled Racist Over Generated Images of People From All 50 States',\n",
       "  'description': \"Artificial intelligence relies on ethnic stereotypes and is completely racist ... that's the majority view on social media after computers imagined what someone from each state looks like. Here's the deal ... one advanced AI model cranked out 50…\",\n",
       "  'url': 'https://www.tmz.com/2023/09/02/artificial-intelligence-model-labeled-racist-over-generated-images-people-all-50-states/',\n",
       "  'urlToImage': 'https://imagez.tmz.com/image/27/16by9/2023/08/24/2790c75d3d214818851a576e5758ea00_xl.jpg',\n",
       "  'publishedAt': '2023-09-02T08:00:45Z',\n",
       "  'content': \"Artificial intelligence relies on ethnic stereotypes and is completely racist ... that's the majority view on social media after computers imagined what someone from each state looks like.\\r\\nHere's th… [+1039 chars]\"},\n",
       " {'source': {'id': None, 'name': 'Digital Trends'},\n",
       "  'author': 'Alan Truly',\n",
       "  'title': 'Watch out, Grammarly — Google Docs gets AI proofreading',\n",
       "  'description': \"Google Docs is challenging Grammarly with an AI-enhanced proofreading feature that's part of Duet for Workspace.\",\n",
       "  'url': 'https://www.digitaltrends.com/computing/google-docs-gets-ai-proofreading-via-duet/',\n",
       "  'urlToImage': 'https://www.digitaltrends.com/wp-content/uploads/2022/05/google-docs-smart-features-2.jpeg?resize=1200%2C630&p=1',\n",
       "  'publishedAt': '2023-08-30T16:49:11Z',\n",
       "  'content': 'Google\\r\\nGoogle announced a full suite of artificial intelligence enhancements for Workspace at its IO event in May. Now, those advanced features are available as Google Duet, and one stands out as a … [+1623 chars]'},\n",
       " {'source': {'id': None, 'name': 'Yahoo Entertainment'},\n",
       "  'author': 'Gillian Tan',\n",
       "  'title': 'Nvidia-Backed CoreWeave\\xa0Seeks Stake Sale at Up to $8 Billion Valuation',\n",
       "  'description': '(Bloomberg) -- CoreWeave, a cloud computing provider that’s becoming one of the hottest startups in the artificial intelligence race, is exploring a minority...',\n",
       "  'url': 'https://finance.yahoo.com/news/nvidia-backed-coreweave-seeks-stake-155748985.html',\n",
       "  'urlToImage': 'https://s.yimg.com/ny/api/res/1.2/tkeJrQ2poDsXG6uiSfuY4Q--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://media.zenfs.com/en/bloomberg_technology_68/9e2ce32ea60cb278aac1c70a1998ee51',\n",
       "  'publishedAt': '2023-08-30T15:57:48Z',\n",
       "  'content': '(Bloomberg) -- CoreWeave, a cloud computing provider thats becoming one of the hottest startups in the artificial intelligence race, is exploring a minority stake sale that values the company at as m… [+1976 chars]'},\n",
       " {'source': {'id': None, 'name': 'Yahoo Entertainment'},\n",
       "  'author': 'Charlotte Hughes-Morgan',\n",
       "  'title': 'UK Needs Tough Rules on AI Using Copyrighted Data, Lawmakers Say',\n",
       "  'description': '(Bloomberg) -- A group of UK lawmakers are calling on the government to enforce clearer and tougher rules on artificial intelligence systems that are trained...',\n",
       "  'url': 'https://finance.yahoo.com/news/uk-needs-tough-rules-ai-230100182.html',\n",
       "  'urlToImage': 'https://s.yimg.com/ny/api/res/1.2/JZFCpOXFcBcFNO755EDB9g--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD03OTk-/https://media.zenfs.com/en/bloomberg_technology_68/6ee2b36882892a0b7c17556281b6aaea',\n",
       "  'publishedAt': '2023-08-29T23:01:00Z',\n",
       "  'content': '(Bloomberg) -- A group of UK lawmakers are calling on the government to enforce clearer and tougher rules on artificial intelligence systems that are trained off the work of artists, writers and othe… [+2745 chars]'},\n",
       " {'source': {'id': None, 'name': 'Yahoo Entertainment'},\n",
       "  'author': 'Zheping Huang and Jane Zhang',\n",
       "  'title': 'Baidu Among First Firms to Win China Approval for AI Models',\n",
       "  'description': '(Bloomberg) -- China will approve the first batch of generative artificial intelligence services for public rollout as soon as this week, freeing up...',\n",
       "  'url': 'https://finance.yahoo.com/news/baidu-among-first-firms-win-164304670.html',\n",
       "  'urlToImage': 'https://s.yimg.com/ny/api/res/1.2/xoZR43zTaadYu9FKQf6Acw--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://media.zenfs.com/en/bloomberg_technology_68/6e7a1ea82507dc412150b40b17f0450c',\n",
       "  'publishedAt': '2023-08-30T16:43:04Z',\n",
       "  'content': '(Bloomberg) -- China will approve the first batch of generative artificial intelligence services for public rollout as soon as this week, freeing up homegrown technology champions including Baidu Inc… [+4025 chars]'},\n",
       " {'source': {'id': None, 'name': 'Yahoo Entertainment'},\n",
       "  'author': 'Liana Baker and Katie Roof',\n",
       "  'title': 'AI Startup Cohere Taps Banks for Fresh Fundraising Round',\n",
       "  'description': '(Bloomberg) -- Artificial intelligence startup Cohere, backed by investors including Oracle Corp. and Nvidia Corp., is working with banks to raise a fresh...',\n",
       "  'url': 'https://finance.yahoo.com/news/ai-startup-cohere-taps-banks-224011228.html',\n",
       "  'urlToImage': 'https://s.yimg.com/ny/api/res/1.2/0epIkJiq7piRfJzJYZVQQQ--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://media.zenfs.com/en/bloomberg_technology_68/12fb541298fb513361987455cb2a0303',\n",
       "  'publishedAt': '2023-08-30T22:40:11Z',\n",
       "  'content': '(Bloomberg) -- Artificial intelligence startup Cohere, backed by investors including Oracle Corp. and Nvidia Corp., is working with banks to raise a fresh funding round, just a few months after its l… [+1595 chars]'},\n",
       " {'source': {'id': None, 'name': 'DIYphotography'},\n",
       "  'author': 'Alex Baker',\n",
       "  'title': 'Google unveils a new weapon against AI-generated disinformation',\n",
       "  'description': 'Google has commenced trials of an innovative digital watermark system designed to identify images produced by artificial intelligence (AI). Spearheaded by DeepMind, Google’s AI division, the watermarking technology, called SynthID, aims to expose images creat…',\n",
       "  'url': 'https://www.diyphotography.net/google-unveils-a-new-weapon-against-ai-generated-disinformation/',\n",
       "  'urlToImage': 'https://www.diyphotography.net/wp-content/uploads/2023/08/fs9ikpamfvy.jpg',\n",
       "  'publishedAt': '2023-08-31T16:04:24Z',\n",
       "  'content': 'Google has commenced trials of an innovative digital watermark system designed to identify images produced by artificial intelligence (AI).\\r\\nSpearheaded by DeepMind, Google’s AI division, the waterma… [+2842 chars]'},\n",
       " {'source': {'id': None, 'name': 'Forbes'},\n",
       "  'author': 'Suresh Menon, Forbes Councils Member, \\n Suresh Menon, Forbes Councils Member\\n https://www.forbes.com/sites/forbestechcouncil/people/sureshmenon/',\n",
       "  'title': 'How Four Retail Basics Are Better With Artificial Intelligence',\n",
       "  'description': 'A personal touch has always been important to retail; today, it’s provided in new, AI-generated ways.',\n",
       "  'url': 'https://www.forbes.com/sites/forbestechcouncil/2023/08/31/how-four-retail-basics-are-better-with-artificial-intelligence/',\n",
       "  'urlToImage': 'https://imageio.forbes.com/specials-images/imageserve/648726ec4c121273abd8baa2/0x0.jpg?format=jpg&width=1200',\n",
       "  'publishedAt': '2023-08-31T13:15:00Z',\n",
       "  'content': 'Suresh Menon, Senior Vice President and General Manager, Software Solutions at Zebra Technologies.\\r\\ngetty\\r\\nRetailers continue to tackle a highly challenging macroeconomic environment. The days of rea… [+6579 chars]'},\n",
       " {'source': {'id': None, 'name': 'Forbes'},\n",
       "  'author': 'Adam Blue, Forbes Councils Member, \\n Adam Blue, Forbes Councils Member\\n https://www.forbes.com/sites/forbestechcouncil/people/adamblue/',\n",
       "  'title': 'Beware Of Artificial Intelligence With Puppy Dog Eyes',\n",
       "  'description': 'We’re not going to solve every problem by being smarter or through the use of technology.',\n",
       "  'url': 'https://www.forbes.com/sites/forbestechcouncil/2023/08/30/beware-of-artificial-intelligence-with-puppy-dog-eyes/',\n",
       "  'urlToImage': 'https://imageio.forbes.com/specials-images/imageserve/649d8709912e06741b6aa415/0x0.jpg?format=jpg&width=1200',\n",
       "  'publishedAt': '2023-08-30T12:30:00Z',\n",
       "  'content': \"As Q2's Chief Technology Officer, Adam Blue drives the overarching innovation strategy to solve current and future customer challenges.\\r\\ngetty\\r\\nThe other night, as I re-watched WALL-E, I thought abou… [+5875 chars]\"},\n",
       " {'source': {'id': None, 'name': 'Small Business Trends'},\n",
       "  'author': 'Small Business Editor',\n",
       "  'title': 'Twilio’s CustomerAI Setting a New Benchmark for Customer Engagement',\n",
       "  'description': 'Twilio unveils CustomerAI at SIGNAL 2023, revolutionizing customer engagement with AI-powered predictions, voice intelligence, and more.',\n",
       "  'url': 'https://smallbiztrends.com/2023/09/twilios-customerai-setting-a-new-benchmark-for-customer-engagement.html',\n",
       "  'urlToImage': 'https://smallbiztrends.com/wp-content/themes/sahifa/images/logo-full.jpg',\n",
       "  'publishedAt': '2023-09-03T09:00:20Z',\n",
       "  'content': 'Twilio has taken a quantum leap in the realm of customer engagement by launching its revolutionary platform, CustomerAI. This powerhouse brings artificial intelligence to the forefront, offering busi… [+2195 chars]'},\n",
       " {'source': {'id': None, 'name': 'AnandTech'},\n",
       "  'author': 'Anton Shilov',\n",
       "  'title': \"Samsung Unveils Industry's First 32Gbit DDR5 Memory Die: 1TB Modules Incoming\",\n",
       "  'description': \"Samsung early on Friday revealed the world's first 32 Gb DDR5 DRAM die. The new memory die is made on the company's\\xa012 nm-class DRAM fabrication process\\xa0and not only offers increased density, but also lowers power consumption. The chip will allow Samsung to b…\",\n",
       "  'url': 'https://www.anandtech.com/show/20039/samsung-unveils-industrys-first-32gbit-ddr5-die-1tb-modules-incoming',\n",
       "  'urlToImage': 'https://images.anandtech.com/doci/20039/32Gb-DDR5-DRAM_PR_dl2_678x452.jpg',\n",
       "  'publishedAt': '2023-09-01T15:00:00Z',\n",
       "  'content': \"Samsung early on Friday revealed the world's first 32 Gb DDR5 DRAM die. The new memory die is made on the company's\\xa012 nm-class DRAM fabrication process\\xa0and not only offers increased density, but als… [+2279 chars]\"},\n",
       " {'source': {'id': None, 'name': 'NPR'},\n",
       "  'author': 'Suzanne Nuyen',\n",
       "  'title': 'Up First briefing: Hurricane Idalia; Proud Boys; Spanish Soccer controversy',\n",
       "  'description': \"Florida braces for Hurricane Idalia's landfall. The ex-leader of the Proud Boys will be sentenced today. An unwanted World Cup kiss sparks controversy in Spain.\",\n",
       "  'url': 'https://www.npr.org/2023/08/30/1196717755/up-first-briefing-hurricane-idalia-proud-boys-spanish-soccer-controversy',\n",
       "  'urlToImage': 'https://media.npr.org/assets/img/2023/08/30/idalia.beach.getty_wide-c22df7f2f16ca66d85d40e8bb70a7067429e84b0-s1400-c100.jpg',\n",
       "  'publishedAt': '2023-08-30T11:45:20Z',\n",
       "  'content': \"Good morning. You're reading the Up First newsletter. Subscribe here to get it delivered to your inbox, and listen to the Up First podcast for all the news you need to start your day.\\r\\nToday's top st… [+5642 chars]\"},\n",
       " {'source': {'id': None, 'name': 'Ritholtz.com'},\n",
       "  'author': 'Barry Ritholtz',\n",
       "  'title': '10 Thursday AM Reads',\n",
       "  'description': 'My end of August, morning train WFH reads: • Money Is Pouring Into AI. Skeptics Say It’s a ‘Grift Shift.’ The move from crypto to artificial intelligence has fueled the markets this year, but some are questioning how much of it is real. (Institutional Investo…',\n",
       "  'url': 'https://ritholtz.com/2023/08/10-thursday-am-reads-445/',\n",
       "  'urlToImage': 'https://ritholtz.com/wp-content/uploads/2023/03/recession.png',\n",
       "  'publishedAt': '2023-08-31T10:55:52Z',\n",
       "  'content': 'Money Is Pouring Into AI. Skeptics Say Its a Grift Shift. The move from crypto to artificial intelligence has fueled the markets this year, but some are questioning how much of it is real. (Instituti… [+3021 chars]'},\n",
       " {'source': {'id': 'business-insider', 'name': 'Business Insider'},\n",
       "  'author': 'Jordan Hart',\n",
       "  'title': 'I used AI to remodel a fixer-upper house my wife and I bought. It potentially saved us thousands of dollars.',\n",
       "  'description': \"My wife and I bought a fixer-upper home for around $460,000 in 2020. We've been using AI to remodel it. AI helped, but maybe not in the expected way.\",\n",
       "  'url': 'https://www.businessinsider.com/ai-helped-me-remodel-my-old-fixer-upper-house-remodel-2023-9',\n",
       "  'urlToImage': 'https://i.insider.com/64f0b0bc57bff700194780df?width=1200&format=jpeg',\n",
       "  'publishedAt': '2023-09-02T10:02:06Z',\n",
       "  'content': 'Tyler Bouldin and his wife document their renovation on TikTok.Courtesy ofTyler Bouldin\\r\\n<ul>\\n<li>My wife and I bought an old home in the middle of nowhere Pennsylvania in 2020.</li>\\n<li>Since then, … [+4482 chars]'},\n",
       " {'source': {'id': None, 'name': 'Digital Trends'},\n",
       "  'author': 'Trevor Mogg',\n",
       "  'title': 'AI drone beats pro drone racers at their own game',\n",
       "  'description': 'An AI drone has beaten pro drone racers at their own game in a world first that could have implications for other drone-related tasks such as search and rescue.',\n",
       "  'url': 'https://www.digitaltrends.com/news/ai-drone-beats-pro-drone-racers-at-their-own-game/',\n",
       "  'urlToImage': 'https://www.digitaltrends.com/wp-content/uploads/2023/08/drone-race-AI.jpeg?resize=1200%2C630&p=1',\n",
       "  'publishedAt': '2023-09-01T01:40:14Z',\n",
       "  'content': 'Champion-level Drone Racing using Deep Reinforcement Learning (Nature, 2023)\\r\\nProfessional drone racers are the latest to suffer the ignominy of being outsmarted by\\xa0artificial intelligence after quad… [+3285 chars]'},\n",
       " {'source': {'id': None, 'name': 'Techmeme.com'},\n",
       "  'author': None,\n",
       "  'title': \"Google adds AI tools from others, like Meta's Llama 2 and Anthropic's Claude 2, to Cloud, expands Duet AI in Workspace, partners with Fox Sports, and more (Bloomberg)\",\n",
       "  'description': \"Bloomberg:\\nGoogle adds AI tools from others, like Meta's Llama 2 and Anthropic's Claude 2, to Cloud, expands Duet AI in Workspace, partners with Fox Sports, and more\\xa0 —\\xa0 Alphabet Inc.'s Google is adding artificial intelligence tools from companies including M…\",\n",
       "  'url': 'https://www.techmeme.com/230829/p14',\n",
       "  'urlToImage': 'https://assets.bwbx.io/images/users/iqjWHBFdfxIU/i_vxVV3jkHoA/v0/1200x800.jpg',\n",
       "  'publishedAt': '2023-08-29T12:35:17Z',\n",
       "  'content': 'About This Page\\r\\nThis is a Techmeme archive page.\\r\\nIt shows how the site appeared at 8:35\\xa0AM\\xa0ET, August\\xa029,\\xa02023.\\r\\nThe most current version of the site as always is available at our home page.\\r\\nTo vi… [+69 chars]'},\n",
       " {'source': {'id': None, 'name': 'Techmeme.com'},\n",
       "  'author': None,\n",
       "  'title': 'Source: OpenAI is on pace to generate more than $1B in revenue over the next 12 months from the sale of AI software and the computing capacity that powers it (Amir Efrati/The Information)',\n",
       "  'description': 'Amir Efrati / The Information:\\nSource: OpenAI is on pace to generate more than $1B in revenue over the next 12 months from the sale of AI software and the computing capacity that powers it\\xa0 —\\xa0 OpenAI is currently on pace to generate more than $1 billion in re…',\n",
       "  'url': 'https://www.techmeme.com/230829/p40',\n",
       "  'urlToImage': 'https://tii.imgix.net/production/articles/11166/9878fb7c-dc5f-4d6c-868d-237bf3efa790.png?fm=jpeg&auto=compress&w=610',\n",
       "  'publishedAt': '2023-08-29T23:25:06Z',\n",
       "  'content': 'About This Page\\r\\nThis is a Techmeme archive page.\\r\\nIt shows how the site appeared at 7:45\\xa0PM\\xa0ET, August\\xa029,\\xa02023.\\r\\nThe most current version of the site as always is available at our home page.\\r\\nTo vi… [+321 chars]'},\n",
       " {'source': {'id': None, 'name': 'ReadWrite'},\n",
       "  'author': 'Brad Anderson',\n",
       "  'title': 'How Does Technology Improve Workplace Safety?',\n",
       "  'description': 'Every business should prioritize workplace safety and increase it to offer employees the best environment. Employees want to feel protected […]\\nThe post How Does Technology Improve Workplace Safety?\\xa0 appeared first on ReadWrite.',\n",
       "  'url': 'https://readwrite.com/how-does-technology-improve-workplace-safety/',\n",
       "  'urlToImage': 'https://readwrite.com/wp-content/uploads/2023/08/woman-staring-at-ipad-workplace-safety.jpeg',\n",
       "  'publishedAt': '2023-08-29T16:09:09Z',\n",
       "  'content': 'Every business should prioritize workplace safety and increase it to offer employees the best environment. Employees want to feel protected to give their best, and it is a company’s responsibility to… [+6407 chars]'},\n",
       " {'source': {'id': None, 'name': 'Scientific American'},\n",
       "  'author': 'Dina Genkina',\n",
       "  'title': 'AI Could Smuggle Secret Messages in Memes',\n",
       "  'description': 'A new technique for sending hidden messages is mathematically proven to escape detection',\n",
       "  'url': 'https://www.scientificamerican.com/article/ai-could-smuggle-secret-messages-in-memes/',\n",
       "  'urlToImage': 'https://static.scientificamerican.com/sciam/cache/file/51D2880D-C9AD-4EE5-864F44F55794A4D8.jpg',\n",
       "  'publishedAt': '2023-09-01T13:00:00Z',\n",
       "  'content': \"In an advance that could benefit spies and dissidents alike, computer scientists have developed a way to communicate confidential information so discreetly that an adversary couldn't even know secret… [+2789 chars]\"},\n",
       " {'source': {'id': 'al-jazeera-english', 'name': 'Al Jazeera English'},\n",
       "  'author': 'Al Jazeera',\n",
       "  'title': 'History Illustrated: The Gabon coup: End of a family dynasty?',\n",
       "  'description': 'The Bongo clan has faced corruption allegations for more than 50 years, so what will the recent military coup change?',\n",
       "  'url': 'https://www.aljazeera.com/gallery/2023/9/4/history-illustrated-the-gabon-coup-end-of-a-family-dynasty',\n",
       "  'urlToImage': 'https://www.aljazeera.com/wp-content/uploads/2023/09/1-copy-1693814628.jpg?resize=1920%2C1440',\n",
       "  'publishedAt': '2023-09-04T09:06:07Z',\n",
       "  'content': 'History Illustrated is a weekly series of insightful perspectives that puts news events and current affairs into historical context using graphics generated with artificial intelligence.'},\n",
       " {'source': {'id': None, 'name': 'Quartz India'},\n",
       "  'author': 'Michelle Cheng',\n",
       "  'title': 'Google’s new AI-powered search results are ripping off news sites',\n",
       "  'description': 'Earlier this year, as part of its experiments with artificial intelligence, Google released a new search feature that provides an AI-generated overview of search results. The idea is to get users to their answers faster, without needing to leave the search re…',\n",
       "  'url': 'https://qz.com/google-s-new-ai-powered-search-results-are-ripping-off-1850786915',\n",
       "  'urlToImage': 'https://i.kinja-img.com/gawker-media/image/upload/c_fill,f_auto,fl_progressive,g_center,h_675,pg_1,q_80,w_1200/d728b3c126841b148d1e837069d260d2.jpg',\n",
       "  'publishedAt': '2023-08-30T18:26:55Z',\n",
       "  'content': 'Earlier this year, as part of its experiments with artificial intelligence, Google released a new search feature that provides an AI-generated overview of search results. The idea is to get users to … [+2615 chars]'},\n",
       " {'source': {'id': None, 'name': 'Forbes'},\n",
       "  'author': 'Greg Licholai, MD, Contributor, \\n Greg Licholai, MD, Contributor\\n https://www.forbes.com/sites/greglicholai/',\n",
       "  'title': 'AI Is Game Changer For Toughest Areas Of Drug Discovery',\n",
       "  'description': 'Pharma drug research and development is undergoing changes thanks to the power of artificial intelligence',\n",
       "  'url': 'https://www.forbes.com/sites/greglicholai/2023/08/31/ai-is-game-changer-for-toughest-areas-of-drug-discovery/',\n",
       "  'urlToImage': 'https://imageio.forbes.com/specials-images/imageserve/64ef7b6f066b9c5eddf3933e/0x0.jpg?format=jpg&width=1200',\n",
       "  'publishedAt': '2023-08-31T11:00:00Z',\n",
       "  'content': \"Artificial intelligence is helping drug discovery's toughest problems \\r\\ngetty\\r\\nThe world of drug research and development (R&amp;D) is undergoing a seismic shift, thanks to the power of artificial in… [+5150 chars]\"},\n",
       " {'source': {'id': None, 'name': 'Forbes'},\n",
       "  'author': 'Tim Newcomb, Contributor, \\n Tim Newcomb, Contributor\\n https://www.forbes.com/sites/timnewcomb/',\n",
       "  'title': 'IBM Mining Millions Of Data Points From U.S. Open Stadiums',\n",
       "  'description': 'Or at least, the artificial intelligence engine run by IBM is learning the game.',\n",
       "  'url': 'https://www.forbes.com/sites/timnewcomb/2023/09/04/ibm-mining-millions-of-data-points-from-us-open-stadiums/',\n",
       "  'urlToImage': 'https://imageio.forbes.com/specials-images/imageserve/64ee4d3413566a1d6c4af6bb/0x0.jpg?format=jpg&width=1200',\n",
       "  'publishedAt': '2023-09-04T13:00:00Z',\n",
       "  'content': 'Arthur Ashe Stadium at the U.S. Open. (Mike Lawrence/USTA)\\r\\nMike Lawrence/USTA\\r\\nWatson knows tennis. Or at least, the artificial intelligence engine run by IBMIBM\\r\\n is learning the game.\\r\\nNew for the… [+3513 chars]'},\n",
       " {'source': {'id': None, 'name': 'Forbes'},\n",
       "  'author': 'David Prosser, Contributor, \\n David Prosser, Contributor\\n https://www.forbes.com/sites/davidprosser/',\n",
       "  'title': 'Voxel Raises Another $12 Million To Makes Workplaces Safer',\n",
       "  'description': \"Voxel's artificial intelligence tools help employers to intervene before workplace accidents happen\",\n",
       "  'url': 'https://www.forbes.com/sites/davidprosser/2023/08/30/voxel-raises-another-12-million-to-makes-workplaces-safer/',\n",
       "  'urlToImage': 'https://imageio.forbes.com/specials-images/imageserve/64ee39469888b21efbd8ae2d/0x0.jpg?format=jpg&crop=3557,1668,x0,y568,safe&width=1200',\n",
       "  'publishedAt': '2023-08-30T11:00:00Z',\n",
       "  'content': \"Voxel's team believe AI can dramatically reduce the number of workplace accidents \\r\\nVoxel\\r\\nSan Francisco-based start-up Voxel promises to hugely reduce the possibility of you suffering an accident at… [+3728 chars]\"},\n",
       " {'source': {'id': None, 'name': 'Forbes'},\n",
       "  'author': 'David Prosser, Contributor, \\n David Prosser, Contributor\\n https://www.forbes.com/sites/davidprosser/',\n",
       "  'title': 'Meet Five New European Unicorns That Have Defied The Gloom',\n",
       "  'description': \"Technology start-ups are having a tough time finding funding as investors become more risk-averse, but that hasn't stopped these businesses making it to unicorn status\",\n",
       "  'url': 'https://www.forbes.com/sites/davidprosser/2023/08/31/meet-five-new-european-unicorns-that-have-defied-the-gloom/',\n",
       "  'urlToImage': 'https://imageio.forbes.com/specials-images/imageserve/64f06646fc38d613e2f39496/0x0.jpg?format=jpg&crop=3000,1406,x0,y295,safe&width=1200',\n",
       "  'publishedAt': '2023-08-31T10:09:58Z',\n",
       "  'content': 'The best European start-ups continue to accelerate\\r\\nGetty Images\\r\\nIt has been a difficult year for European start-ups and technology-focused start-ups in particular. The turbulent economic and geo-po… [+3523 chars]'},\n",
       " {'source': {'id': None, 'name': 'Psychology Today'},\n",
       "  'author': 'Mackenzie Littledale',\n",
       "  'title': 'AI: Friend or Foe?',\n",
       "  'description': \"Artificial intelligence may sound scary, but it's possible to embrace, cope, and grow in the face of it.\",\n",
       "  'url': 'https://www.psychologytoday.com/intl/blog/from-margin-to-mainstream/202309/ai-friend-or-foe',\n",
       "  'urlToImage': 'https://cdn2.psychologytoday.com/assets/styles/manual_crop_1_91_1_1528x800/public/teaser_image/blog_entry/2023-09/2023-0901%20Leno%20Michele%20HEADSHOT%202.jpeg.jpg?itok=EMQ7Bsvr',\n",
       "  'publishedAt': '2023-09-02T15:26:36Z',\n",
       "  'content': 'Michele Leno completed her undergraduate studies at Marygrove College, attended the Michigan School of Professional Psychology, and obtained her M.A. in clinical psychology. She completed her doctora… [+5953 chars]'},\n",
       " {'source': {'id': None, 'name': 'Adweek'},\n",
       "  'author': 'Trishla Ostwal',\n",
       "  'title': 'Google’s New GenAI Marketing Tools Speed Up Campaign Planning and Buying',\n",
       "  'description': 'At the annual Google Cloud Next conference in San Francisco, the tech giant announced a generative artificial intelligence-powered marketing solution that aims to speed up the time it takes for marketers to build and serve marketing campaigns. Google is colla…',\n",
       "  'url': 'https://www.adweek.com/programmatic/google-genai-marketing-tools-speed-up-campaign-planning-buying/',\n",
       "  'urlToImage': 'https://static-prod.adweek.com/wp-content/uploads/2023/08/google-gen-ai-tool-2023-600x315.jpg',\n",
       "  'publishedAt': '2023-08-30T15:00:00Z',\n",
       "  'content': 'At the annual Google Cloud Next conference in San Francisco, the tech giant announced a generative artificial intelligence-powered marketing solution that aims to speed up the time it takes for marke… [+4106 chars]'},\n",
       " {'source': {'id': None, 'name': 'Adafruit.com'},\n",
       "  'author': 'Ben',\n",
       "  'title': 'AI beats champion human pilots in head-to-head drone races #drone #droneday',\n",
       "  'description': 'There is a movie plot in here somewhere. Will humans be able to catch up? From New Scientist:An artificial intelligence has consistently beaten champion drone pilots in races for the first time, achieving lap times no human was able to match. The sport of dro…',\n",
       "  'url': 'https://blog.adafruit.com/2023/09/04/ai-beats-champion-human-pilots-in-head-to-head-drone-races-drone-droneday/',\n",
       "  'urlToImage': 'https://cdn-blog.adafruit.com/uploads/2023/09/ai-beats-champion-human-pilots-i.jpg',\n",
       "  'publishedAt': '2023-09-04T20:00:00Z',\n",
       "  'content': 'There is a movie plot in here somewhere. Will humans be able to catch up? From New Scientist:\\r\\nAn artificial intelligence has consistently beaten champion drone pilots in races for the first time, ac… [+3700 chars]'},\n",
       " {'source': {'id': 'techradar', 'name': 'TechRadar'},\n",
       "  'author': 'Craig Hale',\n",
       "  'title': 'AI could help companies connect with customers like never before',\n",
       "  'description': 'Customers expect the best experience thanks to artificial intelligence, but they also want to know when AI is being used.',\n",
       "  'url': 'https://www.techradar.com/pro/ai-could-help-companies-connect-with-customers-like-never-before',\n",
       "  'urlToImage': 'https://cdn.mos.cms.futurecdn.net/6eVES2TmJ7a6bhVSejKk6S-1200-80.jpg',\n",
       "  'publishedAt': '2023-09-04T13:17:25Z',\n",
       "  'content': 'New research from Salesforce has claimed companies that explicitly detail how they use AI writers and other related tools could prove more popular among customers.\\r\\nThe research of 14,300 consumers a… [+1830 chars]'},\n",
       " {'source': {'id': None, 'name': 'Slashdot.org'},\n",
       "  'author': 'BeauHD',\n",
       "  'title': \"AI Quadcopter 'Swift' Beats Top Human Drone Racers\",\n",
       "  'description': 'An autonomous, artificial-intelligence-powered drone called Swift has beaten humanity\\'s best drone racers. \"The AI-equipped drone, developed by researchers at the University of Zurich, came out on top in 15 out of 25 races and recorded the single fastest lap …',\n",
       "  'url': 'https://slashdot.org/story/23/08/31/2154254/ai-quadcopter-swift-beats-top-human-drone-racers',\n",
       "  'urlToImage': 'https://a.fsdn.com/sd/topics/ai_64.png',\n",
       "  'publishedAt': '2023-08-31T23:20:00Z',\n",
       "  'content': 'Swift beat the humans in the niche but growing sport of first-person view drone racing. Human competitors navigate using a headset connected to a camera on their drones to pilot a quadcopter through … [+1726 chars]'},\n",
       " {'source': {'id': None, 'name': 'ReadWrite'},\n",
       "  'author': 'Prashanth Samudrala',\n",
       "  'title': 'AI Brings New Capabilities and Risks to Healthcare Data Security',\n",
       "  'description': 'Data protection is a critical aspect of properly managing a healthcare organization’s IT environment. Healthcare data remains one of the […]\\nThe post AI Brings New Capabilities and Risks to Healthcare Data Security appeared first on ReadWrite.',\n",
       "  'url': 'https://readwrite.com/ai-brings-new-capabilities-and-risks-to-healthcare-data-security/',\n",
       "  'urlToImage': 'https://readwrite.com/wp-content/uploads/2023/07/Risks-to-Healthcare-Data-Security.jpg',\n",
       "  'publishedAt': '2023-08-31T15:00:10Z',\n",
       "  'content': 'Data protection is a critical aspect of properly managing a healthcare organizations IT environment. Healthcare data remains one of the top targets for cybercriminals due to the sensitivity level of … [+12183 chars]'},\n",
       " {'source': {'id': None, 'name': 'CNET'},\n",
       "  'author': 'Katie Collins',\n",
       "  'title': \"AI Took the Stage at the World's Largest Arts Festival. Here's What Happened - CNET\",\n",
       "  'description': 'A clown, a comic and a computer scientist walk into the Edinburgh Fringe festival. Can AI deliver a satisfying punchline?',\n",
       "  'url': 'https://www.cnet.com/tech/ai-took-the-stage-at-the-worlds-largest-arts-festival-heres-what-happened/',\n",
       "  'urlToImage': 'https://www.cnet.com/a/img/resize/ac4f8c4ce374f400edb17473c697c5505c73b090/hub/2023/09/01/78498ef1-4591-48ef-a32e-d17f83033c46/gettyimages-1258476293.jpg?auto=webp&fit=crop&height=675&width=1200',\n",
       "  'publishedAt': '2023-09-02T19:10:11Z',\n",
       "  'content': 'In a dusty basement, in front of about 100 amused onlookers, sex robot Vanessa 5000 shows off her sexy dance and her preprogrammed personalities for particular tastes. Like all technology, she someti… [+13028 chars]'},\n",
       " {'source': {'id': None, 'name': 'Digiday'},\n",
       "  'author': 'Antoinette Siu',\n",
       "  'title': 'How the growth of click and impression farming are getting worse with AI',\n",
       "  'description': 'While click farming and impression farming have been around in digital advertising, the increased use of artificial intelligence may worsen these larger fraud-related problems.',\n",
       "  'url': 'http://digiday.com/media-buying/the-growth-of-click-and-impression-farming-are-getting-worse-with-ai/',\n",
       "  'urlToImage': 'https://digiday.com/wp-content/uploads/sites/3/2023/02/robot-pencil-digiday.jpeg',\n",
       "  'publishedAt': '2023-08-29T04:01:00Z',\n",
       "  'content': 'While click farming and impression farming have been around for a while in digital advertising, the increased use of artificial intelligence may worsen these larger fraud-related problems. It’s the l… [+5349 chars]'},\n",
       " {'source': {'id': 'bbc-news', 'name': 'BBC News'},\n",
       "  'author': 'https://www.facebook.com/bbcnews',\n",
       "  'title': \"Artificial Intelligence: NI can be 'testing centre' for new technology\",\n",
       "  'description': 'A group representing software firms says NI can lead the way on AI after £10m Kainos investment.',\n",
       "  'url': 'https://www.bbc.co.uk/news/uk-northern-ireland-66662461',\n",
       "  'urlToImage': 'https://ichef.bbci.co.uk/news/1024/branded_news/13039/production/_130718877_gettyimages-1466716029.jpg',\n",
       "  'publishedAt': '2023-09-03T05:40:33Z',\n",
       "  'content': 'Northern Ireland can be a testing centre for artificial intelligence in the UK, according to a group representing the software industry.\\r\\nBelfast-based IT firm Kainos announced it was investing £10m … [+7014 chars]'},\n",
       " {'source': {'id': None, 'name': 'Psychology Today'},\n",
       "  'author': 'Cami Rosso',\n",
       "  'title': 'AI Decodes Brain Activity Into Speech With High Accuracy',\n",
       "  'description': 'Brain-computer interface (BCI) uses artificial intelligence (AI) deep learning to translate brain activity to speech with up to 100% accuracy.',\n",
       "  'url': 'https://www.psychologytoday.com/intl/blog/the-future-brain/202309/ai-decodes-brain-activity-into-speech-with-high-accuracy',\n",
       "  'urlToImage': 'https://cdn2.psychologytoday.com/assets/styles/manual_crop_1_91_1_1528x800/public/teaser_image/blog_entry/2023-09/picxl24u.jpg?itok=Qyv4HLIX',\n",
       "  'publishedAt': '2023-09-04T19:29:13Z',\n",
       "  'content': 'A new study published in this months Journal of Neural Engineering demonstrates how a brain-computer interface (BCI) uses artificial intelligence (AI) deep learning to translate brain activity to spe… [+3601 chars]'},\n",
       " {'source': {'id': None, 'name': 'Cartoon Brew'},\n",
       "  'author': 'Amid Amidi',\n",
       "  'title': 'Tech Bros Want To Replace Storyboard Artists With Artificial Intelligence',\n",
       "  'description': 'Numerous AI tools have been released this year with the aim of replacing storyboard artists. The animation industry needs to respond.',\n",
       "  'url': 'https://www.cartoonbrew.com/tech/tech-bros-want-to-replace-storyboard-artists-with-artificial-intelligence-232124.html',\n",
       "  'urlToImage': 'https://www.cartoonbrew.com/wp-content/uploads/2023/08/texavery_deputydroopy.jpg',\n",
       "  'publishedAt': '2023-08-31T20:37:00Z',\n",
       "  'content': 'Artificial intelligence is upending many industries at the moment, and in the filmmaking world, a key area that is currently under threat of automation is storyboarding. \\r\\nNumerous AI tools have been… [+2983 chars]'},\n",
       " {'source': {'id': None, 'name': 'The Indian Express'},\n",
       "  'author': 'Express News Service',\n",
       "  'title': 'Telerad Group, NIMHANS sign MoU for AI-enhanced radiology diagnostics to improve diagnosis of stroke, head trauma',\n",
       "  'description': 'Dr Arjun Kalyanpur, Founder CEO and Chief Radiologist, Telerad Group, said, “There is currently a nationwide (and global) shortage of radiologists for reporting of scans and Artificial Intelligence has shown the potential to deliver value in this scenario.”',\n",
       "  'url': 'https://indianexpress.com/article/cities/bangalore/telerad-group-nimhans-mou-ai-enhanced-radiology-improve-diagnosis-stroke-head-trauma-8916537/',\n",
       "  'urlToImage': 'https://images.indianexpress.com/2023/08/Telerad-Group-NIMHANS-MoU-1.jpg',\n",
       "  'publishedAt': '2023-08-30T13:19:56Z',\n",
       "  'content': 'The Telerad Group, a provider of remote radiology services, announced Wednesday that it signed a Memorandum of Understanding (MoU) with National Institute of Mental Health and Neurosciences (NIMHANS)… [+3605 chars]'},\n",
       " {'source': {'id': None, 'name': 'CNET'},\n",
       "  'author': 'Stephen Shankland',\n",
       "  'title': 'Google Duet AI: New Features for Gmail, Docs and Sheets, at $30 a Month - CNET',\n",
       "  'description': 'You can sign up for a free 14-day trial now, but prices for small businesses and consumers are yet to be revealed.',\n",
       "  'url': 'https://www.cnet.com/tech/services-and-software/google-duet-ai-new-features-for-gmail-docs-and-sheets-at-30-a-month/',\n",
       "  'urlToImage': 'https://www.cnet.com/a/img/resize/df51716a6d8bb7a303ac4952f7deab4b0ca1845b/hub/2022/03/07/efc09baa-157d-4a9b-af4b-8ed3d0b86283/google-gmail-logo-7142.jpg?auto=webp&fit=crop&height=675&width=1200',\n",
       "  'publishedAt': '2023-09-02T15:00:08Z',\n",
       "  'content': \"Google's Duet artificial intelligence tools for Google Workspace are now available for anyone to try, giving an AI boost to the company's widely used apps, including Gmail, Google Docs, Meet, Sheets … [+2009 chars]\"},\n",
       " {'source': {'id': None, 'name': 'BBC News'},\n",
       "  'author': 'https://www.facebook.com/bbcnews',\n",
       "  'title': 'Google Tests Watermark To Identify AI Images',\n",
       "  'description': \"The tech giant's artificial intelligence firm DeepMind unveils measures to counter disinformation.\",\n",
       "  'url': 'https://www.bbc.com/news/technology-66618852',\n",
       "  'urlToImage': 'https://ichef.bbci.co.uk/news/1024/branded_news/36C4/production/_130902041_copyofcopyofheader1-1.png',\n",
       "  'publishedAt': '2023-08-29T15:41:20Z',\n",
       "  'content': \"Google is trialling a digital watermark to spot images made by artificial intelligence (AI) in a bid to fight disinformation.\\r\\nDeveloped by DeepMind, Google's AI arm, SynthID will identify images gen… [+3894 chars]\"},\n",
       " {'source': {'id': None, 'name': '9to5Mac'},\n",
       "  'author': 'Ben Lovejoy',\n",
       "  'title': 'Apple Impact Accelerator 2023 rewards 12 more Black and Brown owned startups',\n",
       "  'description': 'The third round of the Apple Impact Accelerator program – which provides a range of assistance to tech startups – has just been announced. Applications opened back in April.\\n\\n\\n\\nThis time, 12 business owned by Black and Brown entrepreneurs have been selected f…',\n",
       "  'url': 'https://9to5mac.com/2023/08/30/apple-impact-accelerator-3/',\n",
       "  'urlToImage': 'https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2023/08/Apple-Impact-Accelerator-3.jpg?resize=1200%2C628&quality=82&strip=all&ssl=1',\n",
       "  'publishedAt': '2023-08-30T14:17:54Z',\n",
       "  'content': 'The third round of the Apple Impact Accelerator program which provides a range of assistance to tech startups has just been announced. Applications opened back in April.This time, 12 business owned b… [+7371 chars]'},\n",
       " {'source': {'id': None, 'name': 'Forbes'},\n",
       "  'author': 'Sai Balasubramanian, M.D., J.D., Contributor, \\n Sai Balasubramanian, M.D., J.D., Contributor\\n https://www.forbes.com/sites/saibala/',\n",
       "  'title': 'Bayer Is Rapidly Expanding Its Footprint With Artificial Intelligence',\n",
       "  'description': 'The company is partnering with Google Cloud to integrate cutting edge AI into its core businesses.',\n",
       "  'url': 'https://www.forbes.com/sites/saibala/2023/09/04/bayer-is-rapidly-expanding-its-footprint-with-artificial-intelligence/',\n",
       "  'urlToImage': 'https://imageio.forbes.com/specials-images/imageserve/64f5e994a70f9d57fab0157e/0x0.jpg?format=jpg&crop=2479,1395,x0,y186,safe&width=1200',\n",
       "  'publishedAt': '2023-09-04T14:29:55Z',\n",
       "  'content': 'Bayer is one of the worlds most prominent pharmaceutical and biotechnology companies. It is known for creating household staples such as Aspirin, to more advanced technologies and innovations that ar… [+5154 chars]'},\n",
       " {'source': {'id': None, 'name': 'Forbes'},\n",
       "  'author': 'Bryan Robinson, Ph.D., Contributor, \\n Bryan Robinson, Ph.D., Contributor\\n https://www.forbes.com/sites/bryanrobinson/',\n",
       "  'title': 'Workers Taking Wellness Into Their Own Hands Using AI-Backed Mental Health',\n",
       "  'description': 'Artificial intelligence and mental health might be an odd coupling, but employees are using the new technology to take control of their mental health.',\n",
       "  'url': 'https://www.forbes.com/sites/bryanrobinson/2023/09/02/workers-taking-wellness-into-their-own-hands-using-ai-backed-mental-health/',\n",
       "  'urlToImage': 'https://imageio.forbes.com/specials-images/imageserve/64dcc56ee715cb3003d8ae1f/0x0.jpg?format=jpg&width=1200',\n",
       "  'publishedAt': '2023-09-02T10:47:00Z',\n",
       "  'content': 'Artificial intelligence and mental health sound like an odd coupling, but new technology is allowing ... [+] employees to take their mental wellness into their own hands.\\r\\ngetty\\r\\nEveryone knows that … [+6832 chars]'},\n",
       " {'source': {'id': None, 'name': 'Forbes'},\n",
       "  'author': 'Federico Guerrini, Contributor, \\n Federico Guerrini, Contributor\\n https://www.forbes.com/sites/federicoguerrini/',\n",
       "  'title': 'European Countries Race To Set The AI Regulatory Pace',\n",
       "  'description': 'As artificial intelligence continues its meteoric rise, European nations are moving swiftly to establish regulatory frameworks and ramp up investments in the technology.',\n",
       "  'url': 'https://www.forbes.com/sites/federicoguerrini/2023/09/04/european-countries-race-to-set-the-ai-regulatory-pace/',\n",
       "  'urlToImage': 'https://imageio.forbes.com/specials-images/imageserve/64f616ed78e171de1285cca1/0x0.jpg?format=jpg&width=1200',\n",
       "  'publishedAt': '2023-09-04T18:06:18Z',\n",
       "  'content': 'The flag of the European Union (Photo by Sean Gallup/Getty Images)\\r\\nGetty Images\\r\\nAs artificial intelligence continues its meteoric rise, European nations are moving swiftly to establish regulatory f… [+3663 chars]'},\n",
       " {'source': {'id': None, 'name': 'GameSpot'},\n",
       "  'author': 'Jason Fanelli',\n",
       "  'title': 'Video Game Voice Actors May Go On Strike As Union Authorizes Vote',\n",
       "  'description': 'Actors union SAG-AFTRA has been striking for a new film and TV contract for weeks now, after joining the Writers Guild Of America strike, which began in May. That strike may soon extend to video game voice actors, as the union’s board has sent a vote to all m…',\n",
       "  'url': 'https://www.gamespot.com/articles/video-game-voice-actors-may-go-on-strike-as-union-authorizes-vote/1100-6517441/',\n",
       "  'urlToImage': 'https://www.gamespot.com/a/uploads/screen_kubrick/3/37852/4187325-img_4786.jpeg',\n",
       "  'publishedAt': '2023-09-01T22:16:00Z',\n",
       "  'content': 'Actors union SAG-AFTRA has been striking for a new film and TV contract for weeks now, after joining the Writers Guild Of America strike, which began in May. That strike may soon extend to video game… [+1628 chars]'},\n",
       " {'source': {'id': None, 'name': 'Forbes'},\n",
       "  'author': 'Colin Harper, Senior Contributor, \\n Colin Harper, Senior Contributor\\n https://www.forbes.com/sites/colinharper/',\n",
       "  'title': 'Can Bitcoin Miners ‘Do’ AI? Not Really',\n",
       "  'description': 'Some bitcoin miners are climbing onto the artificial-intelligence hype train as a way to diversify revenue streams. Many cannot deliver on their promises.',\n",
       "  'url': 'https://www.forbes.com/sites/colinharper/2023/08/30/can-bitcoin-miners-do-ai-not-really/',\n",
       "  'urlToImage': 'https://imageio.forbes.com/specials-images/imageserve/618e9d457f00bd1e890f9cff/0x0.jpg?format=jpg&width=1200',\n",
       "  'publishedAt': '2023-08-30T15:45:45Z',\n",
       "  'content': 'Bitcoin miners are trying to pivot to artificial intelligence with varying results\\r\\ngetty\\r\\nThe bitcoin industryparticularly its niche mining sectoris no stranger to hype trains. Now, a new one has ro… [+6426 chars]'},\n",
       " {'source': {'id': None, 'name': 'Forbes'},\n",
       "  'author': 'James Garvert, Forbes Councils Member, \\n James Garvert, Forbes Councils Member\\n https://www.forbes.com/sites/forbestechcouncil/people/jamesgarvert/',\n",
       "  'title': 'As AI Emboldens Fraudsters, Businesses Must Protect Their Customers From New Threats In The Phone Channel',\n",
       "  'description': 'Now, as businesses and consumers toy with the capabilities of generative artificial intelligence in many aspects of their lives, criminals are right there with them.',\n",
       "  'url': 'https://www.forbes.com/sites/forbestechcouncil/2023/09/01/as-ai-emboldens-fraudsters-businesses-must-protect-their-customers-from-new-threats-in-the-phone-channel/',\n",
       "  'urlToImage': 'https://imageio.forbes.com/specials-images/imageserve/64f0d172624333c513b575a3/0x0.jpg?format=jpg&width=1200',\n",
       "  'publishedAt': '2023-09-01T13:30:00Z',\n",
       "  'content': 'James Garvert is the SVP of TruContact Communications Solutions for TransUnion.\\r\\ngetty\\r\\nImpersonation scams once again topped the Federal Trade Commissions list of top frauds in 2022, and the phone c… [+5237 chars]'},\n",
       " {'source': {'id': None, 'name': 'Biztoc.com'},\n",
       "  'author': 'reuters.com',\n",
       "  'title': 'OpenAI releasing version of ChatGPT for large businesses',\n",
       "  'description': 'ChatGPT logo and AI Artificial Intelligence words are seen in this illustration taken May 4, 2023. REUTERS/Dado Ruvic/Illustration/File Photo/File Photo Acquire Licensing Rights Aug 28 (Reuters) - Artificial intelligence leader OpenAI said on Monday it is rel…',\n",
       "  'url': 'https://biztoc.com/x/7b08ca723153a172',\n",
       "  'urlToImage': 'https://c.biztoc.com/p/7b08ca723153a172/s.webp',\n",
       "  'publishedAt': '2023-08-29T01:06:07Z',\n",
       "  'content': 'ChatGPT logo and AI Artificial Intelligence words are seen in this illustration taken May 4, 2023. REUTERS/Dado Ruvic/Illustration/File Photo/File Photo Acquire Licensing RightsAug 28 (Reuters) - Art… [+277 chars]'},\n",
       " {'source': {'id': None, 'name': 'Biztoc.com'},\n",
       "  'author': 'ft.com',\n",
       "  'title': 'US should use chip leadership to enforce AI standards, Deep Mind co-founder says',\n",
       "  'description': 'Mustafa Suleyman says sales of Nvidia chips that dominate artificial intelligence should be tied to safe use pledges #mustafasuleyman #nvidia',\n",
       "  'url': 'https://biztoc.com/x/fafc7299ebfc2c71',\n",
       "  'urlToImage': 'https://c.biztoc.com/236/og.png',\n",
       "  'publishedAt': '2023-09-01T04:46:12Z',\n",
       "  'content': 'Mustafa Suleyman says sales of Nvidia chips that dominate artificial intelligence should be tied to safe use pledges\\r\\n#mustafasuleyman#nvidia\\r\\nThis story appeared on ft.com, 2023-09-01.'},\n",
       " {'source': {'id': None, 'name': 'Science Daily'},\n",
       "  'author': None,\n",
       "  'title': \"Unveiling global warming's impact on daily precipitation with deep learning\",\n",
       "  'description': 'A research team has conclusively demonstrated that global warming stands as primary driver behind the recent increase in heavy rainfall and heatwaves using deep learning convolutional neural network.',\n",
       "  'url': 'https://www.sciencedaily.com/releases/2023/08/230830131932.htm',\n",
       "  'urlToImage': 'https://www.sciencedaily.com/images/scidaily-icon.png',\n",
       "  'publishedAt': '2023-08-30T17:19:32Z',\n",
       "  'content': 'A collaborative international research team led by Professor Yoo-Geun Ham from Chonnam National University and Professor Seung-Ki Min from Pohang University of Science and Technology (POSTECH) has ma… [+2559 chars]'},\n",
       " {'source': {'id': None, 'name': 'Yahoo Entertainment'},\n",
       "  'author': 'Andrew Chuter',\n",
       "  'title': 'Saab buys British artificial intelligence specialist BlueBear',\n",
       "  'description': 'The Swedish-based company looks to expand its footprint in a fast-moving military market.',\n",
       "  'url': 'https://news.yahoo.com/saab-buys-british-artificial-intelligence-161101393.html',\n",
       "  'urlToImage': 'https://s.yimg.com/ny/api/res/1.2/r4RjwhuAqr9LEETG3ZnDlw--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02NzU-/https://media.zenfs.com/en/defense_news_499/93263b6a223751a8e57080424fa5c2e7',\n",
       "  'publishedAt': '2023-08-31T16:11:01Z',\n",
       "  'content': 'LONDON Saab has boosted its footprint in the artificial intelligence sector with the acquisition of British specialists BlueBear Systems.\\r\\nThe British company is a leading provider of AI-enabled auto… [+2044 chars]'},\n",
       " {'source': {'id': None, 'name': 'Digiday'},\n",
       "  'author': 'Sara Guaglione',\n",
       "  'title': 'How publishers like The Marshall Project and The Markup are testing generative AI in their newsrooms',\n",
       "  'description': 'Publishers including The Marshall Project and The Markup shared case studies of how they’re using generative artificial intelligence in their newsrooms for more efficient editorial processes – after some failed tests.',\n",
       "  'url': 'http://digiday.com/media/how-publishers-like-the-marshall-project-and-the-markup-are-testing-generative-ai-in-their-newsrooms/',\n",
       "  'urlToImage': 'https://digiday.com/wp-content/uploads/sites/3/2021/02/robot_nurture-02-02-02-02.jpg',\n",
       "  'publishedAt': '2023-08-29T04:01:00Z',\n",
       "  'content': 'Publishers including The Marshall Project and The Markup shared how reporters are using generative artificial intelligence in their newsrooms in their reporting processes after some failed tests.The … [+2710 chars]'},\n",
       " {'source': {'id': None, 'name': 'The A.V. Club'},\n",
       "  'author': 'William Hughes',\n",
       "  'title': 'SAG-AFTRA is seeking authorization to strike against the video game companies, now, too',\n",
       "  'description': 'Because one major industry-shaking strike just wasn’t enough, SAG-AFTRA president Fran Drescher announced today that the actors union is requesting authorization to strike against several of the planet’s biggest video game publishers, too. Negotiations betwee…',\n",
       "  'url': 'https://www.avclub.com/sag-aftra-is-seeking-authorization-to-strike-against-th-1850798612',\n",
       "  'urlToImage': 'https://i.kinja-img.com/gawker-media/image/upload/c_fill,f_auto,fl_progressive,g_center,h_675,pg_1,q_80,w_1200/642257f44fad04cf10bde10c62abbdb7.jpg',\n",
       "  'publishedAt': '2023-09-02T01:02:00Z',\n",
       "  'content': 'Because one major industry-shaking strike just wasnt enough, SAG-AFTRA president Fran Drescher announced today that the actors union is requesting authorization to strike against several of the plane… [+1893 chars]'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_news['articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4e31adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An AI Moderator Is Coming With ‘Call of Duty: Modern Warfare 3’\n",
      "\n",
      "When Call of Duty: Modern Warfare 3 comes out on Nov. 10, all players’ voice chats will be silently monitored by artificial intelligence. The AI-powered moderation technology, Toxmod, is designed to identify toxic speech in real time in multiplayer games to c…\n",
      "{'source': 'https://lifehacker.com/an-ai-moderator-is-coming-with-call-of-duty-modern-wa-1850793420'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "docs = [\n",
    "    Document(\n",
    "    page_content=article['title'] + '\\n\\n' + article['description'], \n",
    "    metadata={\n",
    "        'source': article['url'],\n",
    "    }\n",
    "    ) for article in latest_news['articles']\n",
    "]\n",
    "\n",
    "print(docs[0].page_content)\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c399a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c7a5a45f7f443539d476a9443b8edcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.chains import create_qa_with_sources_chain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "qa_chain = create_qa_with_sources_chain(llm)\n",
    "\n",
    "doc_prompt = PromptTemplate(\n",
    "    template=\"Content: {page_content}\\nSource: {source}\",\n",
    "    input_variables=[\"page_content\", \"source\"],\n",
    ")\n",
    "\n",
    "final_qa_chain = StuffDocumentsChain(\n",
    "    llm_chain=qa_chain,\n",
    "    document_variable_name=\"context\",\n",
    "    document_prompt=doc_prompt,\n",
    ")\n",
    "\n",
    "index = FAISS.from_documents(docs, embedding=embeddings)\n",
    "\n",
    "\n",
    "chain = RetrievalQA(\n",
    "    retriever=index.as_retriever(), \n",
    "    combine_documents_chain=final_qa_chain\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "753705b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8988497ae4a471cb3180119ec5014a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"answer\": \"The most important news about artificial intelligence from last week is the use of AI to train on the works of authors Stephen King and Margaret Atwood. These authors responded to the revelation that their work is being used to train AI. Additionally, AI took the stage at the Edinburgh Fringe festival, raising the question of whether AI can deliver a satisfying punchline. Furthermore, a tech expert from the University of Oxford highlighted the potential workplace threats of AI, including the possibility of AI becoming a monitoring boss. Finally, AI is being seen as a tool that can help companies connect with customers in a more personalized and efficient way.\",\n",
      "  \"sources\": [\n",
      "    \"https://www.theatlantic.com/newsletters/archive/2023/09/books-briefing-ai-stephen-king-margaret-atwood/675213/?utm_source=feed\",\n",
      "    \"https://www.cnet.com/tech/ai-took-the-stage-at-the-worlds-largest-arts-festival-heres-what-happened/\",\n",
      "    \"https://www.foxnews.com/tech/tech-expert-existential-fears-ai-are-overblown-sees-very-disturbing-workplace-threats\",\n",
      "    \"https://www.techradar.com/pro/ai-could-help-companies-connect-with-customers-like-never-before\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "What is the most important news about artificial intelligence from last week?\n",
    "\"\"\"\n",
    "\n",
    "answer = chain.run(question)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5605d77e",
   "metadata": {},
   "source": [
    "# Indexing a website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7673e4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: apify-client in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (1.4.0)\n",
      "Requirement already satisfied: chromadb in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (0.4.8)\n",
      "Requirement already satisfied: apify-shared~=1.0.0 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from apify-client) (1.0.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from apify-client) (0.24.1)\n",
      "Requirement already satisfied: requests>=2.28 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from chromadb) (2.31.0)\n",
      "Requirement already satisfied: pydantic<2.0,>=1.9 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from chromadb) (1.10.12)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.2 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from chromadb) (0.7.2)\n",
      "Requirement already satisfied: fastapi<0.100.0,>=0.95.2 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from chromadb) (0.99.1)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from chromadb) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from chromadb) (1.25.2)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from chromadb) (3.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from chromadb) (4.7.1)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from chromadb) (3.3.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from chromadb) (1.15.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from chromadb) (0.13.3)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from chromadb) (4.66.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from chromadb) (6.0.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from chromadb) (4.0.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from fastapi<0.100.0,>=0.95.2->chromadb) (0.27.0)\n",
      "Requirement already satisfied: certifi in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from httpx>=0.24.1->apify-client) (2023.7.22)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from httpx>=0.24.1->apify-client) (0.16.3)\n",
      "Requirement already satisfied: idna in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from httpx>=0.24.1->apify-client) (3.4)\n",
      "Requirement already satisfied: sniffio in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from httpx>=0.24.1->apify-client) (1.2.0)\n",
      "Requirement already satisfied: coloredlogs in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
      "Requirement already satisfied: packaging in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (23.0)\n",
      "Requirement already satisfied: protobuf in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (4.24.2)\n",
      "Requirement already satisfied: sympy in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: six>=1.5 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>2.1 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from requests>=2.28->chromadb) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from requests>=2.28->chromadb) (1.26.16)\n",
      "Requirement already satisfied: click>=7.0 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.6)\n",
      "Requirement already satisfied: h11>=0.8 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.0)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.20.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from httpcore<0.18.0,>=0.15.0->httpx>=0.24.1->apify-client) (3.5.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install apify-client chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64bc548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import ApifyWrapper\n",
    "from langchain.document_loaders.base import Document\n",
    "\n",
    "apify = ApifyWrapper()\n",
    "\n",
    "loader = apify.call_actor(\n",
    "    actor_id=\"apify/website-content-crawler\",\n",
    "    run_input={\n",
    "        \"startUrls\": [{\"url\": \"https://newsletter.theaiedge.io/\"}],\n",
    "        \"aggressivePrune\": True,\n",
    "    },\n",
    "    dataset_mapping_function=lambda item: Document(\n",
    "        page_content=item[\"text\"] or \"\", metadata={\"source\": item[\"url\"]}\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "956f93d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreIndexWrapper(vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x1272497d0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "index = VectorstoreIndexCreator(\n",
    "    text_splitter=text_splitter\n",
    ").from_loaders([loader])\n",
    "\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d2ffdbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the main subject of the aiedge newsletter?',\n",
       " 'answer': ' The main subject of the AiEdge newsletter is Machine Learning applications, Machine Learning System Design, MLOps, and the latest techniques and news about the field.\\n',\n",
       " 'sources': ''}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the main subject of the aiedge newsletter?\"\n",
    "\n",
    "index.query_with_sources(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "641ec816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "The AiEdge Newsletter\n",
      "A newsletter for continuous learning about Machine Learning applications, Machine Learning System Design, MLOps, the latest techniques and news. Subscribe and receive a free Machine Learning book PDF!\n",
      "\n",
      "Keep reading with a 7-day free trial\n",
      "Subscribe to \n",
      "The AiEdge Newsletter\n",
      "to keep reading this post and get 7 days of free access to the full post archives.\n",
      "\n",
      "Keep reading with a 7-day free trial\n",
      "Subscribe to \n",
      "The AiEdge Newsletter\n",
      "to keep reading this post and get 7 days of free access to the full post archives.\n",
      "\n",
      "The AiEdge Newsletter is a simple way to keep learning about Artificial Intelligence and Machine Learning. My angle is to make the complex domain of Machine Learning simple to understand such that anybody could enjoy it and learn further from there. I want to address subjects that tend to be less mainstream and not found in typical textbooks. In general, I write about the following subjects:\n",
      "Human: What is the most recent article of the aiedge newsletter?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have access to the specific articles or the most recent content of the AiEdge Newsletter. As an AI language model, I don't have real-time access to current articles or newsletters. It would be best to subscribe to the newsletter and check the latest edition for the most recent article.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = index.vectorstore.as_retriever()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=retriever,\n",
    ")\n",
    "\n",
    "query = \"What is the most recent article of the aiedge newsletter?\"\n",
    "\n",
    "qa.run(\n",
    "    query, \n",
    "    callbacks=[handler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9576391",
   "metadata": {},
   "source": [
    "# Indexing a GitHub repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49a3227f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: GitPython in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (3.1.34)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from GitPython) (4.0.10)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython) (5.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install GitPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36a9efb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GitLoader\n",
    "\n",
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
    "    repo_path=\"./data/repo/\",\n",
    "    file_filter=lambda file_path: file_path.endswith(\".py\"),\n",
    "    branch=\"master\",\n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea14a216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"Configuration file for the Sphinx documentation builder.\"\"\"\n",
      "# Configuration file for the Sphinx documentation builder.\n",
      "#\n",
      "# This file only contains a selection of the most common options. For a full\n",
      "# list see the documentation:\n",
      "# https://www.sphinx-doc.org/en/master/usage/configuration.html\n",
      "\n",
      "# -- Path setup --------------------------------------------------------------\n",
      "\n",
      "import json\n",
      "import os\n",
      "import sys\n",
      "from pathlib import Path\n",
      "\n",
      "import toml\n",
      "from docutils import nodes\n",
      "from sphinx.util.docutils import SphinxDirective\n",
      "\n",
      "# If extensions (or modules to document with autodoc) are in another directory,\n",
      "# add these directories to sys.path here. If the directory is relative to the\n",
      "# documentation root, use os.path.abspath to make it absolute, like shown here.\n",
      "\n",
      "_DIR = Path(__file__).parent.absolute()\n",
      "sys.path.insert(0, os.path.abspath(\".\"))\n",
      "sys.path.insert(0, os.path.abspath(\"../../libs/langchain\"))\n",
      "sys.path.insert(0, os.path.abspath(\"../../libs/experimental\"))\n",
      "\n",
      "with (_DIR.parents[1] / \"libs\" / \"langchain\" / \"pyproject.toml\").open(\"r\") as f:\n",
      "    data = toml.load(f)\n",
      "with (_DIR / \"guide_imports.json\").open(\"r\") as f:\n",
      "    imported_classes = json.load(f)\n",
      "\n",
      "\n",
      "class ExampleLinksDirective(SphinxDirective):\n",
      "    \"\"\"Directive to generate a list of links to examples.\n",
      "\n",
      "    We have a script that extracts links to API reference docs\n",
      "    from our notebook examples. This directive uses that information\n",
      "    to backlink to the examples from the API reference docs.\"\"\"\n",
      "\n",
      "    has_content = False\n",
      "    required_arguments = 1\n",
      "\n",
      "    def run(self):\n",
      "        \"\"\"Run the directive.\n",
      "\n",
      "        Called any time :example_links:`ClassName` is used\n",
      "        in the template *.rst files.\"\"\"\n",
      "        class_or_func_name = self.arguments[0]\n",
      "        links = imported_classes.get(class_or_func_name, {})\n",
      "        list_node = nodes.bullet_list()\n",
      "        for doc_name, link in links.items():\n",
      "            item_node = nodes.list_item()\n",
      "            para_node = nodes.paragraph()\n",
      "            link_node = nodes.reference()\n",
      "            link_node[\"refuri\"] = link\n",
      "            link_node.append(nodes.Text(doc_name))\n",
      "            para_node.append(link_node)\n",
      "            item_node.append(para_node)\n",
      "            list_node.append(item_node)\n",
      "        if list_node.children:\n",
      "            title_node = nodes.title()\n",
      "            title_node.append(nodes.Text(f\"Examples using {class_or_func_name}\"))\n",
      "            return [title_node, list_node]\n",
      "        return [list_node]\n",
      "\n",
      "\n",
      "def setup(app):\n",
      "    app.add_directive(\"example_links\", ExampleLinksDirective)\n",
      "\n",
      "\n",
      "# -- Project information -----------------------------------------------------\n",
      "\n",
      "project = \"🦜🔗 LangChain\"\n",
      "copyright = \"2023, Harrison Chase\"\n",
      "author = \"Harrison Chase\"\n",
      "\n",
      "version = data[\"tool\"][\"poetry\"][\"version\"]\n",
      "release = version\n",
      "\n",
      "html_title = project + \" \" + version\n",
      "html_last_updated_fmt = \"%b %d, %Y\"\n",
      "\n",
      "\n",
      "# -- General configuration ---------------------------------------------------\n",
      "\n",
      "# Add any Sphinx extension module names here, as strings. They can be\n",
      "# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n",
      "# ones.\n",
      "extensions = [\n",
      "    \"sphinx.ext.autodoc\",\n",
      "    \"sphinx.ext.autodoc.typehints\",\n",
      "    \"sphinx.ext.autosummary\",\n",
      "    \"sphinx.ext.napoleon\",\n",
      "    \"sphinx.ext.viewcode\",\n",
      "    \"sphinxcontrib.autodoc_pydantic\",\n",
      "    \"sphinx_copybutton\",\n",
      "    \"sphinx_panels\",\n",
      "    \"IPython.sphinxext.ipython_console_highlighting\",\n",
      "]\n",
      "source_suffix = [\".rst\"]\n",
      "\n",
      "# some autodoc pydantic options are repeated in the actual template.\n",
      "# potentially user error, but there may be bugs in the sphinx extension\n",
      "# with options not being passed through correctly (from either the location in the code)\n",
      "autodoc_pydantic_model_show_json = False\n",
      "autodoc_pydantic_field_list_validators = False\n",
      "autodoc_pydantic_config_members = False\n",
      "autodoc_pydantic_model_show_config_summary = False\n",
      "autodoc_pydantic_model_show_validator_members = False\n",
      "autodoc_pydantic_model_show_validator_summary = False\n",
      "autodoc_pydantic_model_signature_prefix = \"class\"\n",
      "autodoc_pydantic_field_signature_prefix = \"param\"\n",
      "autodoc_member_order = \"groupwise\"\n",
      "autoclass_content = \"both\"\n",
      "autodoc_typehints_format = \"short\"\n",
      "\n",
      "# autodoc_typehints = \"description\"\n",
      "# Add any paths that contain templates here, relative to this directory.\n",
      "templates_path = [\"templates\"]\n",
      "\n",
      "# List of patterns, relative to source directory, that match files and\n",
      "# directories to ignore when looking for source files.\n",
      "# This pattern also affects html_static_path and html_extra_path.\n",
      "exclude_patterns = [\"_build\", \"Thumbs.db\", \".DS_Store\"]\n",
      "\n",
      "\n",
      "# -- Options for HTML output -------------------------------------------------\n",
      "\n",
      "# The theme to use for HTML and HTML Help pages.  See the documentation for\n",
      "# a list of builtin themes.\n",
      "#\n",
      "html_theme = \"scikit-learn-modern\"\n",
      "html_theme_path = [\"themes\"]\n",
      "\n",
      "# redirects dictionary maps from old links to new links\n",
      "html_additional_pages = {}\n",
      "redirects = {\n",
      "    \"index\": \"api_reference\",\n",
      "}\n",
      "for old_link in redirects:\n",
      "    html_additional_pages[old_link] = \"redirects.html\"\n",
      "\n",
      "html_context = {\n",
      "    \"display_github\": True,  # Integrate GitHub\n",
      "    \"github_user\": \"hwchase17\",  # Username\n",
      "    \"github_repo\": \"langchain\",  # Repo name\n",
      "    \"github_version\": \"master\",  # Version\n",
      "    \"conf_py_path\": \"/docs/api_reference\",  # Path in the checkout to the docs root\n",
      "    \"redirects\": redirects,\n",
      "}\n",
      "\n",
      "# Add any paths that contain custom static files (such as style sheets) here,\n",
      "# relative to this directory. They are copied after the builtin static files,\n",
      "# so a file named \"default.css\" will overwrite the builtin \"default.css\".\n",
      "html_static_path = [\"_static\"]\n",
      "\n",
      "# These paths are either relative to html_static_path\n",
      "# or fully qualified paths (e.g. https://...)\n",
      "html_css_files = [\n",
      "    \"css/custom.css\",\n",
      "]\n",
      "html_use_index = False\n",
      "\n",
      "myst_enable_extensions = [\"colon_fence\"]\n",
      "\n",
      "# generate autosummary even if no references\n",
      "autosummary_generate = True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "32151b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1763"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "72caecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import Language\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, \n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "documents = python_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "60369f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='\"\"\"Configuration file for the Sphinx documentation builder.\"\"\"\\n# Configuration file for the Sphinx documentation builder.\\n#\\n# This file only contains a selection of the most common options. For a full\\n# list see the documentation:\\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\\n\\n# -- Path setup --------------------------------------------------------------\\n\\nimport json\\nimport os\\nimport sys\\nfrom pathlib import Path\\n\\nimport toml\\nfrom docutils import nodes\\nfrom sphinx.util.docutils import SphinxDirective\\n\\n# If extensions (or modules to document with autodoc) are in another directory,\\n# add these directories to sys.path here. If the directory is relative to the\\n# documentation root, use os.path.abspath to make it absolute, like shown here.\\n\\n_DIR = Path(__file__).parent.absolute()\\nsys.path.insert(0, os.path.abspath(\".\"))\\nsys.path.insert(0, os.path.abspath(\"../../libs/langchain\"))\\nsys.path.insert(0, os.path.abspath(\"../../libs/experimental\"))', metadata={'source': 'docs/api_reference/conf.py', 'file_path': 'docs/api_reference/conf.py', 'file_name': 'conf.py', 'file_type': '.py'})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1fce3759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10906"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13363ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ff8744c3c04d3caf2ced8ef982529c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e23cdf4ee1a405fbd6aedf3d7fe02db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "def _load_stuff_chain(\n",
      "    llm: BaseLanguageModel,\n",
      "    prompt: BasePromptTemplate = stuff_prompt.PROMPT,\n",
      "    document_prompt: BasePromptTemplate = stuff_prompt.EXAMPLE_PROMPT,\n",
      "    document_variable_name: str = \"summaries\",\n",
      "    verbose: Optional[bool] = None,\n",
      "    **kwargs: Any,\n",
      ") -> StuffDocumentsChain:\n",
      "    llm_chain = LLMChain(llm=llm, prompt=prompt, verbose=verbose)\n",
      "    return StuffDocumentsChain(\n",
      "        llm_chain=llm_chain,\n",
      "        document_variable_name=document_variable_name,\n",
      "        document_prompt=document_prompt,\n",
      "        verbose=verbose,\n",
      "        **kwargs,\n",
      "    )\n",
      "\n",
      "def _load_stuff_chain(\n",
      "    llm: BaseLanguageModel,\n",
      "    prompt: Optional[BasePromptTemplate] = None,\n",
      "    document_variable_name: str = \"context\",\n",
      "    verbose: Optional[bool] = None,\n",
      "    callback_manager: Optional[BaseCallbackManager] = None,\n",
      "    callbacks: Callbacks = None,\n",
      "    **kwargs: Any,\n",
      ") -> StuffDocumentsChain:\n",
      "    _prompt = prompt or stuff_prompt.PROMPT_SELECTOR.get_prompt(llm)\n",
      "    llm_chain = LLMChain(\n",
      "        llm=llm,\n",
      "        prompt=_prompt,\n",
      "        verbose=verbose,\n",
      "        callback_manager=callback_manager,\n",
      "        callbacks=callbacks,\n",
      "    )\n",
      "    # TODO: document prompt\n",
      "    return StuffDocumentsChain(\n",
      "        llm_chain=llm_chain,\n",
      "        document_variable_name=document_variable_name,\n",
      "        verbose=verbose,\n",
      "        callback_manager=callback_manager,\n",
      "        callbacks=callbacks,\n",
      "        **kwargs,\n",
      "    )\n",
      "\n",
      "return StuffDocumentsChain(\n",
      "        llm_chain=llm_chain, document_prompt=document_prompt, **config\n",
      "    )\n",
      "\n",
      "\"\"\"Tests for correct functioning of chains.\"\"\"\n",
      "Human: What is a stuff chain?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A stuff chain is a sequence of operations performed on a language model (LLM) to generate or process text. It typically consists of a language model chain (LLMChain) and a document chain (StuffDocumentsChain). The LLMChain is responsible for generating text based on a prompt, while the StuffDocumentsChain is used to process and manipulate documents or summaries. The specific details and functionality of a stuff chain can vary depending on the context and configuration.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = FAISS.from_documents(documents, embeddings)\n",
    "retriever = index.as_retriever()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=retriever,\n",
    ")\n",
    "\n",
    "query = \"What is a stuff chain?\"\n",
    "\n",
    "qa.run(query, callbacks=[handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ba54e42d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ca3d78cd1a4eddab789b1c40b870cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "class MapReduceChain(Chain):\n",
      "    \"\"\"Map-reduce chain.\"\"\"\n",
      "\n",
      "    combine_documents_chain: BaseCombineDocumentsChain\n",
      "    \"\"\"Chain to use to combine documents.\"\"\"\n",
      "    text_splitter: TextSplitter\n",
      "    \"\"\"Text splitter to use.\"\"\"\n",
      "    input_key: str = \"input_text\"  #: :meta private:\n",
      "    output_key: str = \"output_text\"  #: :meta private:\n",
      "\n",
      "collapse_documents_chain=collapse_chain,\n",
      "        token_max=token_max,\n",
      "        verbose=verbose,\n",
      "    )\n",
      "    return MapReduceDocumentsChain(\n",
      "        llm_chain=map_chain,\n",
      "        document_variable_name=map_reduce_document_variable_name,\n",
      "        reduce_documents_chain=reduce_documents_chain,\n",
      "        verbose=verbose,\n",
      "        callback_manager=callback_manager,\n",
      "        callbacks=callbacks,\n",
      "        **kwargs,\n",
      "    )\n",
      "\n",
      "class MapReduceDocumentsChain(BaseCombineDocumentsChain):\n",
      "    \"\"\"Combining documents by mapping a chain over them, then combining results.\n",
      "\n",
      "    We first call `llm_chain` on each document individually, passing in the\n",
      "    `page_content` and any other kwargs. This is the `map` step.\n",
      "\n",
      "    We then process the results of that `map` step in a `reduce` step. This should\n",
      "    likely be a ReduceDocumentsChain.\n",
      "\n",
      "    Example:\n",
      "        .. code-block:: python\n",
      "\n",
      "            from langchain.chains import (\n",
      "                StuffDocumentsChain,\n",
      "                LLMChain,\n",
      "                ReduceDocumentsChain,\n",
      "                MapReduceDocumentsChain,\n",
      "            )\n",
      "            from langchain.prompts import PromptTemplate\n",
      "            from langchain.llms import OpenAI\n",
      "\n",
      "def _load_map_reduce_chain(\n",
      "    llm: BaseLanguageModel,\n",
      "    map_prompt: BasePromptTemplate = map_reduce_prompt.PROMPT,\n",
      "    combine_prompt: BasePromptTemplate = map_reduce_prompt.PROMPT,\n",
      "    combine_document_variable_name: str = \"text\",\n",
      "    map_reduce_document_variable_name: str = \"text\",\n",
      "    collapse_prompt: Optional[BasePromptTemplate] = None,\n",
      "    reduce_llm: Optional[BaseLanguageModel] = None,\n",
      "    collapse_llm: Optional[BaseLanguageModel] = None,\n",
      "    verbose: Optional[bool] = None,\n",
      "    token_max: int = 3000,\n",
      "    callbacks: Callbacks = None,\n",
      "    **kwargs: Any,\n",
      ") -> MapReduceDocumentsChain:\n",
      "    map_chain = LLMChain(\n",
      "        llm=llm, prompt=map_prompt, verbose=verbose, callbacks=callbacks\n",
      "    )\n",
      "    _reduce_llm = reduce_llm or llm\n",
      "    reduce_chain = LLMChain(\n",
      "        llm=_reduce_llm, prompt=combine_prompt, verbose=verbose, callbacks=callbacks\n",
      "    )\n",
      "    # TODO: document prompt\n",
      "    combine_documents_chain = StuffDocumentsChain(\n",
      "        llm_chain=reduce_chain,\n",
      "\n",
      "def _load_map_reduce_chain(\n",
      "    llm: BaseLanguageModel,\n",
      "    question_prompt: BasePromptTemplate = map_reduce_prompt.QUESTION_PROMPT,\n",
      "    combine_prompt: BasePromptTemplate = map_reduce_prompt.COMBINE_PROMPT,\n",
      "    document_prompt: BasePromptTemplate = map_reduce_prompt.EXAMPLE_PROMPT,\n",
      "    combine_document_variable_name: str = \"summaries\",\n",
      "    map_reduce_document_variable_name: str = \"context\",\n",
      "    collapse_prompt: Optional[BasePromptTemplate] = None,\n",
      "    reduce_llm: Optional[BaseLanguageModel] = None,\n",
      "    collapse_llm: Optional[BaseLanguageModel] = None,\n",
      "    verbose: Optional[bool] = None,\n",
      "    token_max: int = 3000,\n",
      "    **kwargs: Any,\n",
      ") -> MapReduceDocumentsChain:\n",
      "    map_chain = LLMChain(llm=llm, prompt=question_prompt, verbose=verbose)\n",
      "    _reduce_llm = reduce_llm or llm\n",
      "    reduce_chain = LLMChain(llm=_reduce_llm, prompt=combine_prompt, verbose=verbose)\n",
      "    combine_documents_chain = StuffDocumentsChain(\n",
      "        llm_chain=reduce_chain,\n",
      "\n",
      "def _load_map_reduce_documents_chain(\n",
      "    config: dict, **kwargs: Any\n",
      ") -> MapReduceDocumentsChain:\n",
      "    if \"llm_chain\" in config:\n",
      "        llm_chain_config = config.pop(\"llm_chain\")\n",
      "        llm_chain = load_chain_from_config(llm_chain_config)\n",
      "    elif \"llm_chain_path\" in config:\n",
      "        llm_chain = load_chain(config.pop(\"llm_chain_path\"))\n",
      "    else:\n",
      "        raise ValueError(\"One of `llm_chain` or `llm_chain_config` must be present.\")\n",
      "\n",
      "    if not isinstance(llm_chain, LLMChain):\n",
      "        raise ValueError(f\"Expected LLMChain, got {llm_chain}\")\n",
      "\n",
      "    if \"reduce_documents_chain\" in config:\n",
      "        reduce_documents_chain = load_chain_from_config(\n",
      "            config.pop(\"reduce_documents_chain\")\n",
      "        )\n",
      "    elif \"reduce_documents_chain_path\" in config:\n",
      "        reduce_documents_chain = load_chain(config.pop(\"reduce_documents_chain_path\"))\n",
      "    else:\n",
      "        reduce_documents_chain = _load_reduce_documents_chain(config)\n",
      "\n",
      "),\n",
      "            document_variable_name=combine_document_variable_name,\n",
      "        )\n",
      "    reduce_documents_chain = ReduceDocumentsChain(\n",
      "        combine_documents_chain=combine_documents_chain,\n",
      "        collapse_documents_chain=collapse_chain,\n",
      "        token_max=token_max,\n",
      "        verbose=verbose,\n",
      "        callbacks=callbacks,\n",
      "    )\n",
      "    return MapReduceDocumentsChain(\n",
      "        llm_chain=map_chain,\n",
      "        reduce_documents_chain=reduce_documents_chain,\n",
      "        document_variable_name=map_reduce_document_variable_name,\n",
      "        verbose=verbose,\n",
      "        callbacks=callbacks,\n",
      "        **kwargs,\n",
      "    )\n",
      "\n",
      "verbose=verbose,\n",
      "            ),\n",
      "            document_variable_name=combine_document_variable_name,\n",
      "            document_prompt=document_prompt,\n",
      "        )\n",
      "    reduce_documents_chain = ReduceDocumentsChain(\n",
      "        combine_documents_chain=combine_documents_chain,\n",
      "        collapse_documents_chain=collapse_chain,\n",
      "        token_max=token_max,\n",
      "        verbose=verbose,\n",
      "    )\n",
      "    return MapReduceDocumentsChain(\n",
      "        llm_chain=map_chain,\n",
      "        reduce_documents_chain=reduce_documents_chain,\n",
      "        document_variable_name=map_reduce_document_variable_name,\n",
      "        verbose=verbose,\n",
      "        **kwargs,\n",
      "    )\n",
      "\n",
      "def _load_map_reduce_chain(\n",
      "    llm: BaseLanguageModel,\n",
      "    question_prompt: Optional[BasePromptTemplate] = None,\n",
      "    combine_prompt: Optional[BasePromptTemplate] = None,\n",
      "    combine_document_variable_name: str = \"summaries\",\n",
      "    map_reduce_document_variable_name: str = \"context\",\n",
      "    collapse_prompt: Optional[BasePromptTemplate] = None,\n",
      "    reduce_llm: Optional[BaseLanguageModel] = None,\n",
      "    collapse_llm: Optional[BaseLanguageModel] = None,\n",
      "    verbose: Optional[bool] = None,\n",
      "    callback_manager: Optional[BaseCallbackManager] = None,\n",
      "    callbacks: Callbacks = None,\n",
      "    token_max: int = 3000,\n",
      "    **kwargs: Any,\n",
      ") -> MapReduceDocumentsChain:\n",
      "    _question_prompt = (\n",
      "        question_prompt or map_reduce_prompt.QUESTION_PROMPT_SELECTOR.get_prompt(llm)\n",
      "    )\n",
      "    _combine_prompt = (\n",
      "        combine_prompt or map_reduce_prompt.COMBINE_PROMPT_SELECTOR.get_prompt(llm)\n",
      "    )\n",
      "    map_chain = LLMChain(\n",
      "        llm=llm,\n",
      "        prompt=_question_prompt,\n",
      "        verbose=verbose,\n",
      "\n",
      "return MapReduceDocumentsChain(\n",
      "        llm_chain=llm_chain,\n",
      "        reduce_documents_chain=reduce_documents_chain,\n",
      "        **config,\n",
      "    )\n",
      "\n",
      "@property\n",
      "    def _chain_type(self) -> str:\n",
      "        return \"map_reduce_documents_chain\"\n",
      "\n",
      "chain: Runnable = input_map | router\n",
      "    assert dumps(chain, pretty=True) == snapshot\n",
      "\n",
      "    result = chain.invoke({\"key\": \"math\", \"question\": \"2 + 2\"})\n",
      "    assert result == \"4\"\n",
      "\n",
      "    result2 = chain.batch(\n",
      "        [{\"key\": \"math\", \"question\": \"2 + 2\"}, {\"key\": \"english\", \"question\": \"2 + 2\"}]\n",
      "    )\n",
      "    assert result2 == [\"4\", \"2\"]\n",
      "\n",
      "    result = await chain.ainvoke({\"key\": \"math\", \"question\": \"2 + 2\"})\n",
      "    assert result == \"4\"\n",
      "\n",
      "    result2 = await chain.abatch(\n",
      "        [{\"key\": \"math\", \"question\": \"2 + 2\"}, {\"key\": \"english\", \"question\": \"2 + 2\"}]\n",
      "    )\n",
      "    assert result2 == [\"4\", \"2\"]\n",
      "\n",
      ")\n",
      "            chain = ReduceDocumentsChain(\n",
      "                combine_documents_chain=combine_documents_chain,\n",
      "                collapse_documents_chain=collapse_documents_chain,\n",
      "            )\n",
      "    \"\"\"\n",
      "\n",
      "reduce_prompt = PromptTemplate.from_template(\n",
      "                \"Combine these summaries: {context}\"\n",
      "            )\n",
      "            reduce_llm_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
      "            combine_documents_chain = StuffDocumentsChain(\n",
      "                llm_chain=reduce_llm_chain,\n",
      "                document_prompt=document_prompt,\n",
      "                document_variable_name=document_variable_name\n",
      "            )\n",
      "            reduce_documents_chain = ReduceDocumentsChain(\n",
      "                combine_documents_chain=combine_documents_chain,\n",
      "            )\n",
      "            chain = MapReduceDocumentsChain(\n",
      "                llm_chain=llm_chain,\n",
      "                reduce_documents_chain=reduce_documents_chain,\n",
      "            )\n",
      "            # If we wanted to, we could also pass in collapse_documents_chain\n",
      "            # which is specifically aimed at collapsing documents BEFORE\n",
      "            # the final call.\n",
      "            prompt = PromptTemplate.from_template(\n",
      "\n",
      "from langchain.chains import ReduceDocumentsChain\n",
      "from langchain.chains.api.base import APIChain\n",
      "from langchain.chains.base import Chain\n",
      "from langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\n",
      "from langchain.chains.combine_documents.map_rerank import MapRerankDocumentsChain\n",
      "from langchain.chains.combine_documents.refine import RefineDocumentsChain\n",
      "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
      "from langchain.chains.graph_qa.cypher import GraphCypherQAChain\n",
      "from langchain.chains.hyde.base import HypotheticalDocumentEmbedder\n",
      "from langchain.chains.llm import LLMChain\n",
      "from langchain.chains.llm_bash.base import LLMBashChain\n",
      "from langchain.chains.llm_checker.base import LLMCheckerChain\n",
      "from langchain.chains.llm_math.base import LLMMathChain\n",
      "from langchain.chains.llm_requests import LLMRequestsChain\n",
      "from langchain.chains.qa_with_sources.base import QAWithSourcesChain\n",
      "\n",
      "@classmethod\n",
      "    def from_params(\n",
      "        cls,\n",
      "        llm: BaseLanguageModel,\n",
      "        prompt: BasePromptTemplate,\n",
      "        text_splitter: TextSplitter,\n",
      "        callbacks: Callbacks = None,\n",
      "        combine_chain_kwargs: Optional[Mapping[str, Any]] = None,\n",
      "        reduce_chain_kwargs: Optional[Mapping[str, Any]] = None,\n",
      "        **kwargs: Any,\n",
      "    ) -> MapReduceChain:\n",
      "        \"\"\"Construct a map-reduce chain that uses the chain for map and reduce.\"\"\"\n",
      "        llm_chain = LLMChain(llm=llm, prompt=prompt, callbacks=callbacks)\n",
      "        stuff_chain = StuffDocumentsChain(\n",
      "            llm_chain=llm_chain,\n",
      "            callbacks=callbacks,\n",
      "            **(reduce_chain_kwargs if reduce_chain_kwargs else {}),\n",
      "        )\n",
      "        reduce_documents_chain = ReduceDocumentsChain(\n",
      "            combine_documents_chain=stuff_chain\n",
      "        )\n",
      "        combine_documents_chain = MapReduceDocumentsChain(\n",
      "            llm_chain=llm_chain,\n",
      "            reduce_documents_chain=reduce_documents_chain,\n",
      "\n",
      "\"\"\"Map-reduce chain.\n",
      "\n",
      "Splits up a document, sends the smaller parts to the LLM with one prompt,\n",
      "then combines the results with another one.\n",
      "\"\"\"\n",
      "from __future__ import annotations\n",
      "\n",
      "from typing import Any, Dict, List, Mapping, Optional\n",
      "\n",
      "from langchain.callbacks.manager import CallbackManagerForChainRun, Callbacks\n",
      "from langchain.chains import ReduceDocumentsChain\n",
      "from langchain.chains.base import Chain\n",
      "from langchain.chains.combine_documents.base import BaseCombineDocumentsChain\n",
      "from langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\n",
      "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
      "from langchain.chains.llm import LLMChain\n",
      "from langchain.docstore.document import Document\n",
      "from langchain.pydantic_v1 import Extra\n",
      "from langchain.schema import BasePromptTemplate\n",
      "from langchain.schema.language_model import BaseLanguageModel\n",
      "from langchain.text_splitter import TextSplitter\n",
      "\n",
      "def _load_map_rerank_documents_chain(\n",
      "    config: dict, **kwargs: Any\n",
      ") -> MapRerankDocumentsChain:\n",
      "    if \"llm_chain\" in config:\n",
      "        llm_chain_config = config.pop(\"llm_chain\")\n",
      "        llm_chain = load_chain_from_config(llm_chain_config)\n",
      "    elif \"llm_chain_path\" in config:\n",
      "        llm_chain = load_chain(config.pop(\"llm_chain_path\"))\n",
      "    else:\n",
      "        raise ValueError(\"One of `llm_chain` or `llm_chain_config` must be present.\")\n",
      "    return MapRerankDocumentsChain(llm_chain=llm_chain, **config)\n",
      "\n",
      "document_prompt=document_prompt,\n",
      "                document_variable_name=document_variable_name\n",
      "            )\n",
      "            chain = ReduceDocumentsChain(\n",
      "                combine_documents_chain=combine_documents_chain,\n",
      "            )\n",
      "            # If we wanted to, we could also pass in collapse_documents_chain\n",
      "            # which is specifically aimed at collapsing documents BEFORE\n",
      "            # the final call.\n",
      "            prompt = PromptTemplate.from_template(\n",
      "                \"Collapse this content: {context}\"\n",
      "            )\n",
      "            llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
      "            collapse_documents_chain = StuffDocumentsChain(\n",
      "                llm_chain=llm_chain,\n",
      "                document_prompt=document_prompt,\n",
      "                document_variable_name=document_variable_name\n",
      "            )\n",
      "            chain = ReduceDocumentsChain(\n",
      "                combine_documents_chain=combine_documents_chain,\n",
      "                collapse_documents_chain=collapse_documents_chain,\n",
      "\n",
      "reduce_chain = LLMChain(llm=_reduce_llm, prompt=combine_prompt, verbose=verbose)\n",
      "    combine_documents_chain = StuffDocumentsChain(\n",
      "        llm_chain=reduce_chain,\n",
      "        document_variable_name=combine_document_variable_name,\n",
      "        document_prompt=document_prompt,\n",
      "        verbose=verbose,\n",
      "    )\n",
      "    if collapse_prompt is None:\n",
      "        collapse_chain = None\n",
      "        if collapse_llm is not None:\n",
      "            raise ValueError(\n",
      "                \"collapse_llm provided, but collapse_prompt was not: please \"\n",
      "                \"provide one or stop providing collapse_llm.\"\n",
      "            )\n",
      "    else:\n",
      "        _collapse_llm = collapse_llm or llm\n",
      "        collapse_chain = StuffDocumentsChain(\n",
      "            llm_chain=LLMChain(\n",
      "                llm=_collapse_llm,\n",
      "                prompt=collapse_prompt,\n",
      "                verbose=verbose,\n",
      "            ),\n",
      "            document_variable_name=combine_document_variable_name,\n",
      "            document_prompt=document_prompt,\n",
      "        )\n",
      "Human: When should I use a map reduce chain?\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A MapReduce chain is useful when you have a large document that needs to be processed in parallel. It splits the document into smaller parts, applies a map function to each part, and then combines the results using a reduce function. This allows for efficient processing of large amounts of data by distributing the workload across multiple processors or machines.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "retriever.search_kwargs['fetch_k'] = 200\n",
    "retriever.search_kwargs['maximal_marginal_relevance'] = True\n",
    "retriever.search_kwargs['k'] = 20\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=retriever,\n",
    ")\n",
    "\n",
    "query = \"When should I use a map reduce chain?\"\n",
    "\n",
    "qa.run(query, callbacks=[handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f6aaf21a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f2cd734b9a4c8b815a494513061c12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "def _load_map_rerank_documents_chain(\n",
      "    config: dict, **kwargs: Any\n",
      ") -> MapRerankDocumentsChain:\n",
      "    if \"llm_chain\" in config:\n",
      "        llm_chain_config = config.pop(\"llm_chain\")\n",
      "        llm_chain = load_chain_from_config(llm_chain_config)\n",
      "    elif \"llm_chain_path\" in config:\n",
      "        llm_chain = load_chain(config.pop(\"llm_chain_path\"))\n",
      "    else:\n",
      "        raise ValueError(\"One of `llm_chain` or `llm_chain_config` must be present.\")\n",
      "    return MapRerankDocumentsChain(llm_chain=llm_chain, **config)\n",
      "\n",
      "def _load_map_rerank_chain(\n",
      "    llm: BaseLanguageModel,\n",
      "    prompt: BasePromptTemplate = MAP_RERANK_PROMPT,\n",
      "    verbose: bool = False,\n",
      "    document_variable_name: str = \"context\",\n",
      "    rank_key: str = \"score\",\n",
      "    answer_key: str = \"answer\",\n",
      "    callback_manager: Optional[BaseCallbackManager] = None,\n",
      "    callbacks: Callbacks = None,\n",
      "    **kwargs: Any,\n",
      ") -> MapRerankDocumentsChain:\n",
      "    llm_chain = LLMChain(\n",
      "        llm=llm,\n",
      "        prompt=prompt,\n",
      "        verbose=verbose,\n",
      "        callback_manager=callback_manager,\n",
      "        callbacks=callbacks,\n",
      "    )\n",
      "    return MapRerankDocumentsChain(\n",
      "        llm_chain=llm_chain,\n",
      "        rank_key=rank_key,\n",
      "        answer_key=answer_key,\n",
      "        document_variable_name=document_variable_name,\n",
      "        verbose=verbose,\n",
      "        callback_manager=callback_manager,\n",
      "        **kwargs,\n",
      "    )\n",
      "\n",
      "output_parser=output_parser,\n",
      "            )\n",
      "            llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
      "            chain = MapRerankDocumentsChain(\n",
      "                llm_chain=llm_chain,\n",
      "                document_variable_name=document_variable_name,\n",
      "                rank_key=\"score\",\n",
      "                answer_key=\"answer\",\n",
      "            )\n",
      "    \"\"\"\n",
      "\n",
      "class MapRerankDocumentsChain(BaseCombineDocumentsChain):\n",
      "    \"\"\"Combining documents by mapping a chain over them, then reranking results.\n",
      "\n",
      "    This algorithm calls an LLMChain on each input document. The LLMChain is expected\n",
      "    to have an OutputParser that parses the result into both an answer (`answer_key`)\n",
      "    and a score (`rank_key`). The answer with the highest score is then returned.\n",
      "\n",
      "    Example:\n",
      "        .. code-block:: python\n",
      "\n",
      "            from langchain.chains import StuffDocumentsChain, LLMChain\n",
      "            from langchain.prompts import PromptTemplate\n",
      "            from langchain.llms import OpenAI\n",
      "            from langchain.output_parsers.regex import RegexParser\n",
      "\n",
      "\"\"\"Tests for correct functioning of chains.\"\"\"\n",
      "\n",
      "class MapReduceChain(Chain):\n",
      "    \"\"\"Map-reduce chain.\"\"\"\n",
      "\n",
      "    combine_documents_chain: BaseCombineDocumentsChain\n",
      "    \"\"\"Chain to use to combine documents.\"\"\"\n",
      "    text_splitter: TextSplitter\n",
      "    \"\"\"Text splitter to use.\"\"\"\n",
      "    input_key: str = \"input_text\"  #: :meta private:\n",
      "    output_key: str = \"output_text\"  #: :meta private:\n",
      "\n",
      "- Grading the accuracy of a response against ground truth answers: :class:`QAEvalChain <langchain.evaluation.qa.eval_chain.QAEvalChain>`\n",
      "- Comparing the output of two models: :class:`PairwiseStringEvalChain <langchain.evaluation.comparison.eval_chain.PairwiseStringEvalChain>` or :class:`LabeledPairwiseStringEvalChain <langchain.evaluation.comparison.eval_chain.LabeledPairwiseStringEvalChain>` when there is additionally a reference label.\n",
      "- Judging the efficacy of an agent's tool usage: :class:`TrajectoryEvalChain <langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain>`\n",
      "- Checking whether an output complies with a set of criteria: :class:`CriteriaEvalChain <langchain.evaluation.criteria.eval_chain.CriteriaEvalChain>` or :class:`LabeledCriteriaEvalChain <langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain>` when there is additionally a reference label.\n",
      "\n",
      "collapse_documents_chain=collapse_chain,\n",
      "        token_max=token_max,\n",
      "        verbose=verbose,\n",
      "    )\n",
      "    return MapReduceDocumentsChain(\n",
      "        llm_chain=map_chain,\n",
      "        document_variable_name=map_reduce_document_variable_name,\n",
      "        reduce_documents_chain=reduce_documents_chain,\n",
      "        verbose=verbose,\n",
      "        callback_manager=callback_manager,\n",
      "        callbacks=callbacks,\n",
      "        **kwargs,\n",
      "    )\n",
      "\n",
      "document_variable_name = \"context\"\n",
      "            llm = OpenAI()\n",
      "            # The prompt here should take as an input variable the\n",
      "            # `document_variable_name`\n",
      "            # The actual prompt will need to be a lot more complex, this is just\n",
      "            # an example.\n",
      "            prompt_template = (\n",
      "                \"Use the following context to tell me the chemical formula \"\n",
      "                \"for water. Output both your answer and a score of how confident \"\n",
      "                \"you are. Context: {content}\"\n",
      "            )\n",
      "            output_parser = RegexParser(\n",
      "                regex=r\"(.*?)\\nScore: (.*)\",\n",
      "                output_keys=[\"answer\", \"score\"],\n",
      "            )\n",
      "            prompt = PromptTemplate(\n",
      "                template=prompt_template,\n",
      "                input_variables=[\"context\"],\n",
      "                output_parser=output_parser,\n",
      "            )\n",
      "            llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
      "            chain = MapRerankDocumentsChain(\n",
      "\n",
      "def _load_map_reduce_chain(\n",
      "    llm: BaseLanguageModel,\n",
      "    question_prompt: BasePromptTemplate = map_reduce_prompt.QUESTION_PROMPT,\n",
      "    combine_prompt: BasePromptTemplate = map_reduce_prompt.COMBINE_PROMPT,\n",
      "    document_prompt: BasePromptTemplate = map_reduce_prompt.EXAMPLE_PROMPT,\n",
      "    combine_document_variable_name: str = \"summaries\",\n",
      "    map_reduce_document_variable_name: str = \"context\",\n",
      "    collapse_prompt: Optional[BasePromptTemplate] = None,\n",
      "    reduce_llm: Optional[BaseLanguageModel] = None,\n",
      "    collapse_llm: Optional[BaseLanguageModel] = None,\n",
      "    verbose: Optional[bool] = None,\n",
      "    token_max: int = 3000,\n",
      "    **kwargs: Any,\n",
      ") -> MapReduceDocumentsChain:\n",
      "    map_chain = LLMChain(llm=llm, prompt=question_prompt, verbose=verbose)\n",
      "    _reduce_llm = reduce_llm or llm\n",
      "    reduce_chain = LLMChain(llm=_reduce_llm, prompt=combine_prompt, verbose=verbose)\n",
      "    combine_documents_chain = StuffDocumentsChain(\n",
      "        llm_chain=reduce_chain,\n",
      "\n",
      "def _load_map_reduce_chain(\n",
      "    llm: BaseLanguageModel,\n",
      "    question_prompt: Optional[BasePromptTemplate] = None,\n",
      "    combine_prompt: Optional[BasePromptTemplate] = None,\n",
      "    combine_document_variable_name: str = \"summaries\",\n",
      "    map_reduce_document_variable_name: str = \"context\",\n",
      "    collapse_prompt: Optional[BasePromptTemplate] = None,\n",
      "    reduce_llm: Optional[BaseLanguageModel] = None,\n",
      "    collapse_llm: Optional[BaseLanguageModel] = None,\n",
      "    verbose: Optional[bool] = None,\n",
      "    callback_manager: Optional[BaseCallbackManager] = None,\n",
      "    callbacks: Callbacks = None,\n",
      "    token_max: int = 3000,\n",
      "    **kwargs: Any,\n",
      ") -> MapReduceDocumentsChain:\n",
      "    _question_prompt = (\n",
      "        question_prompt or map_reduce_prompt.QUESTION_PROMPT_SELECTOR.get_prompt(llm)\n",
      "    )\n",
      "    _combine_prompt = (\n",
      "        combine_prompt or map_reduce_prompt.COMBINE_PROMPT_SELECTOR.get_prompt(llm)\n",
      "    )\n",
      "    map_chain = LLMChain(\n",
      "        llm=llm,\n",
      "        prompt=_question_prompt,\n",
      "        verbose=verbose,\n",
      "\n",
      "def _load_map_reduce_chain(\n",
      "    llm: BaseLanguageModel,\n",
      "    map_prompt: BasePromptTemplate = map_reduce_prompt.PROMPT,\n",
      "    combine_prompt: BasePromptTemplate = map_reduce_prompt.PROMPT,\n",
      "    combine_document_variable_name: str = \"text\",\n",
      "    map_reduce_document_variable_name: str = \"text\",\n",
      "    collapse_prompt: Optional[BasePromptTemplate] = None,\n",
      "    reduce_llm: Optional[BaseLanguageModel] = None,\n",
      "    collapse_llm: Optional[BaseLanguageModel] = None,\n",
      "    verbose: Optional[bool] = None,\n",
      "    token_max: int = 3000,\n",
      "    callbacks: Callbacks = None,\n",
      "    **kwargs: Any,\n",
      ") -> MapReduceDocumentsChain:\n",
      "    map_chain = LLMChain(\n",
      "        llm=llm, prompt=map_prompt, verbose=verbose, callbacks=callbacks\n",
      "    )\n",
      "    _reduce_llm = reduce_llm or llm\n",
      "    reduce_chain = LLMChain(\n",
      "        llm=_reduce_llm, prompt=combine_prompt, verbose=verbose, callbacks=callbacks\n",
      "    )\n",
      "    # TODO: document prompt\n",
      "    combine_documents_chain = StuffDocumentsChain(\n",
      "        llm_chain=reduce_chain,\n",
      "\n",
      "def test_chain(kv_dataset_name: str, eval_project_name: str, client: Client) -> None:\n",
      "    llm = ChatOpenAI(temperature=0)\n",
      "    chain = LLMChain.from_string(llm, \"The answer to the {question} is: \")\n",
      "    eval_config = RunEvalConfig(evaluators=[EvaluatorType.QA, EvaluatorType.CRITERIA])\n",
      "    with pytest.raises(ValueError, match=\"Must specify reference_key\"):\n",
      "        run_on_dataset(client, kv_dataset_name, lambda: chain, evaluation=eval_config)\n",
      "    eval_config = RunEvalConfig(\n",
      "        evaluators=[EvaluatorType.QA, EvaluatorType.CRITERIA],\n",
      "        reference_key=\"some_output\",\n",
      "    )\n",
      "    with pytest.raises(\n",
      "        InputFormatError, match=\"Example inputs do not match chain input keys\"\n",
      "    ):\n",
      "        run_on_dataset(client, kv_dataset_name, lambda: chain, evaluation=eval_config)\n",
      "\n",
      "    def input_mapper(d: dict) -> dict:\n",
      "        return {\"input\": d[\"some_input\"]}\n",
      "\n",
      "chain: Runnable = input_map | router\n",
      "    assert dumps(chain, pretty=True) == snapshot\n",
      "\n",
      "    result = chain.invoke({\"key\": \"math\", \"question\": \"2 + 2\"})\n",
      "    assert result == \"4\"\n",
      "\n",
      "    result2 = chain.batch(\n",
      "        [{\"key\": \"math\", \"question\": \"2 + 2\"}, {\"key\": \"english\", \"question\": \"2 + 2\"}]\n",
      "    )\n",
      "    assert result2 == [\"4\", \"2\"]\n",
      "\n",
      "    result = await chain.ainvoke({\"key\": \"math\", \"question\": \"2 + 2\"})\n",
      "    assert result == \"4\"\n",
      "\n",
      "    result2 = await chain.abatch(\n",
      "        [{\"key\": \"math\", \"question\": \"2 + 2\"}, {\"key\": \"english\", \"question\": \"2 + 2\"}]\n",
      "    )\n",
      "    assert result2 == [\"4\", \"2\"]\n",
      "\n",
      "_EVALUATOR_MAP: Dict[\n",
      "    EvaluatorType, Union[Type[LLMEvalChain], Type[Chain], Type[StringEvaluator]]\n",
      "] = {\n",
      "    EvaluatorType.QA: QAEvalChain,\n",
      "    EvaluatorType.COT_QA: CotQAEvalChain,\n",
      "    EvaluatorType.CONTEXT_QA: ContextQAEvalChain,\n",
      "    EvaluatorType.PAIRWISE_STRING: PairwiseStringEvalChain,\n",
      "    EvaluatorType.LABELED_PAIRWISE_STRING: LabeledPairwiseStringEvalChain,\n",
      "    EvaluatorType.AGENT_TRAJECTORY: TrajectoryEvalChain,\n",
      "    EvaluatorType.CRITERIA: CriteriaEvalChain,\n",
      "    EvaluatorType.LABELED_CRITERIA: LabeledCriteriaEvalChain,\n",
      "    EvaluatorType.STRING_DISTANCE: StringDistanceEvalChain,\n",
      "    EvaluatorType.PAIRWISE_STRING_DISTANCE: PairwiseStringDistanceEvalChain,\n",
      "    EvaluatorType.EMBEDDING_DISTANCE: EmbeddingDistanceEvalChain,\n",
      "    EvaluatorType.PAIRWISE_EMBEDDING_DISTANCE: PairwiseEmbeddingDistanceEvalChain,\n",
      "    EvaluatorType.JSON_VALIDITY: JsonValidityEvaluator,\n",
      "    EvaluatorType.JSON_EQUALITY: JsonEqualityEvaluator,\n",
      "}\n",
      "\n",
      "\"\"\"All integration tests for chains.\"\"\"\n",
      "\n",
      "- Computing semantic difference between a prediction and reference: :class:`EmbeddingDistanceEvalChain <langchain.evaluation.embedding_distance.base.EmbeddingDistanceEvalChain>` or between two predictions: :class:`PairwiseEmbeddingDistanceEvalChain <langchain.evaluation.embedding_distance.base.PairwiseEmbeddingDistanceEvalChain>` \n",
      "- Measuring the string distance between a prediction and reference :class:`StringDistanceEvalChain <langchain.evaluation.string_distance.base.StringDistanceEvalChain>` or between two predictions :class:`PairwiseStringDistanceEvalChain <langchain.evaluation.string_distance.base.PairwiseStringDistanceEvalChain>`\n",
      "\n",
      "@property\n",
      "    def _chain_type(self) -> str:\n",
      "        return \"map_reduce_documents_chain\"\n",
      "\n",
      "from langsmith import Client\n",
      "    from langchain.chat_models import ChatOpenAI\n",
      "    from langchain.chains import LLMChain\n",
      "    from langchain.smith import RunEvalConfig, run_on_dataset\n",
      "\n",
      "    # Chains may have memory. Passing in a constructor function lets the\n",
      "    # evaluation framework avoid cross-contamination between runs.\n",
      "    def construct_chain():\n",
      "        llm = ChatOpenAI(temperature=0)\n",
      "        chain = LLMChain.from_string(\n",
      "            llm,\n",
      "            \"What's the answer to {your_input_key}\"\n",
      "        )\n",
      "        return chain\n",
      "\n",
      "from langchain.chains import ReduceDocumentsChain\n",
      "from langchain.chains.api.base import APIChain\n",
      "from langchain.chains.base import Chain\n",
      "from langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\n",
      "from langchain.chains.combine_documents.map_rerank import MapRerankDocumentsChain\n",
      "from langchain.chains.combine_documents.refine import RefineDocumentsChain\n",
      "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
      "from langchain.chains.graph_qa.cypher import GraphCypherQAChain\n",
      "from langchain.chains.hyde.base import HypotheticalDocumentEmbedder\n",
      "from langchain.chains.llm import LLMChain\n",
      "from langchain.chains.llm_bash.base import LLMBashChain\n",
      "from langchain.chains.llm_checker.base import LLMCheckerChain\n",
      "from langchain.chains.llm_math.base import LLMMathChain\n",
      "from langchain.chains.llm_requests import LLMRequestsChain\n",
      "from langchain.chains.qa_with_sources.base import QAWithSourcesChain\n",
      "Human: When should I use a map rank chain?\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You should use a MapRerankDocumentsChain when you want to combine multiple documents by mapping a chain over them and then reranking the results. This algorithm calls an LLMChain on each input document and uses an OutputParser to parse the results into an answer and a score. The answer with the highest score is then returned.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"When should I use a map rank chain?\"\n",
    "\n",
    "qa.run(query, callbacks=[handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1817bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596b6897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c070020d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langchain)",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

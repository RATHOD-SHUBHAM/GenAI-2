{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c77c9812",
   "metadata": {},
   "source": [
    "# Agent Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2221026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from wikipedia) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/damienbenveniste/anaconda3/envs/langchain/lib/python3.11/site-packages (from beautifulsoup4->wikipedia) (2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b881019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType, load_tools\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "tools = load_tools(['wikipedia', \"llm-math\"], llm=llm)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools, \n",
    "    llm=llm, \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    return_intermediate_steps=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e7449c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "Wikipedia: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\n",
      "Calculator: Useful for when you need to answer questions about math.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Wikipedia, Calculator]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: \n",
      "What is the current American president's age? \n",
      "Return the age multiplied by 82\n",
      "\n",
      "Thought:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use Wikipedia to find the current American president's age.\n",
      "Action: Wikipedia\n",
      "Action Input: \"current American president\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: President of the United States\n",
      "Summary: The president of the United States (POTUS) is the head of state and head of government of the United States. The president directs the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces.\n",
      "The power of the presidency has grown substantially since the first president, George Washington, took office in 1789. While presidential power has ebbed and flowed over time, the presidency has played an increasingly significant role in American political life since the beginning of the 20th century, with notable expansion during the presidency of Franklin D. Roosevelt. In modern times, the president is one of the world's most powerful political figures – the leader of the only remaining global superpower. As the leader of the nation with the largest economy by nominal GDP, the president possesses significant domestic and international hard and soft power.\n",
      "Article II of the Constitution establishes the executive branch of the federal government and vests executive power in the president. The power includes the execution and enforcement of federal law and the responsibility to appoint federal executive, diplomatic, regulatory, and judicial officers.  Based on constitutional provisions empowering the president to appoint and receive ambassadors and conclude treaties with foreign powers, and on subsequent laws enacted by Congress, the modern presidency has primary responsibility for conducting U.S. foreign policy. The role includes responsibility for directing the world's most expensive military, which has the second largest nuclear arsenal.\n",
      "The president also plays a leading role in federal legislation and domestic policymaking. As part of the system of separation of powers, Article I, Section 7 of the Constitution gives the president the power to sign or veto federal legislation. Since modern presidents are typically viewed as leaders of their political parties, major policymaking is significantly shaped by the outcome of presidential elections, with presidents taking an active role in promoting their policy priorities to members of Congress who are often electorally dependent on the president. In recent decades, presidents have also made increasing use of executive orders, agency regulations, and judicial appointments to shape domestic policy.\n",
      "The president is elected indirectly through the Electoral College to a four-year term, along with the vice president. Under the Twenty-second Amendment, ratified in 1951, no person who has been elected to two presidential terms may be elected to a third. In addition, nine vice presidents have become president by virtue of a president's intra-term death or resignation. In all, 45 individuals have served 46 presidencies spanning 58 four-year terms. Joe Biden is the 46th and current president of the United States, having assumed office on January 20, 2021.\n",
      "\n",
      "\n",
      "\n",
      "Page: List of presidents of the United States\n",
      "Summary: The president of the United States is the head of state and head of government of the United States, indirectly elected to a four-year term via the Electoral College. The officeholder leads the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces. Since the office was established in 1789, 45 men have served in 46 presidencies. The first president, George Washington, won a unanimous vote of the Electoral College. Grover Cleveland served two non-consecutive terms and is therefore counted as the 22nd and 24th president of the United States, giving rise to the discrepancy between the number of presidencies and the number of persons who have served as president. The incumbent president is Joe Biden.The presidency of William Henry Harrison, who died 31 days after taking office in 1841, was the shortest in American history. Franklin D. Roosevelt served the longest, over twelve years, before dying early in his fourth term in 1945. He is the only U.S. p\u001b[0m\n",
      "Thought:\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "Wikipedia: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\n",
      "Calculator: Useful for when you need to answer questions about math.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Wikipedia, Calculator]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: \n",
      "What is the current American president's age? \n",
      "Return the age multiplied by 82\n",
      "\n",
      "Thought:I should use Wikipedia to find the current American president's age.\n",
      "Action: Wikipedia\n",
      "Action Input: \"current American president\"\n",
      "Observation: Page: President of the United States\n",
      "Summary: The president of the United States (POTUS) is the head of state and head of government of the United States. The president directs the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces.\n",
      "The power of the presidency has grown substantially since the first president, George Washington, took office in 1789. While presidential power has ebbed and flowed over time, the presidency has played an increasingly significant role in American political life since the beginning of the 20th century, with notable expansion during the presidency of Franklin D. Roosevelt. In modern times, the president is one of the world's most powerful political figures – the leader of the only remaining global superpower. As the leader of the nation with the largest economy by nominal GDP, the president possesses significant domestic and international hard and soft power.\n",
      "Article II of the Constitution establishes the executive branch of the federal government and vests executive power in the president. The power includes the execution and enforcement of federal law and the responsibility to appoint federal executive, diplomatic, regulatory, and judicial officers.  Based on constitutional provisions empowering the president to appoint and receive ambassadors and conclude treaties with foreign powers, and on subsequent laws enacted by Congress, the modern presidency has primary responsibility for conducting U.S. foreign policy. The role includes responsibility for directing the world's most expensive military, which has the second largest nuclear arsenal.\n",
      "The president also plays a leading role in federal legislation and domestic policymaking. As part of the system of separation of powers, Article I, Section 7 of the Constitution gives the president the power to sign or veto federal legislation. Since modern presidents are typically viewed as leaders of their political parties, major policymaking is significantly shaped by the outcome of presidential elections, with presidents taking an active role in promoting their policy priorities to members of Congress who are often electorally dependent on the president. In recent decades, presidents have also made increasing use of executive orders, agency regulations, and judicial appointments to shape domestic policy.\n",
      "The president is elected indirectly through the Electoral College to a four-year term, along with the vice president. Under the Twenty-second Amendment, ratified in 1951, no person who has been elected to two presidential terms may be elected to a third. In addition, nine vice presidents have become president by virtue of a president's intra-term death or resignation. In all, 45 individuals have served 46 presidencies spanning 58 four-year terms. Joe Biden is the 46th and current president of the United States, having assumed office on January 20, 2021.\n",
      "\n",
      "\n",
      "\n",
      "Page: List of presidents of the United States\n",
      "Summary: The president of the United States is the head of state and head of government of the United States, indirectly elected to a four-year term via the Electoral College. The officeholder leads the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces. Since the office was established in 1789, 45 men have served in 46 presidencies. The first president, George Washington, won a unanimous vote of the Electoral College. Grover Cleveland served two non-consecutive terms and is therefore counted as the 22nd and 24th president of the United States, giving rise to the discrepancy between the number of presidencies and the number of persons who have served as president. The incumbent president is Joe Biden.The presidency of William Henry Harrison, who died 31 days after taking office in 1841, was the shortest in American history. Franklin D. Roosevelt served the longest, over twelve years, before dying early in his fourth term in 1945. He is the only U.S. p\n",
      "Thought:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find the current American president's age to multiply it by 82.\n",
      "Action: Wikipedia\n",
      "Action Input: \"Joe Biden\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Joe Biden\n",
      "Summary: Joseph Robinette Biden Jr. (  BY-dən; born November 20, 1942) is an American politician who is the 46th and current president of the United States. A member of the Democratic Party, he previously served as the 47th vice president from 2009 to 2017 under President Barack Obama and represented Delaware in the United States Senate from 1973 to 2009.\n",
      "Born in Scranton, Pennsylvania, Biden moved with his family to Delaware in 1953. He studied at the University of Delaware before earning his law degree from Syracuse University. He was elected to the New Castle County Council in 1970 and to the U.S. Senate in 1972. As a senator, Biden drafted and led the effort to pass the Violent Crime Control and Law Enforcement Act and the Violence Against Women Act; and oversaw six U.S. Supreme Court confirmation hearings, including the contentious hearings for Robert Bork and Clarence Thomas. Biden ran unsuccessfully for the Democratic presidential nomination in 1988 and 2008. In 2008, Barack Obama chose Biden as his running mate, and Biden was a close counselor to Obama during his two terms as vice president. In the 2020 presidential election, Biden and his running mate, Kamala Harris, defeated incumbents Donald Trump and Mike Pence.\n",
      "Taking office at age 78, Biden is the oldest president in U.S. history, the first to have a female vice president, and the first from Delaware. In 2021, he signed a bipartisan infrastructure bill, as well as a $1.9 trillion economic stimulus package in response to the COVID-19 pandemic and subsequent recession. Biden proposed the Build Back Better Act, which failed in Congress, but aspects of which were incorporated into the Inflation Reduction Act that was signed into law in 2022. Biden also signed the bipartisan CHIPS and Science Act, which focused on manufacturing, appointed Ketanji Brown Jackson to the Supreme Court and worked with congressional Republicans to prevent a first ever national default by negotiating a deal to raise the debt ceiling. In foreign policy, Biden restored America's membership in the Paris Agreement. He oversaw the complete withdrawal of U.S. troops from Afghanistan that ended the war in Afghanistan, during which the Afghan government collapsed and the Taliban seized control. Biden has responded to the Russian invasion of Ukraine by imposing sanctions on Russia and authorizing civilian and military aid to Ukraine. In April 2023, he announced his candidacy for the Democratic Party nomination in the 2024 presidential election. Scholars and historians rank Biden in the upper to mid tier of presidents. \n",
      "\n",
      "\n",
      "\n",
      "Page: Presidency of Joe Biden\n",
      "Summary: Joe Biden's tenure as the 46th president of the United States began with his inauguration on January 20, 2021. Biden, a Democrat from Delaware who previously served as vice president under Barack Obama, took office following his victory in the 2020 presidential election over Republican incumbent president Donald Trump. Upon his inauguration, he became the oldest president in American history. Biden entered office amid the COVID-19 pandemic, an economic crisis, and increased political polarization.On the first day of his presidency, Biden made an effort to revert President Trump's energy policy by restoring U.S. participation in the Paris Agreement and revoking the permit for the Keystone XL pipeline. He also halted funding for Trump's border wall, an expansion of the Mexican border wall. On his second day, he issued a series of executive orders to reduce the impact of COVID-19, including invoking the Defense Production Act of 1950, and set an early goal of achieving one hundred million COVID-19 vaccinations in the United States in his first 100 days.Biden signed into law the American Rescue Plan Act of 2021; a $1.9 trillion stimulus bill that temporarily established expanded unemployment insurance and sent $1,400 stimulus checks to most Americans in response to continued economic pressure from COVID-19. He signed the bipartisan \u001b[0m\n",
      "Thought:\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "Wikipedia: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\n",
      "Calculator: Useful for when you need to answer questions about math.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Wikipedia, Calculator]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: \n",
      "What is the current American president's age? \n",
      "Return the age multiplied by 82\n",
      "\n",
      "Thought:I should use Wikipedia to find the current American president's age.\n",
      "Action: Wikipedia\n",
      "Action Input: \"current American president\"\n",
      "Observation: Page: President of the United States\n",
      "Summary: The president of the United States (POTUS) is the head of state and head of government of the United States. The president directs the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces.\n",
      "The power of the presidency has grown substantially since the first president, George Washington, took office in 1789. While presidential power has ebbed and flowed over time, the presidency has played an increasingly significant role in American political life since the beginning of the 20th century, with notable expansion during the presidency of Franklin D. Roosevelt. In modern times, the president is one of the world's most powerful political figures – the leader of the only remaining global superpower. As the leader of the nation with the largest economy by nominal GDP, the president possesses significant domestic and international hard and soft power.\n",
      "Article II of the Constitution establishes the executive branch of the federal government and vests executive power in the president. The power includes the execution and enforcement of federal law and the responsibility to appoint federal executive, diplomatic, regulatory, and judicial officers.  Based on constitutional provisions empowering the president to appoint and receive ambassadors and conclude treaties with foreign powers, and on subsequent laws enacted by Congress, the modern presidency has primary responsibility for conducting U.S. foreign policy. The role includes responsibility for directing the world's most expensive military, which has the second largest nuclear arsenal.\n",
      "The president also plays a leading role in federal legislation and domestic policymaking. As part of the system of separation of powers, Article I, Section 7 of the Constitution gives the president the power to sign or veto federal legislation. Since modern presidents are typically viewed as leaders of their political parties, major policymaking is significantly shaped by the outcome of presidential elections, with presidents taking an active role in promoting their policy priorities to members of Congress who are often electorally dependent on the president. In recent decades, presidents have also made increasing use of executive orders, agency regulations, and judicial appointments to shape domestic policy.\n",
      "The president is elected indirectly through the Electoral College to a four-year term, along with the vice president. Under the Twenty-second Amendment, ratified in 1951, no person who has been elected to two presidential terms may be elected to a third. In addition, nine vice presidents have become president by virtue of a president's intra-term death or resignation. In all, 45 individuals have served 46 presidencies spanning 58 four-year terms. Joe Biden is the 46th and current president of the United States, having assumed office on January 20, 2021.\n",
      "\n",
      "\n",
      "\n",
      "Page: List of presidents of the United States\n",
      "Summary: The president of the United States is the head of state and head of government of the United States, indirectly elected to a four-year term via the Electoral College. The officeholder leads the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces. Since the office was established in 1789, 45 men have served in 46 presidencies. The first president, George Washington, won a unanimous vote of the Electoral College. Grover Cleveland served two non-consecutive terms and is therefore counted as the 22nd and 24th president of the United States, giving rise to the discrepancy between the number of presidencies and the number of persons who have served as president. The incumbent president is Joe Biden.The presidency of William Henry Harrison, who died 31 days after taking office in 1841, was the shortest in American history. Franklin D. Roosevelt served the longest, over twelve years, before dying early in his fourth term in 1945. He is the only U.S. p\n",
      "Thought:I need to find the current American president's age to multiply it by 82.\n",
      "Action: Wikipedia\n",
      "Action Input: \"Joe Biden\"\n",
      "Observation: Page: Joe Biden\n",
      "Summary: Joseph Robinette Biden Jr. (  BY-dən; born November 20, 1942) is an American politician who is the 46th and current president of the United States. A member of the Democratic Party, he previously served as the 47th vice president from 2009 to 2017 under President Barack Obama and represented Delaware in the United States Senate from 1973 to 2009.\n",
      "Born in Scranton, Pennsylvania, Biden moved with his family to Delaware in 1953. He studied at the University of Delaware before earning his law degree from Syracuse University. He was elected to the New Castle County Council in 1970 and to the U.S. Senate in 1972. As a senator, Biden drafted and led the effort to pass the Violent Crime Control and Law Enforcement Act and the Violence Against Women Act; and oversaw six U.S. Supreme Court confirmation hearings, including the contentious hearings for Robert Bork and Clarence Thomas. Biden ran unsuccessfully for the Democratic presidential nomination in 1988 and 2008. In 2008, Barack Obama chose Biden as his running mate, and Biden was a close counselor to Obama during his two terms as vice president. In the 2020 presidential election, Biden and his running mate, Kamala Harris, defeated incumbents Donald Trump and Mike Pence.\n",
      "Taking office at age 78, Biden is the oldest president in U.S. history, the first to have a female vice president, and the first from Delaware. In 2021, he signed a bipartisan infrastructure bill, as well as a $1.9 trillion economic stimulus package in response to the COVID-19 pandemic and subsequent recession. Biden proposed the Build Back Better Act, which failed in Congress, but aspects of which were incorporated into the Inflation Reduction Act that was signed into law in 2022. Biden also signed the bipartisan CHIPS and Science Act, which focused on manufacturing, appointed Ketanji Brown Jackson to the Supreme Court and worked with congressional Republicans to prevent a first ever national default by negotiating a deal to raise the debt ceiling. In foreign policy, Biden restored America's membership in the Paris Agreement. He oversaw the complete withdrawal of U.S. troops from Afghanistan that ended the war in Afghanistan, during which the Afghan government collapsed and the Taliban seized control. Biden has responded to the Russian invasion of Ukraine by imposing sanctions on Russia and authorizing civilian and military aid to Ukraine. In April 2023, he announced his candidacy for the Democratic Party nomination in the 2024 presidential election. Scholars and historians rank Biden in the upper to mid tier of presidents. \n",
      "\n",
      "\n",
      "\n",
      "Page: Presidency of Joe Biden\n",
      "Summary: Joe Biden's tenure as the 46th president of the United States began with his inauguration on January 20, 2021. Biden, a Democrat from Delaware who previously served as vice president under Barack Obama, took office following his victory in the 2020 presidential election over Republican incumbent president Donald Trump. Upon his inauguration, he became the oldest president in American history. Biden entered office amid the COVID-19 pandemic, an economic crisis, and increased political polarization.On the first day of his presidency, Biden made an effort to revert President Trump's energy policy by restoring U.S. participation in the Paris Agreement and revoking the permit for the Keystone XL pipeline. He also halted funding for Trump's border wall, an expansion of the Mexican border wall. On his second day, he issued a series of executive orders to reduce the impact of COVID-19, including invoking the Defense Production Act of 1950, and set an early goal of achieving one hundred million COVID-19 vaccinations in the United States in his first 100 days.Biden signed into law the American Rescue Plan Act of 2021; a $1.9 trillion stimulus bill that temporarily established expanded unemployment insurance and sent $1,400 stimulus checks to most Americans in response to continued economic pressure from COVID-19. He signed the bipartisan \n",
      "Thought:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI have found the current American president's age, which is 78. Now I can multiply it by 82.\n",
      "Action: Calculator\n",
      "Action Input: 78 * 82\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "78 * 82\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mTranslate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
      "\n",
      "Question: ${Question with math problem.}\n",
      "```text\n",
      "${single line mathematical expression that solves the problem}\n",
      "```\n",
      "...numexpr.evaluate(text)...\n",
      "```output\n",
      "${Output of running the code}\n",
      "```\n",
      "Answer: ${Answer}\n",
      "\n",
      "Begin.\n",
      "\n",
      "Question: What is 37593 * 67?\n",
      "```text\n",
      "37593 * 67\n",
      "```\n",
      "...numexpr.evaluate(\"37593 * 67\")...\n",
      "```output\n",
      "2518731\n",
      "```\n",
      "Answer: 2518731\n",
      "\n",
      "Question: 37593^(1/5)\n",
      "```text\n",
      "37593**(1/5)\n",
      "```\n",
      "...numexpr.evaluate(\"37593**(1/5)\")...\n",
      "```output\n",
      "8.222831614237718\n",
      "```\n",
      "Answer: 8.222831614237718\n",
      "\n",
      "Question: 78 * 82\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```text\n",
      "78 * 82\n",
      "```\n",
      "...numexpr.evaluate(\"78 * 82\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m6396\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 6396\u001b[0m\n",
      "Thought:\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "Wikipedia: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\n",
      "Calculator: Useful for when you need to answer questions about math.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Wikipedia, Calculator]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: \n",
      "What is the current American president's age? \n",
      "Return the age multiplied by 82\n",
      "\n",
      "Thought:I should use Wikipedia to find the current American president's age.\n",
      "Action: Wikipedia\n",
      "Action Input: \"current American president\"\n",
      "Observation: Page: President of the United States\n",
      "Summary: The president of the United States (POTUS) is the head of state and head of government of the United States. The president directs the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces.\n",
      "The power of the presidency has grown substantially since the first president, George Washington, took office in 1789. While presidential power has ebbed and flowed over time, the presidency has played an increasingly significant role in American political life since the beginning of the 20th century, with notable expansion during the presidency of Franklin D. Roosevelt. In modern times, the president is one of the world's most powerful political figures – the leader of the only remaining global superpower. As the leader of the nation with the largest economy by nominal GDP, the president possesses significant domestic and international hard and soft power.\n",
      "Article II of the Constitution establishes the executive branch of the federal government and vests executive power in the president. The power includes the execution and enforcement of federal law and the responsibility to appoint federal executive, diplomatic, regulatory, and judicial officers.  Based on constitutional provisions empowering the president to appoint and receive ambassadors and conclude treaties with foreign powers, and on subsequent laws enacted by Congress, the modern presidency has primary responsibility for conducting U.S. foreign policy. The role includes responsibility for directing the world's most expensive military, which has the second largest nuclear arsenal.\n",
      "The president also plays a leading role in federal legislation and domestic policymaking. As part of the system of separation of powers, Article I, Section 7 of the Constitution gives the president the power to sign or veto federal legislation. Since modern presidents are typically viewed as leaders of their political parties, major policymaking is significantly shaped by the outcome of presidential elections, with presidents taking an active role in promoting their policy priorities to members of Congress who are often electorally dependent on the president. In recent decades, presidents have also made increasing use of executive orders, agency regulations, and judicial appointments to shape domestic policy.\n",
      "The president is elected indirectly through the Electoral College to a four-year term, along with the vice president. Under the Twenty-second Amendment, ratified in 1951, no person who has been elected to two presidential terms may be elected to a third. In addition, nine vice presidents have become president by virtue of a president's intra-term death or resignation. In all, 45 individuals have served 46 presidencies spanning 58 four-year terms. Joe Biden is the 46th and current president of the United States, having assumed office on January 20, 2021.\n",
      "\n",
      "\n",
      "\n",
      "Page: List of presidents of the United States\n",
      "Summary: The president of the United States is the head of state and head of government of the United States, indirectly elected to a four-year term via the Electoral College. The officeholder leads the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces. Since the office was established in 1789, 45 men have served in 46 presidencies. The first president, George Washington, won a unanimous vote of the Electoral College. Grover Cleveland served two non-consecutive terms and is therefore counted as the 22nd and 24th president of the United States, giving rise to the discrepancy between the number of presidencies and the number of persons who have served as president. The incumbent president is Joe Biden.The presidency of William Henry Harrison, who died 31 days after taking office in 1841, was the shortest in American history. Franklin D. Roosevelt served the longest, over twelve years, before dying early in his fourth term in 1945. He is the only U.S. p\n",
      "Thought:I need to find the current American president's age to multiply it by 82.\n",
      "Action: Wikipedia\n",
      "Action Input: \"Joe Biden\"\n",
      "Observation: Page: Joe Biden\n",
      "Summary: Joseph Robinette Biden Jr. (  BY-dən; born November 20, 1942) is an American politician who is the 46th and current president of the United States. A member of the Democratic Party, he previously served as the 47th vice president from 2009 to 2017 under President Barack Obama and represented Delaware in the United States Senate from 1973 to 2009.\n",
      "Born in Scranton, Pennsylvania, Biden moved with his family to Delaware in 1953. He studied at the University of Delaware before earning his law degree from Syracuse University. He was elected to the New Castle County Council in 1970 and to the U.S. Senate in 1972. As a senator, Biden drafted and led the effort to pass the Violent Crime Control and Law Enforcement Act and the Violence Against Women Act; and oversaw six U.S. Supreme Court confirmation hearings, including the contentious hearings for Robert Bork and Clarence Thomas. Biden ran unsuccessfully for the Democratic presidential nomination in 1988 and 2008. In 2008, Barack Obama chose Biden as his running mate, and Biden was a close counselor to Obama during his two terms as vice president. In the 2020 presidential election, Biden and his running mate, Kamala Harris, defeated incumbents Donald Trump and Mike Pence.\n",
      "Taking office at age 78, Biden is the oldest president in U.S. history, the first to have a female vice president, and the first from Delaware. In 2021, he signed a bipartisan infrastructure bill, as well as a $1.9 trillion economic stimulus package in response to the COVID-19 pandemic and subsequent recession. Biden proposed the Build Back Better Act, which failed in Congress, but aspects of which were incorporated into the Inflation Reduction Act that was signed into law in 2022. Biden also signed the bipartisan CHIPS and Science Act, which focused on manufacturing, appointed Ketanji Brown Jackson to the Supreme Court and worked with congressional Republicans to prevent a first ever national default by negotiating a deal to raise the debt ceiling. In foreign policy, Biden restored America's membership in the Paris Agreement. He oversaw the complete withdrawal of U.S. troops from Afghanistan that ended the war in Afghanistan, during which the Afghan government collapsed and the Taliban seized control. Biden has responded to the Russian invasion of Ukraine by imposing sanctions on Russia and authorizing civilian and military aid to Ukraine. In April 2023, he announced his candidacy for the Democratic Party nomination in the 2024 presidential election. Scholars and historians rank Biden in the upper to mid tier of presidents. \n",
      "\n",
      "\n",
      "\n",
      "Page: Presidency of Joe Biden\n",
      "Summary: Joe Biden's tenure as the 46th president of the United States began with his inauguration on January 20, 2021. Biden, a Democrat from Delaware who previously served as vice president under Barack Obama, took office following his victory in the 2020 presidential election over Republican incumbent president Donald Trump. Upon his inauguration, he became the oldest president in American history. Biden entered office amid the COVID-19 pandemic, an economic crisis, and increased political polarization.On the first day of his presidency, Biden made an effort to revert President Trump's energy policy by restoring U.S. participation in the Paris Agreement and revoking the permit for the Keystone XL pipeline. He also halted funding for Trump's border wall, an expansion of the Mexican border wall. On his second day, he issued a series of executive orders to reduce the impact of COVID-19, including invoking the Defense Production Act of 1950, and set an early goal of achieving one hundred million COVID-19 vaccinations in the United States in his first 100 days.Biden signed into law the American Rescue Plan Act of 2021; a $1.9 trillion stimulus bill that temporarily established expanded unemployment insurance and sent $1,400 stimulus checks to most Americans in response to continued economic pressure from COVID-19. He signed the bipartisan \n",
      "Thought:I have found the current American president's age, which is 78. Now I can multiply it by 82.\n",
      "Action: Calculator\n",
      "Action Input: 78 * 82\n",
      "Observation: Answer: 6396\n",
      "Thought:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "Final Answer: The current American president's age multiplied by 82 is 6396.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"\\nWhat is the current American president's age? \\nReturn the age multiplied by 82\\n\",\n",
       " 'output': \"The current American president's age multiplied by 82 is 6396.\",\n",
       " 'intermediate_steps': [(AgentAction(tool='Wikipedia', tool_input='current American president', log='I should use Wikipedia to find the current American president\\'s age.\\nAction: Wikipedia\\nAction Input: \"current American president\"'),\n",
       "   \"Page: President of the United States\\nSummary: The president of the United States (POTUS) is the head of state and head of government of the United States. The president directs the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces.\\nThe power of the presidency has grown substantially since the first president, George Washington, took office in 1789. While presidential power has ebbed and flowed over time, the presidency has played an increasingly significant role in American political life since the beginning of the 20th century, with notable expansion during the presidency of Franklin D. Roosevelt. In modern times, the president is one of the world's most powerful political figures – the leader of the only remaining global superpower. As the leader of the nation with the largest economy by nominal GDP, the president possesses significant domestic and international hard and soft power.\\nArticle II of the Constitution establishes the executive branch of the federal government and vests executive power in the president. The power includes the execution and enforcement of federal law and the responsibility to appoint federal executive, diplomatic, regulatory, and judicial officers.  Based on constitutional provisions empowering the president to appoint and receive ambassadors and conclude treaties with foreign powers, and on subsequent laws enacted by Congress, the modern presidency has primary responsibility for conducting U.S. foreign policy. The role includes responsibility for directing the world's most expensive military, which has the second largest nuclear arsenal.\\nThe president also plays a leading role in federal legislation and domestic policymaking. As part of the system of separation of powers, Article I, Section 7 of the Constitution gives the president the power to sign or veto federal legislation. Since modern presidents are typically viewed as leaders of their political parties, major policymaking is significantly shaped by the outcome of presidential elections, with presidents taking an active role in promoting their policy priorities to members of Congress who are often electorally dependent on the president. In recent decades, presidents have also made increasing use of executive orders, agency regulations, and judicial appointments to shape domestic policy.\\nThe president is elected indirectly through the Electoral College to a four-year term, along with the vice president. Under the Twenty-second Amendment, ratified in 1951, no person who has been elected to two presidential terms may be elected to a third. In addition, nine vice presidents have become president by virtue of a president's intra-term death or resignation. In all, 45 individuals have served 46 presidencies spanning 58 four-year terms. Joe Biden is the 46th and current president of the United States, having assumed office on January 20, 2021.\\n\\n\\n\\nPage: List of presidents of the United States\\nSummary: The president of the United States is the head of state and head of government of the United States, indirectly elected to a four-year term via the Electoral College. The officeholder leads the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces. Since the office was established in 1789, 45 men have served in 46 presidencies. The first president, George Washington, won a unanimous vote of the Electoral College. Grover Cleveland served two non-consecutive terms and is therefore counted as the 22nd and 24th president of the United States, giving rise to the discrepancy between the number of presidencies and the number of persons who have served as president. The incumbent president is Joe Biden.The presidency of William Henry Harrison, who died 31 days after taking office in 1841, was the shortest in American history. Franklin D. Roosevelt served the longest, over twelve years, before dying early in his fourth term in 1945. He is the only U.S. p\"),\n",
       "  (AgentAction(tool='Wikipedia', tool_input='Joe Biden', log='I need to find the current American president\\'s age to multiply it by 82.\\nAction: Wikipedia\\nAction Input: \"Joe Biden\"'),\n",
       "   \"Page: Joe Biden\\nSummary: Joseph Robinette Biden Jr. (  BY-dən; born November 20, 1942) is an American politician who is the 46th and current president of the United States. A member of the Democratic Party, he previously served as the 47th vice president from 2009 to 2017 under President Barack Obama and represented Delaware in the United States Senate from 1973 to 2009.\\nBorn in Scranton, Pennsylvania, Biden moved with his family to Delaware in 1953. He studied at the University of Delaware before earning his law degree from Syracuse University. He was elected to the New Castle County Council in 1970 and to the U.S. Senate in 1972. As a senator, Biden drafted and led the effort to pass the Violent Crime Control and Law Enforcement Act and the Violence Against Women Act; and oversaw six U.S. Supreme Court confirmation hearings, including the contentious hearings for Robert Bork and Clarence Thomas. Biden ran unsuccessfully for the Democratic presidential nomination in 1988 and 2008. In 2008, Barack Obama chose Biden as his running mate, and Biden was a close counselor to Obama during his two terms as vice president. In the 2020 presidential election, Biden and his running mate, Kamala Harris, defeated incumbents Donald Trump and Mike Pence.\\nTaking office at age 78, Biden is the oldest president in U.S. history, the first to have a female vice president, and the first from Delaware. In 2021, he signed a bipartisan infrastructure bill, as well as a $1.9 trillion economic stimulus package in response to the COVID-19 pandemic and subsequent recession. Biden proposed the Build Back Better Act, which failed in Congress, but aspects of which were incorporated into the Inflation Reduction Act that was signed into law in 2022. Biden also signed the bipartisan CHIPS and Science Act, which focused on manufacturing, appointed Ketanji Brown Jackson to the Supreme Court and worked with congressional Republicans to prevent a first ever national default by negotiating a deal to raise the debt ceiling. In foreign policy, Biden restored America's membership in the Paris Agreement. He oversaw the complete withdrawal of U.S. troops from Afghanistan that ended the war in Afghanistan, during which the Afghan government collapsed and the Taliban seized control. Biden has responded to the Russian invasion of Ukraine by imposing sanctions on Russia and authorizing civilian and military aid to Ukraine. In April 2023, he announced his candidacy for the Democratic Party nomination in the 2024 presidential election. Scholars and historians rank Biden in the upper to mid tier of presidents. \\n\\n\\n\\nPage: Presidency of Joe Biden\\nSummary: Joe Biden's tenure as the 46th president of the United States began with his inauguration on January 20, 2021. Biden, a Democrat from Delaware who previously served as vice president under Barack Obama, took office following his victory in the 2020 presidential election over Republican incumbent president Donald Trump. Upon his inauguration, he became the oldest president in American history. Biden entered office amid the COVID-19 pandemic, an economic crisis, and increased political polarization.On the first day of his presidency, Biden made an effort to revert President Trump's energy policy by restoring U.S. participation in the Paris Agreement and revoking the permit for the Keystone XL pipeline. He also halted funding for Trump's border wall, an expansion of the Mexican border wall. On his second day, he issued a series of executive orders to reduce the impact of COVID-19, including invoking the Defense Production Act of 1950, and set an early goal of achieving one hundred million COVID-19 vaccinations in the United States in his first 100 days.Biden signed into law the American Rescue Plan Act of 2021; a $1.9 trillion stimulus bill that temporarily established expanded unemployment insurance and sent $1,400 stimulus checks to most Americans in response to continued economic pressure from COVID-19. He signed the bipartisan \"),\n",
       "  (AgentAction(tool='Calculator', tool_input='78 * 82', log=\"I have found the current American president's age, which is 78. Now I can multiply it by 82.\\nAction: Calculator\\nAction Input: 78 * 82\"),\n",
       "   'Answer: 6396')]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "question = \"\"\"\n",
    "What is the current American president's age? \n",
    "Return the age multiplied by 82\n",
    "\"\"\"\n",
    "\n",
    "handler = StdOutCallbackHandler()\n",
    "response = agent(\n",
    "    {\"input\": question}, \n",
    "    callbacks=[handler]\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13c9b0f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentAction(tool='Wikipedia', tool_input='current American president', log='I should use Wikipedia to find the current American president\\'s age.\\nAction: Wikipedia\\nAction Input: \"current American president\"')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['intermediate_steps'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6167606c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['President of the United States',\n",
       " 'List of presidents of the United States',\n",
       " 'List of current presidents of legislatures',\n",
       " 'List of current vice presidents and designated acting presidents',\n",
       " 'George Washington',\n",
       " 'Abraham Lincoln',\n",
       " 'Vice President of the United States',\n",
       " 'News',\n",
       " 'Joe Biden',\n",
       " 'List of current United States senators']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wikipedia\n",
    "\n",
    "wikipedia.search('current American president')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caa756a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The president of the United States (POTUS) is the head of state and head of government of the United States. The president directs the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces.\\nThe power of the presidency has grown substantially since the first president, George Washington, took office in 1789. While presidential power has ebbed and flowed over time, the presidency has played an increasingly significant role in American political life since the beginning of the 20th century, with notable expansion during the presidency of Franklin D. Roosevelt. In modern times, the president is one of the world's most powerful political figures – the leader of the only remaining global superpower. As the leader of the nation with the largest economy by nominal GDP, the president possesses significant domestic and international hard and soft power.\\nArticle II of the Constitution establishes the executive branch of the federal government and vests executive power in the president. The power includes the execution and enforcement of federal law and the responsibility to appoint federal executive, diplomatic, regulatory, and judicial officers.  Based on constitutional provisions empowering the president to appoint and receive ambassadors and conclude treaties with foreign powers, and on subsequent laws enacted by Congress, the modern presidency has primary responsibility for conducting U.S. foreign policy. The role includes responsibility for directing the world's most expensive military, which has the second largest nuclear arsenal.\\nThe president also plays a leading role in federal legislation and domestic policymaking. As part of the system of separation of powers, Article I, Section 7 of the Constitution gives the president the power to sign or veto federal legislation. Since modern presidents are typically viewed as leaders of their political parties, major policymaking is significantly shaped by the outcome of presidential elections, with presidents taking an active role in promoting their policy priorities to members of Congress who are often electorally dependent on the president. In recent decades, presidents have also made increasing use of executive orders, agency regulations, and judicial appointments to shape domestic policy.\\nThe president is elected indirectly through the Electoral College to a four-year term, along with the vice president. Under the Twenty-second Amendment, ratified in 1951, no person who has been elected to two presidential terms may be elected to a third. In addition, nine vice presidents have become president by virtue of a president's intra-term death or resignation. In all, 45 individuals have served 46 presidencies spanning 58 four-year terms. Joe Biden is the 46th and current president of the United States, having assumed office on January 20, 2021.\\n\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia.summary('President of the United States')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7cbf3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The president of the United States is the head of state and head of government of the United States, indirectly elected to a four-year term via the Electoral College. The officeholder leads the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces. Since the office was established in 1789, 45 men have served in 46 presidencies. The first president, George Washington, won a unanimous vote of the Electoral College. Grover Cleveland served two non-consecutive terms and is therefore counted as the 22nd and 24th president of the United States, giving rise to the discrepancy between the number of presidencies and the number of persons who have served as president. The incumbent president is Joe Biden.The presidency of William Henry Harrison, who died 31 days after taking office in 1841, was the shortest in American history. Franklin D. Roosevelt served the longest, over twelve years, before dying early in his fourth term in 1945. He is the only U.S. president to have served more than two terms. Since the ratification of the Twenty-second Amendment to the United States Constitution in 1951, no person may be elected president more than twice, and no one who has served more than two years of a term to which someone else was elected may be elected more than once.Four presidents died in office of natural causes (William Henry Harrison, Zachary Taylor, Warren G. Harding, and Franklin D. Roosevelt), four were assassinated (Abraham Lincoln, James A. Garfield, William McKinley and John F. Kennedy), and one resigned (Richard Nixon, facing impeachment and removal from office). John Tyler was the first vice president to assume the presidency during a presidential term, and set the precedent that a vice president who does so becomes the fully functioning president with his presidency.Throughout most of its history, American politics has been dominated by political parties. The Constitution is silent on the issue of political parties, and at the time it came into force in 1789, no organized parties existed. Soon after the 1st Congress convened, political factions began rallying around dominant Washington administration officials, such as Alexander Hamilton and Thomas Jefferson. Concerned about the capacity of political parties to destroy the fragile unity holding the nation together, Washington remained unaffiliated with any political faction or party throughout his eight-year presidency. He was, and remains, the only U.S. president never affiliated with a political party.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia.summary('List of presidents of the United States')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08c93863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMMathChain\n",
    "\n",
    "chain = LLMMathChain.from_llm(llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c794d0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "45 * 89\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mTranslate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
      "\n",
      "Question: ${Question with math problem.}\n",
      "```text\n",
      "${single line mathematical expression that solves the problem}\n",
      "```\n",
      "...numexpr.evaluate(text)...\n",
      "```output\n",
      "${Output of running the code}\n",
      "```\n",
      "Answer: ${Answer}\n",
      "\n",
      "Begin.\n",
      "\n",
      "Question: What is 37593 * 67?\n",
      "```text\n",
      "37593 * 67\n",
      "```\n",
      "...numexpr.evaluate(\"37593 * 67\")...\n",
      "```output\n",
      "2518731\n",
      "```\n",
      "Answer: 2518731\n",
      "\n",
      "Question: 37593^(1/5)\n",
      "```text\n",
      "37593**(1/5)\n",
      "```\n",
      "...numexpr.evaluate(\"37593**(1/5)\")...\n",
      "```output\n",
      "8.222831614237718\n",
      "```\n",
      "Answer: 8.222831614237718\n",
      "\n",
      "Question: 45 * 89\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```text\n",
      "45 * 89\n",
      "```\n",
      "...numexpr.evaluate(\"45 * 89\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m4005\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer: 4005'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run('45 * 89', callbacks=[handler])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d1898",
   "metadata": {},
   "source": [
    "# Custom Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "224a6866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Springer Series in Statistics\\nTrevor Hastie\\nRobert TibshiraniJerome FriedmanSpringer Series in Statistics\\nThe Elements of\\nStatistical Learning\\nData Mining, Inference, and Prediction\\nThe Elements of Statistical LearningDuring the past decade there has been an explosion in computation and information tech-\\nnology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining,machine learning, and bioinformatics. Many of these tools have common underpinnings butare often expressed with different terminology. This book describes the important ideas inthese areas in a common conceptual framework. While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples are given, with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book’s coverage is broad, from supervised learning(prediction) to unsupervised learning. The many topics include neural networks, supportvector machines, classification trees and boosting—the first comprehensive treatment of thistopic in any book.\\nThis major new edition features many topics not covered in the original, including graphical\\nmodels, random forests, ensemble methods, least angle regression & path algorithms for thelasso, non-negative matrix factorization, and spectral clustering. There is also a chapter onmethods for “wide” data (p bigger than n), including multiple testing and false discovery rates.\\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at\\nStanford University. They are prominent researchers in this area: Hastie and Tibshiranideveloped generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS andinvented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of thevery successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.\\n›springer.comSTATISTICS\\nisbn 978-0-387-84857-0Trevor Hastie • Robert Tibshirani • Jerome Friedman\\nThe Elements of Statictical Learning\\nHastie • Tibshirani • Friedman\\nSecond Edition', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 0}),\n",
       " Document(page_content='This is page v\\nPrinter: Opaque this\\nTo our parents:\\nValerie and Patrick Hastie\\nVera and Sami Tibshirani\\nFlorence and Harry Friedman\\nand to our families:\\nSamantha, Timothy, and Lynda\\nCharlie, Ryan, Julie, and Cheryl\\nMelanie, Dora, Monika, and Ildiko', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 1}),\n",
       " Document(page_content='vi', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 2}),\n",
       " Document(page_content='This is page vii\\nPrinter: Opaque this\\nPreface to the Second Edition\\nIn God we trust, all others bring data.\\n–William Edwards Deming (1900-1993)1\\nWe have been gratiﬁed by the popularity of the ﬁrst edition of The\\nElements of Statistical Learning. This, along with the fast pace of research\\nin the statistical learning ﬁeld, motivated us to update our book with a\\nsecond edition.\\nWe have added four new chapters and updated some of the existi ng\\nchapters. Because many readers are familiar with the layout of the ﬁrst\\nedition, we have tried to change it as little as possible. Her e is a summary\\nof the main changes:\\n1On the Web, this quote has been widely attributed to both Deming and R obert W.\\nHayden; however Professor Hayden told us that he can claim no credit for th is quote,\\nand ironically we could ﬁnd no “data” conﬁrming that Deming actual ly said this.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 3}),\n",
       " Document(page_content='viii Preface to the Second Edition\\nChapter What’s new\\n1.Introduction\\n2.Overview of Supervised Learning\\n3.Linear Methods for Regression LAR algorithm and generaliza tions\\nof the lasso\\n4.Linear Methods for Classiﬁcation Lasso path for logistic re gression\\n5.Basis Expansions and Regulariza-\\ntionAdditional illustrations of RKHS\\n6.Kernel Smoothing Methods\\n7.Model Assessment and Selection Strengths and pitfalls of cr oss-\\nvalidation\\n8.Model Inference and Averaging\\n9.Additive Models, Trees, and\\nRelated Methods\\n10.Boosting and Additive Trees New example from ecology; some\\nmaterial split oﬀ to Chapter 16.\\n11.Neural Networks Bayesian neural nets and the NIPS\\n2003 challenge\\n12.Support Vector Machines and\\nFlexible DiscriminantsPath algorithm for SVM classiﬁer\\n13.Prototype Methods and\\nNearest-Neighbors\\n14.Unsupervised Learning Spectral clustering, kernel PCA,\\nsparse PCA, non-negative matrix\\nfactorization archetypal analysis,\\nnonlinear dimension reduction,\\nGoogle page rank algorithm, a\\ndirect approach to ICA\\n15.Random Forests New\\n16.Ensemble Learning New\\n17.Undirected Graphical Models New\\n18.High-Dimensional Problems New\\nSome further notes:\\n•Our ﬁrst edition was unfriendly to colorblind readers; in pa rticular,\\nwe tended to favor red/greencontrasts which are particularly trou-\\nblesome. We have changed the color palette in this edition to a large\\nextent, replacing the above with an orange/bluecontrast.\\n•We have changed the name of Chapter 6 from “Kernel Methods” to\\n“Kernel Smoothing Methods”, to avoid confusion with the mac hine-\\nlearningkernelmethodthatisdiscussedinthecontextofsu pportvec-\\ntor machines (Chapter 12) and more generally in Chapters 5 an d 14.\\n•In the ﬁrst edition, the discussion of error-rate estimatio n in Chap-\\nter 7 was sloppy, as we did not clearly diﬀerentiate the notio ns of\\nconditional error rates (conditional on the training set) a nd uncondi-\\ntional rates. We have ﬁxed this in the new edition.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 4}),\n",
       " Document(page_content='Preface to the Second Edition ix\\n•Chapters 15 and 16 follow naturally from Chapter 10, and the c hap-\\nters are probably best read in that order.\\n•In Chapter 17, we have not attempted a comprehensive treatme nt\\nof graphical models, and discuss only undirected models and some\\nnew methods for their estimation. Due to a lack of space, we ha ve\\nspeciﬁcally omitted coverage of directed graphical models .\\n•Chapter 18 explores the “ p≫N” problem, which is learning in high-\\ndimensional feature spaces. These problems arise in many ar eas, in-\\ncluding genomic and proteomic studies, and document classi ﬁcation.\\nWe thank the many readers who have found the (too numerous) er rors in\\nthe ﬁrst edition. We apologize for those and have done our bes t to avoid er-\\nrorsinthisnewedition.WethankMarkSegal,BalaRajaratna m,andLarry\\nWasserman for comments on some of the new chapters, and many S tanford\\ngraduate and post-doctoral students who oﬀered comments, i n particular\\nMohammed AlQuraishi, John Boik, Holger Hoeﬂing, Arian Male ki, Donal\\nMcMahon, Saharon Rosset, Babak Shababa, Daniela Witten, Ji Zhu and\\nHui Zou. We thank John Kimmel for his patience in guiding us th rough this\\nnew edition. RT dedicates this edition to the memory of Anna M cPhee.\\nTrevor Hastie\\nRobert Tibshirani\\nJerome Friedman\\nStanford, California\\nAugust 2008', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 5}),\n",
       " Document(page_content='x Preface to the Second Edition', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 6}),\n",
       " Document(page_content='This is page xi\\nPrinter: Opaque this\\nPreface to the First Edition\\nWe are drowning in information and starving for knowledge.\\n–Rutherford D. Roger\\nThe ﬁeld of Statistics is constantly challenged by the probl ems that science\\nandindustrybringstoitsdoor.Intheearlydays,theseprob lemsoftencame\\nfrom agricultural and industrial experiments and were rela tively small in\\nscope. With the advent of computers and the information age, statistical\\nproblems have exploded both in size and complexity. Challen ges in the\\nareas of data storage, organization and searching have led t o the new ﬁeld\\nof “data mining”; statistical and computational problems i n biology and\\nmedicine have created “bioinformatics.” Vast amounts of da ta are being\\ngenerated in many ﬁelds, and the statistician’s job is to mak e sense of it\\nall: to extract important patterns and trends, and understa nd “what the\\ndata says.” We call this learning from data .\\nThe challenges in learning from data have led to a revolution in the sta-\\ntisticalsciences.Sincecomputationplayssuchakeyrole, itisnotsurprising\\nthat much of this new development has been done by researcher s in other\\nﬁelds such as computer science and engineering.\\nThe learning problems that we consider can be roughly catego rized as\\neithersupervised orunsupervised . In supervised learning, the goal is to pre-\\ndict the value of an outcome measure based on a number of input measures;\\nin unsupervised learning, there is no outcome measure, and t he goal is to\\ndescribe the associations and patterns among a set of input m easures.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 7}),\n",
       " Document(page_content='xii Preface to the First Edition\\nThis book is our attempt to bring together many of the importa nt new\\nideas in learning, and explain them in a statistical framewo rk. While some\\nmathematical details are needed, we emphasize the methods a nd their con-\\nceptual underpinnings rather than their theoretical prope rties. As a result,\\nwe hope that this book will appeal not just to statisticians b ut also to\\nresearchers and practitioners in a wide variety of ﬁelds.\\nJust as we have learned a great deal from researchers outside of the ﬁeld\\nof statistics, our statistical viewpoint may help others to better understand\\ndiﬀerent aspects of learning:\\nThere is no true interpretation of anything; interpretatio n is a\\nvehicle in the service of human comprehension. The value of\\ninterpretation is in enabling others to fruitfully think ab out an\\nidea.\\n–Andreas Buja\\nWe would like to acknowledge the contribution of many people to the\\nconception and completion of this book. David Andrews, Leo B reiman,\\nAndreas Buja, John Chambers, Bradley Efron, Geoﬀrey Hinton , Werner\\nStuetzle, and John Tukey have greatly inﬂuenced our careers . Balasub-\\nramanian Narasimhan gave us advice and help on many computat ional\\nproblems, and maintained an excellent computing environme nt. Shin-Ho\\nBang helped in the production of a number of the ﬁgures. Lee Wi lkinson\\ngavevaluabletipsoncolorproduction.IlanaBelitskaya,E vaCantoni,Maya\\nGupta,MichaelJordan,ShantiGopatam,RadfordNeal,Jorge Picazo,Bog-\\ndan Popescu, Olivier Renaud, Saharon Rosset, John Storey, J i Zhu, Mu\\nZhu, two reviewers and many students read parts of the manusc ript and\\noﬀered helpful suggestions. John Kimmel was supportive, pa tient and help-\\nful at every phase; MaryAnn Brickner and Frank Ganz headed a s uperb\\nproduction team at Springer. Trevor Hastie would like to tha nk the statis-\\ntics department at the University of Cape Town for their hosp itality during\\nthe ﬁnal stages of this book. We gratefully acknowledge NSF a nd NIH for\\ntheir support of this work. Finally, we would like to thank ou r families and\\nour parents for their love and support.\\nTrevor Hastie\\nRobert Tibshirani\\nJerome Friedman\\nStanford, California\\nMay 2001\\nThe quiet statisticians have changed our world; not by disco v-\\nering new facts or technical developments, but by changing t he\\nways that we reason, experiment and form our opinions ....\\n–Ian Hacking', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 8}),\n",
       " Document(page_content='This is page xiii\\nPrinter: Opaque this\\nContents\\nPreface to the Second Edition vii\\nPreface to the First Edition xi\\n1 Introduction 1\\n2 Overview of Supervised Learning 9\\n2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.2 Variable Types and Terminology . . . . . . . . . . . . . . 9\\n2.3 Two Simple Approaches to Prediction:\\nLeast Squares and Nearest Neighbors . . . . . . . . . . . 11\\n2.3.1 Linear Models and Least Squares . . . . . . . . 11\\n2.3.2 Nearest-Neighbor Methods . . . . . . . . . . . . 14\\n2.3.3 From Least Squares to Nearest Neighbors . . . . 16\\n2.4 Statistical Decision Theory . . . . . . . . . . . . . . . . . 18\\n2.5 Local Methods in High Dimensions . . . . . . . . . . . . . 22\\n2.6 Statistical Models, Supervised Learning\\nand Function Approximation . . . . . . . . . . . . . . . . 28\\n2.6.1 A Statistical Model\\nfor the Joint Distribution Pr( X,Y) . . . . . . . 28\\n2.6.2 Supervised Learning . . . . . . . . . . . . . . . . 29\\n2.6.3 Function Approximation . . . . . . . . . . . . . 29\\n2.7 Structured Regression Models . . . . . . . . . . . . . . . 32\\n2.7.1 Diﬃculty of the Problem . . . . . . . . . . . . . 32', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 9}),\n",
       " Document(page_content='xiv Contents\\n2.8 Classes of Restricted Estimators . . . . . . . . . . . . . . 33\\n2.8.1 Roughness Penalty and Bayesian Methods . . . 34\\n2.8.2 Kernel Methods and Local Regression . . . . . . 34\\n2.8.3 Basis Functions and Dictionary Methods . . . . 35\\n2.9 Model Selection and the Bias–Variance Tradeoﬀ . . . . . 37\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 39\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\\n3 Linear Methods for Regression 43\\n3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 43\\n3.2 Linear Regression Models and Least Squares . . . . . . . 44\\n3.2.1 Example: Prostate Cancer . . . . . . . . . . . . 49\\n3.2.2 The Gauss–Markov Theorem . . . . . . . . . . . 51\\n3.2.3 Multiple Regression\\nfrom Simple Univariate Regression . . . . . . . . 52\\n3.2.4 Multiple Outputs . . . . . . . . . . . . . . . . . 56\\n3.3 Subset Selection . . . . . . . . . . . . . . . . . . . . . . . 57\\n3.3.1 Best-Subset Selection . . . . . . . . . . . . . . . 57\\n3.3.2 Forward- and Backward-Stepwise Selection . . . 58\\n3.3.3 Forward-Stagewise Regression . . . . . . . . . . 60\\n3.3.4 Prostate Cancer Data Example (Continued) . . 61\\n3.4 Shrinkage Methods . . . . . . . . . . . . . . . . . . . . . . 61\\n3.4.1 Ridge Regression . . . . . . . . . . . . . . . . . 61\\n3.4.2 The Lasso . . . . . . . . . . . . . . . . . . . . . 68\\n3.4.3 Discussion: Subset Selection, Ridge Regression\\nand the Lasso . . . . . . . . . . . . . . . . . . . 69\\n3.4.4 Least Angle Regression . . . . . . . . . . . . . . 73\\n3.5 Methods Using Derived Input Directions . . . . . . . . . 79\\n3.5.1 Principal Components Regression . . . . . . . . 79\\n3.5.2 Partial Least Squares . . . . . . . . . . . . . . . 80\\n3.6 Discussion: A Comparison of the Selection\\nand Shrinkage Methods . . . . . . . . . . . . . . . . . . . 82\\n3.7 Multiple Outcome Shrinkage and Selection . . . . . . . . 84\\n3.8 More on the Lasso and Related Path Algorithms . . . . . 86\\n3.8.1 Incremental Forward Stagewise Regression . . . 86\\n3.8.2 Piecewise-Linear Path Algorithms . . . . . . . . 89\\n3.8.3 The Dantzig Selector . . . . . . . . . . . . . . . 89\\n3.8.4 The Grouped Lasso . . . . . . . . . . . . . . . . 90\\n3.8.5 Further Properties of the Lasso . . . . . . . . . . 91\\n3.8.6 Pathwise Coordinate Optimization . . . . . . . . 92\\n3.9 Computational Considerations . . . . . . . . . . . . . . . 93\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 94\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 10}),\n",
       " Document(page_content='Contents xv\\n4 Linear Methods for Classiﬁcation 101\\n4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 101\\n4.2 Linear Regression of an Indicator Matrix . . . . . . . . . 103\\n4.3 Linear Discriminant Analysis . . . . . . . . . . . . . . . . 106\\n4.3.1 Regularized Discriminant Analysis . . . . . . . . 112\\n4.3.2 Computations for LDA . . . . . . . . . . . . . . 113\\n4.3.3 Reduced-Rank Linear Discriminant Analysis . . 113\\n4.4 Logistic Regression . . . . . . . . . . . . . . . . . . . . . . 119\\n4.4.1 Fitting Logistic Regression Models . . . . . . . . 120\\n4.4.2 Example: South African Heart Disease . . . . . 122\\n4.4.3 Quadratic Approximations and Inference . . . . 124\\n4.4.4L1Regularized Logistic Regression . . . . . . . . 125\\n4.4.5 Logistic Regression or LDA? . . . . . . . . . . . 127\\n4.5 Separating Hyperplanes . . . . . . . . . . . . . . . . . . . 129\\n4.5.1 Rosenblatt’s Perceptron Learning Algorithm . . 130\\n4.5.2 Optimal Separating Hyperplanes . . . . . . . . . 132\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 135\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\\n5 Basis Expansions and Regularization 139\\n5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 139\\n5.2 Piecewise Polynomials and Splines . . . . . . . . . . . . . 141\\n5.2.1 Natural Cubic Splines . . . . . . . . . . . . . . . 144\\n5.2.2 Example:SouthAfricanHeartDisease(Continued)146\\n5.2.3 Example: Phoneme Recognition . . . . . . . . . 148\\n5.3 Filtering and Feature Extraction . . . . . . . . . . . . . . 150\\n5.4 Smoothing Splines . . . . . . . . . . . . . . . . . . . . . . 151\\n5.4.1 Degrees of Freedom and Smoother Matrices . . . 153\\n5.5 Automatic Selection of the Smoothing Parameters . . . . 15 6\\n5.5.1 Fixing the Degrees of Freedom . . . . . . . . . . 158\\n5.5.2 The Bias–Variance Tradeoﬀ . . . . . . . . . . . . 158\\n5.6 Nonparametric Logistic Regression . . . . . . . . . . . . . 161\\n5.7 Multidimensional Splines . . . . . . . . . . . . . . . . . . 162\\n5.8 Regularization and Reproducing Kernel Hilbert Spaces . 167\\n5.8.1 Spaces of Functions Generated by Kernels . . . 168\\n5.8.2 Examples of RKHS . . . . . . . . . . . . . . . . 170\\n5.9 Wavelet Smoothing . . . . . . . . . . . . . . . . . . . . . 174\\n5.9.1 Wavelet Bases and the Wavelet Transform . . . 176\\n5.9.2 Adaptive Wavelet Filtering . . . . . . . . . . . . 179\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 181\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\\nAppendix: Computational Considerations for Splines . . . . . . 186\\nAppendix:B-splines . . . . . . . . . . . . . . . . . . . . . 186\\nAppendix: Computations for Smoothing Splines . . . . . 189', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 11}),\n",
       " Document(page_content='xvi Contents\\n6 Kernel Smoothing Methods 191\\n6.1 One-Dimensional Kernel Smoothers . . . . . . . . . . . . 192\\n6.1.1 Local Linear Regression . . . . . . . . . . . . . . 194\\n6.1.2 Local Polynomial Regression . . . . . . . . . . . 197\\n6.2 Selecting the Width of the Kernel . . . . . . . . . . . . . 198\\n6.3 Local Regression in IRp. . . . . . . . . . . . . . . . . . . 200\\n6.4 Structured Local Regression Models in IRp. . . . . . . . 201\\n6.4.1 Structured Kernels . . . . . . . . . . . . . . . . . 203\\n6.4.2 Structured Regression Functions . . . . . . . . . 203\\n6.5 Local Likelihood and Other Models . . . . . . . . . . . . 205\\n6.6 Kernel Density Estimation and Classiﬁcation . . . . . . . 20 8\\n6.6.1 Kernel Density Estimation . . . . . . . . . . . . 208\\n6.6.2 Kernel Density Classiﬁcation . . . . . . . . . . . 210\\n6.6.3 The Naive Bayes Classiﬁer . . . . . . . . . . . . 210\\n6.7 Radial Basis Functions and Kernels . . . . . . . . . . . . 212\\n6.8 Mixture Models for Density Estimation and Classiﬁcatio n 214\\n6.9 Computational Considerations . . . . . . . . . . . . . . . 216\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 216\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\\n7 Model Assessment and Selection 219\\n7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 219\\n7.2 Bias, Variance and Model Complexity . . . . . . . . . . . 219\\n7.3 The Bias–Variance Decomposition . . . . . . . . . . . . . 223\\n7.3.1 Example: Bias–Variance Tradeoﬀ . . . . . . . . 226\\n7.4 Optimism of the Training Error Rate . . . . . . . . . . . 228\\n7.5 Estimates of In-Sample Prediction Error . . . . . . . . . . 230\\n7.6 The Eﬀective Number of Parameters . . . . . . . . . . . . 232\\n7.7 The Bayesian Approach and BIC . . . . . . . . . . . . . . 233\\n7.8 Minimum Description Length . . . . . . . . . . . . . . . . 235\\n7.9 Vapnik–Chervonenkis Dimension . . . . . . . . . . . . . . 237\\n7.9.1 Example (Continued) . . . . . . . . . . . . . . . 239\\n7.10 Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . 241\\n7.10.1K-Fold Cross-Validation . . . . . . . . . . . . . 241\\n7.10.2 The Wrong and Right Way\\nto Do Cross-validation . . . . . . . . . . . . . . . 245\\n7.10.3 Does Cross-Validation Really Work? . . . . . . . 247\\n7.11 Bootstrap Methods . . . . . . . . . . . . . . . . . . . . . 249\\n7.11.1 Example (Continued) . . . . . . . . . . . . . . . 252\\n7.12 Conditional or Expected Test Error? . . . . . . . . . . . . 254\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 257\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\\n8 Model Inference and Averaging 261\\n8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 261', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 12}),\n",
       " Document(page_content='Contents xvii\\n8.2 The Bootstrap and Maximum Likelihood Methods . . . . 261\\n8.2.1 A Smoothing Example . . . . . . . . . . . . . . 261\\n8.2.2 Maximum Likelihood Inference . . . . . . . . . . 265\\n8.2.3 Bootstrap versus Maximum Likelihood . . . . . 267\\n8.3 Bayesian Methods . . . . . . . . . . . . . . . . . . . . . . 267\\n8.4 Relationship Between the Bootstrap\\nand Bayesian Inference . . . . . . . . . . . . . . . . . . . 271\\n8.5 The EM Algorithm . . . . . . . . . . . . . . . . . . . . . 272\\n8.5.1 Two-Component Mixture Model . . . . . . . . . 272\\n8.5.2 The EM Algorithm in General . . . . . . . . . . 276\\n8.5.3 EM as a Maximization–Maximization Procedure 277\\n8.6 MCMC for Sampling from the Posterior . . . . . . . . . . 279\\n8.7 Bagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\\n8.7.1 Example: Trees with Simulated Data . . . . . . 283\\n8.8 Model Averaging and Stacking . . . . . . . . . . . . . . . 288\\n8.9 Stochastic Search: Bumping . . . . . . . . . . . . . . . . . 290\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 292\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293\\n9 Additive Models, Trees, and Related Methods 295\\n9.1 Generalized Additive Models . . . . . . . . . . . . . . . . 295\\n9.1.1 Fitting Additive Models . . . . . . . . . . . . . . 297\\n9.1.2 Example: Additive Logistic Regression . . . . . 299\\n9.1.3 Summary . . . . . . . . . . . . . . . . . . . . . . 304\\n9.2 Tree-Based Methods . . . . . . . . . . . . . . . . . . . . . 305\\n9.2.1 Background . . . . . . . . . . . . . . . . . . . . 305\\n9.2.2 Regression Trees . . . . . . . . . . . . . . . . . . 307\\n9.2.3 Classiﬁcation Trees . . . . . . . . . . . . . . . . 308\\n9.2.4 Other Issues . . . . . . . . . . . . . . . . . . . . 310\\n9.2.5 Spam Example (Continued) . . . . . . . . . . . 313\\n9.3 PRIM: Bump Hunting . . . . . . . . . . . . . . . . . . . . 317\\n9.3.1 Spam Example (Continued) . . . . . . . . . . . 320\\n9.4 MARS: Multivariate Adaptive Regression Splines . . . . . 3 21\\n9.4.1 Spam Example (Continued) . . . . . . . . . . . 326\\n9.4.2 Example (Simulated Data) . . . . . . . . . . . . 327\\n9.4.3 Other Issues . . . . . . . . . . . . . . . . . . . . 328\\n9.5 Hierarchical Mixtures of Experts . . . . . . . . . . . . . . 329\\n9.6 Missing Data . . . . . . . . . . . . . . . . . . . . . . . . . 332\\n9.7 Computational Considerations . . . . . . . . . . . . . . . 334\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 334\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\\n10 Boosting and Additive Trees 337\\n10.1 Boosting Methods . . . . . . . . . . . . . . . . . . . . . . 337\\n10.1.1 Outline of This Chapter . . . . . . . . . . . . . . 340', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 13}),\n",
       " Document(page_content='xviii Contents\\n10.2 Boosting Fits an Additive Model . . . . . . . . . . . . . . 341\\n10.3 Forward Stagewise Additive Modeling . . . . . . . . . . . 342\\n10.4 Exponential Loss and AdaBoost . . . . . . . . . . . . . . 343\\n10.5 Why Exponential Loss? . . . . . . . . . . . . . . . . . . . 345\\n10.6 Loss Functions and Robustness . . . . . . . . . . . . . . . 346\\n10.7 “Oﬀ-the-Shelf” Procedures for Data Mining . . . . . . . . 35 0\\n10.8 Example: Spam Data . . . . . . . . . . . . . . . . . . . . 352\\n10.9 Boosting Trees . . . . . . . . . . . . . . . . . . . . . . . . 353\\n10.10 Numerical Optimization via Gradient Boosting . . . . . . 358\\n10.10.1 Steepest Descent . . . . . . . . . . . . . . . . . . 358\\n10.10.2 Gradient Boosting . . . . . . . . . . . . . . . . . 359\\n10.10.3 Implementations of Gradient Boosting . . . . . . 360\\n10.11 Right-Sized Trees for Boosting . . . . . . . . . . . . . . . 361\\n10.12 Regularization . . . . . . . . . . . . . . . . . . . . . . . . 364\\n10.12.1 Shrinkage . . . . . . . . . . . . . . . . . . . . . . 364\\n10.12.2 Subsampling . . . . . . . . . . . . . . . . . . . . 365\\n10.13 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . 367\\n10.13.1 Relative Importance of Predictor Variables . . . 367\\n10.13.2 Partial Dependence Plots . . . . . . . . . . . . . 369\\n10.14 Illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . 371\\n10.14.1 California Housing . . . . . . . . . . . . . . . . . 371\\n10.14.2 New Zealand Fish . . . . . . . . . . . . . . . . . 375\\n10.14.3 Demographics Data . . . . . . . . . . . . . . . . 379\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 380\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384\\n11 Neural Networks 389\\n11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 389\\n11.2 Projection Pursuit Regression . . . . . . . . . . . . . . . 389\\n11.3 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 392\\n11.4 Fitting Neural Networks . . . . . . . . . . . . . . . . . . . 395\\n11.5 Some Issues in Training Neural Networks . . . . . . . . . 397\\n11.5.1 Starting Values . . . . . . . . . . . . . . . . . . . 397\\n11.5.2 Overﬁtting . . . . . . . . . . . . . . . . . . . . . 398\\n11.5.3 Scaling of the Inputs . . . . . . . . . . . . . . . 398\\n11.5.4 Number of Hidden Units and Layers . . . . . . . 400\\n11.5.5 Multiple Minima . . . . . . . . . . . . . . . . . . 400\\n11.6 Example: Simulated Data . . . . . . . . . . . . . . . . . . 401\\n11.7 Example: ZIP Code Data . . . . . . . . . . . . . . . . . . 404\\n11.8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 408\\n11.9 Bayesian Neural Nets and the NIPS 2003 Challenge . . . 409\\n11.9.1 Bayes, Boosting and Bagging . . . . . . . . . . . 410\\n11.9.2 Performance Comparisons . . . . . . . . . . . . 412\\n11.10 Computational Considerations . . . . . . . . . . . . . . . 414\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 415', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 14}),\n",
       " Document(page_content='Contents xix\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415\\n12 Support Vector Machines and\\nFlexible Discriminants 417\\n12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 417\\n12.2 The Support Vector Classiﬁer . . . . . . . . . . . . . . . . 417\\n12.2.1 Computing the Support Vector Classiﬁer . . . . 420\\n12.2.2 Mixture Example (Continued) . . . . . . . . . . 421\\n12.3 Support Vector Machines and Kernels . . . . . . . . . . . 423\\n12.3.1 Computing the SVM for Classiﬁcation . . . . . . 423\\n12.3.2 The SVM as a Penalization Method . . . . . . . 426\\n12.3.3 Function Estimation and Reproducing Kernels . 428\\n12.3.4 SVMs and the Curse of Dimensionality . . . . . 431\\n12.3.5 A Path Algorithm for the SVM Classiﬁer . . . . 432\\n12.3.6 Support Vector Machines for Regression . . . . . 434\\n12.3.7 Regression and Kernels . . . . . . . . . . . . . . 436\\n12.3.8 Discussion . . . . . . . . . . . . . . . . . . . . . 438\\n12.4 Generalizing Linear Discriminant Analysis . . . . . . . . 4 38\\n12.5 Flexible Discriminant Analysis . . . . . . . . . . . . . . . 440\\n12.5.1 Computing the FDA Estimates . . . . . . . . . . 444\\n12.6 Penalized Discriminant Analysis . . . . . . . . . . . . . . 446\\n12.7 Mixture Discriminant Analysis . . . . . . . . . . . . . . . 449\\n12.7.1 Example: Waveform Data . . . . . . . . . . . . . 451\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 455\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455\\n13 Prototype Methods and Nearest-Neighbors 459\\n13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 459\\n13.2 Prototype Methods . . . . . . . . . . . . . . . . . . . . . 459\\n13.2.1K-means Clustering . . . . . . . . . . . . . . . . 460\\n13.2.2 Learning Vector Quantization . . . . . . . . . . 462\\n13.2.3 Gaussian Mixtures . . . . . . . . . . . . . . . . . 463\\n13.3k-Nearest-Neighbor Classiﬁers . . . . . . . . . . . . . . . 463\\n13.3.1 Example: A Comparative Study . . . . . . . . . 468\\n13.3.2 Example: k-Nearest-Neighbors\\nand Image Scene Classiﬁcation . . . . . . . . . . 470\\n13.3.3 Invariant Metrics and Tangent Distance . . . . . 471\\n13.4 Adaptive Nearest-Neighbor Methods . . . . . . . . . . . . 475\\n13.4.1 Example . . . . . . . . . . . . . . . . . . . . . . 478\\n13.4.2 Global Dimension Reduction\\nfor Nearest-Neighbors . . . . . . . . . . . . . . . 479\\n13.5 Computational Considerations . . . . . . . . . . . . . . . 480\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 481\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 15}),\n",
       " Document(page_content='xx Contents\\n14 Unsupervised Learning 485\\n14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 485\\n14.2 Association Rules . . . . . . . . . . . . . . . . . . . . . . 487\\n14.2.1 Market Basket Analysis . . . . . . . . . . . . . . 488\\n14.2.2 The Apriori Algorithm . . . . . . . . . . . . . . 489\\n14.2.3 Example: Market Basket Analysis . . . . . . . . 492\\n14.2.4 Unsupervised as Supervised Learning . . . . . . 495\\n14.2.5 Generalized Association Rules . . . . . . . . . . 497\\n14.2.6 Choice of Supervised Learning Method . . . . . 499\\n14.2.7 Example: Market Basket Analysis (Continued) . 499\\n14.3 Cluster Analysis . . . . . . . . . . . . . . . . . . . . . . . 501\\n14.3.1 Proximity Matrices . . . . . . . . . . . . . . . . 503\\n14.3.2 Dissimilarities Based on Attributes . . . . . . . 503\\n14.3.3 Object Dissimilarity . . . . . . . . . . . . . . . . 505\\n14.3.4 Clustering Algorithms . . . . . . . . . . . . . . . 507\\n14.3.5 Combinatorial Algorithms . . . . . . . . . . . . 507\\n14.3.6K-means . . . . . . . . . . . . . . . . . . . . . . 509\\n14.3.7 Gaussian Mixtures as Soft K-means Clustering . 510\\n14.3.8 Example: Human Tumor Microarray Data . . . 512\\n14.3.9 Vector Quantization . . . . . . . . . . . . . . . . 514\\n14.3.10K-medoids . . . . . . . . . . . . . . . . . . . . . 515\\n14.3.11 Practical Issues . . . . . . . . . . . . . . . . . . 518\\n14.3.12 Hierarchical Clustering . . . . . . . . . . . . . . 520\\n14.4 Self-Organizing Maps . . . . . . . . . . . . . . . . . . . . 528\\n14.5 Principal Components, Curves and Surfaces . . . . . . . . 53 4\\n14.5.1 Principal Components . . . . . . . . . . . . . . . 534\\n14.5.2 Principal Curves and Surfaces . . . . . . . . . . 541\\n14.5.3 Spectral Clustering . . . . . . . . . . . . . . . . 544\\n14.5.4 Kernel Principal Components . . . . . . . . . . . 547\\n14.5.5 Sparse Principal Components . . . . . . . . . . . 550\\n14.6 Non-negative Matrix Factorization . . . . . . . . . . . . . 553\\n14.6.1 Archetypal Analysis . . . . . . . . . . . . . . . . 554\\n14.7 Independent Component Analysis\\nand Exploratory Projection Pursuit . . . . . . . . . . . . 557\\n14.7.1 Latent Variables and Factor Analysis . . . . . . 558\\n14.7.2 Independent Component Analysis . . . . . . . . 560\\n14.7.3 Exploratory Projection Pursuit . . . . . . . . . . 565\\n14.7.4 A Direct Approach to ICA . . . . . . . . . . . . 565\\n14.8 Multidimensional Scaling . . . . . . . . . . . . . . . . . . 570\\n14.9 Nonlinear Dimension Reduction\\nand Local Multidimensional Scaling . . . . . . . . . . . . 572\\n14.10 The Google PageRank Algorithm . . . . . . . . . . . . . 576\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 578\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 16}),\n",
       " Document(page_content='Contents xxi\\n15 Random Forests 587\\n15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 587\\n15.2 Deﬁnition of Random Forests . . . . . . . . . . . . . . . . 587\\n15.3 Details of Random Forests . . . . . . . . . . . . . . . . . 592\\n15.3.1 Out of Bag Samples . . . . . . . . . . . . . . . . 592\\n15.3.2 Variable Importance . . . . . . . . . . . . . . . . 593\\n15.3.3 Proximity Plots . . . . . . . . . . . . . . . . . . 595\\n15.3.4 Random Forests and Overﬁtting . . . . . . . . . 596\\n15.4 Analysis of Random Forests . . . . . . . . . . . . . . . . . 597\\n15.4.1 Variance and the De-Correlation Eﬀect . . . . . 597\\n15.4.2 Bias . . . . . . . . . . . . . . . . . . . . . . . . . 600\\n15.4.3 Adaptive Nearest Neighbors . . . . . . . . . . . 601\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 602\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603\\n16 Ensemble Learning 605\\n16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 605\\n16.2 Boosting and Regularization Paths . . . . . . . . . . . . . 607\\n16.2.1 Penalized Regression . . . . . . . . . . . . . . . 607\\n16.2.2 The “Bet on Sparsity” Principle . . . . . . . . . 610\\n16.2.3 Regularization Paths, Over-ﬁtting and Margins . 613\\n16.3 Learning Ensembles . . . . . . . . . . . . . . . . . . . . . 616\\n16.3.1 Learning a Good Ensemble . . . . . . . . . . . . 617\\n16.3.2 Rule Ensembles . . . . . . . . . . . . . . . . . . 622\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 623\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624\\n17 Undirected Graphical Models 625\\n17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 625\\n17.2 Markov Graphs and Their Properties . . . . . . . . . . . 627\\n17.3 Undirected Graphical Models for Continuous Variables . 630\\n17.3.1 Estimation of the Parameters\\nwhen the Graph Structure is Known . . . . . . . 631\\n17.3.2 Estimation of the Graph Structure . . . . . . . . 635\\n17.4 Undirected Graphical Models for Discrete Variables . . . 638\\n17.4.1 Estimation of the Parameters\\nwhen the Graph Structure is Known . . . . . . . 639\\n17.4.2 Hidden Nodes . . . . . . . . . . . . . . . . . . . 641\\n17.4.3 Estimation of the Graph Structure . . . . . . . . 642\\n17.4.4 Restricted Boltzmann Machines . . . . . . . . . 643\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645\\n18 High-Dimensional Problems: p≫N 649\\n18.1 When pis Much Bigger than N. . . . . . . . . . . . . . 649', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 17}),\n",
       " Document(page_content='xxii Contents\\n18.2 Diagonal Linear Discriminant Analysis\\nand Nearest Shrunken Centroids . . . . . . . . . . . . . . 651\\n18.3 Linear Classiﬁers with Quadratic Regularization . . . . . 654\\n18.3.1 Regularized Discriminant Analysis . . . . . . . . 656\\n18.3.2 Logistic Regression\\nwith Quadratic Regularization . . . . . . . . . . 657\\n18.3.3 The Support Vector Classiﬁer . . . . . . . . . . 657\\n18.3.4 Feature Selection . . . . . . . . . . . . . . . . . . 658\\n18.3.5 Computational Shortcuts When p≫N. . . . . 659\\n18.4 Linear Classiﬁers with L1Regularization . . . . . . . . . 661\\n18.4.1 Application of Lasso\\nto Protein Mass Spectroscopy . . . . . . . . . . 664\\n18.4.2 The Fused Lasso for Functional Data . . . . . . 666\\n18.5 Classiﬁcation When Features are Unavailable . . . . . . . 6 68\\n18.5.1 Example: String Kernels\\nand Protein Classiﬁcation . . . . . . . . . . . . . 668\\n18.5.2 Classiﬁcation and Other Models Using\\nInner-Product Kernels and Pairwise Distances . 670\\n18.5.3 Example: Abstracts Classiﬁcation . . . . . . . . 672\\n18.6 High-Dimensional Regression:\\nSupervised Principal Components . . . . . . . . . . . . . 674\\n18.6.1 Connection to Latent-Variable Modeling . . . . 678\\n18.6.2 Relationship with Partial Least Squares . . . . . 680\\n18.6.3 Pre-Conditioning for Feature Selection . . . . . 681\\n18.7 Feature Assessment and the Multiple-Testing Problem . . 683\\n18.7.1 The False Discovery Rate . . . . . . . . . . . . . 687\\n18.7.2 Asymmetric Cutpoints and the SAM Procedure 690\\n18.7.3 A Bayesian Interpretation of the FDR . . . . . . 692\\n18.8 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . 693\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 694\\nReferences 699\\nAuthor Index 729\\nIndex 737', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 18}),\n",
       " Document(page_content='This is page 1\\nPrinter: Opaque this\\n1\\nIntroduction\\nStatistical learning plays a key role in many areas of science, ﬁnance and\\nindustry. Here are some examples of learning problems:\\n•Predict whether a patient, hospitalized due to a heart attac k, will\\nhave a second heart attack. The prediction is to be based on de mo-\\ngraphic, diet and clinical measurements for that patient.\\n•Predict the price of a stock in 6 months from now, on the basis o f\\ncompany performance measures and economic data.\\n•Identify the numbers in a handwritten ZIP code, from a digiti zed\\nimage.\\n•Estimate the amount of glucose in the blood of a diabetic pers on,\\nfrom the infrared absorption spectrum of that person’s bloo d.\\n•Identify the risk factors for prostate cancer, based on clin ical and\\ndemographic variables.\\nThe science of learning plays a key role in the ﬁelds of statis tics, data\\nmining and artiﬁcial intelligence, intersecting with area s of engineering and\\nother disciplines.\\nThis book is about learning from data. In a typical scenario, we have\\nan outcome measurement, usually quantitative (such as a sto ck price) or\\ncategorical (such as heart attack/no heart attack), that we wish to predict\\nbased on a set of features (such as diet and clinical measurements). We\\nhave atraining set of data, in which we observe the outcome and feature', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 19}),\n",
       " Document(page_content='2 1. Introduction\\nTABLE 1.1. Average percentage of words or characters in an email messag e\\nequal to the indicated word or character. We have chosen the w ords and characters\\nshowing the largest diﬀerence between spamandemail.\\ngeorge you your hp free hpl ! our re edu remove\\nspam 0.00 2.26 1.38 0.02 0.52 0.01 0.51 0.51 0.13 0.01 0.28\\nemail 1.27 1.27 0.44 0.90 0.07 0.43 0.11 0.18 0.42 0.29 0.01\\nmeasurements for a set of objects (such as people). Using thi s data we build\\na prediction model, or learner, which will enable us to predict the outcome\\nfor new unseen objects. A good learner is one that accurately predicts such\\nan outcome.\\nThe examples above describe what is called the supervised learning prob-\\nlem. It is called “supervised” because of the presence of the outcome vari-\\nable to guide the learning process. In the unsupervised learning problem ,\\nwe observe only the features and have no measurements of the o utcome.\\nOur task is rather to describe how the data are organized or cl ustered. We\\ndevote most of this book to supervised learning; the unsuper vised problem\\nis less developed in the literature, and is the focus of Chapt er 14.\\nHere are some examples of real learning problems that are dis cussed in\\nthis book.\\nExample 1: Email Spam\\nThe data for this example consists of information from 4601 e mail mes-\\nsages, in a study to try to predict whether the email was junk e mail, or\\n“spam.” The objective was to design an automatic spam detect or that\\ncould ﬁlter out spam before clogging the users’ mailboxes. F or all 4601\\nemail messages, the true outcome (email type) emailorspamis available,\\nalong with the relative frequencies of 57 of the most commonl y occurring\\nwords and punctuation marks in the email message. This is a su pervised\\nlearning problem, with the outcome the class variable email/spam. It is also\\ncalled aclassiﬁcation problem.\\nTable 1.1 lists the words and characters showing the largest average\\ndiﬀerence between spamandemail.\\nOur learning method has to decide which features to use and ho w: for\\nexample, we might use a rule such as\\nif (%george<0.6) & (%you>1.5) then spam\\nelseemail.\\nAnother form of a rule might be:\\nif (0.2·%you−0.3·%george)>0 then spam\\nelseemail.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 20}),\n",
       " Document(page_content='1. Introduction 3\\nlpsa−1 1 2 3 4\\nooo ooo ooo oo o ooooo o oooo oooooooooo ooooooooooooooo oo ooooooooooooo oo oooo ooooooooooooooooooooooooooooo\\noo o ooo ooo oo o oooooooooooooooooooo ooooo o ooooooooo oo ooooooo oooooo oo oooo ooooooooooooooooooooooooooooo40 50 60 70 80\\noo o ooo ooo oo o oooooo o ooooooooooooo ooooooooooooooo o o o oooooo oo oooo oo oooo oooooooooooooo ooooooooooooooo\\noo o ooo o o o oo ooooo o oo o o o ooooo ooo oo o oo o oo oo oo o o ooo o o oo ooo o o o ooooo o o o ooo o o o o oooo oo oo ooooo o o oo o o oooooo0.0 0.4 0.8\\noo o ooo ooo oo o oooooooooooooooooooo oooooo o ooooooo o oo oooooooooooo o o o oooo oo o o oooo oo o ooo o oo o o ooo o oooooo\\noo o ooo ooo oo o ooooooooo o o o oooooo oo oooo o o oooooooo o oo ooooo ooooooo o oo oooo ooo ooooooo o ooooooo oooo ooooooo6.0 7.0 8.0 9.0\\noo o ooo ooo oo o oooooooooooooooooooo oooooooo o ooooo o oo ooooooooooooo oo oooo ooooo o ooooooooo o ooooooooooooo\\n0 1 2 3 4 5oo o ooo ooo oo o ooooooooooo o oo o o o ooo oooooooo o ooooo o oo o o o oooooooooo oo oo o o oo ooo o ooo o oooooooo oooo oooooo o−1 1 2 3 4ooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\noooo\\noo\\nooo\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\noo\\nooo\\noo\\nooo\\nooo\\nlcavol\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\noooo\\noo\\nooo\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\noo\\nooo\\noo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\noooo\\noo\\nooo\\nooooo\\noo\\nooo\\nooo\\noo\\no oo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooo\\noo oo\\noo\\nooo\\noo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\nooo o\\noo\\no oo\\nooooo\\noo\\no oo\\nooo\\noo\\nooo\\nooo\\no o\\nooo\\nooo\\no oooo\\noo oo\\noooo\\noo\\nooo\\noo\\nooo\\no oo\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\noooo\\noo\\nooo\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\no o\\nooo\\nooo\\no oooo\\noo o o\\noo o o\\noo\\nooo\\noo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\nooo o\\noo\\nooo\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\no o\\nooo\\nooo\\nooooo\\noo o o\\noooo\\noo\\nooo\\noo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\noooo\\noo\\no oo\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooo\\nooo o\\noo\\nooo\\noo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\noooo\\noo\\no oo\\nooooo\\noo\\no oo\\nooo\\noo\\nooo\\nooo\\no o\\noo o\\nooo\\nooooo\\noo oo\\noooo\\noo\\nooo\\noo\\nooo\\no oo\\noo\\noooooo o\\nooo\\nooo\\nooo\\noo\\nooooo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\noooo\\noooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\nooo\\nooo\\noo\\noooooo o\\nooo\\nooo\\nooo\\noo\\nooooo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\noooo\\noooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\nooo\\nooolweight\\noo\\noooooo o\\nooo\\nooo\\nooo\\noo\\nooooo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\noooo\\no ooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\noo o\\nooo\\noo\\noooooo o\\noo o\\nooo\\nooo\\noo\\no oooo\\nooo\\nooo\\no\\noooo\\no\\noo\\noooo o\\noooo\\no ooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\nooo\\nooo\\noo\\noooooo o\\nooo\\nooo\\nooo\\noo\\nooooo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\nooo o\\noooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooo oooo\\noo\\noo\\nooo\\nooo\\noo\\noooooo o\\nooo\\nooo\\nooo\\noo\\no oo oo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\nooo o\\noooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\nooo\\nooo\\noo\\noooooo o\\nooo\\nooo\\nooo\\noo\\nooooo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\nooo o\\noooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooo oooo\\noo\\noo\\nooo\\nooo\\n2.5 3.5 4.5oo\\noooooo o\\nooo\\nooo\\nooo\\noo\\nooo oo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\nooo o\\noooo\\noo\\noo\\nooo\\noo\\nooo\\noo oooo\\no\\noo\\noo\\noo o\\noo\\nooooooo\\noo\\noo\\nooo\\nooo40 50 60 70 80ooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\noooooooo oo o oo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\noo oooooo oo o oo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\noooooooo oo o oo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\noooo\\nage\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\no o oooooo oo o oo\\noo oo\\noo\\nooo\\no oo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\noooooooo oo o oo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\no o oooooo oo o oo\\noooo\\noo\\nooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='ooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\noooooooo oo o oo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\no o oooooo oo o oo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\noooooooo oo o oo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\noooooooo oo o oo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\noo o ooooo\\no ooo\\no o o oo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\nooo\\noo\\noooo\\no\\no oo o o ooo\\no o oo\\no o ooo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\no oo\\noo\\no o oo\\no\\no o o oo ooo\\no o oo\\no o o oo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\no oo\\noo\\no o oo\\no\\no o o o o ooo\\no ooo\\no o o oo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\nooo\\noo\\no o oo\\nolbph\\no o o o o ooo\\no o oo\\no o o oo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\no oo\\noo\\no o oo\\no\\no o o o o ooo\\no o oo\\no o o oo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\no oo\\noo\\no ooo\\no\\no o o o o ooo\\no o oo\\no o o oo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\no oo\\noo\\no o oo\\no\\n−1 0 1 2o o o o o ooo\\no o oo\\no o o oo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\no oo\\noo\\no o oo\\no0.0 0.4 0.8oo o ooo o o o oo o o o o o o o o o o o o o o o o o o o o o o o o o o oo\\no o o o o o oo\\no o o o o o o o o o o o o oo\\noo\\no o o o o oo\\noo oo o\\no oo\\no o oo\\no oo\\nooo o\\noooooo o\\no oo o o o o o o o o o o o oo o oo o o o o o o o o o o o o o o o o o o oo\\no o o o o ooo\\no o o o o o o o o o o o o oo\\noo\\no o oo o oo\\noo o o o\\no oo\\no o oo\\no oo\\noo o o\\nooo o o o o\\no o o oo o o o o o o o o o o o oo o o oo o oo o oo o o o o oo o o ooo\\noo oo o o oo\\no o o o o o o o oo o o o oo\\noo\\noo oo o oo\\noo o o o\\no oo\\no o oo\\no oo\\noo o o\\nooo o o oo\\no o o o o o o o o oo o o o o o o oo o o o o o o o oo o o o o o o o o o oo\\no o o o o o oo\\no o o o o o o o o oo o o oo\\noo\\no o o o ooo\\noo o o o\\no oo\\noo oo\\no oo\\nooo o\\noo oo o o o\\no o o o o o o oo o o oo o o o oo o oo oo o o o o o o o o o oo o oo oo\\no o o oo o oo\\no o oo o ooo oo o o o oo\\noo\\no o o oo oo\\noo o o o\\nooo\\no o oo\\no oo\\noo o o\\noo o o o o o\\nsvi\\no o o o o o o o o o o o o o o o o o o o o oo oo o o o o oo o o o o o ooo\\no o o o o o oo\\no o o o o o oo o o o o o oo\\noo\\no o o o o oo\\noo o o o\\no oo\\no o oo\\no oo\\noo o o\\noo o oo o o\\no o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o oo\\no oo o o o oo\\no o o o o o o o o o o o o oo\\noo\\no o o o o oo\\noo oo o\\no oo\\no o oo\\no oo\\noo o o\\noo o o o o o\\no o o o o o o o o o o o o o o o o o o o o o o oo o o o oo o o o o oo o oo\\no oo oo o oo\\no o o o o o o o o o o o o oo\\noo\\no o oo ooo\\noo oo o\\no oo\\no o oo\\no oo\\noo o o\\noo o o o oo\\noo o ooo o o o oo oo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no o oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\no o o oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\no oooo\\noo\\no oo o o o o o o o o oo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no o oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\no o o oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\no oooo\\noo\\no o o oo o o o o o o oo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no ooo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\no o o oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\no oooo\\noo\\no o o o o o o o o oo oo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no o oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\no o o oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\no oooo\\noo\\no o o o o o o oo o o oo\\noo\\nooo\\no ooo\\noo\\no oo\\nooo\\no\\no o oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\no o o oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\no oo\\noo\\noo\\nooo\\no\\no oooo\\noo\\no o o o o o o o o o o oo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no o oo\\noo\\noo\\no', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='oo\\nooo\\no ooo\\noo\\no oo\\nooo\\no\\no o oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\no o o oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\no oo\\noo\\noo\\nooo\\no\\no oooo\\noo\\no o o o o o o o o o o oo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no o oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\no o o oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\no oooo\\noo\\nlcp\\no o o o o o o o o o o oo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no o oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\no o o oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\no oooo\\noo\\n−1 0 1 2 3o o o o o o o o o o o oo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no o oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\no o o oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\no oooo\\noo6.0 7.0 8.0 9.0ooo\\nooo o o o oo oo o o\\noo\\no o o oo\\noo\\no oo o o\\no o o o o ooo\\no o oo\\no\\noo o oo\\no\\no oo\\noo o o o o\\noo o\\nooo\\no\\noo o o\\noo o o oo\\no o o o o o o o oo\\no o\\nooo o\\noooooo o\\no oo\\no o o o o o o o oo o o\\noo\\no o o oo\\noo\\no oo o o\\no o o o o ooo\\no o oo\\no\\noo ooo\\no\\no oo\\noo o o o o\\noo o\\nooo\\no\\noo oo\\noo o o oo\\no o o o o o o o oo\\no o\\noo o o\\nooo o o o o\\no oo\\noo o o o o o o oo o o\\noo\\no o o oo\\noo\\no ooo o\\no o o oo ooo\\no o oo\\no\\noo o oo\\no\\no oo\\noo o o oo\\noo o\\nooo\\no\\noo oo\\noo o o oo\\no o o o o o o o oo\\no o\\noo o o\\nooo o o oo\\no oo\\no o o o o o oo oo o o\\noo\\no o o oo\\noo\\no ooo o\\no o o o o ooo\\no o oo\\no\\noo o oo\\no\\no oo\\noo o o o o\\noo o\\nooo\\no\\noo o o\\noo o o oo\\no o o o o oo o oo\\no o\\nooo o\\noo oo o o o\\no oo\\no o o o oo o o oo o o\\noo\\no o ooo\\noo\\no oo o o\\no o o oo ooo\\no o oo\\no\\noo o oo\\no\\no oo\\nooo o oo\\noo o\\nooo\\no\\noo o o\\noo o ooo\\no o ooo o o o oo\\no o\\noo o o\\noo o o o o o\\no oo\\no o o o o o o o oo o o\\noo\\no o o oo\\noo\\no oo o o\\no o o o o ooo\\no ooo\\no\\noo o oo\\no\\no oo\\noo o o o o\\noo o\\nooo\\no\\noo o o\\noo oo oo\\no o o o oo o o oo\\no o\\noo o o\\noo o o o o o\\no oo\\no o o o o o o o oo o o\\noo\\no o o oo\\noo\\no oo o o\\no o o o o ooo\\no o oo\\no\\noo o oo\\no\\no oo\\noo oo o o\\noo o\\nooo\\no\\noo o o\\noo oo oo\\no o o o o o o o oo\\no o\\noo o o\\noo o oo o ogleason\\no oo\\no o o o o o o o oo o o\\noo\\no o o oo\\noo\\no oo o o\\no o o o o ooo\\no o oo\\no\\noo o oo\\no\\no oo\\noo o o o o\\noo o\\nooo\\no\\noo oo\\noo o o oo\\no o o o o o o o oo\\no o\\noo o o\\noo o o o oo\\n0 1 2 3 4 5ooo\\nooo o o o oo oo\\no ooo\\no o o oo\\noo\\no oo\\noo\\no o o o o ooo oo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\no oo\\no o o o o o o o oo\\no ooo\\no o o oo\\noo\\no oo\\noo\\no o o o o ooo oo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\n2.5 3.5 4.5o oo\\noo o o o o o o oo\\no ooo\\no o o oo\\noo\\no oo\\noo\\no o o oo ooo oo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\no oo\\no o o o o o oo oo\\no ooo\\no o o oo\\noo\\no oo\\noo\\no o o o o ooo oo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\n−1 0 1 2o oo\\no o o o oo o o oo\\no ooo\\no o ooo\\noo\\no oo\\noo\\no o o oo ooo oo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\no oo\\no o o o o o o o oo\\no ooo\\no o o oo\\noo\\no oo\\noo\\no o o o o ooo oo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\n−1 0 1 2 3o oo\\no o o o o o o o oo\\no ooo\\no o o oo\\noo\\no oo\\noo\\no o o o o ooo oo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\no oo\\no o o o o o o o oo\\no ooo\\no o o oo\\noo\\no oo\\noo\\no o o o o ooo oo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\n0 20 60 100\\n0 20 60 100pgg45\\nFIGURE 1.1. Scatterplot matrix of the prostate cancer data. The ﬁrst row s hows\\nthe response against each of the predictors in turn. Two of th e predictors, sviand\\ngleason, are categorical.\\nFor this problem not all errors are equal; we want to avoid ﬁlt ering out\\ngood email, while letting spam get through is not desirable b ut less serious\\nin its consequences. We discuss a number of diﬀerent methods for tackling\\nthis learning problem in the book.\\nExample 2: Prostate Cancer\\nThe data for this example, displayed in Figure 1.11, come from a study\\nby Stamey et al. (1989) that examined the correlation betwee n the level of\\n1There was an error in these data in the ﬁrst edition of this book. Subj ect 32 had', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='by Stamey et al. (1989) that examined the correlation betwee n the level of\\n1There was an error in these data in the ﬁrst edition of this book. Subj ect 32 had\\na value of 6.1 for lweight, which translates to a 449 gm prostate! The correct value is\\n44.9 gm. We are grateful to Prof. Stephen W. Link for alerting us to this error.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 21}),\n",
       " Document(page_content='4 1. Introduction\\nFIGURE 1.2. Examples of handwritten digits from U.S. postal envelopes.\\nprostate speciﬁc antigen (PSA) and a number of clinical meas ures, in 97\\nmen who were about to receive a radical prostatectomy.\\nThe goal is to predict the log of PSA ( lpsa) from a number of measure-\\nments including log cancer volume ( lcavol), log prostate weight lweight,\\nage, log of benign prostatic hyperplasia amount lbph, seminal vesicle in-\\nvasionsvi, log of capsular penetration lcp, Gleason score gleason, and\\npercent of Gleason scores 4 or 5 pgg45. Figure 1.1 is a scatterplot matrix\\nof the variables. Some correlations with lpsaare evident, but a good pre-\\ndictive model is diﬃcult to construct by eye.\\nThis is a supervised learning problem, known as a regression problem ,\\nbecause the outcome measurement is quantitative.\\nExample 3: Handwritten Digit Recognition\\nThe data from this example come from the handwritten ZIP code s on\\nenvelopes from U.S. postal mail. Each image is a segment from a ﬁve digit\\nZIP code, isolating a single digit. The images are 16 ×16 eight-bit grayscale\\nmaps, with each pixel ranging in intensity from 0 to 255. Some sample\\nimages are shown in Figure 1.2.\\nThe images have been normalized to have approximately the sa me size\\nand orientation. The task is to predict, from the 16 ×16 matrix of pixel\\nintensities, the identity of each image (0 ,1,...,9) quickly and accurately. If\\nit is accurate enough, the resulting algorithm would be used as part of an\\nautomatic sorting procedure for envelopes. This is a classi ﬁcation problem\\nfor which the error rate needs to be kept very low to avoid misd irection of', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 22}),\n",
       " Document(page_content='1. Introduction 5\\nmail. In order to achieve this low error rate, some objects ca n be assigned\\nto a “don’t know” category, and sorted instead by hand.\\nExample 4: DNA Expression Microarrays\\nDNA stands for deoxyribonucleic acid, and is the basic mater ial that makes\\nup human chromosomes. DNA microarrays measure the expressi on of a\\ngene in a cell by measuring the amount of mRNA (messenger ribo nucleic\\nacid) present for that gene. Microarrays are considered a br eakthrough\\ntechnology in biology, facilitating the quantitative stud y of thousands of\\ngenes simultaneously from a single sample of cells.\\nHere is how a DNA microarray works. The nucleotide sequences for a few\\nthousand genes are printed on a glass slide. A target sample a nd a reference\\nsample are labeled with red and green dyes, and each are hybri dized with\\nthe DNA on the slide. Through ﬂuoroscopy, the log (red/green ) intensities\\nof RNA hybridizing at each site is measured. The result is a fe w thousand\\nnumbers, typically ranging from say −6to6, measuring the expressionlevel\\nof each gene in the target relative to the reference sample. P ositive values\\nindicate higher expression in the target versus the referen ce, and vice versa\\nfor negative values.\\nA gene expression dataset collects together the expression values from a\\nseries of DNA microarray experiments, with each column repr esenting an\\nexperiment.Therearethereforeseveralthousandrowsrepr esentingindivid-\\nual genes, and tens of columns representing samples: in the p articular ex-\\nample of Figure 1.3 there are 6830 genes (rows) and 64 samples (columns),\\nalthough for clarity only a random sample of 100 rows are show n. The ﬁg-\\nure displays the data set as a heat map, ranging from green (ne gative) to\\nred (positive). The samples are 64 cancer tumors from diﬀere nt patients.\\nThe challenge here is to understand how the genes and samples are or-\\nganized. Typical questions include the following:\\n(a) which samples are most similar to each other, in terms of t heir expres-\\nsion proﬁles across genes?\\n(b) which genes are most similar to each other, in terms of the ir expression\\nproﬁles across samples?\\n(c) do certain genes show very high (or low) expression for ce rtain cancer\\nsamples?\\nWe could view this task as a regression problem, with two cate gorical\\npredictor variables—genes and samples—with the response var iable being\\nthe level of expression. However, it is probably more useful to view it as\\nunsupervised learning problem. For example, for question (a) above, we\\nthink of the samples as points in 6830–dimensional space, wh ich we want\\ntoclustertogether in some way.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 23}),\n",
       " Document(page_content='6 1. Introduction\\nSID42354SID31984SID301902SIDW128368SID375990SID360097SIDW325120ESTsChr.10SIDW365099SID377133SID381508SIDW308182SID380265SIDW321925ESTsChr.15SIDW362471SIDW417270SIDW298052SID381079SIDW428642TUPLE1TUP1ERLUMENSIDW416621SID43609ESTsSID52979SIDW357197SIDW366311ESTsSMALLNUCSIDW486740ESTsSID297905SID485148SID284853ESTsChr.15SID200394SIDW322806ESTsChr.2SIDW257915SID46536SIDW488221ESTsChr.5SID280066SIDW376394ESTsChr.15SIDW321854WASWiskottHYPOTHETICALSIDW376776SIDW205716SID239012SIDW203464HLACLASSISIDW510534SIDW279664SIDW201620SID297117SID377419SID114241ESTsCh31SIDW376928SIDW310141SIDW298203PTPRCSID289414SID127504ESTsChr.3SID305167SID488017SIDW296310ESTsChr.6SID47116MITOCHONDRIAL60ChrSIDW376586HomosapiensSIDW487261SIDW470459SID167117SIDW31489SID375812DNAPOLYMERSID377451ESTsChr.1MYBPROTOSID471915ESTsSIDW469884HumanmRNASIDW377402ESTsSID207172RASGTPASESID325394H.sapiensmRNAGNALSID73161SIDW380102SIDW299104BREAST\\nRENAL\\nMELANOMAMELANOMA\\nMCF7D-repro\\nCOLONCOLON\\nK562B-repro\\nCOLON\\nNSCLC\\nLEUKEMIA\\nRENAL\\nMELANOMA\\nBREAST\\nCNSCNS\\nRENAL\\nMCF7A-repro\\nNSCLC\\nK562A-repro\\nCOLON\\nCNS\\nNSCLCNSCLC\\nLEUKEMIA\\nCNS\\nOVARIAN\\nBREAST\\nLEUKEMIA\\nMELANOMAMELANOMA\\nOVARIANOVARIAN\\nNSCLC\\nRENAL\\nBREAST\\nMELANOMA\\nOVARIANOVARIAN\\nNSCLC\\nRENAL\\nBREAST\\nMELANOMA\\nLEUKEMIA\\nCOLON\\nBREAST\\nLEUKEMIA\\nCOLON\\nCNS\\nMELANOMA\\nNSCLC\\nPROSTATE\\nNSCLC\\nRENALRENAL\\nNSCLC\\nRENAL\\nLEUKEMIA\\nOVARIAN\\nPROSTATE\\nCOLON\\nBREAST\\nRENAL\\nUNKNOWNFIGURE 1.3. DNA microarray data: expression matrix of 6830genes (rows)\\nand64samples (columns), for the human tumor data. Only a random sample\\nof100rows are shown. The display is a heat map, ranging from bright g reen\\n(negative, under expressed) to bright red (positive, over e xpressed). Missing values\\nare gray. The rows and columns are displayed in a randomly chosen order.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 24}),\n",
       " Document(page_content='1. Introduction 7\\nWho Should Read this Book\\nThis book is designed for researchers and students in a broad variety of\\nﬁelds: statistics, artiﬁcial intelligence, engineering, ﬁnance and others. We\\nexpect that the reader will have had at least one elementary c ourse in\\nstatistics, covering basic topics including linear regres sion.\\nWe have not attempted to write a comprehensive catalog of lea rning\\nmethods, but rather to describe some of the most important te chniques.\\nEqually notable, we describe the underlying concepts and co nsiderations\\nby which a researcher can judge a learning method. We have tri ed to write\\nthis book in an intuitive fashion, emphasizing concepts rat her than math-\\nematical details.\\nAsstatisticians,ourexpositionwillnaturallyreﬂectour backgroundsand\\nareas of expertise. However in the past eight years we have be en attending\\nconferences in neural networks, data mining and machine lea rning, and our\\nthinking has been heavily inﬂuenced by these exciting ﬁelds . This inﬂuence\\nis evident in our current research, and in this book.\\nHow This Book is Organized\\nOur view is that one must understand simple methods before tr ying to\\ngrasp more complex ones. Hence, after giving an overview of t he supervis-\\ning learning problem in Chapter 2 , we discuss linear methods for regression\\nand classiﬁcation in Chapters 3 and4. InChapter 5 we describe splines,\\nwavelets and regularization/penalization methods for a si ngle predictor,\\nwhileChapter 6 covers kernel methods and local regression. Both of these\\nsets of methods are important building blocks for high-dime nsional learn-\\ning techniques. Model assessment and selection is the topic ofChapter 7 ,\\ncovering the concepts of bias and variance, overﬁtting and m ethods such as\\ncross-validation for choosing models. Chapter 8 discusses model inference\\nand averaging, including an overview of maximum likelihood , Bayesian in-\\nference and the bootstrap, the EM algorithm, Gibbs sampling and bagging,\\nA related procedure called boosting is the focus of Chapter 10 .\\nInChapters 9–13 we describe a series of structured methods for su-\\npervised learning, with Chapters 9 and 11 covering regression and Chap-\\nters 12 and 13 focusing on classiﬁcation. Chapter 14 describes methods for\\nunsupervised learning. Two recently proposed techniques, random forests\\nand ensemble learning, are discussed in Chapters 15 and 16 . We describe\\nundirected graphical models in Chapter 17 and ﬁnally we study high-\\ndimensional problems in Chapter 18 .\\nAt the end of each chapter we discuss computational considerations im-\\nportant for data mining applications, including how the com putations scale\\nwith the number of observations and predictors. Each chapte r ends with\\nBibliographic Notes giving background references for the material.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 25}),\n",
       " Document(page_content='8 1. Introduction\\nWe recommend that Chapters 1–4 be ﬁrst read in sequence. Chap ter 7\\nshould also be considered mandatory, as it covers central co ncepts that\\npertain to all learning methods. With this in mind, the rest o f the book\\ncan be read sequentially, or sampled, depending on the reade r’s interest.\\nThe symbol\\n indicates a technically diﬃcult section, one that can\\nbe skipped without interrupting the ﬂow of the discussion.\\nBook Website\\nThe website for this book is located at\\nhttp://www-stat.stanford.edu/ElemStatLearn\\nIt contains a number of resources, including many of the data sets used in\\nthis book.\\nNote for Instructors\\nWe have successively used the ﬁrst edition of this book as the basis for a\\ntwo-quartercourse,andwiththeadditionalmaterialsinth issecondedition,\\nitcouldevenbeusedforathree-quartersequence.Exercise sareprovidedat\\nthe end of each chapter. It is important for students to have a ccess to good\\nsoftware tools for these topics. We used the R and S-PLUS prog ramming\\nlanguages in our courses.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 26}),\n",
       " Document(page_content='This is page 9\\nPrinter: Opaque this\\n2\\nOverview of Supervised Learning\\n2.1 Introduction\\nThe ﬁrst three examples described in Chapter 1 have several c omponents\\nin common. For each there is a set of variables that might be de noted as\\ninputs, which are measured or preset. These have some inﬂuence on on e or\\nmoreoutputs. For each example the goal is to use the inputs to predict the\\nvalues of the outputs. This exercise is called supervised learning .\\nWe have used the more modern language of machine learning. In the\\nstatistical literature the inputs are often called the predictors , a term we\\nwill use interchangeably with inputs, and more classically theindependent\\nvariables .Inthepatternrecognitionliteraturetheterm featuresispreferred,\\nwhich we use as well. The outputs are called the responses , or classically\\nthedependent variables .\\n2.2 Variable Types and Terminology\\nThe outputs vary in nature among the examples. In the glucose prediction\\nexample, the output is a quantitative measurement, where some measure-\\nments are bigger than others, and measurements close in valu e are close\\nin nature. In the famous Iris discrimination example due to R . A. Fisher,\\nthe output is qualitative (species of Iris) and assumes values in a ﬁnite set\\nG={Virginica ,SetosaandVersicolor}. In the handwritten digit example\\nthe output is one of 10 diﬀerent digit classes:G={0,1,...,9}. In both of', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 27}),\n",
       " Document(page_content='10 2. Overview of Supervised Learning\\nthese there is no explicit ordering in the classes, and in fac t often descrip-\\ntive labels rather than numbers are used to denote the classe s. Qualitative\\nvariables are also referred to as categorical ordiscretevariables as well as\\nfactors.\\nFor both types of outputs it makes sense to think of using the i nputs to\\npredict the output. Given some speciﬁc atmospheric measure ments today\\nand yesterday, we want to predict the ozone level tomorrow. G iven the\\ngrayscale values for the pixels of the digitized image of the handwritten\\ndigit, we want to predict its class label.\\nThis distinction in output type has led to a naming conventio n for the\\nprediction tasks: regression when we predict quantitative outputs, and clas-\\nsiﬁcation when we predict qualitative outputs. We will see that these t wo\\ntasks have a lot in common, and in particular both can be viewe d as a task\\nin function approximation.\\nInputs also vary in measurement type; we can have some of each of qual-\\nitative and quantitative input variables. These have also l ed to distinctions\\nin the types of methods that are used for prediction: some met hods are\\ndeﬁned most naturally for quantitative inputs, some most na turally for\\nqualitative and some for both.\\nA third variable type is ordered categorical , such as small, medium and\\nlarge, where there is an ordering between the values, but no metric notion\\nis appropriate (the diﬀerence between medium and small need not be the\\nsame as that between large and medium). These are discussed f urther in\\nChapter 4.\\nQualitative variables are typically represented numerica lly by codes. The\\neasiest case is when there are only two classes or categories , such as “suc-\\ncess” or “failure,” “survived” or “died.” These are often re presented by a\\nsingle binary digit or bit as 0 or 1, or else by −1 and 1. For reasons that will\\nbecome apparent, such numeric codes are sometimes referred to astargets.\\nWhen there are more than two categories, several alternativ es are available.\\nThe most useful and commonly used coding is via dummy variables . Here a\\nK-level qualitative variable is represented by a vector of Kbinary variables\\nor bits, only one of which is “on” at a time. Although more comp act coding\\nschemes are possible, dummy variables are symmetric in the l evels of the\\nfactor.\\nWe will typically denote an input variable by the symbol X. IfXis\\na vector, its components can be accessed by subscripts Xj. Quantitative\\noutputs will be denoted by Y, and qualitative outputs by G(for group).\\nWe use uppercase letters such as X,YorGwhen referring to the generic\\naspects of a variable. Observed values are written in lowerc ase; hence the\\nith observed value of Xis written as xi(wherexiis again a scalar or\\nvector). Matrices are represented by bold uppercase letter s; for example, a\\nset ofNinputp-vectorsxi, i= 1,...,Nwould be represented by the N×p\\nmatrixX. In general, vectors will not be bold, except when they have N\\ncomponents; this convention distinguishes a p-vector of inputs xifor the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 28}),\n",
       " Document(page_content='2.3 Least Squares and Nearest Neighbors 11\\nith observation from the N-vectorxjconsisting of all the observations on\\nvariableXj. Since all vectors are assumed to be column vectors, the ith\\nrow ofXisxT\\ni, the vector transpose of xi.\\nFor the moment we can loosely state the learning task as follo ws: given\\nthe value of an input vector X, make a good prediction of the output Y,\\ndenoted by ˆY(pronounced “y-hat”). If Ytakes values in IR then so should\\nˆY; likewise for categorical outputs, ˆGshould take values in the same set G\\nassociated with G.\\nFor a two-class G, one approach is to denote the binary coded target\\nasY, and then treat it as a quantitative output. The predictions ˆYwill\\ntypically lie in [0 ,1], and we can assign to ˆGthe class label according to\\nwhether ˆy >0.5. This approach generalizes to K-level qualitative outputs\\nas well.\\nWe need data to construct prediction rules, often a lot of it. We thus\\nsuppose we have available a set of measurements ( xi,yi) or (xi,gi), i=\\n1,...,N,knownasthe training data ,withwhichtoconstructourprediction\\nrule.\\n2.3 Two Simple Approaches to Prediction: Least\\nSquares and Nearest Neighbors\\nIn this section we develop two simple but powerful predictio n methods: the\\nlinear model ﬁt by least squares and the k-nearest-neighbor prediction rule.\\nThelinearmodelmakeshugeassumptionsaboutstructureand yieldsstable\\nbut possibly inaccurate predictions. The method of k-nearest neighbors\\nmakes very mild structural assumptions: its predictions ar e often accurate\\nbut can be unstable.\\n2.3.1 Linear Models and Least Squares\\nThe linear model has been a mainstay of statistics for the pas t 30 years\\nand remains one of our most important tools. Given a vector of inputs\\nXT= (X1,X2,...,X p), we predict the output Yvia the model\\nˆY=ˆβ0+p∑\\nj=1Xjˆβj. (2.1)\\nThe term ˆβ0is the intercept, also known as the biasin machine learning.\\nOften it is convenient to include the constant variable 1 in X, include ˆβ0in\\nthe vector of coeﬃcients ˆβ, and then write the linear model in vector form\\nas an inner product\\nˆY=XTˆβ, (2.2)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 29}),\n",
       " Document(page_content='12 2. Overview of Supervised Learning\\nwhereXTdenotes vector or matrix transpose ( Xbeing a column vector).\\nHere we are modeling a single output, so ˆYis a scalar; in general ˆYcan be\\naK–vector, in which case βwould be a p×Kmatrix of coeﬃcients. In the\\n(p+ 1)-dimensional input–output space, ( X,ˆY) represents a hyperplane.\\nIf the constant is included in X, then the hyperplane includes the origin\\nand is a subspace; if not, it is an aﬃne set cutting the Y-axis at the point\\n(0,ˆβ0). From now on we assume that the intercept is included in ˆβ.\\nViewed as a function over the p-dimensional input space, f(X) =XTβ\\nis linear, and the gradient f′(X) =βis a vector in input space that points\\nin the steepest uphill direction.\\nHow do we ﬁt the linear model to a set of training data? There ar e\\nmany diﬀerent methods, but by far the most popular is the meth od of\\nleast squares . In this approach, we pick the coeﬃcients βto minimize the\\nresidual sum of squares\\nRSS(β) =N∑\\ni=1(yi−xT\\niβ)2. (2.3)\\nRSS(β) is a quadratic function of the parameters, and hence its min imum\\nalways exists, but may not be unique. The solution is easiest to characterize\\nin matrix notation. We can write\\nRSS(β) = (y−Xβ)T(y−Xβ), (2.4)\\nwhereXis anN×pmatrix with each row an input vector, and yis an\\nN-vector of the outputs in the training set. Diﬀerentiating w .r.t.βwe get\\nthenormal equations\\nXT(y−Xβ) = 0. (2.5)\\nIfXTXis nonsingular, then the unique solution is given by\\nˆβ= (XTX)−1XTy, (2.6)\\nand the ﬁtted value at the ith inputxiis ˆyi= ˆy(xi) =xT\\niˆβ. At an arbi-\\ntrary input x0the prediction is ˆ y(x0) =xT\\n0ˆβ. The entire ﬁtted surface is\\ncharacterized by the pparameters ˆβ. Intuitively, it seems that we do not\\nneed a very large data set to ﬁt such a model.\\nLet’s look at an example of the linear model in a classiﬁcatio n context.\\nFigure 2.1 shows a scatterplot of training data on a pair of in putsX1and\\nX2. The data are simulated, and for the present the simulation m odel is\\nnot important. The output class variable Ghas the values BLUEorORANGE,\\nand is represented as such in the scatterplot. There are 100 p oints in each\\nof the two classes. The linear regression model was ﬁt to thes e data, with\\nthe response Ycoded as 0 for BLUEand 1 for ORANGE. The ﬁtted values ˆY\\nare converted to a ﬁtted class variable ˆGaccording to the rule\\nˆG={\\nORANGE ifˆY >0.5,\\nBLUE ifˆY≤0.5.(2.7)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 30}),\n",
       " Document(page_content='2.3 Least Squares and Nearest Neighbors 13\\nLinear Regression of 0/1 Response', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='.. . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . .. .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nFIGURE 2.1. A classiﬁcation example in two dimensions. The classes are code d\\nas a binary variable ( BLUE= 0,ORANGE= 1), and then ﬁt by linear regression.\\nThe line is the decision boundary deﬁned by xTˆβ= 0.5. The orange shaded region\\ndenotes that part of input space classiﬁed as ORANGE, while the blue region is\\nclassiﬁed as BLUE.\\nThe set of points in IR2classiﬁed as ORANGEcorresponds to{x:xTˆβ >0.5},\\nindicated in Figure 2.1, and the two predicted classes are se parated by the\\ndecision boundary {x:xTˆβ= 0.5}, which is linear in this case. We see\\nthat for these data there are several misclassiﬁcations on b oth sides of the\\ndecisionboundary.Perhapsourlinearmodelistoorigid—or aresucherrors\\nunavoidable? Remember that these are errors on the training data itself,\\nand we have not said where the constructed data came from. Con sider the\\ntwo possible scenarios:\\nScenario 1: The training data in each class were generated from bivariat e\\nGaussian distributions with uncorrelated components and d iﬀerent\\nmeans.\\nScenario 2: Thetrainingdataineachclasscamefromamixtureof10low-\\nvariance Gaussian distributions, with individual means th emselves\\ndistributed as Gaussian.\\nA mixture of Gaussians is best described in terms of the gener ative\\nmodel. One ﬁrst generates a discrete variable that determin es which of', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 31}),\n",
       " Document(page_content='14 2. Overview of Supervised Learning\\nthe component Gaussians to use, and then generates an observ ation from\\nthe chosen density. In the case of one Gaussian per class, we w ill see in\\nChapter 4 that a linear decision boundary is the best one can d o, and that\\nour estimate is almost optimal. The region of overlap is inev itable, and\\nfuture data to be predicted will be plagued by this overlap as well.\\nIn the case of mixtures of tightly clustered Gaussians the st ory is dif-\\nferent. A linear decision boundary is unlikely to be optimal , and in fact is\\nnot. The optimal decision boundary is nonlinear and disjoin t, and as such\\nwill be much more diﬃcult to obtain.\\nWe now look at another classiﬁcation and regression procedu re that is\\nin some sense at the opposite end of the spectrum to the linear model, and\\nfar better suited to the second scenario.\\n2.3.2 Nearest-Neighbor Methods\\nNearest-neighbor methods use those observations in the tra ining setTclos-\\nest in input space to xto form ˆY. Speciﬁcally, the k-nearest neighbor ﬁt\\nforˆYis deﬁned as follows:\\nˆY(x) =1\\nk∑\\nxi∈Nk(x)yi, (2.8)\\nwhereNk(x) is the neighborhood of xdeﬁned by the kclosest points xiin\\nthe training sample. Closeness implies a metric, which for t he moment we\\nassume is Euclidean distance. So, in words, we ﬁnd the kobservations with\\nxiclosest toxin input space, and average their responses.\\nIn Figure 2.2 we use the same training data as in Figure 2.1, an d use\\n15-nearest-neighbor averaging of the binary coded respons e as the method\\nof ﬁtting. Thus ˆYis the proportion of ORANGE’s in the neighborhood, and\\nso assigning class ORANGEtoˆGifˆY >0.5 amounts to a majority vote in\\nthe neighborhood. The colored regions indicate all those po ints in input\\nspace classiﬁed as BLUEorORANGEby such a rule, in this case found by\\nevaluating the procedure on a ﬁne grid in input space. We see t hat the\\ndecision boundaries that separate the BLUEfrom the ORANGEregions are far\\nmore irregular, and respond to local clusters where one clas s dominates.\\nFigure 2.3 shows the results for 1-nearest-neighbor classi ﬁcation: ˆYis\\nassigned the value yℓof the closest point xℓtoxin the training data. In\\nthis case the regions of classiﬁcation can be computed relat ively easily, and\\ncorrespond to a Voronoi tessellation of the training data. Each point xi\\nhas an associated tile bounding the region for which it is the closest input\\npoint. For all points xin the tile, ˆG(x) =gi. The decision boundary is even\\nmore irregular than before.\\nThe method of k-nearest-neighbor averaging is deﬁned in exactly the\\nsame way for regression of a quantitative output Y, althoughk= 1 would\\nbe an unlikely choice.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 32}),\n",
       " Document(page_content='2.3 Least Squares and Nearest Neighbors 15\\n15-Nearest Neighbor Classifier', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='.. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='... .. .. .. .. . .. . .. . .. . . . .. . . . . .. . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='. . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nFIGURE 2.2. The same classiﬁcation example in two dimensions as in Fig-\\nure 2.1. The classes are coded as a binary variable (BLUE= 0,ORANGE= 1)and\\nthen ﬁt by 15-nearest-neighbor averaging as in (2.8). The predicted clas s is hence\\nchosen by majority vote amongst the 15-nearest neighbors.\\nIn Figure 2.2 we see that far fewer training observations are misclassiﬁed\\nthan in Figure 2.1. This should not give us too much comfort, t hough, since\\nin Figure 2.3 noneof the training data are misclassiﬁed. A little thought\\nsuggests that for k-nearest-neighbor ﬁts, the error on the training data\\nshould be approximately an increasing function of k, and will always be 0\\nfork= 1. An independent test set would give us a more satisfactory means\\nfor comparing the diﬀerent methods.\\nIt appears that k-nearest-neighbor ﬁts have a single parameter, the num-\\nber of neighbors k, compared to the pparameters in least-squares ﬁts. Al-\\nthough this is the case, we will see that the eﬀective number of parameters\\nofk-nearest neighbors is N/kand is generally bigger than p, and decreases\\nwith increasing k. To get an idea of why, note that if the neighborhoods\\nwere nonoverlapping, there would be N/kneighborhoods and we would ﬁt\\none parameter (a mean) in each neighborhood.\\nIt is also clear that we cannot use sum-of-squared errors on t he training\\nset as a criterion for picking k, since we would always pick k= 1! It would\\nseem thatk-nearest-neighbor methods would be more appropriate for th e\\nmixture Scenario 2 described above, while for Gaussian data the decision\\nboundaries of k-nearest neighbors would be unnecessarily noisy.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 33}),\n",
       " Document(page_content='16 2. Overview of Supervised Learning\\n1−Nearest Neighbor Classifier\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\nooooo\\nooo\\no\\no\\nooo\\noooooo\\noo\\no\\noo\\nooo\\no\\nFIGURE 2.3. The same classiﬁcation example in two dimensions as in Fig-\\nure 2.1. The classes are coded as a binary variable (BLUE= 0,ORANGE= 1), and\\nthen predicted by 1-nearest-neighbor classiﬁcation.\\n2.3.3 From Least Squares to Nearest Neighbors\\nThe linear decision boundary from least squares is very smoo th, and ap-\\nparently stable to ﬁt. It does appear to rely heavily on the as sumption\\nthat a linear decision boundary is appropriate. In language we will develop\\nshortly, it has low variance and potentially high bias.\\nOn the other hand, the k-nearest-neighbor procedures do not appear to\\nrelyonanystringentassumptionsabouttheunderlyingdata ,andcanadapt\\nto any situation. However, any particular subregion of the d ecision bound-\\nary depends on a handful of input points and their particular positions,\\nand is thus wiggly and unstable—high variance and low bias.\\nEach method has its own situations for which it works best; in particular\\nlinear regression is more appropriate for Scenario 1 above, while nearest\\nneighbors are more suitable for Scenario 2. The time has come to expose\\nthe oracle! The data in fact were simulated from a model somew here be-\\ntween the two, but closer to Scenario 2. First we generated 10 meansmk\\nfrom a bivariate Gaussian distribution N((1,0)T,I) and labeled this class\\nBLUE. Similarly, 10 more were drawn from N((0,1)T,I) and labeled class\\nORANGE. Then for each class we generated 100 observations as follow s: for\\neach observation, we picked an mkat random with probability 1 /10, and', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 34}),\n",
       " Document(page_content='2.3 Least Squares and Nearest Neighbors 17\\nDegrees of Freedom − N/kTest Error\\n0.10 0.15 0.20 0.25 0.30\\n  2   3   5   8  12  18  29  67 200151 101  69  45  31  21  11   7   5   3   1\\nTrain\\nTest\\nBayesk −  Number of Nearest Neighbors\\nLinear\\nFIGURE 2.4. Misclassiﬁcation curves for the simulation example used in Fig -\\nures 2.1, 2.2 and 2.3. A single training sample of size 200was used, and a test\\nsample of size 10,000. The orange curves are test and the blue are training er-\\nror fork-nearest-neighbor classiﬁcation. The results for linear regr ession are the\\nbigger orange and blue squares at three degrees of freedom. Th e purple line is the\\noptimal Bayes error rate.\\nthen generated a N(mk,I/5), thus leading to a mixture of Gaussian clus-\\nters for each class. Figure 2.4 shows the results of classify ing 10,000 new\\nobservations generated from the model. We compare the resul ts for least\\nsquares and those for k-nearest neighbors for a range of values of k.\\nA large subset of the most popular techniques in use today are variants of\\nthese two simple procedures. In fact 1-nearest-neighbor, t he simplest of all,\\ncaptures a large percentage of the market for low-dimension al problems.\\nThe following list describes some ways in which these simple procedures\\nhave been enhanced:\\n•Kernel methods use weights that decrease smoothly to zero wi th dis-\\ntancefromthetargetpoint,ratherthantheeﬀective0 /1weightsused\\nbyk-nearest neighbors.\\n•In high-dimensional spaces the distance kernels are modiﬁe d to em-\\nphasize some variable more than others.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 35}),\n",
       " Document(page_content='18 2. Overview of Supervised Learning\\n•Local regression ﬁts linear models by locally weighted leas t squares,\\nrather than ﬁtting constants locally.\\n•Linear models ﬁt to a basis expansion of the original inputs a llow\\narbitrarily complex models.\\n•Projection pursuit and neural network models consist of sum s of non-\\nlinearly transformed linear models.\\n2.4 Statistical Decision Theory\\nIn this section we develop a small amount of theory that provi des a frame-\\nwork for developing models such as those discussed informal ly so far. We\\nﬁrst consider the case of a quantitative output, and place ou rselves in the\\nworld of random variables and probability spaces. Let X∈IRpdenote a\\nreal valued random input vector, and Y∈IR a real valued random out-\\nput variable, with joint distribution Pr( X,Y). We seek a function f(X)\\nfor predicting Ygiven values of the input X. This theory requires a loss\\nfunctionL(Y,f(X)) for penalizing errors in prediction, and by far the most\\ncommon and convenient is squared error loss :L(Y,f(X)) = (Y−f(X))2.\\nThis leads us to a criterion for choosing f,\\nEPE(f) = E(Y−f(X))2(2.9)\\n=∫\\n[y−f(x)]2Pr(dx,dy), (2.10)\\nthe expected (squared) prediction error . By conditioning1onX, we can\\nwrite EPE as\\nEPE(f) = EXEY|X(\\n[Y−f(X)]2|X)\\n(2.11)\\nand we see that it suﬃces to minimize EPE pointwise:\\nf(x) = argmincEY|X(\\n[Y−c]2|X=x)\\n. (2.12)\\nThe solution is\\nf(x) = E(Y|X=x), (2.13)\\nthe conditional expectation, also known as the regression function. Thus\\nthe best prediction of Yat any point X=xis the conditional mean, when\\nbest is measured by average squared error.\\nThe nearest-neighbor methods attempt to directly implemen t this recipe\\nusing the training data. At each point x, we might ask for the average of all\\n1Conditioning here amounts to factoring the joint density Pr( X,Y) = Pr(Y|X)Pr(X)\\nwhere Pr( Y|X) = Pr(Y,X)/Pr(X), and splitting up the bivariate integral accordingly.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 36}),\n",
       " Document(page_content='2.4 Statistical Decision Theory 19\\nthoseyis with input xi=x. Since there is typically at most one observation\\nat any point x, we settle for\\nˆf(x) = Ave(yi|xi∈Nk(x)), (2.14)\\nwhere “Ave” denotes average, and Nk(x) is the neighborhood containing\\nthekpoints inTclosest tox. Two approximations are happening here:\\n•expectation is approximated by averaging over sample data;\\n•conditioning at a point is relaxed to conditioning on some re gion\\n“close” to the target point.\\nFor large training sample size N, the points in the neighborhood are likely\\nto be close to x, and askgets large the average will get more stable.\\nIn fact, under mild regularity conditions on the joint proba bility distri-\\nbution Pr(X,Y), one can show that as N,k→∞such thatk/N→0,\\nˆf(x)→E(Y|X=x). In light of this, why look further, since it seems\\nwe have a universal approximator? We often do not have very la rge sam-\\nples. If the linear or some more structured model is appropri ate, then we\\ncan usually get a more stable estimate than k-nearest neighbors, although\\nsuch knowledge has to be learned from the data as well. There a re other\\nproblems though, sometimes disastrous. In Section 2.5 we se e that as the\\ndimensionpgets large, so does the metric size of the k-nearest neighbor-\\nhood. So settling for nearest neighborhood as a surrogate fo r conditioning\\nwill fail us miserably. The convergence above still holds, b ut therateof\\nconvergence decreases as the dimension increases.\\nHow does linear regressionﬁt intothis framework? Thesimpl est explana-\\ntion is that one assumes that the regression function f(x) is approximately\\nlinear in its arguments:\\nf(x)≈xTβ. (2.15)\\nThisisamodel-basedapproach—wespecifyamodelfortheregr essionfunc-\\ntion. Plugging this linear model for f(x) into EPE (2.9) and diﬀerentiating\\nwe can solve for βtheoretically:\\nβ= [E(XXT)]−1E(XY). (2.16)\\nNote we have notconditioned on X; rather we have used our knowledge\\nof the functional relationship to poolover values of X. The least squares\\nsolution (2.6) amounts to replacing the expectation in (2.1 6) by averages\\nover the training data.\\nSo bothk-nearest neighbors and least squares end up approximating\\nconditional expectations by averages. But they diﬀer drama tically in terms\\nof model assumptions:\\n•Least squares assumes f(x) is well approximated by a globally linear\\nfunction.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 37}),\n",
       " Document(page_content='20 2. Overview of Supervised Learning\\n•k-nearest neighbors assumes f(x) is well approximated by a locally\\nconstant function.\\nAlthough the latter seems more palatable, we have already se en that we\\nmay pay a price for this ﬂexibility.\\nMany of the more modern techniques described in this book are model\\nbased, although far more ﬂexible than the rigid linear model . For example,\\nadditive models assume that\\nf(X) =p∑\\nj=1fj(Xj). (2.17)\\nThisretainstheadditivity ofthelinearmodel,buteachcoo rdinatefunction\\nfjis arbitrary. It turns out that the optimal estimate for the a dditive model\\nuses techniques such as k-nearest neighbors to approximate univariate con-\\nditional expectations simultaneously for each of the coordinate functions.\\nThus the problems of estimating a conditional expectation i n high dimen-\\nsionsaresweptawayinthiscasebyimposingsome(oftenunre alistic)model\\nassumptions, in this case additivity.\\nAre we happy with the criterion (2.11)? What happens if we rep lace the\\nL2loss function with the L1: E|Y−f(X)|? The solution in this case is the\\nconditional median,\\nˆf(x) = median( Y|X=x), (2.18)\\nwhich is a diﬀerent measure of location, and its estimates ar e more robust\\nthan those for the conditional mean. L1criteria have discontinuities in\\ntheir derivatives, which have hindered their widespread us e. Other more\\nresistant loss functions will be mentioned in later chapter s, but squared\\nerror is analytically convenient and the most popular.\\nWhat do we do when the output is a categorical variable G? The same\\nparadigm works here, except we need a diﬀerent loss function for penalizing\\nprediction errors. An estimate ˆGwill assume values in G, the set of possible\\nclasses. Our loss function can be represented by a K×KmatrixL, where\\nK= card(G).Lwill be zero on the diagonal and nonnegative elsewhere,\\nwhereL(k,ℓ) is the price paid for classifying an observation belonging to\\nclassGkasGℓ. Most often we use the zero–one loss function, where all\\nmisclassiﬁcations are charged a single unit. The expected p rediction error\\nis\\nEPE = E[L(G,ˆG(X))], (2.19)\\nwhere again the expectation is taken with respect to the join t distribution\\nPr(G,X). Again we condition, and can write EPE as\\nEPE = E XK∑\\nk=1L[Gk,ˆG(X)]Pr(Gk|X) (2.20)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 38}),\n",
       " Document(page_content='2.4 Statistical Decision Theory 21\\nBayes Optimal Classifier', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='... .. . . .. . . . .. . . . . .. . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nFIGURE 2.5. The optimal Bayes decision boundary for the simulation examp le\\nof Figures 2.1, 2.2 and 2.3. Since the generating density is k nown for each class,\\nthis boundary can be calculated exactly (Exercise 2.2).\\nand again it suﬃces to minimize EPE pointwise:\\nˆG(x) = argming∈GK∑\\nk=1L(Gk,g)Pr(Gk|X=x). (2.21)\\nWith the 0–1 loss function this simpliﬁes to\\nˆG(x) = argming∈G[1−Pr(g|X=x)] (2.22)\\nor simply\\nˆG(x) =Gkif Pr(Gk|X=x) = max\\ng∈GPr(g|X=x).(2.23)\\nThis reasonable solution is known as the Bayes classiﬁer , and says that\\nwe classify to the most probable class, using the conditiona l (discrete) dis-\\ntribution Pr( G|X). Figure 2.5 shows the Bayes-optimal decision boundary\\nfor our simulation example. The error rate of the Bayes class iﬁer is called\\ntheBayes rate .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 39}),\n",
       " Document(page_content='22 2. Overview of Supervised Learning\\nAgain we see that the k-nearest neighbor classiﬁer directly approximates\\nthis solution—a majority vote in a nearest neighborhood amou nts to ex-\\nactly this, except that conditional probability at a point i s relaxed to con-\\nditional probability within a neighborhood of a point, and p robabilities are\\nestimated by training-sample proportions.\\nSuppose for a two-class problem we had taken the dummy-varia ble ap-\\nproach and coded Gvia a binary Y, followed by squared error loss estima-\\ntion. Then ˆf(X) = E(Y|X) = Pr(G=G1|X) ifG1corresponded to Y= 1.\\nLikewise for a K-class problem, E( Yk|X) = Pr(G=Gk|X). This shows\\nthat our dummy-variable regression procedure, followed by classiﬁcation to\\nthe largest ﬁtted value, is another way of representing the B ayes classiﬁer.\\nAlthough this theory is exact, in practice problems can occu r, depending\\non the regression model used. For example, when linear regre ssion is used,\\nˆf(X) need not be positive, and we might be suspicious about using it as\\nan estimate of a probability. We will discuss a variety of app roaches to\\nmodeling Pr( G|X) in Chapter 4.\\n2.5 Local Methods in High Dimensions\\nWe have examined two learning techniques for prediction so f ar: the stable\\nbut biased linear model and the less stable but apparently le ss biased class\\nofk-nearest-neighbor estimates. It would seem that with a reas onably large\\nset of training data, we could always approximate the theore tically optimal\\nconditional expectation by k-nearest-neighbor averaging, since we should\\nbe able to ﬁnd a fairly large neighborhood of observations cl ose to any x\\nand average them. This approach and our intuition breaks dow n in high\\ndimensions, and the phenomenon is commonly referred to as th ecurse\\nof dimensionality (Bellman, 1961). There are many manifestations of this\\nproblem, and we will examine a few here.\\nConsiderthenearest-neighborprocedureforinputsunifor mlydistributed\\nin ap-dimensional unit hypercube, as in Figure 2.6. Suppose we se nd out a\\nhypercubical neighborhood about a target point to capture a fractionrof\\nthe observations. Since this corresponds to a fraction rof the unit volume,\\ntheexpectededgelengthwillbe ep(r) =r1/p.Intendimensions e10(0.01) =\\n0.63 ande10(0.1) = 0.80, while the entire range for each input is only 1 .0.\\nSo to capture 1% or 10% of the data to form a local average, we mu st cover\\n63% or 80% of the range of each input variable. Such neighborh oods are no\\nlonger “local.” Reducing rdramatically does not help much either, since\\nthe fewer observations we average, the higher is the varianc e of our ﬁt.\\nAnother consequence of the sparse sampling in high dimensio ns is that\\nall sample points are close to an edge of the sample. Consider Ndata points\\nuniformly distributed in a p-dimensional unit ball centered at the origin.\\nSuppose we consider a nearest-neighbor estimate at the orig in. The median', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 40}),\n",
       " Document(page_content='2.5 Local Methods in High Dimensions 23\\n1\\n10Unit Cube\\nFraction of VolumeDistance\\n0.0 0.2 0.4 0.60.0 0.2 0.4 0.6 0.8 1.0p=1p=2p=3p=10\\nNeighborhood\\nFIGURE 2.6. The curse of dimensionality is well illustrated by a subcubica l\\nneighborhood for uniform data in a unit cube. The ﬁgure on the right shows the\\nside-length of the subcube needed to capture a fraction rof the volume of the data,\\nfor diﬀerent dimensions p. In ten dimensions we need to cover 80%of the range\\nof each coordinate to capture 10%of the data.\\ndistance from the origin to the closest data point is given by the expression\\nd(p,N) =(\\n1−1\\n21/N)1/p\\n(2.24)\\n(Exercise 2.3). A more complicated expression exists for th e mean distance\\nto the closest point. For N= 500,p= 10 ,d(p,N)≈0.52, more than\\nhalfwaytotheboundary.Hencemostdatapointsarecloserto theboundary\\nof the sample space than to any other data point. The reason th at this\\npresents a problem is that prediction is much more diﬃcult ne ar the edges\\nof the training sample. One must extrapolate from neighbori ng sample\\npoints rather than interpolate between them.\\nAnother manifestation of the curse is that the sampling dens ity is pro-\\nportional to N1/p, wherepis the dimension of the input space and Nis the\\nsample size. Thus, if N1= 100 represents a dense sample for a single input\\nproblem, then N10= 10010is the sample size required for the same sam-\\npling density with 10 inputs. Thus in high dimensions all fea sible training\\nsamples sparsely populate the input space.\\nLet us construct another uniform example. Suppose we have 10 00 train-\\ning examples xigenerated uniformly on [ −1,1]p. Assume that the true\\nrelationship between XandYis\\nY=f(X) =e−8||X||2,\\nwithout any measurement error. We use the 1-nearest-neighb or rule to\\npredicty0at the test-point x0= 0. Denote the training set by T. We can', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 41}),\n",
       " Document(page_content='24 2. Overview of Supervised Learning\\ncompute the expected prediction error at x0for our procedure, averaging\\nover all such samples of size 1000. Since the problem is deter ministic, this\\nis the mean squared error (MSE) for estimating f(0):\\nMSE(x0) = E T[f(x0)−ˆy0]2\\n= ET[ˆy0−ET(ˆy0)]2+[ET(ˆy0)−f(x0)]2\\n= Var T(ˆy0)+Bias2(ˆy0). (2.25)\\nFigure 2.7 illustrates the setup. We have broken down the MSE into two\\ncomponents that will become familiar as we proceed: varianc e and squared\\nbias.Suchadecompositionisalwayspossibleandoftenusef ul,andisknown\\nas thebias–variance decomposition . Unless the nearest neighbor is at 0,\\nˆy0will be smaller than f(0) in this example, and so the average estimate\\nwill be biased downward. The variance is due to the sampling v ariance of\\nthe 1-nearest neighbor. In low dimensions and with N= 1000, the nearest\\nneighbor is very close to 0, and so both the bias and variance a re small. As\\nthe dimension increases, the nearest neighbor tends to stra y further from\\nthe target point, and both bias and variance are incurred. By p= 10, for\\nmore than 99% of the samples the nearest neighbor is a distanc e greater\\nthan 0.5 from the origin. Thus as pincreases, the estimate tends to be 0\\nmore often than not, and hence the MSE levels oﬀ at 1 .0, as does the bias,\\nand the variance starts dropping (an artifact of this exampl e).\\nAlthough this is a highly contrived example, similar phenom ena occur\\nmore generally. The complexity of functions of many variabl es can grow\\nexponentially with the dimension, and if we wish to be able to estimate\\nsuch functions with the same accuracy as function in low dime nsions, then\\nwe need the size of our training set to grow exponentially as w ell. In this\\nexample, the function is a complex interaction of all pvariables involved.\\nThe dependence of the bias term on distance depends on the tru th, and\\nit need not always dominate with 1-nearest neighbor. For exa mple, if the\\nfunction always involves only a few dimensions as in Figure 2 .8, then the\\nvariance can dominate instead.\\nSuppose, on the other hand, that we know that the relationshi p between\\nYandXis linear,\\nY=XTβ+ε, (2.26)\\nwhereε∼N(0,σ2) and we ﬁt the model by least squares to the train-\\ning data. For an arbitrary test point x0, we have ˆy0=xT\\n0ˆβ, which can\\nbe written as ˆ y0=xT\\n0β+∑N\\ni=1ℓi(x0)εi, whereℓi(x0) is theith element\\nofX(XTX)−1x0. Since under this model the least squares estimates are', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 42}),\n",
       " Document(page_content='2.5 Local Methods in High Dimensions 25\\nXf(X)\\n-1.0 -0.5 0.0 0.5 1.00.0 0.2 0.4 0.6 0.8 1.0•1-NN in One Dimension\\nX1X2\\n-1.0 -0.5 0.0 0.5 1.0-1.0 -0.5 0.0 0.5 1.0•••\\n•••\\n••\\n•\\n•••\\n•••\\n••\\n••\\n•\\n•\\n•••\\n••1-NN in One vs. Two Dimensions\\nDimensionAverage Distance to Nearest Neighbor\\n2 4 6 8 100.0 0.2 0.4 0.6 0.8••••••••••Distance to 1-NN vs. Dimension\\nDimensionMse\\n2 4 6 8 100.0 0.2 0.4 0.6 0.8 1.0• •••••••••\\n• •••••••• • • •••••••••MSE vs. Dimension\\n•MSE\\n•Variance\\n•Sq. Bias\\nFIGURE 2.7. A simulation example, demonstrating the curse of dimensional-\\nity and its eﬀect on MSE, bias and variance. The input feature s are uniformly\\ndistributed in [−1,1]pforp= 1,...,10The top left panel shows the target func-\\ntion (no noise) in IR:f(X) =e−8||X||2, and demonstrates the error that 1-nearest\\nneighbor makes in estimating f(0). The training point is indicated by the blue tick\\nmark. The top right panel illustrates why the radius of the 1-nearest neighborhood\\nincreases with dimension p. The lower left panel shows the average radius of the\\n1-nearest neighborhoods. The lower-right panel shows the MSE , squared bias and\\nvariance curves as a function of dimension p.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 43}),\n",
       " Document(page_content='26 2. Overview of Supervised Learning\\nXf(X)\\n-1.0 -0.5 0.0 0.5 1.00 1 2 3 4•1-NN in One Dimension\\nDimensionMSE\\n2 4 6 8 100.0 0.05 0.10 0.15 0.20 0.25••••••••••\\n••••••••••\\n• •• • ••••••MSE  vs. Dimension\\n•MSE\\n•Variance\\n•Sq. Bias\\nFIGURE 2.8. A simulation example with the same setup as in Figure 2.7. Here\\nthe function is constant in all but one dimension: f(X) =1\\n2(X1+ 1)3. The\\nvariance dominates.\\nunbiased, we ﬁnd that\\nEPE(x0) = E y0|x0ET(y0−ˆy0)2\\n= Var(y0|x0)+ET[ˆy0−ETˆy0]2+[ETˆy0−xT\\n0β]2\\n= Var(y0|x0)+Var T(ˆy0)+Bias2(ˆy0)\\n=σ2+ETxT\\n0(XTX)−1x0σ2+02. (2.27)\\nHere we have incurred an additional variance σ2in the prediction error,\\nsince our target is not deterministic. There is no bias, and t he variance\\ndepends on x0. IfNis large andTwere selected at random, and assuming\\nE(X) = 0, then XTX→NCov(X) and\\nEx0EPE(x0)∼Ex0xT\\n0Cov(X)−1x0σ2/N+σ2\\n= trace[Cov( X)−1Cov(x0)]σ2/N+σ2\\n=σ2(p/N)+σ2. (2.28)\\nHere we see that the expected EPE increases linearly as a func tion ofp,\\nwith slope σ2/N. IfNis large and/or σ2is small, this growth in vari-\\nance is negligible (0 in the deterministic case). By imposin g some heavy\\nrestrictions on the class of models being ﬁtted, we have avoi ded the curse\\nof dimensionality. Some of the technical details in (2.27) a nd (2.28) are\\nderived in Exercise 2.5.\\nFigure 2.9 compares 1-nearest neighbor vs. least squares in two situa-\\ntions, both of which have the form Y=f(X) +ε,Xuniform as before,\\nandε∼N(0,1). The sample size is N= 500. For the orange curve, f(x)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 44}),\n",
       " Document(page_content='2.5 Local Methods in High Dimensions 27\\nDimensionEPE Ratio\\n2 4 6 8 101.6 1.7 1.8 1.9 2.0 2.1••••••••••••••••••••Expected Prediction Error of 1NN vs. OLS\\n•Linear\\n•Cubic\\nFIGURE 2.9. The curves show the expected prediction error (at x0= 0) for\\n1-nearest neighbor relative to least squares for the model Y=f(X)+ε. For the\\norange curve, f(x) =x1, while for the blue curve f(x) =1\\n2(x1+1)3.\\nis linear in the ﬁrst coordinate, for the blue curve, cubic as in Figure 2.8.\\nShown is the relative EPE of 1-nearest neighbor to least squa res, which\\nappears to start at around 2 for the linear case. Least square s is unbiased\\nin this case, and as discussed above the EPE is slightly above σ2= 1.\\nThe EPE for 1-nearest neighbor is always above 2, since the va riance of\\nˆf(x0) in this case is at least σ2, and the ratio increases with dimension as\\nthe nearest neighbor strays from the target point. For the cu bic case, least\\nsquares is biased, which moderates the ratio. Clearly we cou ld manufacture\\nexamples where the bias of least squares would dominate the v ariance, and\\nthe 1-nearest neighbor would come out the winner.\\nBy relying on rigid assumptions, the linear model has no bias at all and\\nnegligible variance, while the error in 1-nearest neighbor is substantially\\nlarger. However, if the assumptions are wrong, all bets are o ﬀ and the\\n1-nearest neighbor may dominate. We will see that there is a w hole spec-\\ntrum of models between the rigid linear models and the extrem ely ﬂexible\\n1-nearest-neighbor models, each with their own assumption s and biases,\\nwhich have been proposed speciﬁcally to avoid the exponenti al growth in\\ncomplexity of functions in high dimensions by drawing heavi ly on these\\nassumptions.\\nBefore we delve more deeply, let us elaborate a bit on the conc ept of\\nstatistical models and see how they ﬁt into the prediction framework.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 45}),\n",
       " Document(page_content='28 2. Overview of Supervised Learning\\n2.6 Statistical Models, Supervised Learning and\\nFunction Approximation\\nOur goal is to ﬁnd a useful approximation ˆf(x) to the function f(x) that\\nunderlies the predictive relationship between the inputs a nd outputs. In the\\ntheoretical setting of Section 2.4, we saw that squared erro r loss lead us\\nto the regression function f(x) = E(Y|X=x) for a quantitative response.\\nThe class of nearest-neighbor methods can be viewed as direc t estimates\\nof this conditional expectation, but we have seen that they c an fail in at\\nleast two ways:\\n•if the dimension of the input space is high, the nearest neigh bors need\\nnot be close to the target point, and can result in large error s;\\n•if special structure is known to exist, this can be used to red uce both\\nthe bias and the variance of the estimates.\\nWe anticipate using other classes of models for f(x), in many cases specif-\\nically designed to overcome the dimensionality problems, a nd here we dis-\\ncuss a framework for incorporating them into the prediction problem.\\n2.6.1 A Statistical Model for the Joint Distribution Pr(X,Y)\\nSuppose in fact that our data arose from a statistical model\\nY=f(X)+ε, (2.29)\\nwhere the random error εhas E(ε) = 0 and is independent of X. Note that\\nfor this model, f(x) = E(Y|X=x), and in fact the conditional distribution\\nPr(Y|X) depends on Xonlythrough the conditional mean f(x).\\nThe additive error model is a useful approximation to the tru th. For\\nmost systems the input–output pairs ( X,Y) will not have a deterministic\\nrelationship Y=f(X). Generally there will be other unmeasured variables\\nthatalsocontributeto Y,includingmeasurementerror.Theadditivemodel\\nassumes that we can capture all these departures from a deter ministic re-\\nlationship via the error ε.\\nFor some problems a deterministic relationship does hold. M any of the\\nclassiﬁcation problems studied in machine learning are of t his form, where\\nthe response surface can be thought of as a colored map deﬁned in IRp.\\nThe training data consist of colored examples from the map {xi,gi}, and\\nthe goal is to be able to color any point. Here the function is d eterministic,\\nand the randomness enters through the xlocation of the training points.\\nFor the moment we will not pursue such problems, but will see t hat they\\ncan be handled by techniques appropriate for the error-base d models.\\nThe assumption in (2.29) that the errors are independent and identically\\ndistributedisnotstrictlynecessary,butseemstobeatthe backofourmind', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 46}),\n",
       " Document(page_content='2.6 Statistical Models, Supervised Learning and Function Ap proximation 29\\nwhen we average squared errors uniformly in our EPE criterio n. With such\\na model it becomes natural to use least squares as a data crite rion for\\nmodel estimation as in (2.1). Simple modiﬁcations can be mad e to avoid\\nthe independence assumption; for example, we can have Var( Y|X=x) =\\nσ(x), and now both the mean and variance depend on X. In general the\\nconditional distribution Pr( Y|X) can depend on Xin complicated ways,\\nbut the additive error model precludes these.\\nSo far we have concentrated on the quantitative response. Ad ditive error\\nmodels are typically not used for qualitative outputs G; in this case the tar-\\nget function p(X)isthe conditional density Pr( G|X), and this is modeled\\ndirectly. For example, for two-class data, it is often reaso nable to assume\\nthat the data arise from independent binary trials, with the probability of\\none particular outcome being p(X), and the other 1 −p(X). Thus ifYis\\nthe 0–1 coded version of G, then E(Y|X=x) =p(x), but the variance\\ndepends on xas well: Var( Y|X=x) =p(x)[1−p(x)].\\n2.6.2 Supervised Learning\\nBefore we launch into more statistically oriented jargon, w e present the\\nfunction-ﬁtting paradigm from a machine learning point of v iew. Suppose\\nfor simplicity that the errors are additive and that the mode lY=f(X)+ε\\nis a reasonable assumption. Supervised learning attempts t o learnfby\\nexample through a teacher. One observes the system under study, both\\nthe inputs and outputs, and assembles a training set of observations T=\\n(xi,yi), i= 1,...,N. The observed input values to the system xiare also\\nfed into an artiﬁcial system, known as a learning algorithm ( usually a com-\\nputer program), which also produces outputs ˆf(xi) in response to the in-\\nputs. The learning algorithm has the property that it can mod ify its in-\\nput/output relationship ˆfin response to diﬀerences yi−ˆf(xi) between the\\noriginal and generated outputs. This process is known as learning by exam-\\nple. Upon completion of the learning process the hope is that the artiﬁcial\\nand real outputs will be close enough to be useful for all sets of inputs likely\\nto be encountered in practice.\\n2.6.3 Function Approximation\\nThe learning paradigm of the previous section has been the mo tivation\\nfor research into the supervised learning problem in the ﬁel ds of machine\\nlearning (with analogies to human reasoning) and neural net works (with\\nbiological analogies to the brain). The approach taken in ap plied mathe-\\nmatics and statistics has been from the perspective of funct ion approxima-\\ntion and estimation. Here the data pairs {xi,yi}are viewed as points in a\\n(p+1)-dimensional Euclidean space. The function f(x) has domain equal\\nto thep-dimensional input subspace, and is related to the data via a model', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 47}),\n",
       " Document(page_content='30 2. Overview of Supervised Learning\\nsuch asyi=f(xi)+εi. For convenience in this chapter we will assume the\\ndomain is IRp, ap-dimensional Euclidean space, although in general the\\ninputs can be of mixed type. The goal is to obtain a useful appr oximation\\ntof(x) for allxin some region of IRp, given the representations in T.\\nAlthough somewhat less glamorous than the learning paradig m, treating\\nsupervised learning as a problem in function approximation encourages the\\ngeometrical concepts of Euclidean spaces and mathematical concepts of\\nprobabilistic inference to be applied to the problem. This i s the approach\\ntaken in this book.\\nMany of the approximations we will encounter have associate d a set of\\nparameters θthat can be modiﬁed to suit the data at hand. For example,\\nthe linear model f(x) =xTβhasθ=β. Another class of useful approxi-\\nmators can be expressed as linear basis expansions\\nfθ(x) =K∑\\nk=1hk(x)θk, (2.30)\\nwhere thehkare a suitable set of functions or transformations of the inp ut\\nvectorx. Traditional examples are polynomial and trigonometric ex pan-\\nsions, where for example hkmight bex2\\n1,x1x2\\n2, cos(x1) and so on. We\\nalso encounter nonlinear expansions, such as the sigmoid tr ansformation\\ncommon to neural network models,\\nhk(x) =1\\n1+exp(−xTβk). (2.31)\\nWe can use least squares to estimate the parameters θinfθas we did\\nfor the linear model, by minimizing the residual sum-of-squ ares\\nRSS(θ) =N∑\\ni=1(yi−fθ(xi))2(2.32)\\nas a function of θ. This seems a reasonable criterion for an additive error\\nmodel. In terms of function approximation, we imagine our pa rameterized\\nfunction as a surface in p+ 1 space, and what we observe are noisy re-\\nalizations from it. This is easy to visualize when p= 2 and the vertical\\ncoordinate is the output y, as in Figure 2.10. The noise is in the output\\ncoordinate, so we ﬁnd the set of parameters such that the ﬁtte d surface\\ngets as close to the observed points as possible, where close is measured by\\nthe sum of squared vertical errors in RSS( θ).\\nFor the linear model we get a simple closed form solution to th e mini-\\nmization problem. This is also true for the basis function me thods, if the\\nbasis functions themselves do not have any hidden parameter s. Otherwise\\nthe solution requires either iterative methods or numerica l optimization.\\nWhile least squares is generally very convenient, it is not t he only crite-\\nrion used and in some cases would not make much sense. A more ge neral', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 48}),\n",
       " Document(page_content='2.6 Statistical Models, Supervised Learning and Function Ap proximation 31\\n•••\\n•••\\n••••\\n••\\n•••\\n••••\\n•••\\n••\\n•••\\n•\\n•\\n••\\n••• ••••\\n••\\n•••\\n•••\\n•\\n••\\n••\\n•••\\n•\\n••\\n•••\\n•\\n•••\\n••••\\n•••\\n•\\n••\\nFIGURE 2.10. Least squares ﬁtting of a function of two inputs. The paramet ers\\noffθ(x)are chosen so as to minimize the sum-of-squared vertical err ors.\\nprinciple for estimation is maximum likelihood estimation . Suppose we have\\na random sample yi, i= 1,...,Nfrom a density Pr θ(y) indexed by some\\nparameters θ. The log-probability of the observed sample is\\nL(θ) =N∑\\ni=1logPrθ(yi). (2.33)\\nThe principle of maximum likelihood assumes that the most re asonable\\nvalues forθare those for which the probability of the observed sample is\\nlargest. Least squares for the additive error model Y=fθ(X) +ε, with\\nε∼N(0,σ2), is equivalent to maximum likelihood using the conditiona l\\nlikelihood\\nPr(Y|X,θ) =N(fθ(X),σ2). (2.34)\\nSo although the additional assumption of normality seems mo re restrictive,\\nthe results are the same. The log-likelihood of the data is\\nL(θ) =−N\\n2log(2π)−Nlogσ−1\\n2σ2N∑\\ni=1(yi−fθ(xi))2,(2.35)\\nand the only term involving θis the last, which is RSS( θ) up to a scalar\\nnegative multiplier.\\nA more interesting example is the multinomial likelihood fo r the regres-\\nsion function Pr( G|X) for a qualitative output G. Suppose we have a model\\nPr(G=Gk|X=x) =pk,θ(x), k= 1,...,Kfor the conditional probabil-\\nity of each class given X, indexed by the parameter vector θ. Then the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 49}),\n",
       " Document(page_content='32 2. Overview of Supervised Learning\\nlog-likelihood (also referred to as the cross-entropy) is\\nL(θ) =N∑\\ni=1logpgi,θ(xi), (2.36)\\nand when maximized it delivers values of θthat best conform with the data\\nin this likelihood sense.\\n2.7 Structured Regression Models\\nWehaveseenthatalthoughnearest-neighborandotherlocal methodsfocus\\ndirectly on estimating the function at a point, they face pro blems in high\\ndimensions. They may also be inappropriate even in low dimen sions in\\ncases where more structured approaches can make more eﬃcien t use of the\\ndata. This section introduces classes of such structured ap proaches. Before\\nwe proceed, though, we discuss further the need for such clas ses.\\n2.7.1 Diﬃculty of the Problem\\nConsider the RSS criterion for an arbitrary function f,\\nRSS(f) =N∑\\ni=1(yi−f(xi))2. (2.37)\\nMinimizing (2.37) leads to inﬁnitely many solutions: any fu nctionˆfpassing\\nthrough the training points ( xi,yi) is a solution. Any particular solution\\nchosen might be a poor predictor at test points diﬀerent from the training\\npoints. If there are multiple observation pairs xi,yiℓ, ℓ= 1,...,N iat each\\nvalue ofxi, the risk is limited. In this case, the solutions pass throug h\\nthe average values of the yiℓat eachxi; see Exercise 2.6. The situation is\\nsimilar to the one we have already visited in Section 2.4; ind eed, (2.37) is\\nthe ﬁnite sample version of (2.11) on page 18. If the sample si zeNwere\\nsuﬃciently large such that repeats were guaranteed and dens ely arranged,\\nit would seem that these solutions might all tend to the limit ing conditional\\nexpectation.\\nIn order to obtain useful results for ﬁnite N, we must restrict the eligible\\nsolutions to (2.37) to a smaller set of functions. How to deci de on the\\nnature of the restrictions is based on considerations outsi de of the data.\\nTheserestrictions aresometimes encodedviatheparametri crepresentation\\noffθ, or may be built into the learning method itself, either impl icitly or\\nexplicitly. These restricted classes of solutions are the m ajor topic of this\\nbook. One thing should be clear, though. Any restrictions im posed onf\\nthat lead to a unique solution to (2.37) do not really remove t he ambiguity', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 50}),\n",
       " Document(page_content='2.8 Classes of Restricted Estimators 33\\ncaused by the multiplicity of solutions. There are inﬁnitel y many possible\\nrestrictions, each leading to a unique solution, so the ambi guity has simply\\nbeen transferred to the choice of constraint.\\nIn general the constraints imposed by most learning methods can be\\ndescribed as complexity restrictions of one kind or another. This usually\\nmeans some kind of regular behavior in small neighborhoods o f the input\\nspace. That is, for all input points xsuﬃciently close to each other in\\nsome metric, ˆfexhibits some special structure such as nearly constant,\\nlinear or low-order polynomial behavior. The estimator is t hen obtained by\\naveraging or polynomial ﬁtting in that neighborhood.\\nThe strength of the constraint is dictated by the neighborho od size. The\\nlarger the size of the neighborhood, the stronger the constr aint, and the\\nmore sensitive the solution is to the particular choice of co nstraint. For\\nexample, local constant ﬁts in inﬁnitesimally small neighb orhoods is no\\nconstraint at all; local linear ﬁts in very large neighborho ods is almost a\\nglobally linear model, and is very restrictive.\\nThe nature of the constraint depends on the metric used. Some methods,\\nsuch as kernel and local regression and tree-based methods, directly specify\\nthe metric and size of the neighborhood. The nearest-neighb or methods\\ndiscussed so far are based on the assumption that locally the function is\\nconstant;closetoatargetinput x0,thefunctiondoesnotchangemuch,and\\nso close outputs can be averaged to produce ˆf(x0). Other methods such\\nas splines, neural networks and basis-function methods imp licitly deﬁne\\nneighborhoods of local behavior. In Section 5.4.1 we discus s the concept\\nof anequivalent kernel (see Figure 5.8 on page 157), which describes this\\nlocal dependence for any method linear in the outputs. These equivalent\\nkernels in many cases look just like the explicitly deﬁned we ighting kernels\\ndiscussed above—peaked at the target point and falling away s moothly\\naway from it.\\nOne fact should be clear by now. Any method that attempts to pr o-\\nduce locally varying functions in small isotropic neighbor hoods will run\\ninto problems in high dimensions—again the curse of dimensio nality. And\\nconversely, all methods that overcome the dimensionality p roblems have an\\nassociated—andoftenimplicitoradaptive—metricformeasur ingneighbor-\\nhoods, which basically does not allow the neighborhood to be simultane-\\nously small in all directions.\\n2.8 Classes of Restricted Estimators\\nThe variety of nonparametric regressiontechniques or lear ning methods fall\\nintoanumberofdiﬀerentclassesdependingonthenatureoft herestrictions\\nimposed. These classes are not distinct, and indeed some met hods fall in\\nseveral classes. Here we give a brief summary, since detaile d descriptions', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 51}),\n",
       " Document(page_content='34 2. Overview of Supervised Learning\\nare given in later chapters. Each of the classes has associat ed with it one\\nor more parameters, sometimes appropriately called smoothing parameters,\\nthat control the eﬀective size of the local neighborhood. He re we describe\\nthree broad classes.\\n2.8.1 Roughness Penalty and Bayesian Methods\\nHere the class of functions is controlled by explicitly pena lizing RSS( f)\\nwith a roughness penalty\\nPRSS(f;λ) = RSS(f)+λJ(f). (2.38)\\nThe user-selected functional J(f) will be large for functions fthat vary too\\nrapidly over small regions of input space. For example, the p opularcubic\\nsmoothing spline for one-dimensional inputs is the solution to the penalized\\nleast-squares criterion\\nPRSS(f;λ) =N∑\\ni=1(yi−f(xi))2+λ∫\\n[f′′(x)]2dx. (2.39)\\nThe roughness penalty here controls large values of the seco nd derivative\\noff, and the amount of penalty is dictated by λ≥0. Forλ= 0 no penalty\\nis imposed, and any interpolating function will do, while fo rλ=∞only\\nfunctions linear in xare permitted.\\nPenalty functionals Jcan be constructed for functions in any dimension,\\nand special versions can be created to impose special struct ure. For ex-\\nample, additive penalties J(f) =∑p\\nj=1J(fj) are used in conjunction with\\nadditive functions f(X) =∑p\\nj=1fj(Xj) to create additive models with\\nsmooth coordinate functions. Similarly, projection pursuit regression mod-\\nels havef(X) =∑M\\nm=1gm(αT\\nmX) for adaptively chosen directions αm, and\\nthe functions gmcan each have an associated roughness penalty.\\nPenalty function, or regularization methods, express our prior belief that\\nthe type of functions we seek exhibit a certain type of smooth behavior, and\\nindeed can usually be cast in a Bayesian framework. The penal tyJcorre-\\nsponds to a log-prior, and PRSS( f;λ) the log-posterior distribution, and\\nminimizing PRSS( f;λ) amounts to ﬁnding the posterior mode. We discuss\\nroughness-penalty approaches in Chapter 5 and the Bayesian paradigm in\\nChapter 8.\\n2.8.2 Kernel Methods and Local Regression\\nThesemethodscanbethoughtofasexplicitlyprovidingesti matesofthere-\\ngression function or conditional expectation by specifyin g the nature of the\\nlocal neighborhood, and of the class of regular functions ﬁt ted locally. The\\nlocal neighborhood is speciﬁed by a kernel function Kλ(x0,x) which assigns', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 52}),\n",
       " Document(page_content='2.8 Classes of Restricted Estimators 35\\nweights to points xin a region around x0(see Figure 6.1 on page 192). For\\nexample, the Gaussian kernel has a weight function based on t he Gaussian\\ndensity function\\nKλ(x0,x) =1\\nλexp[\\n−||x−x0||2\\n2λ]\\n(2.40)\\nand assigns weights to points that die exponentially with th eir squared\\nEuclidean distance from x0. The parameter λcorresponds to the variance\\nof the Gaussian density, and controls the width of the neighb orhood. The\\nsimplest form of kernel estimate is the Nadaraya–Watson wei ghted average\\nˆf(x0) =∑N\\ni=1Kλ(x0,xi)yi∑N\\ni=1Kλ(x0,xi). (2.41)\\nIn general we can deﬁne a local regression estimate of f(x0) asfˆθ(x0),\\nwhereˆθminimizes\\nRSS(fθ,x0) =N∑\\ni=1Kλ(x0,xi)(yi−fθ(xi))2, (2.42)\\nandfθis some parameterized function, such as a low-order polynom ial.\\nSome examples are:\\n•fθ(x) =θ0, the constant function; this results in the Nadaraya–\\nWatson estimate in (2.41) above.\\n•fθ(x) =θ0+θ1xgives the popular local linear regression model.\\nNearest-neighbor methods can be thought of as kernel method s having a\\nmore data-dependent metric. Indeed, the metric for k-nearest neighbors is\\nKk(x,x0) =I(||x−x0||≤||x(k)−x0||),\\nwherex(k)is the training observation ranked kth in distance from x0, and\\nI(S) is the indicator of the set S.\\nThesemethodsofcourseneedtobemodiﬁedinhighdimensions ,toavoid\\nthe curse of dimensionality. Various adaptations are discu ssed in Chapter 6.\\n2.8.3 Basis Functions and Dictionary Methods\\nThis class of methods includes the familiar linear and polyn omial expan-\\nsions, but more importantly a wide variety of more ﬂexible mo dels. The\\nmodel forfis a linear expansion of basis functions\\nfθ(x) =M∑\\nm=1θmhm(x), (2.43)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 53}),\n",
       " Document(page_content='36 2. Overview of Supervised Learning\\nwhere each of the hmis a function of the input x, and the term linear here\\nrefers to the action of the parameters θ. This class covers a wide variety of\\nmethods. In some cases the sequence of basis functions is pre scribed, such\\nas a basis for polynomials in xof total degree M.\\nForone-dimensional x,polynomialsplinesofdegree Kcanberepresented\\nby an appropriate sequence of Mspline basis functions, determined in turn\\nbyM−K−1knots.Theseproducefunctionsthatarepiecewisepolynomials\\nof degreeKbetween the knots, and joined up with continuity of degree\\nK−1 at the knots. As an example consider linear splines, or piec ewise\\nlinear functions. One intuitively satisfying basis consis ts of the functions\\nb1(x) = 1,b2(x) =x, andbm+2(x) = (x−tm)+,m= 1,...,M−2,\\nwheretmis themth knot, and z+denotes positive part. Tensor products\\nof spline bases can be used for inputs with dimensions larger than one\\n(see Section 5.2, and the CART and MARS models in Chapter 9.) T he\\nparameterMcontrols the degree of the polynomial or the number of knots\\nin the case of splines.\\nRadial basis functions are symmetric p-dimensional kernels located at\\nparticular centroids,\\nfθ(x) =M∑\\nm=1Kλm(µm,x)θm; (2.44)\\nfor example, the Gaussian kernel Kλ(µ,x) =e−||x−µ||2/2λis popular.\\nRadial basis functions have centroids µmand scales λmthat have to\\nbe determined. The spline basis functions have knots. In gen eral we would\\nlike the data to dictate them as well. Including these as para meters changes\\nthe regression problem from a straightforward linear probl em to a combi-\\nnatorially hard nonlinear problem. In practice, shortcuts such as greedy\\nalgorithms or two stage processes are used. Section 6.7 desc ribes some such\\napproaches.\\nA single-layer feed-forward neural network model with line ar output\\nweights can be thought of as an adaptive basis function metho d. The model\\nhas the form\\nfθ(x) =M∑\\nm=1βmσ(αT\\nmx+bm), (2.45)\\nwhereσ(x) = 1/(1 +e−x) is known as the activation function. Here, as\\nin the projection pursuit model, the directions αmand thebiastermsbm\\nhavetobedetermined,andtheirestimationisthemeatofthe computation.\\nDetails are given in Chapter 11.\\nThese adaptively chosen basis function methods are also kno wn asdictio-\\nnarymethods, where one has available a possibly inﬁnite set or di ctionary\\nDof candidate basis functions from which to choose, and model s are built\\nup by employing some kind of search mechanism.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 54}),\n",
       " Document(page_content='2.9 Model Selection and the Bias–Variance Tradeoﬀ 37\\n2.9 Model Selection and the Bias–Variance\\nTradeoﬀ\\nAll the models described above and many others discussed in l ater chapters\\nhave asmoothing orcomplexity parameter that has to be determined:\\n•the multiplier of the penalty term;\\n•the width of the kernel;\\n•or the number of basis functions.\\nInthecaseofthesmoothingspline,theparameter λindexesmodelsranging\\nfrom a straight line ﬁt to the interpolating model. Similarl y a local degree-\\nmpolynomial model ranges between a degree- mglobal polynomial when\\nthe window size is inﬁnitely large, to an interpolating ﬁt wh en the window\\nsize shrinks to zero. This means that we cannot use residual s um-of-squares\\non the training data to determine these parameters as well, s ince we would\\nalways pick those that gave interpolating ﬁts and hence zero residuals. Such\\na model is unlikely to predict future data well at all.\\nThek-nearest-neighbor regression ﬁt ˆfk(x0) usefully illustrates the com-\\npeting forces that aﬀect the predictive ability of such appr oximations. Sup-\\npose the data arise from a model Y=f(X) +ε, with E(ε) = 0 and\\nVar(ε) =σ2. For simplicity here we assume that the values of xiin the\\nsample are ﬁxed in advance (nonrandom). The expected predic tion error\\natx0, also known as testorgeneralization error, can be decomposed:\\nEPEk(x0) = E[(Y−ˆfk(x0))2|X=x0]\\n=σ2+[Bias2(ˆfk(x0))+Var T(ˆfk(x0))] (2.46)\\n=σ2+[\\nf(x0)−1\\nkk∑\\nℓ=1f(x(ℓ))]2\\n+σ2\\nk.(2.47)\\nThe subscripts in parentheses ( ℓ) indicate the sequence of nearest neighbors\\ntox0.\\nThere are three terms in this expression. The ﬁrst term σ2is their-\\nreducible error—the variance of the new test target—and is beyond our\\ncontrol, even if we know the true f(x0).\\nThe second and third terms are under our control, and make up t he\\nmean squared error ofˆfk(x0) in estimating f(x0), which is broken down\\ninto a bias component and a variance component. The bias term is the\\nsquared diﬀerence between the true mean f(x0) and the expected value of\\nthe estimate—[E T(ˆfk(x0))−f(x0)]2—where the expectation averages the\\nrandomness in the training data. This term will most likely i ncrease with\\nk, if the true function is reasonably smooth. For small kthe few closest\\nneighbors will have values f(x(ℓ)) close tof(x0), so their average should', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 55}),\n",
       " Document(page_content='38 2. Overview of Supervised Learning\\nHigh Bias\\nLow VarianceLow Bias\\nHigh VariancePrediction Error\\nModel ComplexityTraining SampleTest Sample\\nLow High\\nFIGURE 2.11. Test and training error as a function of model complexity.\\nbe close to f(x0). Askgrows, the neighbors are further away, and then\\nanything can happen.\\nThe variance term is simply the variance of an average here, a nd de-\\ncreases as the inverse of k. So askvaries, there is a bias–variance tradeoﬀ.\\nMore generally, as the model complexity of our procedureis increased, the\\nvariance tends to increase and the squared bias tends to decr ease. The op-\\nposite behavior occurs as the model complexity is decreased . Fork-nearest\\nneighbors, the model complexity is controlled by k.\\nTypically we would like to choose our model complexity to tra de bias\\noﬀ with variance in such a way as to minimize the test error. An obvious\\nestimate of test error is the training error1\\nN∑\\ni(yi−ˆyi)2. Unfortunately\\ntraining error is not a good estimate of test error, as it does not properly\\naccount for model complexity.\\nFigure 2.11 shows the typical behavior of the test and traini ng error, as\\nmodel complexity is varied. The training error tends to decr ease whenever\\nwe increase the model complexity, that is, whenever we ﬁt the data harder.\\nHowever with too much ﬁtting, the model adapts itself too clo sely to the\\ntraining data, and will not generalize well (i.e., have larg e test error). In\\nthat case the predictions ˆf(x0) will have large variance, as reﬂected in the\\nlast term of expression (2.46). In contrast, if the model is n ot complex\\nenough, it will underﬁt and may have large bias, again resulting in poor\\ngeneralization. In Chapter 7 we discuss methods for estimat ing the test\\nerror of a prediction method, and hence estimating the optim al amount of\\nmodel complexity for a given prediction method and training set.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 56}),\n",
       " Document(page_content='Exercises 39\\nBibliographic Notes\\nSome good general books on the learning problem are Duda et al . (2000),\\nBishop(1995),(Bishop, 2006),Ripley(1996),Cherkasskya ndMulier(2007)\\nand Vapnik (1996). Parts of this chapter are based on Friedma n (1994b).\\nExercises\\nEx. 2.1Suppose each of K-classes has an associated target tk, which is a\\nvector of all zeros, except a one in the kth position. Show that classifying to\\nthe largest element of ˆ yamounts to choosing the closest target, min k||tk−\\nˆy||, if the elements of ˆ ysum to one.\\nEx. 2.2Show how to compute the Bayes decision boundary for the simul a-\\ntion example in Figure 2.5.\\nEx. 2.3Derive equation (2.24).\\nEx. 2.4The edge eﬀect problem discussed on page 23 is not peculiar to\\nuniform sampling from bounded domains. Consider inputs dra wn from a\\nspherical multinormal distribution X∼N(0,Ip). The squared distance\\nfrom any sample point to the origin has a χ2\\npdistribution with mean p.\\nConsider a prediction point x0drawn from this distribution, and let a=\\nx0/||x0||be an associated unit vector. Let zi=aTxibe the projection of\\neach of the training points on this direction.\\nShow that the ziare distributed N(0,1) with expected squared distance\\nfrom the origin 1, while the target point has expected square d distancep\\nfrom the origin.\\nHence forp= 10, a randomly drawn test point is about 3 .1 standard\\ndeviations from the origin, while all the training points ar e on average\\none standard deviation along direction a. So most prediction points see\\nthemselves as lying on the edge of the training set.\\nEx. 2.5\\n(a) Derive equation (2.27). The last line makes use of (3.8) t hrough a\\nconditioning argument.\\n(b) Derive equation (2.28), making use of the cyclicproperty of the trace\\noperator [trace( AB) = trace(BA)], and its linearity (which allows us\\nto interchange the order of trace and expectation).\\nEx. 2.6Consider a regression problem with inputs xiand outputs yi, and a\\nparameterized model fθ(x) to be ﬁt by least squares. Show that if there are\\nobservations with tiedoridentical values ofx, then the ﬁt can be obtained\\nfrom a reduced weighted least squares problem.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 57}),\n",
       " Document(page_content='40 2. Overview of Supervised Learning\\nEx. 2.7Suppose we have a sample of Npairsxi,yidrawn i.i.d. from the\\ndistribution characterized as follows:\\nxi∼h(x),the design density\\nyi=f(xi)+εi, fis the regression function\\nεi∼(0,σ2) (mean zero, variance σ2)\\nWe construct an estimator for flinearin theyi,\\nˆf(x0) =N∑\\ni=1ℓi(x0;X)yi,\\nwhere the weights ℓi(x0;X) do not depend on the yi, but do depend on the\\nentire training sequence of xi, denoted here by X.\\n(a)Showthatlinearregressionand k-nearest-neighborregressionaremem-\\nbersofthisclassofestimators.Describeexplicitlythewe ightsℓi(x0;X)\\nin each of these cases.\\n(b) Decompose the conditional mean-squared error\\nEY|X(f(x0)−ˆf(x0))2\\nintoaconditionalsquaredbiasandaconditionalvariancec omponent.\\nLikeX,Yrepresents the entire training sequence of yi.\\n(c) Decompose the (unconditional) mean-squared error\\nEY,X(f(x0)−ˆf(x0))2\\ninto a squared bias and a variance component.\\n(d) Establish a relationship between the squared biases and variances in\\nthe above two cases.\\nEx. 2.8Compare the classiﬁcation performance of linear regressio n andk–\\nnearest neighbor classiﬁcation on the zipcodedata. In particular, consider\\nonly the 2’s and3’s, andk= 1,3,5,7 and 15. Show both the training and\\ntest error for each choice. The zipcodedata are available from the book\\nwebsitewww-stat.stanford.edu/ElemStatLearn .\\nEx. 2.9Consider a linear regression model with pparameters, ﬁt by least\\nsquares to a set of training data ( x1,y1),...,(xN,yN) drawn at random\\nfrom a population. Let ˆβbe the least squares estimate. Suppose we have\\nsome test data (˜ x1,˜y1),...,(˜xM,˜yM) drawn at random from the same pop-\\nulation as the training data. If Rtr(β) =1\\nN∑N\\n1(yi−βTxi)2andRte(β) =\\n1\\nM∑M\\n1(˜yi−βT˜xi)2, prove that\\nE[Rtr(ˆβ)]≤E[Rte(ˆβ)],', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 58}),\n",
       " Document(page_content='Exercises 41\\nwhere the expectations are over all that is random in each exp ression. [This\\nexercisewasbroughttoourattentionbyRyanTibshirani,fr omahomework\\nassignment given by Andrew Ng.]', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 59}),\n",
       " Document(page_content='42 2. Overview of Supervised Learning', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 60}),\n",
       " Document(page_content='This is page 43\\nPrinter: Opaque this\\n3\\nLinear Methods for Regression\\n3.1 Introduction\\nA linear regression model assumes that the regression funct ion E(Y|X) is\\nlinear in the inputs X1,...,X p. Linear models were largely developed in\\nthe precomputer age of statistics, but even in today’s compu ter era there\\nare still good reasons to study and use them. They are simple a nd often\\nprovide an adequate and interpretable description of how th e inputs aﬀect\\nthe output. For prediction purposes they can sometimes outp erform fancier\\nnonlinear models, especially in situations with small numb ers of training\\ncases,lowsignal-to-noiseratioorsparsedata.Finally,l inearmethodscanbe\\nappliedtotransformationsoftheinputsandthisconsidera blyexpandstheir\\nscope. These generalizations are sometimes called basis-f unction methods,\\nand are discussed in Chapter 5.\\nIn this chapter we describe linear methods for regression, w hile in the\\nnextchapter wediscusslinear methods for classiﬁcation. O nsometopics we\\ngo into considerable detail, as it is our ﬁrm belief that an un derstanding\\nof linear methods is essential for understanding nonlinear ones. In fact,\\nmany nonlinear techniques are direct generalizations of th e linear methods\\ndiscussed here.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 61}),\n",
       " Document(page_content='44 3. Linear Methods for Regression\\n3.2 Linear Regression Models and Least Squares\\nAsintroducedinChapter2,wehaveaninputvector XT= (X1,X2,...,X p),\\nand want to predict a real-valued output Y. The linear regression model\\nhas the form\\nf(X) =β0+p∑\\nj=1Xjβj. (3.1)\\nThe linear model either assumes that the regression functio n E(Y|X) is\\nlinear, or that the linear model is a reasonable approximati on. Here the\\nβj’s are unknown parameters or coeﬃcients, and the variables Xjcan come\\nfrom diﬀerent sources:\\n•quantitative inputs;\\n•transformations of quantitative inputs, such as log, squar e-root or\\nsquare;\\n•basisexpansions,suchas X2=X2\\n1,X3=X3\\n1,leadingtoapolynomial\\nrepresentation;\\n•numeric or “dummy” coding of the levels of qualitative input s. For\\nexample, if Gis a ﬁve-level factor input, we might create Xj, j=\\n1,...,5,such thatXj=I(G=j). Together this group of Xjrepre-\\nsents the eﬀect of Gby a set of level-dependent constants, since in∑5\\nj=1Xjβj, one of the Xjs is one, and the others are zero.\\n•interactions between variables, for example, X3=X1·X2.\\nNo matter the source of the Xj, the model is linear in the parameters.\\nTypically we have a set of training data ( x1,y1)...(xN,yN) from which\\nto estimate the parameters β. Eachxi= (xi1,xi2,...,x ip)Tis a vector\\nof feature measurements for the ith case. The most popular estimation\\nmethodis least squares ,inwhichwepickthecoeﬃcients β= (β0,β1,...,β p)T\\nto minimize the residual sum of squares\\nRSS(β) =N∑\\ni=1(yi−f(xi))2\\n=N∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2\\n. (3.2)\\nFrom a statistical point of view, this criterion is reasonab le if the training\\nobservations ( xi,yi) represent independent random draws from their popu-\\nlation. Even if the xi’s were not drawn randomly, the criterion is still valid\\nif theyi’s are conditionally independent given the inputs xi. Figure 3.1\\nillustrates the geometry of least-squares ﬁtting in the IRp+1-dimensional', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 62}),\n",
       " Document(page_content='3.2 Linear Regression Models and Least Squares 45\\n•• •\\n••••\\n••••\\n•••\\n••\\n•••\\n•\\n••\\n••\\n•••\\n••••••\\n••••\\n•••\\n••\\n••\\n•\\n••\\n••\\n••••\\n••\\n••\\n•\\n•••\\n•\\n••••\\n••••\\n••\\n••\\nX1X2Y\\nFIGURE 3.1. Linear least squares ﬁtting with X∈IR2. We seek the linear\\nfunction of Xthat minimizes the sum of squared residuals from Y.\\nspace occupied by the pairs ( X,Y). Note that (3.2) makes no assumptions\\nabout the validity of model (3.1); it simply ﬁnds the best lin ear ﬁt to the\\ndata. Least squares ﬁtting is intuitively satisfying no mat ter how the data\\narise; the criterion measures the average lack of ﬁt.\\nHow do we minimize (3.2)? Denote by XtheN×(p+ 1) matrix with\\neach row an input vector (with a 1 in the ﬁrst position), and si milarly let\\nybe theN-vector of outputs in the training set. Then we can write the\\nresidual sum-of-squares as\\nRSS(β) = (y−Xβ)T(y−Xβ). (3.3)\\nThis is a quadratic function in the p+1 parameters. Diﬀerentiating with\\nrespect toβwe obtain\\n∂RSS\\n∂β=−2XT(y−Xβ)\\n∂2RSS\\n∂β∂βT= 2XTX.(3.4)\\nAssuming (for the moment) that Xhas full column rank, and hence XTX\\nis positive deﬁnite, we set the ﬁrst derivative to zero\\nXT(y−Xβ) = 0 (3.5)\\nto obtain the unique solution\\nˆβ= (XTX)−1XTy. (3.6)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 63}),\n",
       " Document(page_content='46 3. Linear Methods for Regression\\nx1x2y\\nˆ y\\nFIGURE 3.2. TheN-dimensional geometry of least squares regression with two\\npredictors. The outcome vector yis orthogonally projected onto the hyperplane\\nspanned by the input vectors x1andx2. The projection ˆyrepresents the vector\\nof the least squares predictions\\nThe predicted values at an input vector x0are given by ˆf(x0) = (1 :x0)Tˆβ;\\nthe ﬁtted values at the training inputs are\\nˆy=Xˆβ=X(XTX)−1XTy, (3.7)\\nwhere ˆyi=ˆf(xi). The matrix H=X(XTX)−1XTappearing in equation\\n(3.7) is sometimes called the “hat” matrix because it puts th e hat on y.\\nFigure3.2showsadiﬀerentgeometricalrepresentationoft heleastsquares\\nestimate,thistimeinIRN.Wedenotethecolumnvectorsof Xbyx0,x1,...,xp,\\nwithx0≡1. For much of what follows, this ﬁrst column is treated like a ny\\nother. These vectors span a subspace of IRN, also referred to as the column\\nspace of X. We minimize RSS( β) =∥y−Xβ∥2by choosing ˆβso that the\\nresidual vector y−ˆyis orthogonal to this subspace. This orthogonality is\\nexpressed in (3.5), and the resulting estimate ˆyis hence the orthogonal pro-\\njectionofyonto this subspace. The hat matrix Hcomputes the orthogonal\\nprojection, and hence it is also known as a projection matrix .\\nIt might happen that the columns of Xare not linearly independent, so\\nthatXis not of full rank. This would occur, for example, if two of th e\\ninputs were perfectly correlated, (e.g., x2= 3x1). ThenXTXis singular\\nand the least squares coeﬃcients ˆβare not uniquely deﬁned. However,\\nthe ﬁtted values ˆy=Xˆβare still the projection of yonto the column\\nspace of X; there is just more than one way to express that projection\\nin terms of the column vectors of X. The non-full-rank case occurs most\\noftenwhenoneormorequalitative inputsarecodedinaredun dantfashion.\\nThere is usually a natural way to resolve the non-unique repr esentation,\\nby recoding and/or dropping redundant columns in X. Most regression\\nsoftware packages detect these redundancies and automatic ally implement', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 64}),\n",
       " Document(page_content='3.2 Linear Regression Models and Least Squares 47\\nsome strategy for removing them. Rank deﬁciencies can also o ccur in signal\\nand image analysis, where the number of inputs pcan exceed the number\\nof training cases N. In this case, the features are typically reduced by\\nﬁltering or else the ﬁtting is controlled by regularization (Section 5.2.3 and\\nChapter 18).\\nUp to now we have made minimal assumptions about the true dist ribu-\\ntion of the data. In order to pin down the sampling properties ofˆβ, we now\\nassume that the observations yiare uncorrelated and have constant vari-\\nanceσ2, and that the xiare ﬁxed (non random). The variance–covariance\\nmatrix of the least squares parameter estimates is easily de rived from (3.6)\\nand is given by\\nVar(ˆβ) = (XTX)−1σ2. (3.8)\\nTypically one estimates the variance σ2by\\nˆσ2=1\\nN−p−1N∑\\ni=1(yi−ˆyi)2.\\nTheN−p−1 rather than Nin the denominator makes ˆ σ2an unbiased\\nestimate of σ2: E(ˆσ2) =σ2.\\nTo draw inferences about the parameters and the model, addit ional as-\\nsumptions are needed. We now assume that (3.1) is the correct model for\\nthe mean; that is, the conditional expectation of Yis linear inX1,...,X p.\\nWealsoassumethatthedeviationsof Yarounditsexpectationareadditive\\nand Gaussian. Hence\\nY= E(Y|X1,...,X p)+ε\\n=β0+p∑\\nj=1Xjβj+ε, (3.9)\\nwhere the error εis a Gaussian random variable with expectation zero and\\nvarianceσ2, writtenε∼N(0,σ2).\\nUnder (3.9), it is easy to show that\\nˆβ∼N(β,(XTX)−1σ2). (3.10)\\nThis is a multivariate normal distribution with mean vector and variance–\\ncovariance matrix as shown. Also\\n(N−p−1)ˆσ2∼σ2χ2\\nN−p−1, (3.11)\\na chi-squared distribution with N−p−1 degrees of freedom. In addition ˆβ\\nand ˆσ2are statistically independent. We use these distributiona l properties\\nto form tests of hypothesis and conﬁdence intervals for the p arametersβj.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 65}),\n",
       " Document(page_content='48 3. Linear Methods for Regression\\nZTail Probabilities\\n2.0 2.2 2.4 2.6 2.8 3.00.01 0.02 0.03 0.04 0.05 0.06t30\\nt100\\nnormal\\nFIGURE 3.3. The tail probabilities Pr(|Z|>z)for three distributions, t30,t100\\nand standard normal. Shown are the appropriate quantiles for t esting signiﬁcance\\nat thep= 0.05and0.01levels. The diﬀerence between tand the standard normal\\nbecomes negligible for Nbigger than about 100.\\nTo test the hypothesis that a particular coeﬃcient βj= 0, we form the\\nstandardized coeﬃcient or Z-score\\nzj=ˆβj\\nˆσ√vj, (3.12)\\nwherevjisthejthdiagonalelementof( XTX)−1.Underthenullhypothesis\\nthatβj= 0,zjis distributed as tN−p−1(atdistribution with N−p−1\\ndegrees of freedom), and hence a large (absolute) value of zjwill lead to\\nrejection of this null hypothesis. If ˆ σis replaced by a known value σ, then\\nzjwould have a standard normal distribution. The diﬀerence be tween the\\ntail quantiles of a t-distribution and a standard normal become negligible\\nas the sample size increases, and so we typically use the norm al quantiles\\n(see Figure 3.3).\\nOften we need to test for the signiﬁcance of groups of coeﬃcie nts simul-\\ntaneously. For example, to test if a categorical variable wi thklevels can\\nbe excluded from a model, we need to test whether the coeﬃcien ts of the\\ndummy variables used to represent the levels can all be set to zero. Here\\nwe use the Fstatistic,\\nF=(RSS0−RSS1)/(p1−p0)\\nRSS1/(N−p1−1), (3.13)\\nwhereRSS 1istheresidualsum-of-squaresfortheleastsquaresﬁtofth ebig-\\nger model with p1+1 parameters, and RSS 0the same for the nested smaller\\nmodel with p0+1 parameters, having p1−p0parameters constrained to be', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 66}),\n",
       " Document(page_content='3.2 Linear Regression Models and Least Squares 49\\nzero. TheFstatistic measures the change in residual sum-of-squares p er\\nadditional parameter in the bigger model, and it is normaliz ed by an esti-\\nmate ofσ2. Under the Gaussian assumptions, and the null hypothesis th at\\nthe smaller model is correct, the Fstatistic will have a Fp1−p0,N−p1−1dis-\\ntribution. It can be shown (Exercise 3.1) that the zjin (3.12) are equivalent\\nto theFstatistic for dropping the single coeﬃcient βjfrom the model. For\\nlargeN, the quantiles of Fp1−p0,N−p1−1approach those of χ2\\np1−p0/(p1−p0).\\nSimilarly, we can isolate βjin (3.10) to obtain a 1 −2αconﬁdence interval\\nforβj:\\n(ˆβj−z(1−α)v1\\n2\\njˆσ,ˆβj+z(1−α)v1\\n2\\njˆσ). (3.14)\\nHerez(1−α)is the 1−αpercentile of the normal distribution:\\nz(1−0.025)= 1.96,\\nz(1−.05)= 1.645,etc.\\nHence the standard practice of reporting ˆβ±2·se(ˆβ) amounts to an ap-\\nproximate 95% conﬁdence interval. Even if the Gaussian erro r assumption\\ndoes not hold, this interval will be approximately correct, with its coverage\\napproaching 1−2αas the sample size N→∞.\\nIn a similar fashion we can obtain an approximate conﬁdence s et for the\\nentire parameter vector β, namely\\nCβ={β|(ˆβ−β)TXTX(ˆβ−β)≤ˆσ2χ2\\np+1(1−α)}, (3.15)\\nwhereχ2\\nℓ(1−α)is the 1−αpercentile of the chi-squared distribution on ℓ\\ndegrees of freedom: for example, χ2\\n5(1−0.05)= 11.1,χ2\\n5(1−0.1)= 9.2. This\\nconﬁdence set for βgenerates a corresponding conﬁdence set for the true\\nfunctionf(x) =xTβ, namely{xTβ|β∈Cβ}(Exercise 3.2; see also Fig-\\nure 5.4 in Section 5.2.2 for examples of conﬁdence bands for f unctions).\\n3.2.1 Example: Prostate Cancer\\nThe data for this example come from a study by Stamey et al. (19 89). They\\nexamined the correlation between the level of prostate-spe ciﬁc antigen and\\na number of clinical measures in men who were about to receive a radical\\nprostatectomy. The variables are log cancer volume ( lcavol), log prostate\\nweight ( lweight),age, log of the amount of benign prostatic hyperplasia\\n(lbph), seminal vesicle invasion ( svi), log of capsular penetration ( lcp),\\nGleason score ( gleason), and percent of Gleason scores 4 or 5 ( pgg45).\\nThe correlation matrix of the predictors given in Table 3.1 s hows many\\nstrong correlations. Figure 1.1 (page 3) of Chapter 1 is a sca tterplot matrix\\nshowing every pairwise plot between the variables. We see th atsviis a\\nbinary variable, and gleasonis an ordered categorical variable. We see, for', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 67}),\n",
       " Document(page_content='50 3. Linear Methods for Regression\\nTABLE 3.1. Correlations of predictors in the prostate cancer data.\\nlcavol lweight age lbph svi lcp gleason\\nlweight 0.300\\nage0.286 0.317\\nlbph0.063 0.437 0.287\\nsvi0.593 0.181 0.129 −0.139\\nlcp0.692 0.157 0.173 −0.089 0.671\\ngleason 0.426 0.024 0.366 0.033 0.307 0.476\\npgg45 0.483 0.074 0.276 −0.030 0.481 0.663 0.757\\nTABLE 3.2. Linear model ﬁt to the prostate cancer data. The Z score is the\\ncoeﬃcient divided by its standard error (3.12). Roughly a Zscore larger than two\\nin absolute value is signiﬁcantly nonzero at the p= 0.05level.\\nTerm Coeﬃcient Std. Error ZScore\\nIntercept 2.46 0.09 27.60\\nlcavol 0.68 0.13 5.37\\nlweight 0.26 0.10 2.75\\nage−0.14 0.10 −1.40\\nlbph 0.21 0.10 2.06\\nsvi 0.31 0.12 2.47\\nlcp−0.29 0.15 −1.87\\ngleason−0.02 0.15 −0.15\\npgg45 0.27 0.15 1.74\\nexample, that both lcavolandlcpshow a strong relationship with the\\nresponse lpsa, and with each other. We need to ﬁt the eﬀects jointly to\\nuntangle the relationships between the predictors and the r esponse.\\nWe ﬁt a linear model to the log of prostate-speciﬁc antigen, lpsa, after\\nﬁrst standardizing the predictors to have unit variance. We randomly split\\nthe dataset into a training set of size 67 and a test set of size 30. We ap-\\nplied least squares estimation to the training set, produci ng the estimates,\\nstandard errors and Z-scores shown in Table 3.2. The Z-scores are deﬁned\\nin (3.12), and measure the eﬀect of dropping that variable fr om the model.\\nAZ-score greater than 2 in absolute value is approximately sig niﬁcant at\\nthe 5% level. (For our example, we have nine parameters, and t he 0.025 tail\\nquantiles of the t67−9distribution are±2.002!) The predictor lcavolshows\\nthe strongest eﬀect, with lweightandsvialso strong. Notice that lcpis\\nnot signiﬁcant, once lcavolis in the model (when used in a model without\\nlcavol,lcpis strongly signiﬁcant). We can also test for the exclusion o f\\na number of terms at once, using the F-statistic (3.13). For example, we\\nconsider dropping all the non-signiﬁcant terms in Table 3.2 , namely age,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 68}),\n",
       " Document(page_content='3.2 Linear Regression Models and Least Squares 51\\nlcp,gleason, andpgg45. We get\\nF=(32.81−29.43)/(9−5)\\n29.43/(67−9)= 1.67, (3.16)\\nwhich has a p-value of 0.17 (Pr(F4,58>1.67) = 0.17), and hence is not\\nsigniﬁcant.\\nThe mean prediction error on the test data is 0 .521. In contrast, predic-\\ntion using the mean training value of lpsahas a test error of 1 .057, which\\nis called the “base error rate.” Hence the linear model reduc es the base\\nerror rate by about 50%. We will return to this example later t o compare\\nvarious selection and shrinkage methods.\\n3.2.2 The Gauss–Markov Theorem\\nOne of the most famous results in statistics asserts that the least squares\\nestimates of the parameters βhave the smallest variance among all linear\\nunbiased estimates. We will make this precise here, and also make clear\\nthat the restriction to unbiased estimates is not necessari ly a wise one. This\\nobservationwillleadustoconsiderbiasedestimatessucha sridgeregression\\nlater in the chapter. We focus on estimation of any linear com bination of\\nthe parameters θ=aTβ; for example, predictions f(x0) =xT\\n0βare of this\\nform. The least squares estimate of aTβis\\nˆθ=aTˆβ=aT(XTX)−1XTy. (3.17)\\nConsidering Xto be ﬁxed, this is a linear function cT\\n0yof the response\\nvectory. If we assume that the linear model is correct, aTˆβis unbiased\\nsince\\nE(aTˆβ) = E(aT(XTX)−1XTy)\\n=aT(XTX)−1XTXβ\\n=aTβ. (3.18)\\nThe Gauss–Markov theorem states that if we have any other lin ear estima-\\ntor˜θ=cTythat is unbiased for aTβ, that is, E( cTy) =aTβ, then\\nVar(aTˆβ)≤Var(cTy). (3.19)\\nThe proof (Exercise 3.3) uses the triangle inequality. For s implicity we have\\nstated the result in terms of estimation of a single paramete raTβ, but with\\na few more deﬁnitions one can state it in terms of the entire pa rameter\\nvectorβ(Exercise 3.3).\\nConsider the mean squared error of an estimator ˜θin estimating θ:\\nMSE(˜θ) = E( ˜θ−θ)2\\n= Var( ˜θ)+[E(˜θ)−θ]2. (3.20)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 69}),\n",
       " Document(page_content='52 3. Linear Methods for Regression\\nThe ﬁrst term is the variance, while the second term is the squ ared bias.\\nTheGauss-Markovtheoremimpliesthattheleastsquaresest imatorhasthe\\nsmallest mean squared error of all linear estimators with no bias. However,\\nthere may well exist a biased estimator with smaller mean squ ared error.\\nSuchanestimatorwouldtradealittlebiasforalargerreduc tioninvariance.\\nBiased estimates are commonly used. Any method that shrinks or sets to\\nzero some of the least squares coeﬃcients may result in a bias ed estimate.\\nWe discuss many examples, including variable subset select ion and ridge\\nregression, later in this chapter. From a more pragmatic poi nt of view, most\\nmodels are distortions of the truth, and hence are biased; pi cking the right\\nmodel amounts to creating the right balance between bias and variance.\\nWe go into these issues in more detail in Chapter 7.\\nMean squared error is intimately related to prediction accu racy, as dis-\\ncussed in Chapter 2. Consider the prediction of the new respo nse at input\\nx0,\\nY0=f(x0)+ε0. (3.21)\\nThen the expected prediction error of an estimate ˜f(x0) =xT\\n0˜βis\\nE(Y0−˜f(x0))2=σ2+E(xT\\n0˜β−f(x0))2\\n=σ2+MSE( ˜f(x0)). (3.22)\\nTherefore, expected prediction error and mean squared erro r diﬀer only by\\nthe constant σ2, representing the variance of the new observation y0.\\n3.2.3 Multiple Regression from Simple Univariate Regressi on\\nThe linear model (3.1) with p >1 inputs is called the multiple linear\\nregression model . The least squares estimates (3.6) for this model are best\\nunderstood in terms of the estimates for the univariate (p= 1) linear\\nmodel, as we indicate in this section.\\nSuppose ﬁrst that we have a univariate model with no intercep t, that is,\\nY=Xβ+ε. (3.23)\\nThe least squares estimate and residuals are\\nˆβ=∑N\\n1xiyi∑N\\n1x2\\ni,\\nri=yi−xiˆβ.(3.24)\\nIn convenient vector notation, we let y= (y1,...,y N)T,x= (x1,...,x N)T\\nand deﬁne\\n⟨x,y⟩=N∑\\ni=1xiyi,\\n=xTy, (3.25)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 70}),\n",
       " Document(page_content='3.2 Linear Regression Models and Least Squares 53\\ntheinner product between xandy1. Then we can write\\nˆβ=⟨x,y⟩\\n⟨x,x⟩,\\nr=y−xˆβ.(3.26)\\nAs we will see, this simple univariate regression provides t he building block\\nfor multiple linear regression. Suppose next that the input sx1,x2,...,xp\\n(the columns of the data matrix X) are orthogonal; that is ⟨xj,xk⟩= 0\\nfor allj̸=k. Then it is easy to check that the multiple least squares esti -\\nmatesˆβjare equal to⟨xj,y⟩/⟨xj,xj⟩—the univariate estimates. In other\\nwords, when the inputs are orthogonal, they have no eﬀect on e ach other’s\\nparameter estimates in the model.\\nOrthogonal inputs occur most often with balanced, designed experiments\\n(where orthogonality is enforced), but almost never with ob servational\\ndata. Hence we will have to orthogonalize them in order to car ry this idea\\nfurther. Suppose next that we have an intercept and a single i nputx. Then\\nthe least squares coeﬃcient of xhas the form\\nˆβ1=⟨x−¯x1,y⟩\\n⟨x−¯x1,x−¯x1⟩, (3.27)\\nwhere ¯x=∑\\nixi/N, and1=x0, the vector of Nones. We can view the\\nestimate (3.27) as the result of two applications of the simp le regression\\n(3.26). The steps are:\\n1. regress xon1to produce the residual z=x−¯x1;\\n2. regress yon the residual zto give the coeﬃcient ˆβ1.\\nInthisprocedure,“regress bona”meansasimpleunivariateregressionof b\\nonawith no intercept, producing coeﬃcient ˆ γ=⟨a,b⟩/⟨a,a⟩and residual\\nvectorb−ˆγa. We say that bis adjusted for a, or is “orthogonalized” with\\nrespect to a.\\nStep 1 orthogonalizes xwith respect to x0=1. Step 2 is just a simple\\nunivariate regression, using the orthogonal predictors 1andz. Figure 3.4\\nshows this process for two general inputs x1andx2. The orthogonalization\\ndoes not change the subspace spanned by x1andx2, it simply produces an\\northogonal basis for representing it.\\nThis recipe generalizes to the case of pinputs, as shown in Algorithm 3.1.\\nNote that the inputs z0,...,zj−1in step 2 are orthogonal, hence the simple\\nregression coeﬃcients computed there are in fact also the mu ltiple regres-\\nsion coeﬃcients.\\n1The inner-product notation is suggestive of generalizations of linea r regression to\\ndiﬀerent metric spaces, as well as to probability spaces.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 71}),\n",
       " Document(page_content='54 3. Linear Methods for Regression\\nx1x2y\\nˆ yz z z z z\\nFIGURE 3.4. Least squares regression by orthogonalization of the inputs . The\\nvectorx2is regressed on the vector x1, leaving the residual vector z. The regres-\\nsion ofyonzgives the multiple regression coeﬃcient of x2. Adding together the\\nprojections of yon each of x1andzgives the least squares ﬁt ˆy.\\nAlgorithm 3.1 Regression by Successive Orthogonalization.\\n1. Initialize z0=x0=1.\\n2. Forj= 1,2,...,p\\nRegress xjonz0,z1,...,,zj−1to produce coeﬃcients ˆ γℓj=\\n⟨zℓ,xj⟩/⟨zℓ,zℓ⟩,ℓ= 0,...,j−1 and residual vector zj=\\nxj−∑j−1\\nk=0ˆγkjzk.\\n3. Regress yon the residual zpto give the estimate ˆβp.\\nThe result of this algorithm is\\nˆβp=⟨zp,y⟩\\n⟨zp,zp⟩. (3.28)\\nRe-arranging the residual in step 2, we can see that each of th exjis a linear\\ncombination of the zk, k≤j. Since the zjare all orthogonal, they form\\na basis for the column space of X, and hence the least squares projection\\nonto this subspace is ˆy. Sincezpalone involves xp(with coeﬃcient 1), we\\nsee that the coeﬃcient (3.28) is indeed the multiple regress ion coeﬃcient of\\nyonxp. This key result exposes the eﬀect of correlated inputs in mu ltiple\\nregression. Note also that by rearranging the xj, any one of them could\\nbe in the last position, and a similar results holds. Hence st ated more\\ngenerally, we have shown that the jth multiple regression coeﬃcient is the\\nunivariate regression coeﬃcient of yonxj·012...(j−1)(j+1)...,p, the residual\\nafter regressing xjonx0,x1,...,xj−1,xj+1,...,xp:', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 72}),\n",
       " Document(page_content='3.2 Linear Regression Models and Least Squares 55\\nThe multiple regression coeﬃcient ˆβjrepresents the additional\\ncontribution of xjony, afterxjhas been adjusted for x0,x1,...,xj−1,\\nxj+1,...,xp.\\nIfxpis highly correlated with some of the other xk’s, the residual vector\\nzpwill be close to zero, and from (3.28) the coeﬃcient ˆβpwill be very\\nunstable. This will be true for all the variables in the corre lated set. In\\nsuch situations, we might have all the Z-scores (as in Table 3 .2) be small—\\nany one of the set can be deleted—yet we cannot delete them all. From\\n(3.28) we also obtain an alternate formula for the variance e stimates (3.8),\\nVar(ˆβp) =σ2\\n⟨zp,zp⟩=σ2\\n∥zp∥2. (3.29)\\nIn other words, the precision with which we can estimate ˆβpdepends on\\nthe length of the residual vector zp; this represents how much of xpis\\nunexplained by the other xk’s.\\nAlgorithm 3.1 is known as the Gram–Schmidt procedure for multiple\\nregression, and is also a useful numerical strategy for comp uting the esti-\\nmates. We can obtain from it not just ˆβp, but also the entire multiple least\\nsquares ﬁt, as shown in Exercise 3.4.\\nWe can represent step 2 of Algorithm 3.1 in matrix form:\\nX=ZΓ, (3.30)\\nwhereZhas as columns the zj(in order), and Γis the upper triangular ma-\\ntrix with entries ˆ γkj. Introducing the diagonal matrix Dwithjth diagonal\\nentryDjj=∥zj∥, we get\\nX=ZD−1DΓ\\n=QR, (3.31)\\nthe so-called QRdecomposition of X. HereQis anN×(p+1) orthogonal\\nmatrix,QTQ=I, andRis a (p+1)×(p+1) upper triangular matrix.\\nTheQRdecomposition represents a convenient orthogonal basis fo r the\\ncolumn space of X. It is easy to see, for example, that the least squares\\nsolution is given by\\nˆβ=R−1QTy, (3.32)\\nˆy=QQTy. (3.33)\\nEquation (3.32) is easy to solve because Ris upper triangular\\n(Exercise 3.4).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 73}),\n",
       " Document(page_content='56 3. Linear Methods for Regression\\n3.2.4 Multiple Outputs\\nSuppose we have multiple outputs Y1,Y2,...,Y Kthat we wish to predict\\nfrom our inputs X0,X1,X2,...,X p. We assume a linear model for each\\noutput\\nYk=β0k+p∑\\nj=1Xjβjk+εk (3.34)\\n=fk(X)+εk. (3.35)\\nWithNtraining cases we can write the model in matrix notation\\nY=XB+E. (3.36)\\nHereYis theN×Kresponse matrix, with ikentryyik,Xis theN×(p+1)\\ninput matrix, Bis the (p+ 1)×Kmatrix of parameters and Eis the\\nN×Kmatrix of errors. A straightforward generalization of the u nivariate\\nloss function (3.2) is\\nRSS(B) =K∑\\nk=1N∑\\ni=1(yik−fk(xi))2(3.37)\\n= tr[(Y−XB)T(Y−XB)]. (3.38)\\nThe least squares estimates have exactly the same form as bef ore\\nˆB= (XTX)−1XTY. (3.39)\\nHence the coeﬃcients for the kth outcome are just the least squares es-\\ntimates in the regression of ykonx0,x1,...,xp. Multiple outputs do not\\naﬀect one another’s least squares estimates.\\nIf the errors ε= (ε1,...,ε K) in (3.34) are correlated, then it might seem\\nappropriate to modify (3.37) in favor of a multivariate vers ion. Speciﬁcally,\\nsuppose Cov( ε) =Σ, then the multivariate weighted criterion\\nRSS(B;Σ) =N∑\\ni=1(yi−f(xi))TΣ−1(yi−f(xi)) (3.40)\\narises naturally from multivariate Gaussian theory. Here f(x) is the vector\\nfunction (f1(x),...,f K(x))T, andyithe vector of Kresponses for obser-\\nvationi. However, it can be shown that again the solution is given by\\n(3.39);Kseparate regressions that ignore the correlations (Exerci se 3.11).\\nIf theΣivary among observations, then this is no longer the case, and the\\nsolution for Bno longer decouples.\\nIn Section 3.7 we pursue the multiple outcome problem, and co nsider\\nsituations where it does pay to combine the regressions.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 74}),\n",
       " Document(page_content='3.3 Subset Selection 57\\n3.3 Subset Selection\\nThere are two reasons why we are often not satisﬁed with the le ast squares\\nestimates (3.6).\\n•The ﬁrst is prediction accuracy : the least squares estimates often have\\nlow bias but large variance. Prediction accuracy can someti mes be\\nimproved by shrinking or setting some coeﬃcients to zero. By doing\\nsowesacriﬁcealittlebitofbiastoreducethevarianceofth epredicted\\nvalues, and hence may improve the overall prediction accura cy.\\n•The second reason is interpretation . With a large number of predic-\\ntors, we often would like to determine a smaller subset that e xhibit\\nthe strongest eﬀects. In order to get the “big picture,” we ar e willing\\nto sacriﬁce some of the small details.\\nIn this section we describe a number of approaches to variabl e subset selec-\\ntionwithlinearregression.Inlatersectionswediscusssh rinkageandhybrid\\napproaches for controlling variance, as well as other dimen sion-reduction\\nstrategies. These all fall under the general heading model selection . Model\\nselection is not restricted to linear models; Chapter 7 cove rs this topic in\\nsome detail.\\nWith subset selection we retain only a subset of the variable s, and elim-\\ninate the rest from the model. Least squares regression is us ed to estimate\\nthe coeﬃcients of the inputs that are retained. There are a nu mber of dif-\\nferent strategies for choosing the subset.\\n3.3.1 Best-Subset Selection\\nBest subset regression ﬁnds for each k∈{0,1,2,...,p}the subset of size k\\nthat gives smallest residual sum of squares (3.2). An eﬃcien t algorithm—\\ntheleaps and bounds procedure (Furnival and Wilson, 1974)—makes this\\nfeasible for pas large as 30 or 40. Figure 3.5 shows all the subset models\\nfor the prostate cancer example. The lower boundary represe nts the models\\nthat are eligible for selection by the best-subsets approac h. Note that the\\nbest subset of size 2, for example, need not include the varia ble that was\\nin the best subset of size 1 (for this example all the subsets a re nested).\\nThe best-subset curve (red lower boundary in Figure 3.5) is n ecessarily\\ndecreasing, so cannot be used to select the subset size k. The question of\\nhow to choose kinvolves the tradeoﬀ between bias and variance, along with\\nthe more subjective desire for parsimony. There are a number of criteria\\nthat one may use; typically we choose the smallest model that minimizes\\nan estimate of the expected prediction error.\\nMany of the other approaches that we discuss in this chapter a re similar,\\nin that they use the training data to produce a sequence of mod els varying\\nin complexity and indexed by a single parameter. In the next s ection we use', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 75}),\n",
       " Document(page_content='58 3. Linear Methods for Regression\\nSubset Size kResidual Sum−of−Squares\\n0 20 40 60 80 100\\n0 1 2 3 4 5 6 7 8•\\n••••••••\\n••••••••••••••••••••••••••\\n•••••••••••••••••••••••••••••••••••••••••••••••••\\n••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\\n•••••••••••••••••••••••••••••••••••••••••••••\\n••••••••••••••••••••••••••\\n••••••••\\n••\\n•\\n•••••• •\\nFIGURE 3.5. All possible subset models for the prostate cancer example. At\\neach subset size is shown the residual sum-of-squares for ea ch model of that size.\\ncross-validation to estimate prediction error and select k; the AIC criterion\\nis a popular alternative. We defer more detailed discussion of these and\\nother approaches to Chapter 7.\\n3.3.2 Forward- and Backward-Stepwise Selection\\nRather than search through all possible subsets (which beco mes infeasible\\nforpmuchlargerthan40),wecanseekagoodpaththroughthem. Forward-\\nstepwise selection starts with the intercept, and then sequentially adds into\\nthe model the predictor that most improves the ﬁt. With many c andidate\\npredictors, this might seem like a lot of computation; howev er, clever up-\\ndating algorithms can exploit the QR decomposition for the c urrent ﬁt to\\nrapidly establish the next candidate (Exercise 3.9). Like b est-subset re-\\ngression, forward stepwise produces a sequence of models in dexed byk, the\\nsubset size, which must be determined.\\nForward-stepwise selection is a greedy algorithm , producing a nested se-\\nquence of models. In this sense it might seem sub-optimal com pared to\\nbest-subset selection. However, there are several reasons why it might be\\npreferred:', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 76}),\n",
       " Document(page_content='3.3 Subset Selection 59\\n•Computational; for largepwe cannot compute the best subset se-\\nquence, but we can always compute the forward stepwise seque nce\\n(even when p≫N).\\n•Statistical; a price is paid in variance for selecting the best subset\\nof each size; forward stepwise is a more constrained search, and will\\nhave lower variance, but perhaps more bias.\\n0 5 10 15 20 25 300.65 0.70 0.75 0.80 0.85 0.90 0.95Best Subset\\nForward Stepwise\\nBackward Stepwise\\nForward StagewiseE||ˆβ(k)−β||2\\nSubset Size k\\nFIGURE 3.6. Comparison of four subset-selection techniques on a simulate d lin-\\near regression problem Y=XTβ+ε. There are N= 300observations on p= 31\\nstandard Gaussian variables, with pairwise correlations all e qual to0.85. For10of\\nthe variables, the coeﬃcients are drawn at random from a N(0,0.4)distribution;\\nthe rest are zero. The noise ε∼N(0,6.25), resulting in a signal-to-noise ratio of\\n0.64. Results are averaged over 50simulations. Shown is the mean-squared error\\nof the estimated coeﬃcient ˆβ(k)at each step from the true β.\\nBackward-stepwise selection starts with the full model, and sequentially\\ndeletes the predictor that has the least impact on the ﬁt. The candidate for\\ndroppingisthevariablewiththesmallestZ-score(Exercis e3.10).Backward\\nselection can only be used when N >p, while forward stepwise can always\\nbe used.\\nFigure 3.6 shows the results of a small simulation study to co mpare\\nbest-subset regression with the simpler alternatives forw ard and backward\\nselection. Their performance is very similar, as is often th e case. Included in\\nthe ﬁgure is forward stagewise regression (next section), w hich takes longer\\nto reach minimum error.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 77}),\n",
       " Document(page_content='60 3. Linear Methods for Regression\\nOn the prostate cancer example, best-subset, forward and ba ckward se-\\nlection all gave exactly the same sequence of terms.\\nSome software packages implement hybrid stepwise-selecti on strategies\\nthat consider both forward and backward moves at each step, a nd select\\nthe “best” of the two. For example in the Rpackage the stepfunction uses\\nthe AIC criterion for weighing the choices, which takes prop er account of\\nthe number of parameters ﬁt; at each step an add or drop will be performed\\nthat minimizes the AIC score.\\nOthermoretraditionalpackagesbasetheselectionon F-statistics,adding\\n“signiﬁcant” terms, and dropping “non-signiﬁcant” terms. These are out\\nof fashion, since they do not take proper account of the multi ple testing\\nissues. It is also tempting after a model search to print out a summary of\\nthe chosen model, such as in Table 3.2; however, the standard errors are\\nnot valid, since they do not account for the search process. T he bootstrap\\n(Section 8.2) can be useful in such settings.\\nFinally, we note that often variables come in groups (such as the dummy\\nvariables that code a multi-level categorical predictor). Smart stepwise pro-\\ncedures (such as stepinR) will add or drop whole groups at a time, taking\\nproper account of their degrees-of-freedom.\\n3.3.3 Forward-Stagewise Regression\\nForward-stagewise regression (FS) is even more constraine d than forward-\\nstepwise regression. It starts like forward-stepwise regr ession, with an in-\\ntercept equal to ¯ y, and centered predictors with coeﬃcients initially all 0.\\nAt each step the algorithm identiﬁes the variable most corre lated with the\\ncurrent residual. It then computes the simple linear regres sion coeﬃcient\\nof the residual on this chosen variable, and then adds it to th e current co-\\neﬃcient for that variable. This is continued till none of the variables have\\ncorrelation with the residuals—i.e. the least-squares ﬁt wh enN >p.\\nUnlike forward-stepwise regression, none of the other vari ables are ad-\\njusted when a term is added to the model. As a consequence, for ward\\nstagewise can take many more than psteps to reach the least squares ﬁt,\\nand historically has been dismissed as being ineﬃcient. It t urns out that\\nthis “slow ﬁtting” can pay dividends in high-dimensional pr oblems. We\\nsee in Section 3.8.1 that both forward stagewise and a varian t which is\\nslowed down even further are quite competitive, especially in very high-\\ndimensional problems.\\nForward-stagewise regression is included in Figure 3.6. In this example it\\ntakes over 1000 steps to get all the correlations below 10−4. For subset size\\nk, we plotted the error for the last step for which there where knonzero\\ncoeﬃcients. Although it catches up with the best ﬁt, it takes longer to\\ndo so.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 78}),\n",
       " Document(page_content='3.4 Shrinkage Methods 61\\n3.3.4 Prostate Cancer Data Example (Continued)\\nTable 3.3 shows the coeﬃcients from a number of diﬀerent sele ction and\\nshrinkagemethods.Theyare best-subset selection usinganall-subsetssearch,\\nridge regression , thelasso,principal components regression andpartial least\\nsquares. Each method has a complexity parameter, and this was chosen to\\nminimize an estimate of prediction error based on tenfold cr oss-validation;\\nfulldetailsaregiveninSection7.10.Brieﬂy,cross-valid ationworksbydivid-\\ning the training data randomly into ten equal parts. The lear ning method\\nis ﬁt—for a range of values of the complexity parameter—to nine -tenths of\\nthe data, and the prediction error is computed on the remaini ng one-tenth.\\nThis is done in turn for each one-tenth of the data, and the ten prediction\\nerror estimates are averaged. From this we obtain an estimat ed prediction\\nerror curve as a function of the complexity parameter.\\nNote that we have already divided these data into a training s et of size\\n67 and a test set of size 30. Cross-validation is applied to th e training set,\\nsince selecting the shrinkage parameter is part of the train ing process. The\\ntest set is there to judge the performance of the selected mod el.\\nThe estimated prediction error curves are shown in Figure 3. 7. Many of\\nthe curves are very ﬂat over large ranges near their minimum. Included\\nare estimated standard error bands for each estimated error rate, based on\\nthe ten error estimates computed by cross-validation. We ha ve used the\\n“one-standard-error” rule—we pick the most parsimonious mo del within\\none standard error of the minimum (Section 7.10, page 244). S uch a rule\\nacknowledges the fact that the tradeoﬀ curve is estimated wi th error, and\\nhence takes a conservative approach.\\nBest-subset selection chose to use the two predictors lcvolandlweight.\\nThe last two lines of the table give the average prediction er ror (and its\\nestimated standard error) over the test set.\\n3.4 Shrinkage Methods\\nBy retaining a subset of the predictors and discarding the re st, subset selec-\\ntion produces a model that is interpretable and has possibly lower predic-\\ntion error than the full model. However, because it is a discr ete process—\\nvariables are either retained or discarded—it often exhibit s high variance,\\nand so doesn’t reduce the prediction error of the full model. Shrinkage\\nmethods are more continuous, and don’t suﬀer as much from hig h\\nvariability.\\n3.4.1 Ridge Regression\\nRidge regression shrinks the regression coeﬃcients by impo sing a penalty\\non their size. The ridge coeﬃcients minimize a penalized res idual sum of', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 79}),\n",
       " Document(page_content='62 3. Linear Methods for Regression\\nSubset SizeCV Error\\n0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•\\n•\\n•••••••All Subsets\\nDegrees of FreedomCV Error\\n0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•\\n•\\n•\\n••••••Ridge Regression\\nShrinkage Factor sCV Error\\n0.0 0.2 0.4 0.6 0.8 1.00.6 0.8 1.0 1.2 1.4 1.6 1.8•\\n•\\n•\\n••••••Lasso\\nNumber of DirectionsCV Error\\n0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•\\n••••••••Principal Components Regression\\nNumber of  DirectionsCV Error\\n0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•\\n••••••• •Partial Least Squares\\nFIGURE 3.7. Estimated prediction error curves and their standard error s for\\nthe various selection and shrinkage methods. Each curve is plo tted as a function\\nof the corresponding complexity parameter for that method. T he horizontal axis\\nhas been chosen so that the model complexity increases as we mo ve from left to\\nright. The estimates of prediction error and their standard errors were obtained by\\ntenfold cross-validation; full details are given in Section 7. 10. The least complex\\nmodel within one standard error of the best is chosen, indica ted by the purple\\nvertical broken lines.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 80}),\n",
       " Document(page_content='3.4 Shrinkage Methods 63\\nTABLE 3.3. Estimated coeﬃcients and test error results, for diﬀerent su bset\\nand shrinkage methods applied to the prostate data. The blank e ntries correspond\\nto variables omitted.\\nTerm LS Best Subset Ridge Lasso PCR PLS\\nIntercept 2.465 2.477 2.452 2.468 2.497 2.452\\nlcavol 0.680 0.740 0.420 0.533 0.543 0.419\\nlweight 0.263 0.316 0.238 0.169 0.289 0.344\\nage−0.141 −0.046 −0.152−0.026\\nlbph 0.210 0.162 0.002 0.214 0.220\\nsvi0.305 0.227 0.094 0.315 0.243\\nlcp−0.288 0.000 −0.051 0.079\\ngleason −0.021 0.040 0.232 0.011\\npgg45 0.267 0.133 −0.056 0.084\\nTest Error 0.521 0.492 0.492 0.479 0.449 0.528\\nStd Error 0.179 0.143 0.165 0.164 0.105 0.152\\nsquares,\\nˆβridge= argmin\\nβ{N∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2+λp∑\\nj=1β2\\nj}\\n.(3.41)\\nHereλ≥0 is a complexity parameter that controls the amount of shrin k-\\nage: the larger the value of λ, the greater the amount of shrinkage. The\\ncoeﬃcients are shrunk toward zero (and each other). The idea of penaliz-\\ning by the sum-of-squares of the parameters is also used in ne ural networks,\\nwhere it is known as weight decay (Chapter 11).\\nAn equivalent way to write the ridge problem is\\nˆβridge= argmin\\nβN∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2\\n,\\nsubject top∑\\nj=1β2\\nj≤t,(3.42)\\nwhich makes explicit the size constraint on the parameters. There is a one-\\nto-one correspondence between the parameters λin (3.41) and tin (3.42).\\nWhen there are many correlated variables in a linear regress ion model,\\ntheir coeﬃcients can become poorly determined and exhibit h igh variance.\\nA wildly large positive coeﬃcient on one variable can be canc eled by a\\nsimilarly large negative coeﬃcient on its correlated cousi n. By imposing a\\nsize constraint on the coeﬃcients, as in (3.42), this proble m is alleviated.\\nThe ridge solutions are not equivariant under scaling of the inputs, and\\nso one normally standardizes the inputs before solving (3.4 1). In addition,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 81}),\n",
       " Document(page_content='64 3. Linear Methods for Regression\\nnotice that the intercept β0has been left out of the penalty term. Penal-\\nization of the intercept would make the procedure depend on t he origin\\nchosen forY; that is, adding a constant cto each of the targets yiwould\\nnot simply result in a shift of the predictions by the same amo untc. It\\ncan be shown (Exercise 3.5) that the solution to (3.41) can be separated\\ninto two parts, after reparametrization using centered inputs: each xijgets\\nreplaced by xij−¯xj. We estimate β0by ¯y=1\\nN∑N\\n1yi. The remaining co-\\neﬃcients get estimated by a ridge regression without interc ept, using the\\ncenteredxij. Henceforth we assume that this centering has been done, so\\nthat the input matrix Xhasp(rather than p+1) columns.\\nWriting the criterion in (3.41) in matrix form,\\nRSS(λ) = (y−Xβ)T(y−Xβ)+λβTβ, (3.43)\\nthe ridge regression solutions are easily seen to be\\nˆβridge= (XTX+λI)−1XTy, (3.44)\\nwhereIis thep×pidentity matrix. Notice that with the choice of quadratic\\npenaltyβTβ, the ridge regression solution is again a linear function of\\ny. The solution adds a positive constant to the diagonal of XTXbefore\\ninversion. This makes the problem nonsingular, even if XTXis not of full\\nrank, and was the main motivation for ridge regression when i t was ﬁrst\\nintroducedinstatistics(HoerlandKennard,1970).Tradit ionaldescriptions\\nof ridge regression start with deﬁnition (3.44). We choose t o motivate it via\\n(3.41) and (3.42), as these provide insight into how it works .\\nFigure 3.8 shows the ridge coeﬃcient estimates for the prost ate can-\\ncer example, plotted as functions of df( λ), theeﬀective degrees of freedom\\nimplied by the penalty λ(deﬁned in (3.50) on page 68). In the case of or-\\nthonormal inputs, the ridge estimates are just a scaled vers ion of the least\\nsquares estimates, that is, ˆβridge=ˆβ/(1+λ).\\nRidge regression can also be derived as the mean or mode of a po ste-\\nrior distribution, with a suitably chosen prior distributi on. In detail, sup-\\nposeyi∼N(β0+xT\\niβ,σ2), and the parameters βjare each distributed as\\nN(0,τ2), independently of one another. Then the (negative) log-po sterior\\ndensity ofβ, withτ2andσ2assumed known, is equal to the expression\\nin curly braces in (3.41), with λ=σ2/τ2(Exercise 3.6). Thus the ridge\\nestimate is the mode of the posterior distribution; since th e distribution is\\nGaussian, it is also the posterior mean.\\nThesingular value decomposition (SVD) of the centered input matrix X\\ngives us some additional insight into the nature of ridge reg ression. This de-\\ncomposition is extremely useful in the analysis of many stat istical methods.\\nThe SVD of the N×pmatrixXhas the form\\nX=UDVT. (3.45)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 82}),\n",
       " Document(page_content='3.4 Shrinkage Methods 65Coefficients\\n0 2 4 6 8−0.2 0.0 0.2 0.4 0.6•\\n•••••\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n••••••\\n•lcavol\\n••••••••••••••••••••••••\\n•lweight\\n•••••••••••••••••••••••••\\nage••••••••••••••••••••••••\\n•lbph••••••••••••••••••••••••\\n•svi\\n••••••••••••••••••••••••\\n•\\nlcp••••••••••••••••••••••••\\n•gleason•\\n•••••••••••••••••••••••\\n•pgg45\\ndf(λ)\\nFIGURE 3.8. Proﬁles of ridge coeﬃcients for the prostate cancer example, a s\\nthe tuning parameter λis varied. Coeﬃcients are plotted versus df(λ), the eﬀective\\ndegrees of freedom. A vertical line is drawn at df = 5.0, the value chosen by\\ncross-validation.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 83}),\n",
       " Document(page_content='66 3. Linear Methods for Regression\\nHereUandVareN×pandp×porthogonal matrices, with the columns\\nofUspanning the column space of X, and the columns of Vspanning the\\nrow space. Dis ap×pdiagonal matrix, with diagonal entries d1≥d2≥\\n···≥dp≥0 called the singular values of X. If one or more values dj= 0,\\nXis singular.\\nUsing the singular value decomposition we can write the leas t squares\\nﬁtted vector as\\nXˆβls=X(XTX)−1XTy\\n=UUTy, (3.46)\\nafter some simpliﬁcation. Note that UTyare the coordinates of ywith\\nrespect to the orthonormal basis U. Note also the similarity with (3.33);\\nQandUare generally diﬀerent orthogonal bases for the column spac e of\\nX(Exercise 3.8).\\nNow the ridge solutions are\\nXˆβridge=X(XTX+λI)−1XTy\\n=U D(D2+λI)−1D UTy\\n=p∑\\nj=1ujd2\\nj\\nd2\\nj+λuT\\njy, (3.47)\\nwhere the ujare the columns of U. Note that since λ≥0, we haved2\\nj/(d2\\nj+\\nλ)≤1. Like linear regression, ridge regression computes the co ordinates of\\nywithrespecttotheorthonormalbasis U.Itthenshrinksthesecoordinates\\nby the factors d2\\nj/(d2\\nj+λ). This means that a greater amount of shrinkage\\nis applied to the coordinates of basis vectors with smaller d2\\nj.\\nWhat does a small value of d2\\njmean? The SVD of the centered matrix\\nXis another way of expressing the principal components of the variables\\ninX. The sample covariance matrix is given by S=XTX/N, and from\\n(3.45) we have\\nXTX=VD2VT, (3.48)\\nwhich is the eigen decomposition ofXTX(and ofS, up to a factor N).\\nThe eigenvectors vj(columns of V) are also called the principal compo-\\nnents(or Karhunen–Loeve) directions of X. The ﬁrst principal component\\ndirectionv1has the property that z1=Xv1has the largest sample vari-\\nance amongst all normalized linear combinations of the colu mns ofX. This\\nsample variance is easily seen to be\\nVar(z1) = Var(Xv1) =d2\\n1\\nN, (3.49)\\nand in fact z1=Xv1=u1d1. The derived variable z1is called the ﬁrst\\nprincipal component of X, and hence u1is the normalized ﬁrst principal', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 84}),\n",
       " Document(page_content='3.4 Shrinkage Methods 67\\n-4 -2 0 2 4-4 -2 0 2 4ooo\\nooooooo\\no\\no\\nooo\\noo\\noo\\noo\\nooo\\no\\no\\noooo\\noo\\no\\nooo\\nooo\\nooooo\\noo\\nooo\\nooo\\noo\\noooo\\noo\\no\\noo\\noo\\nooo\\nooo\\nooo\\nooo\\no\\noo\\nooooo\\no\\noooo\\nooo\\no\\noo\\no\\noo\\noo\\no\\noo\\noooooo\\nooo\\noo\\nooo\\noo\\no\\nooo\\noooo\\no\\nooo\\nooo\\no\\nooo\\nooo\\noo\\no\\noooo\\no\\noooo\\noo\\noo\\noo\\nooo\\no\\nooo\\noo\\noo\\no\\noo\\noo\\noooo\\no\\nooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooLargest Principal\\nComponent\\nSmallest Principal\\nComponent\\nX1X2\\nFIGURE 3.9. Principal components of some input data points. The largest p rin-\\ncipal component is the direction that maximizes the varianc e of the projected data,\\nand the smallest principal component minimizes that varianc e. Ridge regression\\nprojects yonto these components, and then shrinks the coeﬃcients of th e low–\\nvariance components more than the high-variance component s.\\ncomponent. Subsequent principal components zjhave maximum variance\\nd2\\nj/N, subject to being orthogonal to the earlier ones. Conversel y the last\\nprincipal component has minimum variance. Hence the small singular val-\\nuesdjcorrespond to directions in the column space of Xhaving small\\nvariance, and ridge regression shrinks these directions th e most.\\nFigure 3.9 illustrates the principal components of some dat a points in\\ntwo dimensions. If we consider ﬁtting a linear surface over t his domain\\n(theY-axis is sticking out of the page), the conﬁguration of the da ta allow\\nus to determine its gradient more accurately in the long dire ction than\\nthe short. Ridge regression protects against the potential ly high variance\\nof gradients estimated in the short directions. The implici t assumption is\\nthat the response will tend to vary most in the directions of h igh variance\\nof the inputs. This is often a reasonable assumption, since p redictors are\\noften chosen for study because they vary with the response va riable, but\\nneed not hold in general.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 85}),\n",
       " Document(page_content='68 3. Linear Methods for Regression\\nIn Figure 3.7 we have plotted the estimated prediction error versus the\\nquantity\\ndf(λ) = tr[ X(XTX+λI)−1XT],\\n= tr(Hλ)\\n=p∑\\nj=1d2\\nj\\nd2\\nj+λ. (3.50)\\nThis monotone decreasing function of λis theeﬀective degrees of freedom\\nof the ridge regression ﬁt. Usually in a linear-regression ﬁ t withpvariables,\\nthe degrees-of-freedom of the ﬁt is p, the number of free parameters. The\\nidea is that although all pcoeﬃcients in a ridge ﬁt will be non-zero, they\\nare ﬁt in a restricted fashion controlled by λ. Note that df( λ) =pwhen\\nλ= 0 (no regularization) and df( λ)→0 asλ→ ∞. Of course there\\nis always an additional one degree of freedom for the interce pt, which was\\nremoved apriori. This deﬁnition is motivated in more detail in Section 3.4.4\\nand Sections 7.4–7.6. In Figure 3.7 the minimum occurs at df( λ) = 5.0.\\nTable 3.3 shows that ridge regression reduces the test error of the full least\\nsquares estimates by a small amount.\\n3.4.2 The Lasso\\nThe lasso is a shrinkage method like ridge, with subtle but im portant dif-\\nferences. The lasso estimate is deﬁned by\\nˆβlasso= argmin\\nβN∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2\\nsubject top∑\\nj=1|βj|≤t. (3.51)\\nJust as in ridge regression, we can re-parametrize the const antβ0by stan-\\ndardizing the predictors; the solution for ˆβ0is ¯y, and thereafter we ﬁt a\\nmodel without an intercept (Exercise 3.5). In the signal pro cessing litera-\\nture, the lasso is also known as basis pursuit (Chen et al., 1998).\\nWe can also write the lasso problem in the equivalent Lagrangian form\\nˆβlasso= argmin\\nβ{1\\n2N∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2+λp∑\\nj=1|βj|}\\n.(3.52)\\nNotice the similarity to the ridge regression problem (3.42 ) or (3.41): the\\nL2ridge penalty∑p\\n1β2\\njis replaced by the L1lasso penalty∑p\\n1|βj|. This\\nlatter constraint makes the solutions nonlinear in the yi, and there is no\\nclosed form expression as in ridge regression. Computing th e lasso solution', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 86}),\n",
       " Document(page_content='3.4 Shrinkage Methods 69\\nis a quadratic programming problem, although we see in Secti on 3.4.4 that\\neﬃcient algorithms are available for computing the entire p ath of solutions\\nasλis varied, with the same computational cost as for ridge regr ession.\\nBecause of the nature of the constraint, making tsuﬃciently small will\\ncause some of the coeﬃcients to be exactly zero. Thus the lass o does a kind\\nof continuous subset selection. If tis chosen larger than t0=∑p\\n1|ˆβj|(where\\nˆβj=ˆβls\\nj, the least squares estimates), then the lasso estimates are theˆβj’s.\\nOn the other hand, for t=t0/2 say, then the least squares coeﬃcients are\\nshrunk by about 50% on average. However, the nature of the shr inkage\\nis not obvious, and we investigate it further in Section 3.4. 4 below. Like\\nthe subset size in variable subset selection, or the penalty parameter in\\nridge regression, tshould be adaptively chosen to minimize an estimate of\\nexpected prediction error.\\nIn Figure 3.7, for ease of interpretation, we have plotted th e lasso pre-\\ndiction error estimates versus the standardized parameter s=t/∑p\\n1|ˆβj|.\\nA value ˆs≈0.36 was chosen by 10-fold cross-validation; this caused four\\ncoeﬃcients to be set to zero (ﬁfth column of Table 3.3). The re sulting\\nmodel has the second lowest test error, slightly lower than t he full least\\nsquares model, but the standard errors of the test error esti mates (last line\\nof Table 3.3) are fairly large.\\nFigure 3.10 shows the lasso coeﬃcients as the standardized t uning pa-\\nrameters=t/∑p\\n1|ˆβj|is varied. At s= 1.0 these are the least squares\\nestimates; they decrease to 0 as s→0. This decrease is not always strictly\\nmonotonic, although it is in this example. A vertical line is drawn at\\ns= 0.36, the value chosen by cross-validation.\\n3.4.3 Discussion: Subset Selection, Ridge Regression and t he\\nLasso\\nInthissectionwediscussandcomparethethreeapproachesd iscussedsofar\\nfor restricting the linear regression model: subset select ion, ridge regression\\nand the lasso.\\nIn the case of an orthonormal input matrix Xthe three procedures have\\nexplicit solutions. Each method applies a simple transform ation to the least\\nsquares estimate ˆβj, as detailed in Table 3.4.\\nRidge regression does a proportional shrinkage. Lasso tran slates each\\ncoeﬃcient by a constant factor λ, truncating at zero. This is called “soft\\nthresholding,”andisusedinthecontextofwavelet-baseds moothinginSec-\\ntion 5.9. Best-subset selection drops all variables with co eﬃcients smaller\\nthan theMth largest; this is a form of “hard-thresholding.”\\nBack to the nonorthogonal case; some pictures help understa nd their re-\\nlationship. Figure 3.11 depicts the lasso (left) and ridge r egression (right)\\nwhen there are only two parameters. The residual sum of squar es has ellip-\\ntical contours, centered at the full least squares estimate . The constraint', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 87}),\n",
       " Document(page_content='70 3. Linear Methods for Regression\\n0.0 0.2 0.4 0.6 0.8 1.0−0.2 0.0 0.2 0.4 0.6\\nShrinkage Factor sCoefficientslcavol\\nlweight\\nagelbphsvi\\nlcpgleasonpgg45\\nFIGURE 3.10. Proﬁles of lasso coeﬃcients, as the tuning parameter tis varied.\\nCoeﬃcients are plotted versus s=t/∑p\\n1|ˆβj|. A vertical line is drawn at s= 0.36,\\nthe value chosen by cross-validation. Compare Figure 3.8 on pa ge 65; the lasso\\nproﬁles hit zero, while those for ridge do not. The proﬁles are pi ece-wise linear,\\nand so are computed only at the points displayed; see Section 3. 4.4 for details.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 88}),\n",
       " Document(page_content='3.4 Shrinkage Methods 71\\nTABLE 3.4. Estimators of βjin the case of orthonormal columns of X.Mandλ\\nare constants chosen by the corresponding techniques; signdenotes the sign of its\\nargument ( ±1), andx+denotes “positive part” of x. Below the table, estimators\\nare shown by broken red lines. The 45◦line in gray shows the unrestricted estimate\\nfor reference.\\nEstimator Formula\\nBest subset (size M)ˆβj·I(|ˆβj|≥|ˆβ(M)|)\\nRidge ˆβj/(1+λ)\\nLasso sign( ˆβj)(|ˆβj|−λ)+\\n(0,0) (0,0) (0,0)|ˆβ(M)|λBest Subset Ridge Lasso\\nβ^β^ 2. . β\\n1β2\\nβ1β\\nFIGURE 3.11. Estimation picture for the lasso (left) and ridge regression\\n(right). Shown are contours of the error and constraint func tions. The solid blue\\nareas are the constraint regions |β1|+|β2| ≤tandβ2\\n1+β2\\n2≤t2, respectively,\\nwhile the red ellipses are the contours of the least squares erro r function.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 89}),\n",
       " Document(page_content='72 3. Linear Methods for Regression\\nregion for ridge regression is the disk β2\\n1+β2\\n2≤t, while that for lasso is\\nthe diamond|β1|+|β2|≤t. Both methods ﬁnd the ﬁrst point where the\\nelliptical contours hit the constraint region. Unlike the d isk, the diamond\\nhas corners; if the solution occurs at a corner, then it has on e parameter\\nβjequal to zero. When p>2, the diamond becomes a rhomboid, and has\\nmany corners, ﬂat edges and faces; there are many more opport unities for\\nthe estimated parameters to be zero.\\nWe can generalize ridge regression and the lasso, and view th em as Bayes\\nestimates. Consider the criterion\\n˜β= argmin\\nβ{N∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2+λp∑\\nj=1|βj|q}\\n(3.53)\\nforq≥0. The contours of constant value of∑\\nj|βj|qare shown in Fig-\\nure 3.12, for the case of two inputs.\\nThinking of|βj|qas the log-prior density for βj, these are also the equi-\\ncontours of the prior distribution of the parameters. The va lueq= 0 corre-\\nspondstovariablesubsetselection,asthepenaltysimplyc ountsthenumber\\nof nonzero parameters; q= 1 corresponds to the lasso, while q= 2 to ridge\\nregression. Notice that for q≤1, the prior is not uniform in direction, but\\nconcentrates more mass in the coordinate directions. The pr ior correspond-\\ning to theq= 1 case is an independent double exponential (or Laplace)\\ndistribution for each input, with density (1 /2τ)exp(−|β|/τ) andτ= 1/λ.\\nThe caseq= 1 (lasso) is the smallest qsuch that the constraint region\\nis convex; non-convex constraint regions make the optimiza tion problem\\nmore diﬃcult.\\nIn this view, the lasso, ridge regression and best subset sel ection are\\nBayes estimates with diﬀerent priors. Note, however, that t hey are derived\\nas posterior modes, that is, maximizers of the posterior. It is more common\\nto use the mean of the posterior as the Bayes estimate. Ridge r egression is\\nalso the posterior mean, but the lasso and best subset select ion are not.\\nLooking again at the criterion (3.53), we might try using oth er values\\nofqbesides 0, 1, or 2. Although one might consider estimating qfrom\\nthe data, our experience is that it is not worth the eﬀort for t he extra\\nvariance incurred. Values of q∈(1,2) suggest a compromise between the\\nlasso and ridge regression. Although this is the case, with q >1,|βj|qis\\ndiﬀerentiable at 0, and so does not share the ability of lasso (q= 1) for\\nq= 4 q= 2 q= 1 q= 0.5 q= 0.1\\nFIGURE 3.12. Contours of constant value of∑\\nj|βj|qfor given values of q.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 90}),\n",
       " Document(page_content='3.4 Shrinkage Methods 73\\nq= 1.2 α= 0.2\\nLq Elastic Net\\nFIGURE 3.13. Contours of constant value of∑\\nj|βj|qforq= 1.2(left plot),\\nand the elastic-net penalty∑\\nj(αβ2\\nj+(1−α)|βj|)forα= 0.2(right plot). Although\\nvisually very similar, the elastic-net has sharp (non-diﬀeren tiable) corners, while\\ntheq= 1.2penalty does not.\\nsetting coeﬃcients exactly to zero. Partly for this reason a s well as for\\ncomputational tractability, Zou and Hastie (2005) introdu ced theelastic-\\nnetpenalty\\nλp∑\\nj=1(\\nαβ2\\nj+(1−α)|βj|)\\n, (3.54)\\na diﬀerent compromise between ridge and lasso. Figure 3.13 c ompares the\\nLqpenalty with q= 1.2 and the elastic-net penalty with α= 0.2; it is\\nhard to detect the diﬀerence by eye. The elastic-net selects variables like\\nthe lasso, and shrinks together the coeﬃcients of correlate d predictors like\\nridge. It also has considerable computational advantages o ver theLqpenal-\\nties. We discuss the elastic-net further in Section 18.4.\\n3.4.4 Least Angle Regression\\nLeast angle regression (LAR) is a relative newcomer (Efron e t al., 2004),\\nand can be viewed as a kind of “democratic” version of forward stepwise\\nregression (Section 3.3.2). As we will see, LAR is intimatel y connected\\nwith the lasso, and in fact provides an extremely eﬃcient alg orithm for\\ncomputing the entire lasso path as in Figure 3.10.\\nForward stepwise regression builds a model sequentially, a dding one vari-\\nable at a time. At each step, it identiﬁes the best variable to include in the\\nactive set , and then updates the least squares ﬁt to include all the acti ve\\nvariables.\\nLeast angle regression uses a similar strategy, but only ent ers “as much”\\nof a predictor as it deserves. At the ﬁrst step it identiﬁes th e variable\\nmost correlated with the response. Rather than ﬁt this varia ble completely,\\nLAR moves the coeﬃcient of this variable continuously towar d its least-\\nsquares value (causing its correlation with the evolving re sidual to decrease\\nin absolute value). As soon as another variable “catches up” in terms of\\ncorrelation with the residual, the process is paused. The se cond variable\\nthen joins the active set, and their coeﬃcients are moved tog ether in a way\\nthat keeps their correlations tied and decreasing. This pro cess is continued', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 91}),\n",
       " Document(page_content='74 3. Linear Methods for Regression\\nuntil all the variables are in the model, and ends at the full l east-squares\\nﬁt. Algorithm 3.2 provides the details. The termination con dition in step 5\\nrequires some explanation. If p>N−1, the LAR algorithm reaches a zero\\nresidual solution after N−1 steps (the−1 is because we have centered the\\ndata).\\nAlgorithm 3.2 Least Angle Regression.\\n1. Standardize the predictors to have mean zero and unit norm . Start\\nwith the residual r=y−¯y,β1,β2,...,β p= 0.\\n2. Find the predictor xjmost correlated with r.\\n3. Moveβjfrom 0 towards its least-squares coeﬃcient ⟨xj,r⟩, until some\\nother competitor xkhas as much correlation with the current residual\\nas doesxj.\\n4. Moveβjandβkin the direction deﬁned by their joint least squares\\ncoeﬃcient of the current residual on ( xj,xk), until some other com-\\npetitorxlhas as much correlation with the current residual.\\n5. Continue in this way until all ppredictors have been entered. After\\nmin(N−1,p) steps, we arrive at the full least-squares solution.\\nSupposeAkis the active set of variables at the beginning of the kth\\nstep, and let βAkbe the coeﬃcient vector for these variables at this step;\\nthere will be k−1 nonzero values, and the one just entered will be zero. If\\nrk=y−XAkβAkis the current residual, then the direction for this step is\\nδk= (XT\\nAkXAk)−1XT\\nAkrk. (3.55)\\nThe coeﬃcient proﬁle then evolves as βAk(α) =βAk+α·δk. Exercise 3.23\\nveriﬁes that the directions chosen in this fashion do what is claimed: keep\\nthe correlations tied and decreasing. If the ﬁt vector at the beginning of\\nthis step is ˆfk, then it evolves as ˆfk(α) =ˆfk+α·uk, whereuk=XAkδk\\nis the new ﬁt direction. The name “least angle” arises from a g eometrical\\ninterpretation of this process; ukmakes the smallest (and equal) angle\\nwith each of the predictors in Ak(Exercise 3.24). Figure 3.14 shows the\\nabsolute correlations decreasing and joining ranks with ea ch step of the\\nLAR algorithm, using simulated data.\\nBy construction the coeﬃcients in LAR change in a piecewise l inear fash-\\nion. Figure 3.15 [left panel] shows the LAR coeﬃcient proﬁle evolving as a\\nfunction of their L1arc length2. Note that we do not need to take small\\n2TheL1arc-length of a diﬀerentiable curve β(s) fors∈[0,S] is given by TV( β,S) =∫S\\n0||˙β(s)||1ds,where˙β(s) =∂β(s)/∂s. For the piecewise-linear LAR coeﬃcient proﬁle,\\nthis amounts to summing the L1norms of the changes in coeﬃcients from step to step.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 92}),\n",
       " Document(page_content='3.4 Shrinkage Methods 75\\n0 5 10 150.0 0.1 0.2 0.3 0.4v2 v6 v4 v5 v3 v1\\nL1Arc LengthAbsolute Correlations\\nFIGURE 3.14. Progression of the absolute correlations during each step of t he\\nLAR procedure, using a simulated data set with six predictors . The labels at the\\ntop of the plot indicate which variables enter the active set at each step. The step\\nlength are measured in units of L1arc length.\\n0 5 10 15−1.5 −1.0 −0.5 0.0 0.5Least Angle Regression\\n0 5 10 15−1.5 −1.0 −0.5 0.0 0.5Lasso\\nL1Arc Length L1Arc Length\\nCoeﬃcientsCoeﬃcients\\nFIGURE 3.15. Left panel shows the LAR coeﬃcient proﬁles on the simulated\\ndata, as a function of the L1arc length. The right panel shows the Lasso proﬁle.\\nThey are identical until the dark-blue coeﬃcient crosses zer o at an arc length of\\nabout18.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 93}),\n",
       " Document(page_content='76 3. Linear Methods for Regression\\nsteps and recheck the correlations in step 3; using knowledg e of the covari-\\nance of the predictors and the piecewise linearity of the alg orithm, we can\\nworkouttheexactsteplengthatthebeginningofeachstep(E xercise3.25).\\nThe right panel of Figure 3.15 shows the lasso coeﬃcient proﬁ les on the\\nsame data. They are almost identical to those in the left pane l, and diﬀer\\nfor the ﬁrst time when the blue coeﬃcient passes back through zero. For the\\nprostate data, the LAR coeﬃcient proﬁle turns out to be ident ical to the\\nlasso proﬁle in Figure 3.10, which never crosses zero. These observations\\nlead to a simple modiﬁcation of the LAR algorithm that gives t he entire\\nlasso path, which is also piecewise-linear.\\nAlgorithm 3.2a Least Angle Regression: Lasso Modiﬁcation .\\n4a. If a non-zero coeﬃcient hits zero, drop its variable from the active set\\nof variables and recompute the current joint least squares d irection.\\nTheLAR(lasso)algorithmisextremelyeﬃcient,requiringt hesameorder\\nof computation as that of a single least squares ﬁt using the ppredictors.\\nLeast angle regression always takes psteps to get to the full least squares\\nestimates. The lasso path can have more than psteps, although the two\\nare often quite similar. Algorithm 3.2 with the lasso modiﬁc ation 3.2a is\\nan eﬃcient way of computing the solution to any lasso problem , especially\\nwhenp≫N. Osborne et al. (2000a) also discovered a piecewise-linear path\\nfor computing the lasso, which they called a homotopy algorithm.\\nWenowgiveaheuristicargumentforwhytheseproceduresare sosimilar.\\nAlthough the LAR algorithm is stated in terms of correlation s, if the input\\nfeatures are standardized, it is equivalent and easier to wo rk with inner-\\nproducts. Suppose Ais the active set of variables at some stage in the\\nalgorithm, tied in their absolute inner-product with the cu rrent residuals\\ny−Xβ. We can express this as\\nxT\\nj(y−Xβ) =γ·sj,∀j∈A (3.56)\\nwheresj∈{−1,1}indicates the sign of the inner-product, and γis the\\ncommon value. Also |xT\\nk(y−Xβ)|≤γ∀k̸∈A. Now consider the lasso\\ncriterion (3.52), which we write in vector form\\nR(β) =1\\n2||y−Xβ||2\\n2+λ||β||1. (3.57)\\nLetBbe the active set of variables in the solution for a given valu e ofλ.\\nFor these variables R(β) is diﬀerentiable, and the stationarity conditions\\ngive\\nxT\\nj(y−Xβ) =λ·sign(βj),∀j∈B (3.58)\\nComparing (3.58) with (3.56), we see that they are identical only if the\\nsign ofβjmatches the sign of the inner product. That is why the LAR', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 94}),\n",
       " Document(page_content='3.4 Shrinkage Methods 77\\nalgorithm and lasso start to diﬀer when an active coeﬃcient p asses through\\nzero; condition (3.58) is violated for that variable, and it is kicked out of the\\nactive setB. Exercise 3.23 shows that these equations imply a piecewise -\\nlinear coeﬃcient proﬁle as λdecreases. The stationarity conditions for the\\nnon-active variables require that\\n|xT\\nk(y−Xβ)|≤λ,∀k̸∈B, (3.59)\\nwhich again agrees with the LAR algorithm.\\nFigure 3.16 compares LAR and lasso to forward stepwise and st agewise\\nregression. The setup is the same as in Figure 3.6 on page 59, e xcept here\\nN= 100 here rather than 300, so the problem is more diﬃcult. We s ee\\nthat the more aggressive forward stepwise starts to overﬁt q uite early (well\\nbefore the 10 true variables can enter the model), and ultima tely performs\\nworse than the slower forward stagewise regression. The beh avior of LAR\\nand lasso is similar to that of forward stagewise regression . Incremental\\nforward stagewise is similar to LAR and lasso, and is describ ed in Sec-\\ntion 3.8.1.\\nDegrees-of-Freedom Formula for LAR and Lasso\\nSuppose that we ﬁt a linear model via the least angle regressi on procedure,\\nstoppingatsomenumberofsteps k<p,orequivalentlyusingalassobound\\ntthat produces a constrained version of the full least square s ﬁt. How many\\nparameters, or “degrees of freedom” have we used?\\nConsiderﬁrstalinearregressionusingasubsetof kfeatures.Ifthissubset\\nis prespeciﬁed in advance without reference to the training data, then the\\ndegrees of freedom used in the ﬁtted model is deﬁned to be k. Indeed, in\\nclassical statistics, the number of linearly independent p arameters is what\\nis meant by “degrees of freedom.” Alternatively, suppose th at we carry out\\na best subset selection to determine the “optimal” set of kpredictors. Then\\nthe resulting model has kparameters, but in some sense we have used up\\nmore thankdegrees of freedom.\\nWe need a more general deﬁnition for the eﬀective degrees of f reedom of\\nan adaptively ﬁtted model. We deﬁne the degrees of freedom of the ﬁtted\\nvectorˆy= (ˆy1,ˆy2,...,ˆyN) as\\ndf(ˆy) =1\\nσ2N∑\\ni=1Cov(ˆyi,yi). (3.60)\\nHere Cov(ˆyi,yi) refers to the sampling covariance between the predicted\\nvalue ˆyiand its corresponding outcome value yi. This makes intuitive sense:\\nthe harder that we ﬁt to the data, the larger this covariance a nd hence\\ndf(ˆy). Expression (3.60) is a useful notion of degrees of freedom , one that\\ncan be applied to any model prediction ˆy. This includes models that are', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 95}),\n",
       " Document(page_content='78 3. Linear Methods for Regression\\n0.0 0.2 0.4 0.6 0.8 1.00.55 0.60 0.65Forward Stepwise\\nLAR\\nLasso\\nForward Stagewise\\nIncremental Forward StagewiseE||ˆβ(k)−β||2\\nFraction of L1arc-length\\nFIGURE 3.16. Comparison of LAR and lasso with forward stepwise, forward\\nstagewise (FS) and incremental forward stagewise (FS 0) regression. The setup\\nis the same as in Figure 3.6, except N= 100here rather than 300. Here the\\nslower FS regression ultimately outperforms forward stepwise . LAR and lasso\\nshow similar behavior to FS and FS 0. Since the procedures take diﬀerent numbers\\nof steps (across simulation replicates and methods), we plot th e MSE as a function\\nof the fraction of total L1arc-length toward the least-squares ﬁt.\\nadaptively ﬁtted to the training data. This deﬁnition is mot ivated and\\ndiscussed further in Sections 7.4–7.6.\\nNow for a linear regression with kﬁxed predictors, it is easy to show\\nthat df(ˆy) =k. Likewise for ridge regression, this deﬁnition leads to the\\nclosed-form expression (3.50) on page 68: df( ˆy) = tr(Sλ). In both these\\ncases, (3.60) is simple to evaluate because the ﬁt ˆy=Hλyis linear in y.\\nIf we think about deﬁnition (3.60) in the context of a best sub set selection\\nof sizek, it seems clear that df( ˆy) will be larger than k, and this can be\\nveriﬁed by estimating Cov(ˆ yi,yi)/σ2directly by simulation. However there\\nis no closed form method for estimating df( ˆy) for best subset selection.\\nFor LAR and lasso, something magical happens. These techniq ues are\\nadaptiveinasmootherwaythanbestsubsetselection,andhe nceestimation\\nof degrees of freedom is more tractable. Speciﬁcally it can b e shown that\\nafter thekth step of the LAR procedure, the eﬀective degrees of freedom of\\nthe ﬁt vector is exactly k. Now for the lasso, the (modiﬁed) LAR procedure', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 96}),\n",
       " Document(page_content='3.5 Methods Using Derived Input Directions 79\\noften takes more than psteps, since predictors can drop out. Hence the\\ndeﬁnitionisalittlediﬀerent;forthelasso,atanystagedf (ˆy)approximately\\nequals the number of predictors in the model. While this appr oximation\\nworks reasonably well anywhere in the lasso path, for each kit works best\\nat thelastmodel in the sequence that contains kpredictors. A detailed\\nstudy of the degrees of freedom for the lasso may be found in Zo u et al.\\n(2007).\\n3.5 Methods Using Derived Input Directions\\nIn many situations we have a large number of inputs, often ver y correlated.\\nThe methods in this section produce a small number of linear c ombinations\\nZm, m= 1,...,Mof the original inputs Xj, and theZmare then used in\\nplace of the Xjas inputs in the regression. The methods diﬀer in how the\\nlinear combinations are constructed.\\n3.5.1 Principal Components Regression\\nIn this approach the linear combinations Zmused are the principal com-\\nponents as deﬁned in Section 3.4.1 above.\\nPrincipal component regression forms the derived input col umnszm=\\nXvm, and then regresses yonz1,z2,...,zMfor someM≤p. Since the zm\\nare orthogonal, this regression is just a sum of univariate r egressions:\\nˆypcr\\n(M)= ¯y1+M∑\\nm=1ˆθmzm, (3.61)\\nwhereˆθm=⟨zm,y⟩/⟨zm,zm⟩. Since the zmare each linear combinations\\nof the original xj, we can express the solution (3.61) in terms of coeﬃcients\\nof thexj(Exercise 3.13):\\nˆβpcr(M) =M∑\\nm=1ˆθmvm. (3.62)\\nAs with ridge regression, principal components depend on th e scaling of\\nthe inputs, so typically we ﬁrst standardize them. Note that ifM=p, we\\nwould just get back the usual least squares estimates, since the columns of\\nZ=UDspan the column space of X. ForM <pwe get a reduced regres-\\nsion. We see that principal components regression is very si milar to ridge\\nregression: both operate via the principal components of th e input ma-\\ntrix. Ridge regression shrinks the coeﬃcients of the princi pal components\\n(Figure 3.17), shrinking more depending on the size of the co rresponding\\neigenvalue; principal components regression discards the p−Msmallest\\neigenvalue components. Figure 3.17 illustrates this.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 97}),\n",
       " Document(page_content='80 3. Linear Methods for Regression\\nIndexShrinkage Factor\\n2 4 6 80.0 0.2 0.4 0.6 0.8 1.0•\\n••\\n••••\\n•• • • • • • •\\n• •ridge\\npcr\\nFIGURE 3.17. Ridge regression shrinks the regression coeﬃcients of the p rin-\\ncipal components, using shrinkage factors d2\\nj/(d2\\nj+λ)as in (3.47). Principal\\ncomponent regression truncates them. Shown are the shrinka ge and truncation\\npatterns corresponding to Figure 3.7, as a function of the pr incipal component\\nindex.\\nIn Figure 3.7 we see that cross-validation suggests seven te rms; the re-\\nsulting model has the lowest test error in Table 3.3.\\n3.5.2 Partial Least Squares\\nThis technique also constructs a set of linear combinations of the inputs\\nfor regression, but unlike principal components regressio n it uses y(in ad-\\ndition to X) for this construction. Like principal component regressi on,\\npartial least squares (PLS) is not scale invariant, so we ass ume that each\\nxjis standardized to have mean 0 and variance 1. PLS begins by co m-\\nputing ˆϕ1j=⟨xj,y⟩for eachj. From this we construct the derived input\\nz1=∑\\njˆϕ1jxj, which is the ﬁrst partial least squares direction. Hence\\nin the construction of each zm, the inputs are weighted by the strength\\nof their univariate eﬀect on y3. The outcome yis regressed on z1giving\\ncoeﬃcient ˆθ1, and then we orthogonalize x1,...,xpwith respect to z1. We\\ncontinue this process, until M≤pdirections have been obtained. In this\\nmanner, partial least squares produces a sequence of derive d, orthogonal\\ninputs or directions z1,z2,...,zM. As with principal-component regres-\\nsion, if we were to construct all M=pdirections, we would get back a\\nsolution equivalent to the usual least squares estimates; u singM < pdi-\\nrections produces a reduced regression. The procedure is de scribed fully in\\nAlgorithm 3.3.\\n3Since the xjare standardized, the ﬁrst directions ˆ ϕ1jare the univariate regression\\ncoeﬃcients (up to an irrelevant constant); this is not the case for subsequen t directions.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 98}),\n",
       " Document(page_content='3.5 Methods Using Derived Input Directions 81\\nAlgorithm 3.3 Partial Least Squares.\\n1. Standardize each xjto have mean zero and variance one. Set ˆy(0)=\\n¯y1, andx(0)\\nj=xj, j= 1,...,p.\\n2. Form= 1,2,...,p\\n(a)zm=∑p\\nj=1ˆϕmjx(m−1)\\nj, where ˆϕmj=⟨x(m−1)\\nj,y⟩.\\n(b)ˆθm=⟨zm,y⟩/⟨zm,zm⟩.\\n(c)ˆy(m)=ˆy(m−1)+ˆθmzm.\\n(d) Orthogonalize each x(m−1)\\njwith respect to zm:x(m)\\nj=x(m−1)\\nj−\\n[⟨zm,x(m−1)\\nj⟩/⟨zm,zm⟩]zm,j= 1,2,...,p.\\n3. Output the sequence of ﬁtted vectors {ˆy(m)}p\\n1. Since the{zℓ}m\\n1are\\nlinear in the original xj, so isˆy(m)=Xˆβpls(m). These linear coeﬃ-\\ncients can be recovered from the sequence of PLS transformat ions.\\nIn the prostate cancer example, cross-validation chose M= 2 PLS direc-\\ntions in Figure 3.7. This produced the model given in the righ tmost column\\nof Table 3.3.\\nWhat optimization problem is partial least squares solving ? Since it uses\\nthe response yto construct its directions, its solution path is a nonlinea r\\nfunction of y. It can be shown (Exercise 3.15) that partial least squares\\nseeks directions that have high variance andhave high correlation with the\\nresponse, in contrast to principal components regression w hich keys only\\non high variance (Stone and Brooks, 1990; Frank and Friedman , 1993). In\\nparticular, the mth principal component direction vmsolves:\\nmaxαVar(Xα) (3.63)\\nsubject to||α||= 1, αTSvℓ= 0, ℓ= 1,...,m−1,\\nwhereSis the sample covariance matrix of the xj. The conditions αTSvℓ=\\n0 ensures that zm=Xαis uncorrelated with all the previous linear com-\\nbinations zℓ=Xvℓ. Themth PLS direction ˆ ϕmsolves:\\nmaxαCorr2(y,Xα)Var(Xα) (3.64)\\nsubject to||α||= 1, αTSˆϕℓ= 0, ℓ= 1,...,m−1.\\nFurther analysis reveals that the variance aspect tends to d ominate, and\\nso partial least squares behaves much like ridge regression and principal\\ncomponents regression. We discuss this further in the next s ection.\\nIf the input matrix Xis orthogonal, then partial least squares ﬁnds the\\nleast squares estimates after m= 1 steps. Subsequent steps have no eﬀect', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 99}),\n",
       " Document(page_content='82 3. Linear Methods for Regression\\nsince the ˆϕmjare zero for m>1 (Exercise 3.14). It can also be shown that\\nthe sequence of PLS coeﬃcients for m= 1,2,...,prepresents the conjugate\\ngradient sequence for computing the least squares solution s (Exercise 3.18).\\n3.6 Discussion: A Comparison of the Selection and\\nShrinkage Methods\\nThere are some simple settings where we can understand bette r the rela-\\ntionship between the diﬀerent methods described above. Con sider an exam-\\nple with two correlated inputs X1andX2, with correlation ρ. We assume\\nthat the true regression coeﬃcients are β1= 4 andβ2= 2. Figure 3.18\\nshows the coeﬃcient proﬁles for the diﬀerent methods, as the ir tuning pa-\\nrameters are varied. The top panel has ρ= 0.5, the bottom panel ρ=−0.5.\\nThe tuning parameters for ridge and lasso vary over a continu ous range,\\nwhile best subset, PLS and PCR take just two discrete steps to the least\\nsquares solution. In the top panel, starting at the origin, r idge regression\\nshrinks the coeﬃcients together until it ﬁnally converges t o least squares.\\nPLS and PCR show similar behavior to ridge, although are disc rete and\\nmore extreme. Best subset overshoots the solution and then b acktracks.\\nThe behavior of the lasso is intermediate to the other method s. When the\\ncorrelation is negative (lower panel), again PLS and PCR rou ghly track\\nthe ridge path, while all of the methods are more similar to on e another.\\nIt is interesting to compare the shrinkage behavior of these diﬀerent\\nmethods. Recall that ridge regression shrinks all directio ns, but shrinks\\nlow-variance directions more. Principal components regre ssion leaves M\\nhigh-variance directions alone, and discards the rest. Int erestingly, it can\\nbe shown that partial least squares also tends to shrink the l ow-variance\\ndirections, but can actually inﬂate some of the higher varia nce directions.\\nThis can make PLS a little unstable, and cause it to have sligh tly higher\\nprediction error compared to ridge regression. A full study is given in Frank\\nand Friedman (1993). These authors conclude that for minimi zing predic-\\ntion error, ridge regression is generally preferable to var iable subset selec-\\ntion, principal components regression and partial least sq uares. However\\nthe improvement over the latter two methods was only slight.\\nTo summarize, PLS, PCR and ridge regression tend to behave si milarly.\\nRidge regression may be preferred because it shrinks smooth ly, rather than\\nin discrete steps. Lasso falls somewhere between ridge regr ession and best\\nsubset regression, and enjoys some of the properties of each .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 100}),\n",
       " Document(page_content='3.6 Discussion: A Comparison of the Selection and Shrinkage Methods 83\\n0 1 2 3 4 5 6-1 0 1 2 3Least Squares\\n0Ridge\\nLasso\\nBest SubsetPLS PCR\\n•\\n0 1 2 3 4 5 6-1 0 1 2 3Least Squares\\nRidge\\nBest Subset\\nPLS\\nPCRLasso•\\n0ρ= 0.5\\nρ=−0.5\\nβ1β1β2 β2\\nFIGURE 3.18. Coeﬃcient proﬁles from diﬀerent methods for a simple problem:\\ntwo inputs with correlation ±0.5, and the true regression coeﬃcients β= (4,2).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 101}),\n",
       " Document(page_content='84 3. Linear Methods for Regression\\n3.7 Multiple Outcome Shrinkage and Selection\\nAs noted in Section 3.2.4, the least squares estimates in a mu ltiple-output\\nlinear model are simply the individual least squares estima tes for each of\\nthe outputs.\\nTo apply selection and shrinkage methods in the multiple out put case,\\none could apply a univariate technique individually to each outcome or si-\\nmultaneously to all outcomes. With ridge regression, for ex ample, we could\\napply formula (3.44) to each of the Kcolumns of the outcome matrix Y,\\nusing possibly diﬀerent parameters λ, or apply it to all columns using the\\nsame value of λ. The former strategy would allow diﬀerent amounts of\\nregularization to be applied to diﬀerent outcomes but requi re estimation\\nofkseparate regularization parameters λ1,...,λ k, while the latter would\\npermit allkoutputs to be used in estimating the sole regularization pa-\\nrameterλ.\\nOther more sophisticated shrinkage and selection strategi es that exploit\\ncorrelations in the diﬀerent responses can be helpful in the multiple output\\ncase. Suppose for example that among the outputs we have\\nYk=f(X)+εk (3.65)\\nYℓ=f(X)+εℓ; (3.66)\\ni.e., (3.65) and (3.66) share the same structural part f(X) in their models.\\nIt is clear in this case that we should pool our observations o nYkandYl\\nto estimate the common f.\\nCombining responses is at the heart of canonical correlation analysis\\n(CCA), a data reduction technique developed for the multipl e output case.\\nSimilar to PCA, CCA ﬁnds a sequence of uncorrelated linear co mbina-\\ntionsXvm, m= 1,...,M of thexj, and a corresponding sequence of\\nuncorrelated linear combinations Yumof the responses yk, such that the\\ncorrelations\\nCorr2(Yum,Xvm) (3.67)\\nare successively maximized. Note that at most M= min(K,p) directions\\ncan be found. The leading canonical response variates are th ose linear com-\\nbinations (derived responses) best predicted by the xj; in contrast, the\\ntrailing canonical variates can be poorly predicted by the xj, and are can-\\ndidates for being dropped. The CCA solution is computed usin g a general-\\nized SVD of the sample cross-covariance matrix YTX/N(assuming Yand\\nXare centered; Exercise 3.20).\\nReduced-rank regression (Izenman,1975;vanderMerweandZidek,1980)\\nformalizes this approach in terms of a regression model that explicitly pools\\ninformation. Given an error covariance Cov( ε) =Σ, we solve the following', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 102}),\n",
       " Document(page_content='3.7 Multiple Outcome Shrinkage and Selection 85\\nrestricted multivariate regression problem:\\nˆBrr(m) = argmin\\nrank(B)=mN∑\\ni=1(yi−BTxi)TΣ−1(yi−BTxi).(3.68)\\nWithΣreplaced by the estimate YTY/N, one can show (Exercise 3.21)\\nthat the solution is given by a CCA of YandX:\\nˆBrr(m) =ˆBUmU−\\nm, (3.69)\\nwhereUmis theK×msub-matrix of Uconsisting of the ﬁrst mcolumns,\\nandUis theK×Mmatrix of leftcanonical vectors u1,u2,...,u M.U−\\nm\\nis its generalized inverse. Writing the solution as\\nˆBrr(M) = (XTX)−1XT(YUm)U−\\nm, (3.70)\\nwe see that reduced-rank regression performs a linear regre ssion on the\\npooled response matrix YUm, and then maps the coeﬃcients (and hence\\nthe ﬁts as well) back to the original response space. The redu ced-rank ﬁts\\nare given by\\nˆYrr(m) =X(XTX)−1XTYUmU−\\nm\\n=HYPm,(3.71)\\nwhereHis the usual linear regression projection operator, and Pmis the\\nrank-mCCA response projection operator. Although a better estima te of\\nΣwould be( Y−XˆB)T(Y−XˆB)/(N−pK), onecanshowthat thesolution\\nremains the same (Exercise 3.22).\\nReduced-rank regression borrows strength among responses by truncat-\\ning the CCA. Breiman and Friedman (1997) explored with some s uccess\\nshrinkage of the canonical variates between XandY, a smooth version of\\nreduced rank regression. Their proposal has the form (compare (3.69))\\nˆBc+w=ˆBUΛU−1, (3.72)\\nwhereΛis a diagonal shrinkage matrix (the “c+w” stands for “Curds\\nand Whey,” the name they gave to their procedure). Based on op timal\\nprediction in the population setting, they show that Λhas diagonal entries\\nλm=c2\\nm\\nc2m+p\\nN(1−c2m), m= 1,...,M, (3.73)\\nwherecmis themth canonical correlation coeﬃcient. Note that as the ratio\\nof the number of input variables to sample size p/Ngets small, the shrink-\\nage factors approach 1. Breiman and Friedman (1997) propose d modiﬁed\\nversions of Λbased on training data and cross-validation, but the genera l\\nform is the same. Here the ﬁtted response has the form\\nˆYc+w=HYSc+w, (3.74)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 103}),\n",
       " Document(page_content='86 3. Linear Methods for Regression\\nwhereSc+w=UΛU−1is the response shrinkage operator.\\nBreiman and Friedman (1997) also suggested shrinking in bot h theY\\nspace andXspace. This leads to hybrid shrinkage models of the form\\nˆYridge,c+w=AλYSc+w, (3.75)\\nwhereAλ=X(XTX+λI)−1XTis the ridge regression shrinkage operator,\\nas in (3.46) on page 66. Their paper and the discussions there of contain\\nmany more details.\\n3.8 More on the Lasso and Related Path\\nAlgorithms\\nSince the publication of the LAR algorithm (Efron et al., 200 4) there has\\nbeen a lot of activity in developing algorithms for ﬁtting re gularization\\npaths for a variety of diﬀerent problems. In addition, L1regularization has\\ntaken on a life of its own, leading to the development of the ﬁe ldcompressed\\nsensingin the signal-processing literature. (Donoho, 2006a; Cand es, 2006).\\nInthissectionwediscusssomerelatedproposalsandotherp athalgorithms,\\nstarting oﬀ with a precursor to the LAR algorithm.\\n3.8.1 Incremental Forward Stagewise Regression\\nHere we present another LAR-like algorithm, this time focus ed on forward\\nstagewiseregression.Interestingly,eﬀortstounderstan daﬂexiblenonlinear\\nregression procedure (boosting) led to a new algorithm for l inear models\\n(LAR). In reading the ﬁrst edition of this book and the forwar d stagewise\\nAlgorithm 3.4 Incremental Forward Stagewise Regression—FS ǫ.\\n1. Start with the residual requal to yandβ1,β2,...,β p= 0. All the\\npredictors are standardized to have mean zero and unit norm.\\n2. Find the predictor xjmost correlated with r\\n3. Updateβj←βj+δj, whereδj=ǫ·sign[⟨xj,r⟩] andǫ>0 is a small\\nstep size, and set r←r−δjxj.\\n4. Repeat steps 2 and 3 many times, until the residuals are unc orrelated\\nwith all the predictors.\\nAlgorithm 16.1 of Chapter 164, our colleague Brad Efron realized that with\\n4In the ﬁrst edition, this was Algorithm 10.4 in Chapter 10.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 104}),\n",
       " Document(page_content='3.8 More on the Lasso and Related Path Algorithms 87−0.2 0.0 0.2 0.4 0.6lcavol\\nlweight\\nagelbphsvi\\nlcpgleasonpgg45\\n0 50 100 150 200\\n−0.2 0.0 0.2 0.4 0.6lcavol\\nlweight\\nagelbphsvi\\nlcpgleasonpgg45\\n0.0 0.5 1.0 1.5 2.0FSǫ FS0\\nIteration\\nCoeﬃcientsCoeﬃcients\\nL1Arc-length of Coeﬃcients\\nFIGURE 3.19. Coeﬃcient proﬁles for the prostate data. The left panel shows\\nincremental forward stagewise regression with step size ǫ= 0.01. The right panel\\nshows the inﬁnitesimal version FS 0obtained letting ǫ→0. This proﬁle was ﬁt by\\nthe modiﬁcation 3.2b to the LAR Algorithm 3.2. In this example t he FS 0proﬁles\\nare monotone, and hence identical to those of lasso and LAR.\\nlinearmodels,onecouldexplicitlyconstructthepiecewis e-linearlassopaths\\nof Figure 3.10. This led him to propose the LAR procedure of Se ction 3.4.4,\\nas well as the incremental version of forward-stagewise reg ression presented\\nhere.\\nConsider the linear-regression version of the forward-sta gewise boosting\\nalgorithm16.1proposedinSection16.1(page608).Itgener atesacoeﬃcient\\nproﬁle by repeatedly updating (by a small amount ǫ) the coeﬃcient of the\\nvariable most correlated with the current residuals. Algor ithm 3.4 gives\\nthe details. Figure 3.19 (left panel) shows the progress of t he algorithm on\\nthe prostate data with step size ǫ= 0.01. Ifδj=⟨xj,r⟩(the least-squares\\ncoeﬃcient of the residual on jth predictor), then this is exactly the usual\\nforward stagewise procedure (FS) outlined in Section 3.3.3 .\\nHere we are mainly interested in small values of ǫ. Lettingǫ→0 gives\\nthe right panel of Figure 3.19, which in this case is identica l to the lasso\\npath in Figure 3.10. We call this limiting procedure inﬁnitesimal forward\\nstagewise regression or FS0. This procedure plays an important role in\\nnon-linear, adaptive methods like boosting (Chapters 10 an d 16) and is the\\nversion of incremental forward stagewise regression that i s most amenable\\nto theoretical analysis. B¨ uhlmann and Hothorn (2007) refe r to the same\\nprocedure as “L2boost”, because of its connections to boost ing.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 105}),\n",
       " Document(page_content='88 3. Linear Methods for Regression\\nEfron originally thought that the LAR Algorithm 3.2 was an im plemen-\\ntation of FS 0, allowing each tied predictor a chance to update their coeﬃ-\\ncients in a balanced way, while remaining tied in correlatio n. However, he\\nthen realized that the LAR least-squares ﬁt amongst the tied predictors\\ncan result in coeﬃcients moving in the opposite direction to their correla-\\ntion, which cannot happen in Algorithm 3.4. The following mo diﬁcation of\\nthe LAR algorithm implements FS 0:\\nAlgorithm 3.2b Least Angle Regression: FS 0Modiﬁcation .\\n4. Find the new direction by solving the constrained least sq uares prob-\\nlem\\nmin\\nb||r−XAb||2\\n2subject tobjsj≥0, j∈A,\\nwheresjis the sign of⟨xj,r⟩.\\nThe modiﬁcation amounts to a non-negative least squares ﬁt, keeping the\\nsigns of the coeﬃcients the same as those of the correlations . One can show\\nthat this achieves the optimal balancing of inﬁnitesimal “u pdate turns”\\nfor the variables tied for maximal correlation (Hastie et al ., 2007). Like\\nlasso, the entire FS 0path can be computed very eﬃciently via the LAR\\nalgorithm.\\nAs a consequence of these results, if the LAR proﬁles are mono tone non-\\nincreasing or non-decreasing, as they are in Figure 3.19, th en all three\\nmethods—LAR, lasso, and FS 0—give identical proﬁles. If the proﬁles are\\nnot monotone but do not cross the zero axis, then LAR and lasso are\\nidentical.\\nSince FS 0is diﬀerent from the lasso, it is natural to ask if it optimize s\\na criterion. The answer is more complex than for lasso; the FS 0coeﬃcient\\nproﬁle is the solution to a diﬀerential equation. While the l asso makes op-\\ntimal progress in terms of reducing the residual sum-of-squ ares per unit\\nincrease in L1-norm of the coeﬃcient vector β, FS0is optimal per unit\\nincrease in L1arc-length traveled along the coeﬃcient path. Hence its co-\\neﬃcient path is discouraged from changing directions too of ten.\\nFS0is more constrained than lasso, and in fact can be viewed as a m ono-\\ntone version of the lasso; see Figure 16.3 on page 614 for a dra matic exam-\\nple. FS 0may be useful in p≫Nsituations, where its coeﬃcient proﬁles\\nare much smoother and hence have less variance than those of l asso. More\\ndetails on FS 0are given in Section 16.2.3 and Hastie et al. (2007). Fig-\\nure 3.16 includes FS 0where its performance is very similar to that of the\\nlasso.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 106}),\n",
       " Document(page_content='3.8 More on the Lasso and Related Path Algorithms 89\\n3.8.2 Piecewise-Linear Path Algorithms\\nThe least angle regression procedure exploits the piecewis e linear nature of\\nthe lasso solution paths. It has led to similar “path algorit hms” for other\\nregularized problems. Suppose we solve\\nˆβ(λ) = argminβ[R(β)+λJ(β)], (3.76)\\nwith\\nR(β) =N∑\\ni=1L(yi,β0+p∑\\nj=1xijβj), (3.77)\\nwhere both the loss function Land the penalty function Jare convex.\\nThen the following are suﬃcient conditions for the solution pathˆβ(λ) to\\nbe piecewise linear (Rosset and Zhu, 2007):\\n1.Ris quadratic or piecewise-quadratic as a function of β, and\\n2.Jis piecewise linear in β.\\nThis also implies (in principle) that the solution path can b e eﬃciently\\ncomputed. Examples include squared- and absolute-error lo ss, “Huberized”\\nlosses, and the L1,L∞penalties on β. Another example is the “hinge loss”\\nfunction used in the support vector machine. There the loss i s piecewise\\nlinear, and the penalty is quadratic. Interestingly, this l eads to a piecewise-\\nlinear path algorithm in the dual space ; more details are given in Sec-\\ntion 12.3.5.\\n3.8.3 The Dantzig Selector\\nCandes and Tao (2007) proposed the following criterion:\\nminβ||β||1subject to||XT(y−Xβ)||∞≤s. (3.78)\\nThey call the solution the Dantzig selector (DS). It can be written equiva-\\nlently as\\nminβ||XT(y−Xβ)||∞subject to||β||1≤t. (3.79)\\nHere||·||∞denotes the L∞norm, the maximum absolute value of the\\ncomponents of the vector. In this form it resembles the lasso , replacing\\nsquared error loss by the maximum absolute value of its gradi ent. Note\\nthat astgets large, both procedures yield the least squares solutio n if\\nN <p. Ifp≥N, they both yield the least squares solution with minimum\\nL1norm. However for smaller values of t, the DS procedure produces a\\ndiﬀerent path of solutions than the lasso.\\nCandes and Tao (2007) show that the solution to DS is a linear p ro-\\ngramming problem; hence the name Dantzig selector, in honor of the late', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 107}),\n",
       " Document(page_content='90 3. Linear Methods for Regression\\nGeorge Dantzig, the inventor of the simplex method for linea r program-\\nming. They also prove a number of interesting mathematical p roperties for\\nthe method, related to its ability to recover an underlying s parse coeﬃ-\\ncient vector. These same properties also hold for the lasso, as shown later\\nby Bickel et al. (2008).\\nUnfortunately the operating properties of the DS method are somewhat\\nunsatisfactory. The method seems similar in spirit to the la sso, especially\\nwhen we look at the lasso’s stationary conditions (3.58). Li ke the LAR al-\\ngorithm, the lasso maintains the same inner product (and cor relation) with\\nthe current residual for all variables in the active set, and moves their co-\\neﬃcients to optimally decrease the residual sum of squares. In the process,\\nthis common correlation is decreased monotonically (Exerc ise 3.23), and at\\nall times this correlation is larger than that for non-activ e variables. The\\nDantzig selector instead tries to minimize the maximum inne r product of\\nthe current residual with all the predictors. Hence it can ac hieve a smaller\\nmaximum than the lasso, but in the process a curious phenomen on can\\noccur. If the size of the active set is m, there will be mvariables tied with\\nmaximum correlation. However, these need not coincide with the active set!\\nHence it can include a variable in the model that has smaller c orrelation\\nwith the current residual than some of the excluded variable s (Efron et\\nal., 2007). This seems unreasonable and may be responsible f or its some-\\ntimes inferior prediction accuracy. Efron et al. (2007) als o show that DS\\ncanyieldextremelyerraticcoeﬃcientpathsastheregulari zation parameter\\nsis varied.\\n3.8.4 The Grouped Lasso\\nIn some problems, the predictors belong to pre-deﬁned group s; for example\\ngenes that belong to the same biological pathway, or collect ions of indicator\\n(dummy) variables for representing the levels of a categori cal predictor. In\\nthis situation it may be desirable to shrink and select the me mbers of a\\ngroup together. The grouped lasso is one way to achieve this. Suppose that\\ntheppredictors are divided into Lgroups, with pℓthe number in group\\nℓ. For ease of notation, we use a matrix Xℓto represent the predictors\\ncorresponding to the ℓth group, with corresponding coeﬃcient vector βℓ.\\nThe grouped-lasso minimizes the convex criterion\\nmin\\nβ∈IRp(\\n||y−β01−L∑\\nℓ=1Xℓβℓ||2\\n2+λL∑\\nℓ=1√pℓ||βℓ||2)\\n, (3.80)\\nwhere the√pℓterms accounts for the varying group sizes, and ||·||2is\\nthe Euclidean norm (not squared). Since the Euclidean norm o f a vector\\nβℓis zero only if all of its components are zero, this procedure encourages\\nsparsity at both the group and individual levels. That is, fo r some values of\\nλ, an entire group of predictors may drop out of the model. This procedure', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 108}),\n",
       " Document(page_content='3.8 More on the Lasso and Related Path Algorithms 91\\nwas proposed by Bakin (1999) and Lin and Zhang (2006), and stu died and\\ngeneralized by Yuan and Lin (2007). Generalizations includ e more general\\nL2norms||η||K= (ηTKη)1/2, as well as allowing overlapping groups of\\npredictors (Zhao et al., 2008). There are also connections t o methods for\\nﬁtting sparse additive models (Lin and Zhang, 2006; Ravikum ar et al.,\\n2008).\\n3.8.5 Further Properties of the Lasso\\nA number of authors have studied the ability of the lasso and r elated pro-\\ncedures to recover the correct model, as Nandpgrow. Examples of this\\nwork include Knight and Fu (2000), Greenshtein and Ritov (20 04), Tropp\\n(2004), Donoho (2006b), Meinshausen (2007), Meinshausen a nd B¨ uhlmann\\n(2006), Tropp (2006), Zhao and Yu (2006), Wainwright (2006) , and Bunea\\net al. (2007). For example Donoho (2006b) focuses on the p>Ncase and\\nconsiders the lasso solution as the bound tgets large. In the limit this gives\\nthe solution with minimum L1norm among all models with zero training\\nerror. He shows that under certain assumptions on the model m atrixX, if\\nthe true model is sparse, this solution identiﬁes the correc t predictors with\\nhigh probability.\\nMany of the results in this area assume a condition on the mode l matrix\\nof the form\\nmax\\nj∈Sc||xT\\njXS(XSTXS)−1||1≤(1−ǫ) for someǫ∈(0,1].(3.81)\\nHereSindexes the subset of features with non-zero coeﬃcients in t he true\\nunderlying model, and XSare the columns of Xcorresponding to those\\nfeatures. Similarly Scare the features with true coeﬃcients equal to zero,\\nandXScthe corresponding columns. This says that the least squares coef-\\nﬁcients for the columns of XSconXSare not too large, that is, the “good”\\nvariablesSare not too highly correlated with the nuisance variables Sc.\\nRegarding the coeﬃcients themselves, the lasso shrinkage c auses the esti-\\nmates of the non-zero coeﬃcients to be biased towards zero, a nd in general\\nthey are not consistent5. One approach for reducing this bias is to run\\nthe lasso to identify the set of non-zero coeﬃcients, and the n ﬁt an un-\\nrestricted linear model to the selected set of features. Thi s is not always\\nfeasible, if the selected set is large. Alternatively, one c an use the lasso to\\nselect the set of non-zero predictors, and then apply the las so again, but\\nusing only the selected predictors from the ﬁrst step. This i s known as the\\nrelaxed lasso (Meinshausen, 2007). The idea is to use cross-validation to\\nestimate the initial penalty parameter for the lasso, and th en again for a\\nsecond penalty parameter applied to the selected set of pred ictors. Since\\n5Statistical consistency means as the sample size grows, the estimates converge to\\nthe true values.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 109}),\n",
       " Document(page_content='92 3. Linear Methods for Regression\\nthe variables in the second step have less “competition” fro m noise vari-\\nables, cross-validation will tend to pick a smaller value fo rλ, and hence\\ntheir coeﬃcients will be shrunken less than those in the init ial estimate.\\nAlternatively,onecanmodifythelassopenaltyfunctionso thatlargerco-\\neﬃcients are shrunken less severely; the smoothly clipped absolute deviation\\n(SCAD) penalty of Fan and Li (2005) replaces λ|β|byJa(β,λ), where\\ndJa(β,λ)\\ndβ=λ·sign(β)[\\nI(|β|≤λ)+(aλ−|β|)+\\n(a−1)λI(|β|>λ)]\\n(3.82)\\nfor somea≥2. The second term in square-braces reduces the amount of\\nshrinkage in the lasso for larger values of β, with ultimately no shrinkage\\nasa→∞. Figure 3.20 shows the SCAD penalty, along with the lasso and\\n−4 −2 0 2 40 1 2 3 4 5\\n−4 −2 0 2 40.0 0.5 1.0 1.5 2.0 2.5\\n−4 −2 0 2 40.5 1.0 1.5 2.0|β| SCAD |β|1−ν\\nβ β β\\nFIGURE 3.20. The lasso and two alternative non-convex penalties designed to\\npenalize large coeﬃcients less. For SCAD we use λ= 1anda= 4, andν=1\\n2in\\nthe last panel.\\n|β|1−ν. However this criterion is non-convex, which is a drawback s ince it\\nmakes the computation much more diﬃcult. The adaptive lasso (Zou, 2006)\\nuses a weighted penalty of the form∑p\\nj=1wj|βj|wherewj= 1/|ˆβj|ν,ˆβjis\\nthe ordinary least squares estimate and ν >0. This is a practical approxi-\\nmation to the|β|qpenalties (q= 1−νhere) discussed in Section 3.4.3. The\\nadaptive lasso yields consistent estimates of the paramete rs while retaining\\nthe attractive convexity property of the lasso.\\n3.8.6 Pathwise Coordinate Optimization\\nAn alternate approach to the LARS algorithm for computing th e lasso\\nsolution is simple coordinate descent. This idea was propos ed by Fu (1998)\\nandDaubechiesetal.(2004),andlaterstudiedandgenerali zedbyFriedman\\netal.(2007),WuandLange(2008)andothers.Theideaistoﬁx thepenalty\\nparameterλin the Lagrangian form (3.52) and optimize successively ove r\\neach parameter, holding the other parameters ﬁxed at their c urrent values.\\nSuppose the predictors are all standardized to have mean zer o and unit\\nnorm. Denote by ˜βk(λ) the current estimate for βkat penalty parameter', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 110}),\n",
       " Document(page_content='3.9 Computational Considerations 93\\nλ. We can rearrange (3.52) to isolate βj,\\nR(˜β(λ),βj) =1\\n2N∑\\ni=1(\\nyi−∑\\nk̸=jxik˜βk(λ)−xijβj)2\\n+λ∑\\nk̸=j|˜βk(λ)|+λ|βj|,\\n(3.83)\\nwhere we have suppressed the intercept and introduced a fact or1\\n2for con-\\nvenience. This can be viewed as a univariate lasso problem wi th response\\nvariable the partial residual yi−˜y(j)\\ni=yi−∑\\nk̸=jxik˜βk(λ). This has an\\nexplicit solution, resulting in the update\\n˜βj(λ)←S(N∑\\ni=1xij(yi−˜y(j)\\ni),λ)\\n. (3.84)\\nHereS(t,λ) = sign(t)(|t|−λ)+isthesoft-thresholdingoperatorinTable3.4\\non page 71. The ﬁrst argument to S(·) is the simple least-squares coeﬃcient\\nof the partial residual on the standardized variable xij. Repeated iteration\\nof (3.84)—cycling through each variable in turn until conver gence—yields\\nthe lasso estimate ˆβ(λ).\\nWe can also use this simple algorithm to eﬃciently compute th e lasso\\nsolutions at a grid of values of λ. We start with the smallest value λmax\\nfor which ˆβ(λmax) = 0, decrease it a little and cycle through the variables\\nuntil convergence. Then λis decreased again and the process is repeated,\\nusing the previous solution as a “warm start” for the new valu e ofλ. This\\ncan be faster than the LARS algorithm, especially in large pr oblems. A\\nkey to its speed is the fact that the quantities in (3.84) can b e updated\\nquickly asjvaries, and often the update is to leave ˜βj= 0. On the other\\nhand, it delivers solutions over a grid of λvalues, rather than the entire\\nsolution path. The same kind of algorithm can be applied to th e elastic\\nnet, the grouped lasso and many other models in which the pena lty is a\\nsum of functions of the individual parameters (Friedman et a l., 2010). It\\ncan also be applied, with some substantial modiﬁcations, to the fused lasso\\n(Section 18.4.2); details are in Friedman et al. (2007).\\n3.9 Computational Considerations\\nLeast squares ﬁtting is usually done via the Cholesky decomp osition of\\nthe matrix XTXor a QR decomposition of X. WithNobservations and p\\nfeatures, the Cholesky decomposition requires p3+Np2/2 operations, while\\nthe QR decomposition requires Np2operations. Depending on the relative\\nsize ofNandp, the Cholesky can sometimes be faster; on the other hand,\\nit can be less numerically stable (Lawson and Hansen, 1974). Computation\\nof the lasso via the LAR algorithm has the same order of comput ation as\\na least squares ﬁt.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 111}),\n",
       " Document(page_content='94 3. Linear Methods for Regression\\nBibliographic Notes\\nLinear regression is discussed in many statistics books, fo r example, Seber\\n(1984), Weisberg (1980) and Mardia et al. (1979). Ridge regr ession was\\nintroduced by Hoerl and Kennard (1970), while the lasso was p roposed by\\nTibshirani (1996). Around the same time, lasso-type penalt ies were pro-\\nposed in the basis pursuit method for signal processing (Chen et al., 1998).\\nThe least angle regression procedure was proposed in Efron e t al. (2004);\\nrelated to this is the earlier homotopy procedure of Osborne et al. (2000a)\\nand Osborne et al. (2000b). Their algorithm also exploits th e piecewise\\nlinearity used in the LAR/lasso algorithm, but lacks its tra nsparency. The\\ncriterion for the forward stagewise criterion is discussed in Hastie et al.\\n(2007). Park and Hastie (2007) develop a path algorithm simi lar to least\\nangle regression for generalized regression models. Parti al least squares\\nwas introduced by Wold (1975). Comparisons of shrinkage met hods may\\nbe found in Copas (1983) and Frank and Friedman (1993).\\nExercises\\nEx. 3.1Show that the Fstatistic (3.13) for dropping a single coeﬃcient\\nfrom a model is equal to the square of the corresponding z-score (3.12).\\nEx. 3.2Given data on two variables XandY, consider ﬁtting a cubic\\npolynomial regression model f(X) =∑3\\nj=0βjXj. In addition to plotting\\nthe ﬁtted curve, you would like a 95% conﬁdence band about the curve.\\nConsider the following two approaches:\\n1. At each point x0, form a 95% conﬁdence interval for the linear func-\\ntionaTβ=∑3\\nj=0βjxj\\n0.\\n2. Form a 95% conﬁdence set for βas in (3.15), which in turn generates\\nconﬁdence intervals for f(x0).\\nHow do these approaches diﬀer? Which band is likely to be wide r? Conduct\\na small simulation experiment to compare the two methods.\\nEx. 3.3Gauss–Markov theorem:\\n(a) Prove the Gauss–Markov theorem: the least squares estim ate of a\\nparameteraTβhas variance no bigger than that of any other linear\\nunbiased estimate of aTβ(Section 3.2.2).\\n(b) The matrix inequality B⪯Aholds ifA−Bis positive semideﬁnite.\\nShow that if ˆVis the variance-covariance matrix of the least squares\\nestimate of βand˜Vis the variance-covariance matrix of any other\\nlinear unbiased estimate, then ˆV⪯˜V.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 112}),\n",
       " Document(page_content='Exercises 95\\nEx. 3.4Show how the vector of least squares coeﬃcients can be obtain ed\\nfrom a single pass of the Gram–Schmidt procedure (Algorithm 3.1). Rep-\\nresent your solution in terms of the QR decomposition of X.\\nEx. 3.5Consider the ridge regression problem (3.41). Show that thi s prob-\\nlem is equivalent to the problem\\nˆβc= argmin\\nβc{N∑\\ni=1[\\nyi−βc\\n0−p∑\\nj=1(xij−¯xj)βc\\nj]2+λp∑\\nj=1βc\\nj2}\\n.(3.85)\\nGive the correspondence between βcand the original βin (3.41). Char-\\nacterize the solution to this modiﬁed criterion. Show that a similar result\\nholds for the lasso.\\nEx. 3.6Show that the ridge regression estimate is the mean (and mode )\\nof the posterior distribution, under a Gaussian prior β∼N(0,τI), and\\nGaussian sampling model y∼N(Xβ,σ2I). Find the relationship between\\nthe regularization parameter λin the ridge formula, and the variances τ\\nandσ2.\\nEx. 3.7Assumeyi∼N(β0+xT\\niβ,σ2),i= 1,2,...,N, and the parameters\\nβj, j= 1,...,pare each distributed as N(0,τ2), independently of one\\nanother. Assuming σ2andτ2are known, and β0is not governed by a\\nprior (or has a ﬂat improper prior), show that the (minus) log -posterior\\ndensity ofβis proportional to∑N\\ni=1(yi−β0−∑\\njxijβj)2+λ∑p\\nj=1β2\\nj\\nwhereλ=σ2/τ2.\\nEx. 3.8Consider the QR decomposition of the uncentered N×(p+ 1)\\nmatrixX(whose ﬁrst column is all ones), and the SVD of the N×p\\ncentered matrix ˜X. Show that Q2andUspan the same subspace, where\\nQ2is the sub-matrix of Qwith the ﬁrst column removed. Under what\\ncircumstances will they be the same, up to sign ﬂips?\\nEx. 3.9Forward stepwise regression. Suppose we have the QR decomposi-\\ntion for the N×qmatrixX1in a multiple regression problem with response\\ny, and we have an additional p−qpredictors in the matrix X2. Denote the\\ncurrent residual by r. We wish to establish which one of these additional\\nvariables will reduce the residual-sum-of squares the most when included\\nwith those in X1. Describe an eﬃcient procedure for doing this.\\nEx. 3.10 Backward stepwise regression. Suppose we have the multiple re-\\ngression ﬁt of yonX, along with the standard errors and Z-scores as in\\nTable 3.2. We wish to establish which variable, when dropped , will increase\\nthe residual sum-of-squares the least. How would you do this ?\\nEx. 3.11 Show that the solution to the multivariate linear regressio n prob-\\nlem (3.40) is given by (3.39). What happens if the covariance matrices Σi\\nare diﬀerent for each observation?', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 113}),\n",
       " Document(page_content='96 3. Linear Methods for Regression\\nEx. 3.12 Show that the ridge regression estimates can be obtained by\\nordinary least squares regression on an augmented data set. We augment\\nthe centered matrix Xwithpadditional rows√\\nλI, and augment ywithp\\nzeros. By introducing artiﬁcial data having response value zero, the ﬁtting\\nprocedure is forced to shrink the coeﬃcients toward zero. Th is is related to\\nthe idea of hintsdue to Abu-Mostafa (1995), where model constraints are\\nimplemented by adding artiﬁcial data examples that satisfy them.\\nEx. 3.13 Derive the expression (3.62), and show that ˆβpcr(p) =ˆβls.\\nEx. 3.14 Show that in the orthogonal case, PLS stops after m= 1 steps,\\nbecause subsequent ˆ ϕmjin step 2 in Algorithm 3.3 are zero.\\nEx. 3.15 Verify expression (3.64), and hence show that the partial le ast\\nsquares directions are a compromise between the ordinary re gression coef-\\nﬁcient and the principal component directions.\\nEx. 3.16 Derive the entries in Table 3.4, the explicit forms for estim ators\\nin the orthogonal case.\\nEx. 3.17 Repeat the analysis of Table 3.3 on the spam data discussed in\\nChapter 1.\\nEx.3.18 Readaboutconjugategradientalgorithms(Murrayetal.,19 81,for\\nexample), and establish a connection between these algorit hms and partial\\nleast squares.\\nEx. 3.19 Show that∥ˆβridge∥increases as its tuning parameter λ→0. Does\\nthe same property hold for the lasso and partial least square s estimates?\\nFor the latter, consider the “tuning parameter” to be the suc cessive steps\\nin the algorithm.\\nEx. 3.20 Consider the canonical-correlation problem (3.67). Show t hat the\\nleading pair of canonical variates u1andv1solve the problem\\nmax\\nuT(YTY)u=1\\nvT(XTX)v=1uT(YTX)v, (3.86)\\na generalized SVD problem. Show that the solution is given by u1=\\n(YTY)−1\\n2u∗\\n1, andv1= (XTX)−1\\n2v∗\\n1, whereu∗\\n1andv∗\\n1are the leading left\\nand right singular vectors in\\n(YTY)−1\\n2(YTX)(XTX)−1\\n2=U∗D∗V∗T. (3.87)\\nShow that the entire sequence um, vm, m= 1,...,min(K,p) is also given\\nby (3.87).\\nEx. 3.21 Show that the solution to the reduced-rank regression probl em\\n(3.68), with Σestimated by YTY/N, is given by (3.69). Hint:Transform', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 114}),\n",
       " Document(page_content='Exercises 97\\nYtoY∗=YΣ−1\\n2, and solved in terms of the canonical vectors u∗\\nm. Show\\nthatUm=Σ−1\\n2U∗\\nm, and a generalized inverse is U−\\nm=U∗\\nmTΣ1\\n2.\\nEx. 3.22 Show that the solution in Exercise 3.21 does not change if Σis\\nestimated by the more natural quantity ( Y−XˆB)T(Y−XˆB)/(N−pK).\\nEx. 3.23 Consider a regression problem with all variables and respon se hav-\\ning mean zero and standard deviation one. Suppose also that e ach variable\\nhas identical absolute correlation with the response:\\n1\\nN|⟨xj,y⟩|=λ, j= 1,...,p.\\nLetˆβbe the least-squares coeﬃcient of yonX, and let u(α) =αXˆβfor\\nα∈[0,1] be the vector that moves a fraction αtoward the least squares ﬁt\\nu. LetRSSbe the residual sum-of-squares from the full least squares ﬁ t.\\n(a) Show that\\n1\\nN|⟨xj,y−u(α)⟩|= (1−α)λ, j= 1,...,p,\\nand hence the correlations of each xjwith the residuals remain equal\\nin magnitude as we progress toward u.\\n(b) Show that these correlations are all equal to\\nλ(α) =(1−α)√\\n(1−α)2+α(2−α)\\nN·RSS·λ,\\nand hence they decrease monotonically to zero.\\n(c) Use these results to show that the LAR algorithm in Sectio n 3.4.4\\nkeeps the correlations tied and monotonically decreasing, as claimed\\nin (3.55).\\nEx. 3.24 LAR directions. Using the notation around equation (3.55) on\\npage 74, show that the LAR direction makes an equal angle with each of\\nthe predictors in Ak.\\nEx. 3.25 LAR look-ahead (Efron et al., 2004, Sec. 2). Starting at the be-\\nginning of the kth step of the LAR algorithm, derive expressions to identify\\nthe next variable to enter the active set at step k+1, and the value of αat\\nwhich this occurs (using the notation around equation (3.55 ) on page 74).\\nEx. 3.26 Forward stepwise regression enters the variable at each ste p that\\nmost reduces the residual sum-of-squares. LAR adjusts vari ables that have\\nthe most (absolute) correlation with the current residuals . Show that these\\ntwo entry criteria are not necessarily the same. [Hint: let xj.Abe thejth', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 115}),\n",
       " Document(page_content='98 3. Linear Methods for Regression\\nvariable, linearly adjusted for all the variables currentl y in the model. Show\\nthat the ﬁrst criterion amounts to identifying the jfor which Cor( xj.A,r)\\nis largest in magnitude.\\nEx. 3.27 Lasso and LAR : Consider thelassoprobleminLagrange multiplier\\nform: with L(β) =1\\n2∑\\ni(yi−∑\\njxijβj)2, we minimize\\nL(β)+λ∑\\nj|βj| (3.88)\\nfor ﬁxedλ>0.\\n(a) Setting βj=β+\\nj−β−\\njwithβ+\\nj,β−\\nj≥0, expression (3.88) becomes\\nL(β)+λ∑\\nj(β+\\nj+β−\\nj). Show that the Lagrange dual function is\\nL(β)+λ∑\\nj(β+\\nj+β−\\nj)−∑\\njλ+\\njβ+\\nj−∑\\njλ−\\njβ−\\nj(3.89)\\nand the Karush–Kuhn–Tucker optimality conditions are\\n∇L(β)j+λ−λ+\\nj= 0\\n−∇L(β)j+λ−λ−\\nj= 0\\nλ+\\njβ+\\nj= 0\\nλ−\\njβ−\\nj= 0,\\nalong with the non-negativity constraints on the parameter s and all\\nthe Lagrange multipliers.\\n(b) Show that|∇L(β)j|≤λ∀j,and that the KKT conditions imply one\\nof the following three scenarios:\\nλ= 0⇒ ∇L(β)j= 0∀j\\nβ+\\nj>0, λ>0⇒λ+\\nj= 0,∇L(β)j=−λ<0, β−\\nj= 0\\nβ−\\nj>0, λ>0⇒λ−\\nj= 0,∇L(β)j=λ>0, β+\\nj= 0.\\nHence show that for any “active” predictor having βj̸= 0, we must\\nhave∇L(β)j=−λifβj>0, and∇L(β)j=λifβj<0. Assuming\\nthe predictors are standardized, relate λto the correlation between\\nthejth predictor and the current residuals.\\n(c) Suppose that the set of active predictors is unchanged fo rλ0≥λ≥λ1.\\nShow that there is a vector γ0such that\\nˆβ(λ) =ˆβ(λ0)−(λ−λ0)γ0 (3.90)\\nThus the lasso solution path is linear as λranges from λ0toλ1(Efron\\net al., 2004; Rosset and Zhu, 2007).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 116}),\n",
       " Document(page_content='Exercises 99\\nEx. 3.28 Suppose for a given tin (3.51), the ﬁtted lasso coeﬃcient for\\nvariableXjisˆβj=a. Suppose we augment our set of variables with an\\nidentical copy X∗\\nj=Xj. Characterize the eﬀect of this exact collinearity\\nby describing the set of solutions for ˆβjandˆβ∗\\nj, using the same value of t.\\nEx. 3.29 Suppose we run a ridge regression with parameter λon a single\\nvariableX, and get coeﬃcient a. We now include an exact copy X∗=X,\\nand reﬁt our ridge regression. Show that both coeﬃcients are identical, and\\nderive their value. Show in general that if mcopies of a variable Xjare\\nincluded in a ridge regression, their coeﬃcients are all the same.\\nEx. 3.30 Consider the elastic-net optimization problem:\\nmin\\nβ||y−Xβ||2+λ[\\nα||β||2\\n2+(1−α)||β||1]\\n. (3.91)\\nShow how one can turn this into a lasso problem, using an augme nted\\nversion of Xandy.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 117}),\n",
       " Document(page_content='100 3. Linear Methods for Regression', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 118}),\n",
       " Document(page_content='This is page 101\\nPrinter: Opaque this\\n4\\nLinear Methods for Classiﬁcation\\n4.1 Introduction\\nIn this chapter we revisit the classiﬁcation problem and foc us on linear\\nmethods for classiﬁcation. Since our predictor G(x) takes values in a dis-\\ncrete setG, we can always divide the input space into a collection of reg ions\\nlabeledaccordingtotheclassiﬁcation.WesawinChapter2t hatthebound-\\naries of these regions can be rough or smooth, depending on th e prediction\\nfunction. For an important class of procedures, these decision boundaries\\nare linear; this is what we will mean by linear methods for cla ssiﬁcation.\\nThere are several diﬀerent ways in which linear decision bou ndaries can\\nbe found. In Chapter 2 we ﬁt linear regression models to the cl ass indicator\\nvariables, and classify to the largest ﬁt. Suppose there are Kclasses, for\\nconvenience labeled 1 ,2,...,K, and the ﬁtted linear model for the kth\\nindicator response variable is ˆfk(x) =ˆβk0+ˆβT\\nkx. The decision boundary\\nbetween class kandℓis that set of points for which ˆfk(x) =ˆfℓ(x), that is,\\nthe set{x: (ˆβk0−ˆβℓ0)+(ˆβk−ˆβℓ)Tx= 0}, an aﬃne set or hyperplane.1\\nSince the same is true for any pair of classes, the input space is divided\\ninto regions of constant classiﬁcation, with piecewise hyp erplanar decision\\nboundaries. This regression approach is a member of a class o f methods\\nthat model discriminant functions δk(x) for each class, and then classify x\\nto the class with the largest value for its discriminant func tion. Methods\\n1Strictly speaking, a hyperplane passes through the origin, while an aﬃne set need\\nnot. We sometimes ignore the distinction and refer in general to hyperpl anes.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 119}),\n",
       " Document(page_content='102 4. Linear Methods for Classiﬁcation\\nthat model the posterior probabilities Pr( G=k|X=x) are also in this\\nclass. Clearly, if either the δk(x) or Pr(G=k|X=x) are linear in x, then\\nthe decision boundaries will be linear.\\nActually, all we require is that some monotone transformati on ofδkor\\nPr(G=k|X=x) be linear for the decision boundaries to be linear. For\\nexample, if there are two classes, a popular model for the pos terior proba-\\nbilities is\\nPr(G= 1|X=x) =exp(β0+βTx)\\n1+exp(β0+βTx),\\nPr(G= 2|X=x) =1\\n1+exp(β0+βTx).(4.1)\\nHerethemonotonetransformationisthe logittransformation:log[ p/(1−p)],\\nand in fact we see that\\nlogPr(G= 1|X=x)\\nPr(G= 2|X=x)=β0+βTx. (4.2)\\nThe decision boundary is the set of points for which the log-odds are zero,\\nand this is a hyperplane deﬁned by{\\nx|β0+βTx= 0}\\n. We discuss two very\\npopular but diﬀerent methods that result in linear log-odds or logits: linear\\ndiscriminant analysis and linear logistic regression. Alt hough they diﬀer in\\ntheir derivation, the essential diﬀerence between them is i n the way the\\nlinear function is ﬁt to the training data.\\nA more direct approach is to explicitly model the boundaries between\\nthe classes as linear. For a two-class problem in a p-dimensional input\\nspace, this amounts to modeling the decision boundary as a hy perplane—in\\nother words, a normal vector and a cut-point. We will look at t wo methods\\nthat explicitly look for “separating hyperplanes.” The ﬁrs t is the well-\\nknownperceptron model of Rosenblatt (1958), with an algorithm that ﬁnds\\na separating hyperplane in the training data, if one exists. The second\\nmethod, due to Vapnik (1996), ﬁnds an optimally separating hyperplane if\\none exists, else ﬁnds a hyperplane that minimizes some measu re of overlap\\nin the training data. We treat the separable case here, and de fer treatment\\nof the nonseparable case to Chapter 12.\\nWhilethisentirechapterisdevotedtolineardecisionboun daries,thereis\\nconsiderable scope for generalization. For example, we can expand our vari-\\nablesetX1,...,X pbyincludingtheirsquaresandcross-products X2\\n1,X2\\n2,...,\\nX1X2,..., thereby adding p(p+1)/2 additional variables. Linear functions\\nin the augmented space map down to quadratic functions in the original\\nspace—hence linear decision boundaries to quadratic decisi on boundaries.\\nFigure 4.1 illustrates the idea. The data are the same: the le ft plot uses\\nlinear decision boundaries in the two-dimensional space sh own, while the\\nrightplotuseslineardecisionboundariesintheaugmented ﬁve-dimensional\\nspace described above. This approach can be used with any bas is transfor-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 120}),\n",
       " Document(page_content='4.2 Linear Regression of an Indicator Matrix 103\\n1\\n11\\n11\\n111\\n1\\n111\\n1\\n11\\n11\\n1111 1\\n111\\n1\\n11\\n111\\n1111\\n11\\n1\\n11\\n111\\n11\\n1111\\n1 1\\n111\\n11\\n1\\n1\\n1 1\\n11\\n1111\\n11111\\n1\\n11\\n111\\n111\\n1111\\n1\\n111\\n11\\n11\\n11\\n1\\n11\\n11\\n111\\n11\\n1\\n1\\n11\\n111\\n111\\n11\\n11\\n11\\n11 1\\n11\\n11 111\\n1 11\\n1\\n11\\n1\\n11\\n1\\n11\\n1\\n11 1\\n1\\n11\\n11\\n1111\\n1111\\n1\\n111\\n11\\n111\\n1\\n111\\n111\\n1111\\n11\\n1 11\\n111\\n11\\n1\\n11\\n11\\n11\\n111\\n12\\n22\\n22\\n222\\n2\\n2\\n22\\n2 2\\n2\\n22\\n22\\n22\\n2222\\n2\\n22\\n222\\n2\\n22\\n2 2\\n22\\n222\\n2\\n222\\n22222\\n22\\n2\\n222\\n22\\n2\\n22\\n22\\n22\\n22\\n22\\n2222\\n22\\n22\\n222\\n222\\n22222\\n22\\n222\\n22\\n22\\n22\\n2\\n22\\n2222\\n22\\n2\\n22\\n2\\n222\\n2\\n222\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n2222 2\\n222\\n22\\n2\\n22\\n22\\n222\\n222\\n2\\n22\\n22\\n22222\\n22\\n222\\n22\\n2\\n222\\n22\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n22222\\n22\\n3\\n33\\n33\\n33\\n3\\n3333\\n33\\n3\\n33\\n3\\n333\\n33\\n33\\n333\\n333\\n3\\n3\\n33\\n333\\n333\\n333\\n333\\n3333\\n333\\n3333\\n3\\n33333\\n33\\n3\\n33\\n3\\n33\\n3333\\n333\\n3\\n333\\n333\\n33\\n333\\n3\\n3333\\n33\\n333\\n33\\n3\\n3333\\n333\\n33333\\n3\\n3\\n3\\n333\\n33\\n3\\n33\\n3\\n33\\n33\\n333\\n33\\n3\\n3333\\n33\\n333\\n3\\n333\\n33333\\n33\\n333\\n33\\n33\\n3333\\n333\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n3\\n333\\n33\\n33\\n33\\n1\\n11\\n11\\n111\\n1\\n111\\n1\\n11\\n11\\n1111 1\\n111\\n1\\n11\\n111\\n1111\\n11\\n1\\n11\\n111\\n11\\n1111\\n1 1\\n111\\n11\\n1\\n1\\n1 1\\n11\\n1111\\n11111\\n1\\n11\\n111\\n111\\n1111\\n1\\n111\\n11\\n11\\n11\\n1\\n11\\n11\\n111\\n11\\n1\\n1\\n11\\n111\\n111\\n11\\n11\\n11\\n11 1\\n11\\n11 111\\n1 11\\n1\\n11\\n1\\n11\\n1\\n11\\n1\\n11 1\\n1\\n11\\n11\\n1111\\n1111\\n1\\n111\\n11\\n111\\n1\\n111\\n111\\n1111\\n11\\n1 11\\n111\\n11\\n1\\n11\\n11\\n11\\n111\\n12\\n22\\n22\\n222\\n2\\n2\\n22\\n2 2\\n2\\n22\\n22\\n22\\n2222\\n2\\n22\\n222\\n2\\n22\\n2 2\\n22\\n222\\n2\\n222\\n22222\\n22\\n2\\n222\\n22\\n2\\n22\\n22\\n22\\n22\\n22\\n2222\\n22\\n22\\n222\\n222\\n22222\\n22\\n222\\n22\\n22\\n22\\n2\\n22\\n2222\\n22\\n2\\n22\\n2\\n222\\n2\\n222\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n2222 2\\n222\\n22\\n2\\n22\\n22\\n222\\n222\\n2\\n22\\n22\\n22222\\n22\\n222\\n22\\n2\\n222\\n22\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n22222\\n22\\n3\\n33\\n33\\n33\\n3\\n3333\\n33\\n3\\n33\\n3\\n333\\n33\\n33\\n333\\n333\\n3\\n3\\n33\\n333\\n333\\n333\\n333\\n3333\\n333\\n3333\\n3\\n33333\\n33\\n3\\n33\\n3\\n33\\n3333\\n333\\n3\\n333\\n333\\n33\\n333\\n3\\n3333\\n33\\n333\\n33\\n3\\n3333\\n333\\n33333\\n3\\n3\\n3\\n333\\n33\\n3\\n33\\n3\\n33\\n33\\n333\\n33\\n3\\n3333\\n33\\n333\\n3\\n333\\n33333\\n33\\n333\\n33\\n33\\n3333\\n333\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n3\\n333\\n33\\n33\\n33\\nFIGURE 4.1. The left plot shows some data from three classes, with linear\\ndecision boundaries found by linear discriminant analysis. T he right plot shows\\nquadratic decision boundaries. These were obtained by ﬁndi ng linear boundaries\\nin the ﬁve-dimensional space X1,X2,X1X2,X2\\n1,X2\\n2. Linear inequalities in this\\nspace are quadratic inequalities in the original space.\\nmationh(X) whereh: IRp↦→IRqwithq>p, and will be explored in later\\nchapters.\\n4.2 Linear Regression of an Indicator Matrix\\nHere each of the response categories are coded via an indicat or variable.\\nThus ifGhasKclasses, there will be Ksuch indicators Yk, k= 1,...,K,\\nwithYk= 1 ifG=kelse 0. These are collected together in a vector\\nY= (Y1,...,Y K), and theNtraining instances of these form an N×K\\nindicator response matrix Y.Yis a matrix of 0’s and 1’s, with each row\\nhaving a single 1. We ﬁt a linear regression model to each of th e columns\\nofYsimultaneously, and the ﬁt is given by\\nˆY=X(XTX)−1XTY. (4.3)\\nChapter 3 has more details on linear regression. Note that we have a coeﬃ-\\ncient vector for each response column yk, and hence a ( p+1)×Kcoeﬃcient\\nmatrixˆB= (XTX)−1XTY. HereXis the model matrix with p+1 columns\\ncorresponding to the pinputs, and a leading column of 1’s for the intercept.\\nA new observation with input xis classiﬁed as follows:\\n•compute the ﬁtted output ˆf(x)T= (1,xT)ˆB, aKvector;\\n•identify the largest component and classify accordingly:\\nˆG(x) = argmaxk∈Gˆfk(x). (4.4)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 121}),\n",
       " Document(page_content='104 4. Linear Methods for Classiﬁcation\\nWhat is the rationale for this approach? One rather formal ju stiﬁcation\\nis to view the regression as an estimate of conditional expec tation. For the\\nrandom variable Yk,E(Yk|X=x) = Pr(G=k|X=x), so conditional\\nexpectation of each of the Ykseems a sensible goal. The real issue is: how\\ngood an approximation to conditional expectation is the rat her rigid linear\\nregression model? Alternatively, are the ˆfk(x) reasonable estimates of the\\nposterior probabilities Pr( G=k|X=x), and more importantly, does this\\nmatter?\\nIt is quite straightforward to verify that∑\\nk∈Gˆfk(x) = 1 for any x, as\\nlong as there is an intercept in the model (column of 1’s in X). However,\\ntheˆfk(x) can be negative or greater than 1, and typically some are. Th is\\nis a consequence of the rigid nature of linear regression, es pecially if we\\nmake predictions outside the hull of the training data. Thes e violations in\\nthemselves do not guarantee that this approach will not work , and in fact\\non many problems it gives similar results to more standard li near meth-\\nods for classiﬁcation. If we allow linear regression onto ba sis expansions\\nh(X) of the inputs, this approach can lead to consistent estimat es of the\\nprobabilities. As the size of the training set Ngrows bigger, we adaptively\\ninclude more basis elements so that linear regression onto t hese basis func-\\ntions approaches conditional expectation. We discuss such approaches in\\nChapter 5.\\nA more simplistic viewpoint is to construct targetstkfor each class,\\nwheretkis thekth column of the K×Kidentity matrix. Our prediction\\nproblem is to try and reproduce the appropriate target for an observation.\\nWith the same coding as before, the response vector yi(ith row of Y) for\\nobservation ihas the value yi=tkifgi=k. We might then ﬁt the linear\\nmodel by least squares:\\nmin\\nBN∑\\ni=1||yi−[(1,xT\\ni)B]T||2. (4.5)\\nThe criterion is a sum-of-squared Euclidean distances of th e ﬁtted vectors\\nfrom their targets. A new observation is classiﬁed by comput ing its ﬁtted\\nvectorˆf(x) and classifying to the closest target:\\nˆG(x) = argmin\\nk||ˆf(x)−tk||2. (4.6)\\nThis is exactly the same as the previous approach:\\n•The sum-of-squared-norm criterion is exactly the criterio n for multi-\\nple response linear regression, just viewed slightly diﬀer ently. Since\\na squared norm is itself a sum of squares, the components deco uple\\nand can be rearranged as a separate linear model for each elem ent.\\nNote that this is only possible because there is nothing in th e model\\nthat binds the diﬀerent responses together.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 122}),\n",
       " Document(page_content='4.2 Linear Regression of an Indicator Matrix 105\\nLinear Regression\\n1\\n11\\n1\\n1\\n11111\\n11\\n1\\n11111\\n111\\n11 111\\n111\\n1111\\n111\\n1\\n11111\\n111\\n11\\n1111\\n11\\n11\\n11111\\n11\\n11\\n11\\n1\\n11\\n11\\n1\\n11\\n11\\n1\\n11\\n1\\n1\\n11\\n111\\n111111\\n11\\n111\\n11 1\\n111\\n1\\n1\\n11\\n1\\n11\\n11\\n11\\n111\\n111\\n11\\n11\\n1 1\\n1\\n11\\n1111\\n11\\n1\\n111\\n11111\\n1\\n1111\\n11\\n111\\n111\\n111\\n1\\n11\\n1111\\n11\\n1\\n111 11\\n11\\n11\\n11\\n11\\n1\\n1\\n11\\n11\\n1\\n111\\n1\\n11\\n11\\n1\\n111\\n111 111\\n11\\n11\\n11\\n111111\\n1\\n11\\n11\\n11\\n1111\\n1111\\n111\\n1\\n11 1\\n111\\n1111\\n11\\n111\\n111\\n11\\n1\\n11\\n111\\n111\\n111\\n11\\n11\\n11\\n111\\n11 1\\n111\\n11\\n11\\n11\\n1\\n11\\n11\\n1\\n11\\n11\\n1\\n111\\n1\\n11111\\n11111\\n1\\n11\\n111\\n1\\n11\\n111 1\\n11\\n1\\n11 11\\n1\\n111\\n111\\n11\\n1\\n111\\n1\\n1\\n1111\\n1111\\n11\\n11\\n1111\\n11\\n1\\n11\\n11\\n11\\n1 1\\n111\\n1 11\\n1\\n11\\n11\\n11 1\\n11\\n1\\n1111111\\n1\\n11\\n11\\n1\\n11\\n111\\n11\\n111\\n1111\\n11\\n11\\n11\\n1 1\\n11\\n11111\\n1\\n11\\n11\\n111\\n11\\n11\\n11\\n1\\n1\\n11\\n11\\n111\\n111\\n11\\n111\\n11\\n111\\n1\\n11\\n1\\n11\\n11\\n1\\n111\\n1111\\n11\\n1\\n11\\n1\\n11\\n1 12\\n22\\n2\\n222\\n22\\n222\\n2\\n22\\n222\\n222\\n222\\n2222\\n2\\n222\\n22\\n2\\n2\\n222\\n2\\n222\\n2\\n222\\n2222\\n2\\n22\\n22\\n22\\n22222\\n2222\\n2\\n222\\n222\\n22\\n2\\n22\\n22\\n222\\n2\\n2222222\\n22\\n22\\n222\\n222\\n222\\n22\\n2\\n222\\n22\\n2\\n22\\n22\\n2 2\\n22\\n2222\\n2\\n2\\n2\\n22\\n2222\\n22\\n222\\n2\\n22\\n222\\n222\\n22\\n222\\n2\\n2\\n222\\n222\\n2\\n22\\n2222\\n222\\n2222\\n22\\n222\\n2\\n2222\\n222\\n22\\n2\\n222\\n2222\\n222\\n22\\n2\\n222\\n22\\n2\\n22222\\n2222\\n22\\n222\\n22\\n22\\n22\\n2222\\n2222\\n2\\n22\\n22\\n22\\n222\\n22222\\n22\\n22\\n2\\n22\\n2\\n2\\n22\\n2\\n22\\n22\\n2\\n22\\n22\\n22\\n22\\n2\\n222\\n22\\n2222\\n2222\\n222\\n222\\n222\\n2222\\n2\\n222\\n2\\n22\\n222\\n222\\n22\\n222 22\\n222\\n222\\n22222\\n22 222\\n222\\n2\\n22\\n22 2\\n2\\n2222\\n2\\n22\\n22\\n2\\n22\\n22\\n22\\n2\\n22 22\\n2\\n222\\n2222\\n222\\n2\\n2 2222 2\\n22\\n22\\n2222\\n22\\n222\\n2\\n22\\n22\\n2\\n22\\n22\\n222\\n22\\n22\\n2222\\n22\\n2222\\n2222\\n2\\n222\\n22 2\\n22\\n2\\n22\\n22222\\n22\\n22\\n22\\n222\\n2\\n222\\n22\\n2\\n2 22 222\\n22\\n22\\n22 2\\n22 22\\n222\\n22\\n22\\n22 22\\n22\\n223\\n33\\n33\\n33\\n3\\n3\\n333\\n3\\n33\\n3\\n333\\n33\\n3\\n33\\n333\\n33\\n333\\n3\\n333\\n3333\\n3\\n33\\n33\\n333\\n33\\n33\\n33\\n3\\n33\\n33\\n33333\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n3\\n33\\n3\\n333\\n33\\n3333\\n33\\n33\\n3\\n33\\n33\\n3333\\n33\\n3333\\n3\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n333\\n33\\n33\\n3333\\n3\\n33\\n33\\n333\\n3\\n3\\n33\\n33\\n3\\n33333\\n33\\n333\\n333\\n3333\\n33\\n33\\n333\\n3\\n33\\n33\\n33\\n333\\n3\\n3\\n3333\\n333\\n333\\n3\\n333\\n3\\n3\\n33333333333\\n33\\n333\\n3\\n33\\n333\\n33\\n33\\n3\\n3\\n333\\n33\\n3\\n33\\n3\\n333\\n3\\n3333\\n333\\n33\\n33\\n33\\n333\\n33\\n3\\n33\\n33\\n33\\n33\\n33333\\n33\\n33\\n33\\n33\\n3 33\\n333\\n3\\n33 33\\n3\\n33\\n333\\n333333\\n3\\n3333\\n33\\n33\\n3333\\n33\\n33\\n33\\n333\\n333\\n3\\n3333\\n3\\n33\\n3\\n3\\n33333\\n333\\n3333\\n3\\n33\\n333\\n3\\n333\\n3\\n333\\n33\\n3\\n33\\n33\\n3333\\n333\\n33\\n33\\n33 3\\n3\\n3333\\n333\\n3\\n3\\n3\\n333\\n33\\n3\\n333\\n33\\n3\\n3333\\n33\\n3\\n33\\n3333\\n333\\n33\\n333\\n33\\n3\\n3\\n33\\n333\\n3\\n3\\n3333333\\n3333\\n33\\n3\\n3\\n33\\n3\\n3333\\n3\\n33\\n33 3\\n33\\n3\\n3333\\n333\\n33\\n3\\n3\\n333\\n33\\n3\\n3333\\n3\\n33Linear Discriminant Analysis\\n1\\n11\\n1\\n1\\n11111\\n11\\n1\\n11111\\n111\\n11 111\\n111\\n1111\\n111\\n1\\n11111\\n111\\n11\\n1111\\n11\\n11\\n11111\\n11\\n11\\n11\\n1\\n11\\n11\\n1\\n11\\n11\\n1\\n11\\n1\\n1\\n11\\n111\\n111111\\n11\\n111\\n11 1\\n111\\n1\\n1\\n11\\n1\\n11\\n11\\n11\\n111\\n111\\n11\\n11\\n1 1\\n1\\n11\\n1111\\n11\\n1\\n111\\n11111\\n1\\n1111\\n11\\n111\\n111\\n111\\n1\\n11\\n1111\\n11\\n1\\n111 11\\n11\\n11\\n11\\n11\\n1\\n1\\n11\\n11\\n1\\n111\\n1\\n11\\n11\\n1\\n111\\n111 111\\n11\\n11\\n11\\n111111\\n1\\n11\\n11\\n11\\n1111\\n1111\\n111\\n1\\n11 1\\n111\\n1111\\n11\\n111\\n111\\n11\\n1\\n11\\n111\\n111\\n111\\n11\\n11\\n11\\n111\\n11 1\\n111\\n11\\n11\\n11\\n1\\n11\\n11\\n1\\n11\\n11\\n1\\n111\\n1\\n11111\\n11111\\n1\\n11\\n111\\n1\\n11\\n111 1\\n11\\n1\\n11 11\\n1\\n111\\n111\\n11\\n1\\n111\\n1\\n1\\n1111\\n1111\\n11\\n11\\n1111\\n11\\n1\\n11\\n11\\n11\\n1 1\\n111\\n1 11\\n1\\n11\\n11\\n11 1\\n11\\n1\\n1111111\\n1\\n11\\n11\\n1\\n11\\n111\\n11\\n111\\n1111\\n11\\n11\\n11\\n1 1\\n11\\n11111\\n1\\n11\\n11\\n111\\n11\\n11\\n11\\n1\\n1\\n11\\n11\\n111\\n111\\n11\\n111\\n11\\n111\\n1\\n11\\n1\\n11\\n11\\n1\\n111\\n1111\\n11\\n1\\n11\\n1\\n11\\n1 12\\n22\\n2\\n222\\n22\\n222\\n2\\n22\\n222\\n222\\n222\\n2222\\n2\\n222\\n22\\n2\\n2\\n222\\n2\\n222\\n2\\n222\\n2222\\n2\\n22\\n22\\n22\\n22222\\n2222\\n2\\n222\\n222\\n22\\n2\\n22\\n22\\n222\\n2\\n2222222\\n22\\n22\\n222\\n222\\n222\\n22\\n2\\n222\\n22\\n2\\n22\\n22\\n2 2\\n22\\n2222\\n2\\n2\\n2\\n22\\n2222\\n22\\n222\\n2\\n22\\n222\\n222\\n22\\n222\\n2\\n2\\n222\\n222\\n2\\n22\\n2222\\n222\\n2222\\n22\\n222\\n2\\n2222\\n222\\n22\\n2\\n222\\n2222\\n222\\n22\\n2\\n222\\n22\\n2\\n22222\\n2222\\n22\\n222\\n22\\n22\\n22\\n2222\\n2222\\n2\\n22\\n22\\n22\\n222\\n22222\\n22\\n22\\n2\\n22\\n2\\n2\\n22\\n2\\n22\\n22\\n2\\n22\\n22\\n22\\n22\\n2\\n222\\n22\\n2222\\n2222\\n222\\n222\\n222\\n2222\\n2\\n222\\n2\\n22\\n222\\n222\\n22\\n222 22\\n222\\n222\\n22222\\n22 222\\n222\\n2\\n22\\n22 2\\n2\\n2222\\n2\\n22\\n22\\n2\\n22\\n22\\n22\\n2\\n22 22\\n2\\n222\\n2222\\n222\\n2\\n2 2222 2\\n22\\n22\\n2222\\n22\\n222\\n2\\n22\\n22\\n2\\n22\\n22\\n222\\n22\\n22\\n2222\\n22\\n2222\\n2222\\n2\\n222\\n22 2\\n22\\n2\\n22\\n22222\\n22\\n22\\n22\\n222\\n2\\n222\\n22\\n2\\n2 22 222\\n22\\n22\\n22 2\\n22 22\\n222\\n22\\n22\\n22 22\\n22\\n223\\n33\\n33\\n33\\n3\\n3\\n333\\n3\\n33\\n3\\n333\\n33\\n3\\n33\\n333\\n33\\n333\\n3\\n333\\n3333\\n3\\n33\\n33\\n333\\n33\\n33\\n33\\n3\\n33\\n33\\n33333\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n3\\n33\\n3\\n333\\n33\\n3333\\n33\\n33\\n3\\n33\\n33\\n3333\\n33\\n3333\\n3\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n333\\n33\\n33\\n3333\\n3\\n33\\n33\\n333\\n3\\n3\\n33\\n33\\n3\\n33333\\n33\\n333\\n333\\n3333\\n33\\n33\\n333\\n3\\n33\\n33\\n33\\n333\\n3\\n3', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 123}),\n",
       " Document(page_content='3\\n33\\n33\\n33333\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n3\\n33\\n3\\n333\\n33\\n3333\\n33\\n33\\n3\\n33\\n33\\n3333\\n33\\n3333\\n3\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n333\\n33\\n33\\n3333\\n3\\n33\\n33\\n333\\n3\\n3\\n33\\n33\\n3\\n33333\\n33\\n333\\n333\\n3333\\n33\\n33\\n333\\n3\\n33\\n33\\n33\\n333\\n3\\n3\\n3333\\n333\\n333\\n3\\n333\\n3\\n3\\n33333333333\\n33\\n333\\n3\\n33\\n333\\n33\\n33\\n3\\n3\\n333\\n33\\n3\\n33\\n3\\n333\\n3\\n3333\\n333\\n33\\n33\\n33\\n333\\n33\\n3\\n33\\n33\\n33\\n33\\n33333\\n33\\n33\\n33\\n33\\n3 33\\n333\\n3\\n33 33\\n3\\n33\\n333\\n333333\\n3\\n3333\\n33\\n33\\n3333\\n33\\n33\\n33\\n333\\n333\\n3\\n3333\\n3\\n33\\n3\\n3\\n33333\\n333\\n3333\\n3\\n33\\n333\\n3\\n333\\n3\\n333\\n33\\n3\\n33\\n33\\n3333\\n333\\n33\\n33\\n33 3\\n3\\n3333\\n333\\n3\\n3\\n3\\n333\\n33\\n3\\n333\\n33\\n3\\n3333\\n33\\n3\\n33\\n3333\\n333\\n33\\n333\\n33\\n3\\n3\\n33\\n333\\n3\\n3\\n3333333\\n3333\\n33\\n3\\n3\\n33\\n3\\n3333\\n3\\n33\\n33 3\\n33\\n3\\n3333\\n333\\n33\\n3\\n3\\n333\\n33\\n3\\n3333\\n3\\n33\\nX1 X1\\nX2X2\\nFIGURE 4.2. The data come from three classes in IR2and are easily separated\\nby linear decision boundaries. The right plot shows the bounda ries found by linear\\ndiscriminant analysis. The left plot shows the boundaries foun d by linear regres-\\nsion of the indicator response variables. The middle class is co mpletely masked\\n(never dominates).\\n•The closest target classiﬁcation rule (4.6) is easily seen t o be exactly\\nthe same as the maximum ﬁtted component criterion (4.4).\\nThereisaseriousproblemwiththeregressionapproachwhen thenumber\\nof classesK≥3, especially prevalent when Kis large. Because of the rigid\\nnature of the regression model, classes can be maskedby others. Figure 4.2\\nillustrates anextremesituation when K= 3.Thethreeclasses areperfectly\\nseparated by linear decision boundaries, yet linear regres sion misses the\\nmiddle class completely.\\nIn Figure 4.3 we have projected the data onto the line joining the three\\ncentroids (there is no information in the orthogonal direct ion in this case),\\nand we have included and coded the three response variables Y1,Y2and\\nY3. The three regression lines (left panel) are included, and w e see that\\nthe line corresponding to the middle class is horizontal and its ﬁtted values\\nare never dominant! Thus, observations from class 2 are clas siﬁed either\\nas class 1 or class 3. The right panel uses quadratic regressi on rather than\\nlinear regression. For this simple example a quadratic rath er than linear\\nﬁt (for the middle class at least) would solve the problem. Ho wever, it\\ncan be seen that if there were four rather than three classes l ined up like\\nthis, a quadratic would not come down fast enough, and a cubic would\\nbe needed as well. A loose but general rule is that if K≥3 classes are\\nlined up, polynomial terms up to degree K−1 might be needed to resolve\\nthem. Note also that these are polynomials along the derived direction\\npassing through the centroids, which can have arbitrary ori entation. So in', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 123}),\n",
       " Document(page_content='106 4. Linear Methods for Classiﬁcation\\n111\\n111\\n1\\n1\\n1111\\n11\\n11\\n1\\n11\\n11111\\n1\\n11111\\n11\\n1\\n11\\n11\\n11\\n11111\\n111\\n111\\n1\\n1111\\n111\\n111\\n111\\n111111\\n11\\n1\\n11\\n111\\n1111\\n11\\n111\\n111\\n111\\n111\\n111\\n1\\n11\\n11\\n111\\n111\\n11\\n11\\n1111\\n11\\n1\\n111\\n1\\n11\\n1\\n11\\n1\\n11\\n11\\n111\\n1\\n1111\\n111\\n111\\n12222 2222222 2 2 222 2 2 22 2222 222222 2 2 2222 22 22222 2222222222222 22 22222 222 2222222 2 22 222 2 22222 22 22 222 222222 22222 222 22 2 222222222 22222222 222 222 2 22222222222222 22\\n3\\n3\\n33\\n3\\n3333\\n33\\n33\\n33\\n333\\n333\\n3\\n3\\n33333\\n3\\n33\\n333\\n33\\n333333\\n333333\\n3\\n3333\\n3\\n33\\n3\\n33\\n3\\n33\\n333\\n33\\n33\\n33333\\n33\\n3\\n3333\\n33\\n333\\n33\\n3333\\n33333\\n3333\\n33333\\n33\\n3\\n33\\n33\\n33\\n33\\n33\\n333\\n3\\n333\\n333\\n333\\n3333\\n3\\n3333\\n333\\n3\\n33333\\n0.00.51.0\\n0.0 0.2 0.4 0.6 0.8 1.0111\\n111\\n1\\n1\\n1111\\n11\\n11\\n1\\n11\\n11111\\n1\\n11\\n111\\n11\\n1\\n11\\n11\\n11\\n11111\\n11\\n1\\n111\\n1\\n1111\\n111\\n111\\n111\\n111111\\n111\\n11111\\n111111\\n111\\n111\\n111\\n111\\n111\\n1\\n1111111 111\\n111111111111\\n1 111 1111111 11111111 111111112\\n2\\n22\\n2\\n2222\\n22\\n22\\n22\\n222\\n222\\n2\\n2\\n22222\\n2\\n22\\n222\\n22\\n222222\\n222222\\n2\\n22222\\n2\\n22 2\\n222222222 222222\\n22\\n22222222 222222222 22 22\\n22 22\\n22\\n22\\n222\\n222\\n22\\n22\\n2222\\n22\\n2\\n222\\n2\\n22\\n2\\n22\\n2\\n22\\n22\\n222\\n2\\n222\\n222\\n222\\n233 3333\\n333333\\n3333\\n33333333\\n333333\\n33\\n33333\\n3 333333333333 3333\\n33\\n3\\n33\\n3\\n33\\n3333 333\\n33333\\n33\\n3\\n333333333\\n33\\n3333\\n33333\\n3333\\n33333\\n33\\n3\\n33\\n33\\n33\\n33\\n33\\n333\\n3\\n333\\n333\\n333\\n3333\\n3\\n333\\n3\\n33\\n3\\n33333\\n0.00.51.0\\n0.0 0.2 0.4 0.6 0.8 1.0Degree = 1; Error = 0.33 Degree = 2; Error = 0.04\\nFIGURE 4.3. The eﬀects of masking on linear regression in IRfor a three-class\\nproblem. The rug plot at the base indicates the positions and class membership of\\neach observation. The three curves in each panel are the ﬁtte d regressions to the\\nthree-class indicator variables; for example, for the blue class ,yblueis1for the\\nblue observations, and 0for the green and orange. The ﬁts are linear and quadratic\\npolynomials. Above each plot is the training error rate. The Bay es error rate is\\n0.025for this problem, as is the LDA error rate.\\np-dimensional input space, one would need general polynomia l terms and\\ncross-products of total degree K−1,O(pK−1) terms in all, to resolve such\\nworst-case scenarios.\\nThe example is extreme, but for large Kand smallpsuch maskings\\nnaturally occur. As a more realistic illustration, Figure 4 .4 is a projection\\nof the training data for a vowel recognition problem onto an i nformative\\ntwo-dimensional subspace. There are K= 11 classes in p= 10 dimensions.\\nThis is a diﬃcult classiﬁcation problem, and the best method s achieve\\naround 40% errors on the test data. The main point here is summ arized in\\nTable 4.1; linear regression has an error rate of 67%, while a close relative,\\nlineardiscriminantanalysis,hasanerrorrateof56%.Itse emsthatmasking\\nhas hurt in this case. While all the other methods in this chap ter are based\\non linear functions of xas well, they use them in such a way that avoids\\nthis masking problem.\\n4.3 Linear Discriminant Analysis\\nDecision theory for classiﬁcation (Section 2.4) tells us th at we need to know\\nthe class posteriors Pr( G|X) for optimal classiﬁcation. Suppose fk(x) is\\nthe class-conditional density of Xin classG=k, and letπkbe the prior\\nprobability of class k, with∑K\\nk=1πk= 1. A simple application of Bayes', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 124}),\n",
       " Document(page_content='4.3 Linear Discriminant Analysis 107\\nCoordinate 1 for Training DataCoordinate 2 for Training Data\\n-4 -2 0 2 4-6 -4 -2 0 2 4oooo oo\\nooooooo\\no\\noooo\\noo\\no\\no\\no\\no\\noooooooooooo\\no\\no\\no\\no\\no ooooooo\\nooooooo\\no\\noo\\no\\nooooo\\no\\nooooooo\\noo\\noooo\\no\\no\\nooooooooooooo\\no\\noooooo\\no\\nooo\\noooo\\noooo\\no\\nooooo\\noooooo o\\no\\no\\noooooooooo\\no\\noo\\no\\noo\\nooooooooooooo\\noooooooooooooooooo\\noooooooooooo\\no\\no\\nooooooo\\no\\no\\nooooooo\\nooooo\\nooooooo\\no\\no\\noooooooooooooooo\\no\\no\\no\\no\\noooooooo\\noooo\\noo\\noooooooooo\\no\\nooo\\no\\no\\no\\noooo\\no\\no\\nooooo\\no\\no o\\no\\no\\no\\no\\nooooooooo oooo\\no\\no\\noooooo\\no\\noooooooo\\no\\noo\\no\\noooooooo\\no\\no\\no\\no\\nooo\\nooooooooooooooo\\nooo\\noooooooo\\no\\nooo\\nooooooooooooo\\no\\noooo\\nooooo\\nooooo\\no\\noo\\no\\no\\no\\no\\nooo\\nooo\\no\\nooooo\\no\\noooo\\no\\nooooooooooooo\\no ooooooooooooooooo\\no\\no\\nooooo\\nooooooooooo\\no\\no\\nooo\\nooo\\noo oo\\noooooo\\noooooo\\nooooooooo\\no\\no\\nooooooo\\noooooooooooo\\noo\\no\\no\\noo\\n••••••••••••••\\n••\\n••\\n••••Linear Discriminant Analysis\\nFIGURE 4.4. A two-dimensional plot of the vowel training data. There are\\neleven classes with X∈IR10, and this is the best view in terms of a LDA model\\n(Section 4.3.3). The heavy circles are the projected mean vec tors for each class.\\nThe class overlap is considerable.\\nTABLE 4.1. Training and test error rates using a variety of linear techni ques\\non the vowel data. There are eleven classes in ten dimensions, o f which three\\naccount for 90%of the variance (via a principal components analysis). We see\\nthat linear regression is hurt by masking, increasing the tes t and training error\\nby over10%.\\nTechnique Error Rates\\nTraining Test\\nLinear regression 0.48 0.67\\nLinear discriminant analysis 0.32 0.56\\nQuadratic discriminant analysis 0.01 0.53\\nLogistic regression 0.22 0.51', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 125}),\n",
       " Document(page_content='108 4. Linear Methods for Classiﬁcation\\ntheorem gives us\\nPr(G=k|X=x) =fk(x)πk∑K\\nℓ=1fℓ(x)πℓ. (4.7)\\nWe see that in terms of ability to classify, having the fk(x) is almost equiv-\\nalent to having the quantity Pr( G=k|X=x).\\nMany techniques are based on models for the class densities:\\n•linear and quadratic discriminant analysis use Gaussian de nsities;\\n•moreﬂexiblemixturesofGaussiansallowfornonlineardeci sionbound-\\naries (Section 6.8);\\n•general nonparametric density estimates for each class den sity allow\\nthe most ﬂexibility (Section 6.6.2);\\n•Naive Bayes models are a variant of the previous case, and assume\\nthat each of the class densities are products of marginal den sities;\\nthat is, they assume that the inputs are conditionally indep endent in\\neach class (Section 6.6.3).\\nSuppose that we model each class density as multivariate Gau ssian\\nfk(x) =1\\n(2π)p/2|Σk|1/2e−1\\n2(x−µk)TΣ−1\\nk(x−µk). (4.8)\\nLinear discriminant analysis (LDA) arises in the special ca se when we\\nassume that the classes have a common covariance matrix Σk=Σ∀k. In\\ncomparing two classes kandℓ, it is suﬃcient to look at the log-ratio, and\\nwe see that\\nlogPr(G=k|X=x)\\nPr(G=ℓ|X=x)= logfk(x)\\nfℓ(x)+logπk\\nπℓ\\n= logπk\\nπℓ−1\\n2(µk+µℓ)TΣ−1(µk−µℓ)\\n+xTΣ−1(µk−µℓ),(4.9)\\nan equation linear in x. The equal covariance matrices cause the normal-\\nization factors to cancel, as well as the quadratic part in th e exponents.\\nThis linear log-odds function implies that the decision bou ndary between\\nclasseskandℓ—the set where Pr( G=k|X=x) = Pr(G=ℓ|X=x)—is\\nlinear inx; inpdimensions a hyperplane. This is of course true for any pair\\nof classes, so all the decision boundaries are linear. If we d ivide IRpinto\\nregions that are classiﬁed as class 1, class 2, etc., these re gions will be sep-\\narated by hyperplanes. Figure 4.5 (left panel) shows an idea lized example\\nwith three classes and p= 2. Here the data do arise from three Gaus-\\nsian distributions with a common covariance matrix. We have included in', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 126}),\n",
       " Document(page_content='4.3 Linear Discriminant Analysis 109\\n+++\\n3\\n21\\n11\\n233\\n3\\n123\\n32\\n11211\\n33\\n12 1\\n23\\n23\\n3\\n12\\n211\\n1\\n13\\n222\\n21 3\\n2 23\\n13\\n13\\n32\\n13\\n3\\n23\\n133\\n2133\\n22\\n3\\n22\\n21\\n11\\n11\\n2\\n133\\n1\\n13\\n32\\n222 3\\n12\\nFIGURE 4.5. The left panel shows three Gaussian distributions, with the sa me\\ncovariance and diﬀerent means. Included are the contours of c onstant density\\nenclosing 95% of the probability in each case. The Bayes decision boundari es\\nbetween each pair of classes are shown (broken straight lines) , and the Bayes\\ndecision boundaries separating all three classes are the thi cker solid lines (a subset\\nof the former). On the right we see a sample of 30drawn from each Gaussian\\ndistribution, and the ﬁtted LDA decision boundaries.\\nthe ﬁgure the contours corresponding to 95% highest probabi lity density,\\nas well as the class centroids. Notice that the decision boun daries are not\\nthe perpendicular bisectors of the line segments joining th e centroids. This\\nwould be the case if the covariance Σwere spherical σ2I, and the class\\npriors were equal. From (4.9) we see that the linear discriminant functions\\nδk(x) =xTΣ−1µk−1\\n2µT\\nkΣ−1µk+logπk (4.10)\\nareanequivalentdescriptionofthedecisionrule,with G(x) = argmaxkδk(x).\\nIn practice we do not know the parameters of the Gaussian dist ributions,\\nand will need to estimate them using our training data:\\n•ˆπk=Nk/N, whereNkis the number of class- kobservations;\\n•ˆµk=∑\\ngi=kxi/Nk;\\n•ˆΣ=∑K\\nk=1∑\\ngi=k(xi−ˆµk)(xi−ˆµk)T/(N−K).\\nFigure 4.5 (right panel) shows the estimated decision bound aries based on\\na sample of size 30 each from three Gaussian distributions. F igure 4.1 on\\npage 103 is another example, but here the classes are not Gaus sian.\\nWith two classes there is a simple correspondence between li near dis-\\ncriminant analysis and classiﬁcation by linear regression , as in (4.5). The\\nLDA rule classiﬁes to class 2 if\\nxTˆΣ−1(ˆµ2−ˆµ1)>1\\n2(ˆµ2+ ˆµ1)TˆΣ−1(ˆµ2−ˆµ1)−log(N2/N1),(4.11)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 127}),\n",
       " Document(page_content='110 4. Linear Methods for Classiﬁcation\\nand class 1 otherwise. Suppose we code the targets in the two c lasses as +1\\nand−1, respectively. It is easy to show that the coeﬃcient vector from least\\nsquares is proportional to the LDA direction given in (4.11) (Exercise 4.2).\\n[In fact, this correspondence occurs for any (distinct) cod ing of the targets;\\nsee Exercise 4.2]. However unless N1=N2the intercepts are diﬀerent and\\nhence the resulting decision rules are diﬀerent.\\nSincethisderivationoftheLDAdirectionvialeastsquares doesnotusea\\nGaussian assumption for the features, its applicability ex tends beyond the\\nrealm of Gaussian data. However the derivation of the partic ular intercept\\nor cut-point given in (4.11) doesrequire Gaussian data. Thus it makes\\nsense to instead choose the cut-point that empirically mini mizes training\\nerror for a given dataset. This is something we have found to w ork well in\\npractice, but have not seen it mentioned in the literature.\\nWith more than two classes, LDA is not the same as linear regre ssion of\\nthe class indicator matrix, and it avoids the masking proble ms associated\\nwith that approach (Hastie et al., 1994). A correspondence b etween regres-\\nsion and LDA can be established through the notion of optimal scoring ,\\ndiscussed in Section 12.5.\\nGetting back to the general discriminant problem (4.8), if t heΣkare\\nnot assumed to be equal, then the convenient cancellations i n (4.9) do not\\noccur; in particular the pieces quadratic in xremain. We then get quadratic\\ndiscriminant functions (QDA),\\nδk(x) =−1\\n2log|Σk|−1\\n2(x−µk)TΣ−1\\nk(x−µk)+logπk.(4.12)\\nThe decision boundary between each pair of classes kandℓis described by\\na quadratic equation {x:δk(x) =δℓ(x)}.\\nFigure 4.6 shows an example (from Figure 4.1 on page 103) wher e the\\nthree classes are Gaussian mixtures (Section 6.8) and the de cision bound-\\naries are approximated by quadratic equations in x. Here we illustrate\\ntwo popular ways of ﬁtting these quadratic boundaries. The r ight plot\\nuses QDA as described here, while the left plot uses LDA in the enlarged\\nﬁve-dimensional quadratic polynomial space. The diﬀerenc es are generally\\nsmall; QDA is the preferred approach, with the LDA method a co nvenient\\nsubstitute2.\\nTheestimatesforQDAaresimilartothoseforLDA,excepttha tseparate\\ncovariance matrices must be estimated for each class. When pis large this\\ncan mean a dramatic increase in parameters. Since the decisi on boundaries\\nare functions of the parameters of the densities, counting t he number of\\nparameters must be done with care. For LDA, it seems there are (K−\\n1)×(p+1) parameters, since we only need the diﬀerences δk(x)−δK(x)\\n2For this ﬁgure and many similar ﬁgures in the book we compute the d ecision bound-\\naries by an exhaustive contouring method. We compute the decision rule on a ﬁne lattice\\nof points, and then use contouring algorithms to compute the bound aries.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 128}),\n",
       " Document(page_content='4.3 Linear Discriminant Analysis 111\\n1\\n11\\n11\\n111\\n1\\n111\\n1\\n11\\n11\\n1111 1\\n111\\n1\\n11\\n111\\n1111\\n11\\n1\\n11\\n111\\n11\\n1111\\n1 1\\n111\\n11\\n1\\n1\\n1 1\\n11\\n1111\\n11111\\n1\\n11\\n111\\n111\\n1111\\n1\\n111\\n11\\n11\\n11\\n1\\n11\\n11\\n111\\n11\\n1\\n1\\n11\\n111\\n111\\n11\\n11\\n11\\n11 1\\n11\\n11 111\\n1 11\\n1\\n11\\n1\\n11\\n1\\n11\\n1\\n11 1\\n1\\n11\\n11\\n1111\\n1111\\n1\\n111\\n11\\n111\\n1\\n111\\n111\\n1111\\n11\\n1 11\\n111\\n11\\n1\\n11\\n11\\n11\\n111\\n12\\n22\\n22\\n222\\n2\\n2\\n22\\n2 2\\n2\\n22\\n22\\n22\\n2222\\n2\\n22\\n222\\n2\\n22\\n2 2\\n22\\n222\\n2\\n222\\n22222\\n22\\n2\\n222\\n22\\n2\\n22\\n22\\n22\\n22\\n22\\n2222\\n22\\n22\\n222\\n222\\n22222\\n22\\n222\\n22\\n22\\n22\\n2\\n22\\n2222\\n22\\n2\\n22\\n2\\n222\\n2\\n222\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n2222 2\\n222\\n22\\n2\\n22\\n22\\n222\\n222\\n2\\n22\\n22\\n22222\\n22\\n222\\n22\\n2\\n222\\n22\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n22222\\n22\\n3\\n33\\n33\\n33\\n3\\n3333\\n33\\n3\\n33\\n3\\n333\\n33\\n33\\n333\\n333\\n3\\n3\\n33\\n333\\n333\\n333\\n333\\n3333\\n333\\n3333\\n3\\n33333\\n33\\n3\\n33\\n3\\n33\\n3333\\n333\\n3\\n333\\n333\\n33\\n333\\n3\\n3333\\n33\\n333\\n33\\n3\\n3333\\n333\\n33333\\n3\\n3\\n3\\n333\\n33\\n3\\n33\\n3\\n33\\n33\\n333\\n33\\n3\\n3333\\n33\\n333\\n3\\n333\\n33333\\n33\\n333\\n33\\n33\\n3333\\n333\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n3\\n333\\n33\\n33\\n33\\n1\\n11\\n11\\n111\\n1\\n111\\n1\\n11\\n11\\n1111 1\\n111\\n1\\n11\\n111\\n1111\\n11\\n1\\n11\\n111\\n11\\n1111\\n1 1\\n111\\n11\\n1\\n1\\n1 1\\n11\\n1111\\n11111\\n1\\n11\\n111\\n111\\n1111\\n1\\n111\\n11\\n11\\n11\\n1\\n11\\n11\\n111\\n11\\n1\\n1\\n11\\n111\\n111\\n11\\n11\\n11\\n11 1\\n11\\n11 111\\n1 11\\n1\\n11\\n1\\n11\\n1\\n11\\n1\\n11 1\\n1\\n11\\n11\\n1111\\n1111\\n1\\n111\\n11\\n111\\n1\\n111\\n111\\n1111\\n11\\n1 11\\n111\\n11\\n1\\n11\\n11\\n11\\n111\\n12\\n22\\n22\\n222\\n2\\n2\\n22\\n2 2\\n2\\n22\\n22\\n22\\n2222\\n2\\n22\\n222\\n2\\n22\\n2 2\\n22\\n222\\n2\\n222\\n22222\\n22\\n2\\n222\\n22\\n2\\n22\\n22\\n22\\n22\\n22\\n2222\\n22\\n22\\n222\\n222\\n22222\\n22\\n222\\n22\\n22\\n22\\n2\\n22\\n2222\\n22\\n2\\n22\\n2\\n222\\n2\\n222\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n2222 2\\n222\\n22\\n2\\n22\\n22\\n222\\n222\\n2\\n22\\n22\\n22222\\n22\\n222\\n22\\n2\\n222\\n22\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n22222\\n22\\n3\\n33\\n33\\n33\\n3\\n3333\\n33\\n3\\n33\\n3\\n333\\n33\\n33\\n333\\n333\\n3\\n3\\n33\\n333\\n333\\n333\\n333\\n3333\\n333\\n3333\\n3\\n33333\\n33\\n3\\n33\\n3\\n33\\n3333\\n333\\n3\\n333\\n333\\n33\\n333\\n3\\n3333\\n33\\n333\\n33\\n3\\n3333\\n333\\n33333\\n3\\n3\\n3\\n333\\n33\\n3\\n33\\n3\\n33\\n33\\n333\\n33\\n3\\n3333\\n33\\n333\\n3\\n333\\n33333\\n33\\n333\\n33\\n33\\n3333\\n333\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n3\\n333\\n33\\n33\\n33\\nFIGURE 4.6. Two methods for ﬁtting quadratic boundaries. The left plot sho ws\\nthe quadratic decision boundaries for the data in Figure 4.1 (obtained using LDA\\nin the ﬁve-dimensional space X1,X2,X1X2,X2\\n1,X2\\n2). The right plot shows the\\nquadratic decision boundaries found by QDA. The diﬀerences are small, as is\\nusually the case.\\nbetween the discriminant functions where Kis some pre-chosen class (here\\nwe have chosen the last), and each diﬀerence requires p+ 1 parameters3.\\nLikewise for QDA there will be ( K−1)×{p(p+ 3)/2 + 1}parameters.\\nBoth LDA and QDA perform well on an amazingly large and divers e set\\nof classiﬁcation tasks. For example, in the STATLOG project (Michie et\\nal., 1994) LDA was among the top three classiﬁers for 7 of the 2 2 datasets,\\nQDA among the top three for four datasets, and one of the pair w ere in the\\ntop threefor 10datasets. Both techniques arewidely used,a ndentirebooks\\nare devoted to LDA. It seems that whatever exotic tools are th e rage of the\\nday, we should always have available these two simple tools. The question\\narises why LDA and QDA have such a good track record. The reaso n is not\\nlikely to be that the data are approximately Gaussian, and in addition for\\nLDA that the covariances are approximately equal. More like ly a reason is\\nthat the data can only support simple decision boundaries su ch as linear or\\nquadratic, and the estimates provided via the Gaussian mode ls are stable.\\nThis is a bias variance tradeoﬀ—we can put up with the bias of a l inear\\ndecision boundary because it can be estimated with much lowe r variance\\nthan more exotic alternatives. This argument is less believ able for QDA,\\nsince it can have many parameters itself, although perhaps f ewer than the\\nnon-parametric alternatives.\\n3Althoughweﬁtthecovariancematrix ˆΣtocomputetheLDAdiscriminantfunctions,\\na much reduced function of it is all that is required to estimate the O(p) parameters\\nneeded to compute the decision boundaries.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 129}),\n",
       " Document(page_content='112 4. Linear Methods for Classiﬁcation\\nMisclassification Rate\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••Regularized Discriminant Analysis on the Vowel Data\\nTest Data\\nTrain Data\\nα\\nFIGURE 4.7. Test and training errors for the vowel data, using regularize d\\ndiscriminant analysis with a series of values of α∈[0,1]. The optimum for the\\ntest data occurs around α= 0.9, close to quadratic discriminant analysis.\\n4.3.1 Regularized Discriminant Analysis\\nFriedman (1989) proposed a compromise between LDA and QDA, w hich\\nallows one to shrink the separate covariances of QDA toward a common\\ncovariance as in LDA. These methods are very similar in ﬂavor to ridge\\nregression. The regularized covariance matrices have the f orm\\nˆΣk(α) =αˆΣk+(1−α)ˆΣ, (4.13)\\nwhereˆΣis the pooled covariance matrix as used in LDA. Here α∈[0,1]\\nallows a continuum of models between LDA and QDA, and needs to be\\nspeciﬁed. In practice αcan be chosen based on the performance of the\\nmodel on validation data, or by cross-validation.\\nFigure 4.7 shows the results of RDA applied to the vowel data. Both\\nthe training and test error improve with increasing α, although the test\\nerror increases sharply after α= 0.9. The large discrepancy between the\\ntraining and test error is partly due to the fact that there ar e many repeat\\nmeasurements on a small number of individuals, diﬀerent in t he training\\nand test set.\\nSimilar modiﬁcations allow ˆΣitself to be shrunk toward the scalar\\ncovariance,\\nˆΣ(γ) =γˆΣ+(1−γ)ˆσ2I (4.14)\\nforγ∈[0,1]. Replacing ˆΣin (4.13) by ˆΣ(γ) leads to a more general family\\nof covariances ˆΣ(α,γ) indexed by a pair of parameters.\\nIn Chapter 12, we discuss other regularized versions of LDA, which are\\nmore suitable when the data arise from digitized analog sign als and images.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 130}),\n",
       " Document(page_content='4.3 Linear Discriminant Analysis 113\\nInthesesituationsthefeaturesarehigh-dimensionalandc orrelated,andthe\\nLDA coeﬃcients can be regularized to be smooth or sparse in th e original\\ndomain of the signal. This leads to better generalization an d allows for\\neasier interpretation of the coeﬃcients. In Chapter 18 we al so deal with\\nvery high-dimensional problems, where for example the feat ures are gene-\\nexpression measurements in microarray studies. There the m ethods focus\\non the case γ= 0 in (4.14), and other severely regularized versions of LDA .\\n4.3.2 Computations for LDA\\nAs a lead-in to the next topic, we brieﬂy digress on the comput ations\\nrequired for LDA and especially QDA. Their computations are simpliﬁed\\nby diagonalizing ˆΣorˆΣk. For the latter, suppose we compute the eigen-\\ndecomposition for each ˆΣk=UkDkUT\\nk, whereUkisp×porthonormal,\\nandDka diagonal matrix of positive eigenvalues dkℓ. Then the ingredients\\nforδk(x) (4.12) are\\n•(x−ˆµk)TˆΣ−1\\nk(x−ˆµk) = [UT\\nk(x−ˆµk)]TD−1\\nk[UT\\nk(x−ˆµk)];\\n•log|ˆΣk|=∑\\nℓlogdkℓ.\\nIn light of the computational steps outlined above, the LDA c lassiﬁer\\ncan be implemented by the following pair of steps:\\n•Spherethe data with respect to the common covariance estimate ˆΣ:\\nX∗←D−1\\n2UTX, whereˆΣ=UDUT. The common covariance esti-\\nmate ofX∗will now be the identity.\\n•Classifytotheclosestclass centroidinthetransformedsp ace,modulo\\nthe eﬀect of the class prior probabilities πk.\\n4.3.3 Reduced-Rank Linear Discriminant Analysis\\nSo far we have discussed LDA as a restricted Gaussian classiﬁ er. Part of\\nits popularity is due to an additional restriction that allo ws us to view\\ninformative low-dimensional projections of the data.\\nTheKcentroids in p-dimensional input space lie in an aﬃne subspace\\nof dimension≤K−1, and ifpis much larger than K, this will be a con-\\nsiderable drop in dimension. Moreover, in locating the clos est centroid, we\\ncan ignore distances orthogonal to this subspace, since the y will contribute\\nequally to each class. Thus we might just as well project the X∗onto this\\ncentroid-spanning subspace HK−1, and make distance comparisons there.\\nThus there is a fundamental dimension reduction in LDA, name ly, that we\\nneed only consider the data in a subspace of dimension at most K−1.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 131}),\n",
       " Document(page_content='114 4. Linear Methods for Classiﬁcation\\nIfK= 3, for instance, this could allow us to view the data in a two-\\ndimensional plot, color-coding the classes. In doing so we w ould not have\\nrelinquished any of the information needed for LDA classiﬁc ation.\\nWhat ifK >3? We might then ask for a L<K−1 dimensional subspace\\nHL⊆HK−1optimal for LDA in some sense. Fisher deﬁned optimal to\\nmean that the projected centroids were spread out as much as p ossible in\\nterms of variance. This amounts to ﬁnding principal compone nt subspaces\\nof the centroids themselves (principal components are desc ribed brieﬂy in\\nSection3.5.1,andinmoredetailinSection14.5.1).Figure 4.4showssuchan\\noptimal two-dimensional subspace for the vowel data. Here t here are eleven\\nclasses, each a diﬀerent vowel sound, in a ten-dimensional i nput space. The\\ncentroids require the full space in this case, since K−1 =p, but we have\\nshown an optimal two-dimensional subspace. The dimensions are ordered,\\nsowecancomputeadditionaldimensionsinsequence.Figure 4.8showsfour\\nadditional pairs of coordinates, also known as canonical ordiscriminant\\nvariables. In summary then, ﬁnding the sequences of optimal subspaces\\nfor LDA involves the following steps:\\n•compute the K×pmatrix of class centroids Mand the common\\ncovariance matrix W(forwithin-class covariance);\\n•compute M∗=MW−1\\n2using the eigen-decomposition of W;\\n•compute B∗,thecovariancematrixof M∗(Bforbetween-class covari-\\nance), and its eigen-decomposition B∗=V∗DBV∗T. The columns\\nv∗\\nℓofV∗in sequence from ﬁrst to last deﬁne the coordinates of the\\noptimal subspaces.\\nCombining all these operations the ℓthdiscriminant variable is given by\\nZℓ=vT\\nℓXwithvℓ=W−1\\n2v∗\\nℓ.\\nFisher arrived at this decomposition via a diﬀerent route, w ithout refer-\\nring to Gaussian distributions at all. He posed the problem:\\nFind the linear combination Z=aTXsuch that the between-\\nclass variance is maximized relative to the within-class va riance.\\nAgain, the between class variance is the variance of the clas s means of\\nZ, and the within class variance is the pooled variance about t he means.\\nFigure 4.9 shows why this criterion makes sense. Although th e direction\\njoining the centroids separates the means as much as possibl e (i.e., max-\\nimizes the between-class variance), there is considerable overlap between\\nthe projected classes due to the nature of the covariances. B y taking the\\ncovariance into account as well, a direction with minimum ov erlap can be\\nfound.\\nThe between-class variance of ZisaTBaand the within-class variance\\naTWa, whereWis deﬁned earlier, and Bis the covariance matrix of the\\nclass centroid matrix M. Note that B+W=T, where Tis thetotal\\ncovariance matrix of X, ignoring class information.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 132}),\n",
       " Document(page_content='4.3 Linear Discriminant Analysis 115\\nCoordinate 1 Coordinate 3 \\n-4 -2 0 2 4-2 0 2o\\nooooo\\no\\no\\nooo\\nooo\\nooooooo\\no\\noo\\nooo\\nooo\\noo\\nooooooooo\\nooo\\no\\nooo\\nooooo\\noooooo ooooooo\\no\\no\\nooo oo\\no\\no\\nooo\\no\\no\\noooooooooooo\\noooo\\noooooo\\noo\\nooo\\noooooooo\\nooo\\noooo\\noo ooo\\noooo\\noooo\\noo\\noooo\\no\\noo\\noooooooo\\noooo\\noooooooooo\\noo\\nooo\\no\\nooo\\noooooo\\nooo\\noooooo\\no\\no\\noooooo\\noooooooooooo\\no\\nooo\\noo\\nooo\\nooooo\\noo\\nooo\\nooooo\\no\\nooooo\\no\\nooooooo\\noooo\\no\\noooo\\noo\\nooooo\\noooo\\no\\noo\\nooooo\\no\\no\\no\\nooo\\no\\nooooo\\nooooooo\\no\\noooo\\noooooo\\noooo\\no\\no\\noo ooo\\noo\\nooo\\no\\noooo\\no\\nooooooo\\no\\noo\\noo\\no\\nooo\\nooooo\\no\\nooooo\\no\\noo\\no\\noo\\no\\noooooo\\no\\noo\\nooooooooooo\\noo\\noooooo\\noo\\no\\no\\nooo o\\no\\nooooo\\no\\no\\no\\nooo\\noooooooooo\\noooooo\\noooooo\\noooooo\\no\\no\\nooo\\nooooooooooo\\noo oo\\nooo\\noooooo\\no\\no\\noooooooo\\nooo\\noooooo\\no\\nooooo\\nooo\\no\\nooo\\no\\noooooooooo oooo ooooooo\\nooooooo\\n••••••••\\n••••••••••••••\\nCoordinate 2 Coordinate 3 \\n-6 -4 -2 0 2 4-2 0 2o\\nooooo\\no\\no\\nooo\\nooo\\nooooooo\\no\\noo\\nooo\\nooo\\noo\\nooooooooo\\nooo\\no\\nooo\\nooooo\\noooooo ooooooo\\no\\no\\nooo oo\\no\\no\\nooo\\no\\no\\noooooooooooo\\noooo\\noooooo\\noo\\nooo\\noooooooo\\nooo\\noooo\\nooooo\\noooo\\noooo\\noo\\noooo\\no\\noo\\noooooooo\\noooo\\noooooooooo\\noo\\nooo\\no\\nooo\\noooooo\\nooo\\noooooo\\no\\no\\noooooo\\noooooooooooo\\no\\nooo\\noo\\nooo\\nooooo\\noo\\nooo\\nooooo\\no\\nooooo\\no\\nooooooo\\noooo\\no\\noooo\\noo\\nooooo\\noooo\\no\\noo\\nooooo\\no\\no\\no\\nooo\\no\\nooooo\\nooooooo\\no\\noooo\\noooooo\\noooo\\no\\no\\noo ooo\\noo\\nooo\\no\\noooo\\no\\nooooooo\\no\\noo\\noo\\no\\nooo\\nooooo\\no\\nooooo\\no\\noo\\no\\noo\\no\\noooooo\\no\\noo\\nooooooooooo\\noo\\noooooo\\noo\\no\\no\\nooo o\\no\\nooooo\\no\\no\\no\\nooo\\noooooooooo\\noooooo\\noooooo\\noooooo\\no\\no\\nooo\\nooooooooooo\\noo oo\\nooo\\noooooo\\no\\no\\noooooooo\\nooo\\noooooo\\no\\nooooo\\nooo\\no\\nooo\\no\\noooooooooo ooooooooooo\\nooooooo\\n••••••••\\n••••••••••••••\\nCoordinate 1 Coordinate 7 \\n-4 -2 0 2 4-3 -2 -1 0 1 2 3ooo ooo\\no\\nooooooo\\noooo\\nooooooooooooooo\\nooo\\nooo\\no\\no\\nooooooooooooo\\noooo\\no\\nooooo\\no\\nooooooo\\no\\no\\nooooo\\nooooooooooo\\nooo\\no\\noooo\\noooo\\noooo\\noo\\noo\\no\\nooo\\noooooooooo\\nooo\\nooooo\\noooooooooooo\\noo oooo\\noooooo\\noooooo\\noooooooo\\no\\nooooooooo\\no\\nooooooooooo\\no\\nooooo\\noooooo\\noo\\noooo\\nooooooooooo\\nooo o\\nooo\\nooooooooo\\nooo\\noooooo\\noooo\\noooooooo\\no\\noooooo\\nooo\\nooo\\no\\noooo\\nooooooooooo\\no\\noooooo\\nooooo\\nooooooooooooo\\no\\nooooooo\\noooo\\noooooooo\\no ooo\\noooooo\\no\\no\\noooo\\nooooo\\no\\noooooo\\noooooooooooo\\noooo o\\nooooooooooooo\\nooo\\no\\nooo\\noo\\no\\noooooooo\\nooooooo\\nooo\\noo\\noooooooooooo\\nooooooooo\\nooo\\noo\\nooooo\\no\\nooo\\no\\no\\nooo\\no\\noo\\no\\noooooo\\nooooo\\noo\\noo\\no\\noooooo\\nooo\\noo\\nooooooooooo\\no\\noooooo\\no\\nooooooooooo\\noooo\\no\\noo\\n••••••••••••••••••••••\\nCoordinate 9 Coordinate 10 \\n-2 -1 0 1 2 3-2 -1 0 1 2oo\\no\\no\\noooooooo\\nooooo\\no\\nooooooo\\nooooo\\nooooo\\no\\nooooooo\\no\\no\\no\\noo\\noo\\no\\nooo\\noooooooooo\\no\\no\\noooooooooooo o\\noo\\nooo\\nooooooo\\nooooo\\noo\\noooo\\nooooooo\\noo\\no\\nooo\\noooo\\nooooooo\\noooooo\\noo\\no\\noooooo\\no\\noo\\no\\noooooooooooooooooo\\nooo\\nooo\\nooo\\noo\\nooooooo\\no\\noooo\\nooooo\\no\\nooooooooooo\\nooooo\\nooooooooooooo\\no\\noooo\\nooooo\\no\\no\\no\\no\\no\\noooo\\nooooo\\no\\nooo\\no\\noooo\\noo\\no\\noo\\noo\\noooo\\noooo\\no\\nooooooooooo\\no\\nooo\\noo\\no\\nooo\\no\\no oo\\noooooooo\\nooo\\nooooo\\nooooo\\no\\no\\nooo\\noooooooo\\nooo\\noo\\noo\\noooooooooooo\\no\\no\\no\\noo\\nooooooooo\\noooo\\no\\noooooooooo\\nooo\\no\\no\\no\\no\\no\\noo\\nooooooo\\no\\no\\noo\\no\\no\\noooooooooo\\noooo\\nooo\\noooooooo\\nooo\\noooo\\nooo\\no\\nooo\\nooooo\\nooooooooooooo\\nooooo\\no\\no\\nooo\\nooooooo\\nooo\\no\\noo\\noooo\\nooooo\\noo\\no\\no\\no\\noooooooooooooooo\\nooo\\no\\no ooooo\\noo\\noo\\no\\no\\no\\no••••••••••••••••••••••Linear Discriminant Analysis\\nFIGURE 4.8. Four projections onto pairs of canonical variates. Notice t hat as\\nthe rank of the canonical variates increases, the centroids become less spread out.\\nIn the lower right panel they appear to be superimposed, and th e classes most\\nconfused.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 133}),\n",
       " Document(page_content='116 4. Linear Methods for Classiﬁcation\\n++\\n++\\nFIGURE 4.9. Although the line joining the centroids deﬁnes the direction o f\\ngreatest centroid spread, the projected data overlap becaus e of the covariance\\n(left panel). The discriminant direction minimizes this over lap for Gaussian data\\n(right panel).\\nFisher’s problem therefore amounts to maximizing the Rayleigh quotient ,\\nmax\\naaTBa\\naTWa, (4.15)\\nor equivalently\\nmax\\naaTBasubject toaTWa= 1. (4.16)\\nThis is a generalized eigenvalue problem, with agiven by the largest\\neigenvalue of W−1B. It is not hard to show (Exercise 4.1) that the optimal\\na1is identical to v1deﬁned above. Similarly one can ﬁnd the next direction\\na2, orthogonal in Wtoa1, such that aT\\n2Ba2/aT\\n2Wa2is maximized; the\\nsolution is a2=v2, and so on. The aℓare referred to as discriminant\\ncoordinates , not to be confused with discriminant functions. They are al so\\nreferred to as canonical variates , since an alternative derivation of these\\nresults is through a canonical correlation analysis of the i ndicator response\\nmatrixYon the predictor matrix X. This line is pursued in Section 12.5.\\nTo summarize the developments so far:\\n•Gaussian classiﬁcation with common covariances leads to li near deci-\\nsion boundaries. Classiﬁcation can be achieved by sphering the data\\nwith respect to W, and classifying to the closest centroid (modulo\\nlogπk) in the sphered space.\\n•Since only the relative distances to the centroids count, on e can con-\\nﬁne the data to the subspace spanned by the centroids in the sp hered\\nspace.\\n•This subspace can be further decomposed into successively o ptimal\\nsubspaces in term of centroid separation. This decompositi on is iden-\\ntical to the decomposition due to Fisher.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 134}),\n",
       " Document(page_content='4.3 Linear Discriminant Analysis 117\\nDimensionMisclassification Rate\\n2 4 6 8 100.3 0.4 0.5 0.6 0.7LDA and Dimension Reduction on the Vowel Data\\n•\\n••••• • ••••\\n•\\n• •\\n•\\n•••••Test Data\\nTrain Data\\nFIGURE 4.10. Training and test error rates for the vowel data, as a functio n\\nof the dimension of the discriminant subspace. In this case t he best error rate is\\nfor dimension 2. Figure 4.11 shows the decision boundaries in this space.\\nThe reduced subspaces have been motivated as a data reductio n (for\\nviewing) tool. Can they also be used for classiﬁcation, and w hat is the\\nrationale? Clearly they can, as in our original derivation; we simply limit\\nthe distance-to-centroid calculations to the chosen subsp ace. One can show\\nthat this is a Gaussian classiﬁcation rule with the addition al restriction\\nthat the centroids of the Gaussians lie in a L-dimensional subspace of IRp.\\nFitting such a model by maximum likelihood, and then constru cting the\\nposterior probabilities using Bayes’ theorem amounts to th e classiﬁcation\\nrule described above (Exercise 4.8).\\nGaussian classiﬁcation dictates the log πkcorrection factor in the dis-\\ntance calculation. The reason for this correction can be see n in Figure 4.9.\\nThe misclassiﬁcation rate is based on the area of overlap bet ween the two\\ndensities. If the πkare equal (implicit in that ﬁgure), then the optimal\\ncut-point is midway between the projected means. If the πkare not equal,\\nmoving the cut-point toward the smallerclass will improve the error rate.\\nAs mentioned earlier for two classes, one can derive the line ar rule using\\nLDA (or any other method), and then choose the cut-point to mi nimize\\nmisclassiﬁcation error over the training data.\\nAs an example of the beneﬁt of the reduced-rank restriction, we return\\nto the vowel data. There are 11 classes and 10 variables, and h ence 10\\npossible dimensions for the classiﬁer. We can compute the tr aining and\\ntest error in each of these hierarchical subspaces; Figure 4 .10 shows the\\nresults. Figure 4.11 shows the decision boundaries for the c lassiﬁer based\\non the two-dimensional LDA solution.\\nThere is a close connection between Fisher’s reduced rank di scriminant\\nanalysis and regression of an indicator response matrix. It turns out that', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 135}),\n",
       " Document(page_content='118 4. Linear Methods for Classiﬁcation\\noooo\\noo o\\noo\\noo\\no\\noo o\\noo\\noooo\\no\\noo\\nooooo\\noo\\nooo\\no\\nooo\\noooo\\noo\\noooo\\no\\nooo\\no\\noo\\nooo\\no\\no\\no\\nooo\\no\\noo\\noo oo\\noo\\nooo\\no\\noo\\noo\\noo\\no\\noo\\no\\noooo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\no\\no\\no\\noo\\no\\noo\\noo\\no\\no\\nooo\\no\\nooo\\nooo\\noo\\no\\nooo\\noo\\noooo\\noo\\noo\\no\\nooo\\no\\no\\nooo\\no\\nooo\\noo\\noo\\noooo\\noo\\noo\\no\\noo\\no\\nooo\\no\\noooo\\noooo\\noo\\no o\\nooo\\no\\no\\no\\noo\\noooo\\noo\\noo\\no\\noo\\nooo\\no\\noo\\noo\\noo\\no oo\\nooo\\no\\noo\\noo\\noooo\\noo\\noo\\noo\\no\\noo\\no\\noo\\noooo\\no\\nooo\\noooooo\\nooo\\noo\\noo\\nooooo\\nooo\\no\\nooo\\no oo\\noo\\nooo\\no\\noooo\\noo\\no\\nooo\\no\\nooo\\nooooo\\no\\noo\\noo\\no oo\\noo\\noo\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\noooo\\no\\no\\noooo\\no\\noo\\noo\\no\\noo\\noo\\no\\noo\\nooo\\noooo\\noo\\noo\\nooo\\noo\\noo\\no\\noo\\noo\\nooo\\no\\noo\\noo\\no\\noooo\\noooo\\no\\noo\\noo\\no\\nooo\\nooooo\\nooo\\noo\\noo\\noo\\noo\\no\\no\\no\\no\\no\\noo\\noo\\no\\noo\\nooo\\no\\no o\\noo\\no o\\noo\\no\\noo\\no\\nooo\\noo\\nooo\\no\\noo\\no\\noo o\\nooo\\noo\\noo\\no\\noo\\noo\\noo\\noo\\nooo\\nooo\\no\\noo\\no\\noo\\noo\\noo\\noo\\no\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\nooo\\noo\\noo\\no\\noo\\no\\no\\no\\nCanonical Coordinate 1Canonical Coordinate 2Classification in Reduced Subspace\\n••••••••••••••\\n••\\n••\\n••••\\nFIGURE 4.11. Decision boundaries for the vowel training data, in the two- di-\\nmensional subspace spanned by the ﬁrst two canonical variat es. Note that in\\nany higher-dimensional subspace, the decision boundaries are higher-dimensional\\naﬃne planes, and could not be represented as lines.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 136}),\n",
       " Document(page_content='4.4 Logistic Regression 119\\nLDA amounts to the regression followed by an eigen-decompos ition of\\nˆYTY. In the case of two classes, there is a single discriminant va riable\\nthat is identical up to a scalar multiplication to either of t he columns of ˆY.\\nThese connections are developed in Chapter 12. A related fac t is that if one\\ntransforms the original predictors XtoˆY, then LDA using ˆYis identical\\nto LDA in the original space (Exercise 4.3).\\n4.4 Logistic Regression\\nThe logistic regression model arises from the desire to mode l the posterior\\nprobabilities of the Kclasses via linear functions in x, while at the same\\ntime ensuring that they sum to one and remain in [0 ,1]. The model has\\nthe form\\nlogPr(G= 1|X=x)\\nPr(G=K|X=x)=β10+βT\\n1x\\nlogPr(G= 2|X=x)\\nPr(G=K|X=x)=β20+βT\\n2x\\n...\\nlogPr(G=K−1|X=x)\\nPr(G=K|X=x)=β(K−1)0+βT\\nK−1x.(4.17)\\nThe model is speciﬁed in terms of K−1 log-odds or logit transformations\\n(reﬂecting the constraint that the probabilities sum to one ). Although the\\nmodel uses the last class as the denominator in the odds-rati os, the choice\\nof denominator is arbitrary in that the estimates are equiva riant under this\\nchoice. A simple calculation shows that\\nPr(G=k|X=x) =exp(βk0+βT\\nkx)\\n1+∑K−1\\nℓ=1exp(βℓ0+βT\\nℓx), k= 1,...,K−1,\\nPr(G=K|X=x) =1\\n1+∑K−1\\nℓ=1exp(βℓ0+βT\\nℓx), (4.18)\\nand they clearly sum toone. Toemphasize the dependenceon th eentire pa-\\nrameter set θ={β10,βT\\n1,...,β (K−1)0,βT\\nK−1}, we denote the probabilities\\nPr(G=k|X=x) =pk(x;θ).\\nWhenK= 2, this model is especially simple, since there is only a sin gle\\nlinear function. It is widely used in biostatistical applic ations where binary\\nresponses(twoclasses)occurquitefrequently.Forexampl e,patientssurvive\\nor die, have heart disease or not, or a condition is present or absent.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 137}),\n",
       " Document(page_content='120 4. Linear Methods for Classiﬁcation\\n4.4.1 Fitting Logistic Regression Models\\nLogistic regression models are usually ﬁt by maximum likeli hood, using the\\nconditional likelihood of GgivenX. Since Pr(G|X) completely speciﬁes the\\nconditional distribution, the multinomial distribution is appropriate. The\\nlog-likelihood for Nobservations is\\nℓ(θ) =N∑\\ni=1logpgi(xi;θ), (4.19)\\nwherepk(xi;θ) = Pr(G=k|X=xi;θ).\\nWe discuss in detail the two-class case, since the algorithm s simplify\\nconsiderably. It is convenient to code the two-class givia a 0/1 responseyi,\\nwhereyi= 1 whengi= 1, andyi= 0 whengi= 2. Letp1(x;θ) =p(x;θ),\\nandp2(x;θ) = 1−p(x;θ). The log-likelihood can be written\\nℓ(β) =N∑\\ni=1{\\nyilogp(xi;β)+(1−yi)log(1−p(xi;β))}\\n=N∑\\ni=1{\\nyiβTxi−log(1+eβTxi)}\\n. (4.20)\\nHereβ={β10,β1}, and we assume that the vector of inputs xiincludes\\nthe constant term 1 to accommodate the intercept.\\nTo maximize the log-likelihood, we set its derivatives to ze ro. These score\\nequations are\\n∂ℓ(β)\\n∂β=N∑\\ni=1xi(yi−p(xi;β)) = 0, (4.21)\\nwhich arep+1 equations nonlinear inβ. Notice that since the ﬁrst compo-\\nnentofxiis1,theﬁrstscoreequationspeciﬁesthat∑N\\ni=1yi=∑N\\ni=1p(xi;β);\\ntheexpected number of class ones matches the observed number (and hence\\nalso class twos.)\\nTo solve the score equations (4.21), we use the Newton–Raphs on algo-\\nrithm, which requires the second-derivative or Hessian mat rix\\n∂2ℓ(β)\\n∂β∂βT=−N∑\\ni=1xixiTp(xi;β)(1−p(xi;β)). (4.22)\\nStarting with βold, a single Newton update is\\nβnew=βold−(∂2ℓ(β)\\n∂β∂βT)−1∂ℓ(β)\\n∂β, (4.23)\\nwhere the derivatives are evaluated at βold.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 138}),\n",
       " Document(page_content='4.4 Logistic Regression 121\\nIt is convenient to write the score and Hessian in matrix nota tion. Let\\nydenote the vector of yivalues,XtheN×(p+ 1) matrix of xivalues,\\npthe vector of ﬁtted probabilities with ith element p(xi;βold) andWa\\nN×Ndiagonal matrix of weights with ith diagonal element p(xi;βold)(1−\\np(xi;βold)). Then we have\\n∂ℓ(β)\\n∂β=XT(y−p) (4.24)\\n∂2ℓ(β)\\n∂β∂βT=−XTWX (4.25)\\nThe Newton step is thus\\nβnew=βold+(XTWX)−1XT(y−p)\\n= (XTWX)−1XTW(\\nXβold+W−1(y−p))\\n= (XTWX)−1XTWz. (4.26)\\nIn the second and third line we have re-expressed the Newton s tep as a\\nweighted least squares step, with the response\\nz=Xβold+W−1(y−p), (4.27)\\nsometimes known as the adjusted response . These equations get solved re-\\npeatedly, since at each iteration pchanges, and hence so does Wandz.\\nThis algorithm is referred to as iteratively reweighted least squares or IRLS,\\nsince each iteration solves the weighted least squares prob lem:\\nβnew←argmin\\nβ(z−Xβ)TW(z−Xβ). (4.28)\\nIt seems that β= 0 is a good starting value for the iterative procedure,\\nalthough convergence is never guaranteed. Typically the al gorithm does\\nconverge, since the log-likelihood is concave, but oversho oting can occur.\\nIn the rare cases that the log-likelihood decreases, step si ze halving will\\nguarantee convergence.\\nFor the multiclass case ( K≥3) the Newton algorithm can also be ex-\\npressed as an iteratively reweighted least squares algorit hm, but with a\\nvectorofK−1 responses and a nondiagonal weight matrix per observation .\\nThe latter precludes any simpliﬁed algorithms, and in this c ase it is numer-\\nically more convenient to work with the expanded vector θdirectly (Ex-\\nercise 4.4). Alternatively coordinate-descent methods (S ection 3.8.6) can\\nbe used to maximize the log-likelihood eﬃciently. The Rpackageglmnet\\n(Friedman et al., 2010) can ﬁt very large logistic regressio n problems ef-\\nﬁciently, both in Nandp. Although designed to ﬁt regularized models,\\noptions allow for unregularized ﬁts.\\nLogistic regression models are used mostly as a data analysi s and infer-\\nence tool, where the goal is to understand the role of the inpu t variables', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 139}),\n",
       " Document(page_content='122 4. Linear Methods for Classiﬁcation\\nTABLE 4.2. Results from a logistic regression ﬁt to the South African hear t\\ndisease data.\\nCoeﬃcient Std. Error ZScore\\n(Intercept) −4.130 0 .964−4.285\\nsbp 0.006 0 .006 1.023\\ntobacco 0.080 0 .026 3.034\\nldl 0.185 0 .057 3.219\\nfamhist 0.939 0 .225 4.178\\nobesity -0.035 0 .029−1.187\\nalcohol 0.001 0 .004 0.136\\nage 0.043 0 .010 4.184\\ninexplaining the outcome. Typically many models are ﬁt in a search for a\\nparsimonious model involving a subset of the variables, pos sibly with some\\ninteractions terms. The following example illustrates som e of the issues\\ninvolved.\\n4.4.2 Example: South African Heart Disease\\nHere we present an analysis of binary data to illustrate the t raditional\\nstatistical use of the logistic regression model. The data i n Figure 4.12 are a\\nsubset of the Coronary Risk-Factor Study (CORIS) baseline s urvey, carried\\nout in three rural areas of the Western Cape, South Africa (Ro usseauw et\\nal., 1983). The aim of the study was to establish the intensit y of ischemic\\nheart disease risk factors in that high-incidence region. T he data represent\\nwhite males between 15 and 64, and the response variable is th e presence or\\nabsence of myocardial infarction (MI) at the time of the surv ey (the overall\\nprevalence of MI was 5.1% in this region). There are 160 cases in our data\\nset, and a sample of 302 controls. These data are described in more detail\\nin Hastie and Tibshirani (1987).\\nWe ﬁt a logistic-regression model by maximum likelihood, gi ving the\\nresults shown in Table 4.2. This summary includes Zscores for each of the\\ncoeﬃcients in the model (coeﬃcients divided by their standa rd errors); a\\nnonsigniﬁcant Zscoresuggestsacoeﬃcientcanbedroppedfromthemodel.\\nEach of these correspond formally to a test of the null hypoth esis that the\\ncoeﬃcient in question is zero, while all the others are not (a lso known as\\nthe Wald test). A Zscore greater than approximately 2 in absolute value\\nis signiﬁcant at the 5% level.\\nThere are some surprises in this table of coeﬃcients, which m ust be in-\\nterpreted with caution. Systolic blood pressure ( sbp) is not signiﬁcant! Nor\\nisobesity, and its sign is negative. This confusion is a result of the co rre-\\nlation between the set of predictors. On their own, both sbpandobesity\\nare signiﬁcant, and with positive sign. However, in the pres ence of many', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 140}),\n",
       " Document(page_content='4.4 Logistic Regression 123\\nsbp0 10 20 30\\no\\no\\noo\\nooo\\noooo\\no\\noo\\nooooo\\no\\nooo\\noo\\nooooooo\\nooo\\noo\\nooo\\noo\\noo\\nooo\\noooooo\\nooo\\noo\\nooooooooo\\noooooooooooo\\nooooo\\nooooooooo\\noo\\nooooo\\noo\\noooo\\noo\\noooo\\noo\\nooooo\\noooooo\\noooooo\\noooo\\noooooo\\nooo\\noo\\nooo\\noo\\noo\\nooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noo\\noo\\no\\nooo\\noooo\\no\\noooooooo\\nooooooo\\noooooooooo\\nooo\\noo\\noooo\\noooo\\noooooooo\\nooooo\\no\\nooooo\\noo\\noo\\nooo\\noooo\\nooooo\\nooo\\noooooo\\nooo\\nooo\\nooo\\noo\\nooo\\noooooo\\noooooo\\nooo\\noooo\\noooo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\nooooo\\nooo\\noooo\\no oo\\no\\nooo\\nooo\\nooooooo\\nooooo\\noo\\noooo\\nooooooooo\\noooooo\\noooo\\noooo\\noooo\\noooooo\\no\\nooo\\noooooo\\no\\nooo\\noooo\\noo\\no\\nooo\\noooooooooooo\\nooo\\noooo\\noooo\\noo\\noo\\nooo\\no\\noooo\\no\\noo\\nooo\\noooo\\no\\noo\\noo ooo\\no\\nooooo\\nooooooo\\nooo\\noo\\nooo\\noo\\noo\\nooo\\noooooo\\nooo\\noo\\nooooo\\nooo\\nooooooooooooo\\nooooo\\nooooooooo\\noo\\nooooo\\noo\\noooo\\noo\\noooo\\noo\\nooooo\\noooooo\\noooooo\\noooooooooo\\nooo\\noo\\nooo\\noo\\noo\\nooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noo\\noo\\no\\nooo\\nooooo\\noooooooo\\nooooooo\\noooooooooo\\nooo\\noo\\noooo\\noooo\\noooooooo\\nooooo\\no\\nooooo\\noo\\noo\\nooo\\noooo\\noooooo oo\\noooooo\\nooo\\noooooo\\noo\\noooo\\nooooo\\noooooo\\nooo\\noooo\\noooo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\nooooo\\nooo\\noooo\\nooo\\no\\nooo\\nooo\\noooo ooo\\nooooo\\noo\\noooo\\noooo\\nooooo\\noooooo\\noooo\\noooo\\noooo\\noooooo\\no\\nooo\\noooo oo\\noooo\\noooo\\noo\\noooo\\nooooo\\nooooooo\\nooo\\noooooooo\\noo\\noo\\nooo\\no\\nooo0.0 0.4 0.8\\no\\no\\noo\\nooo\\noooo\\no\\noo\\noo ooo\\no\\nooo\\noo\\nooo o ooo\\nooo\\noo\\nooo\\noo\\noo\\nooo\\nooo ooo\\nooo\\noo\\nooo oo\\noooo\\noooooo o ooo oo\\noooo o\\no ooo\\no oooo\\noo\\no oo\\noo\\noo\\noooo\\noo\\noooo\\noo\\nooooo\\noooooo\\noo\\no ooo\\noooooooo oo\\no oo\\noo\\noo ooo\\noo\\nooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noo\\nooo\\nooo\\noooo\\no\\nooo o\\noo oo\\noo ooooo\\noo o ooooooo\\nooo\\noo\\noo oo\\noo oo\\noooooooo\\nooooo\\no\\no ooo o\\noo\\noo\\nooo\\noo oo\\noooooooo\\noooo oo\\nooo\\nooo\\nooo\\noo\\nooooooooo\\noooooo\\no oo\\noooo\\noo\\noo\\noooo\\no\\nooo\\noooo\\no oo\\noo\\nooo\\nooooo\\nooo\\noo oo\\nooo\\no\\no oo\\nooo\\nooo oo oo\\no oooo\\noo\\nooo o\\no o oo\\no\\no o oo\\nooo\\nooo\\no ooo\\noooo\\noooo\\no oo ooo\\no\\nooo\\noooooo\\noooo\\no ooo\\noo\\noooo\\nooooo\\nooo o ooo\\nooo\\noo o ooooo\\noo\\noo\\nooo\\no\\noooo\\no\\noo\\nooo\\noooo\\no\\noo\\noo ooo\\no\\nooooo\\nooooooo\\nooo\\noo\\nooo\\noo\\noo\\nooo\\noooooo\\nooo\\noo\\nooooooooooooooooooooo\\nooooo\\nooooooooo\\noo\\nooo\\noo\\noo\\noooo\\noo\\noooo\\noo\\nooooo\\noooooo\\noo\\noooo\\noooo\\noooooo\\nooo\\noo\\nooooo\\nooooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noo\\noo\\no\\nooo\\noooo\\no\\noooooooo\\nooooooo\\noooo\\noooooo\\nooo\\noo\\noooo\\noooo\\noooooooo\\nooooo\\no\\nooooo\\noo\\noo\\nooo\\noooo\\noooooooo\\noooooo\\nooo\\noooooo\\noo\\nooo\\noooooo\\noooooo\\nooo\\noooo\\noooo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\nooooo\\nooo\\noooo\\nooo\\no\\nooo\\nooo\\nooooooo\\nooooo\\noo\\noooo\\noooo\\nooooo\\noooooo\\noooo\\noooo\\noooo\\noooooo\\no\\nooo\\noooooo\\noooo\\noooo\\noo\\no\\nooo\\nooooo\\nooooooo\\nooo\\noooooooo\\noo\\noo\\nooo\\no\\nooo0 50 100\\no\\no\\noo\\nooo\\noooo\\no\\noo\\nooooo\\no\\nooooo\\nooooooo\\nooo\\noo\\nooo\\noo\\noo\\nooo\\noooooo\\nooo\\noo\\noooooooo\\nooooooooooooo\\nooooo\\noooo\\nooooo\\noo\\nooooo\\noo\\noooo\\noo\\noo\\noo\\noo\\nooooo\\nooo ooo\\noo\\noooo\\noooooooooo\\nooo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noooo\\no\\nooo\\nooooo\\noooooooo\\nooooooo\\noooooooooo\\nooo\\noo\\noooo\\noooo\\noooooooo\\nooo\\noo\\no\\nooooo\\noo\\noo\\nooo\\noooo\\noooooooo\\noooooo\\nooo\\noooooo\\noo\\noooo\\nooooo\\noooooo\\nooo\\noooo\\noo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\nooooo\\nooo\\noooo\\nooo\\no\\nooo\\nooo\\nooooooo\\nooooo\\noo\\noooooooo\\nooooo\\noooooo\\noooo\\noooo\\noooo\\noooooo\\no\\nooo\\noooooo\\noooo\\noooo\\noo\\noooo\\nooooo\\nooooooo\\nooo\\noooooooo\\noo\\noo\\nooo\\no\\nooo\\n100 160 220o\\no\\noo\\nooo\\no ooo\\no\\noo\\nooooo\\no\\nooooo\\nooooooo\\nooo\\noo\\nooo\\noo\\noo\\nooo\\noooooo\\nooo\\noo\\nooooooo o\\noooooooooo ooo\\nooooo\\noooo\\nooooo\\noo\\nooo\\noo\\noo\\noooo\\noo\\noooo\\noo\\nooooo\\no ooooo\\noooooo\\noooo\\noooo oo\\nooo\\noo\\nooooo\\nooooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noo\\noo\\no\\nooo\\nooooo\\nooooooo\\no\\nooo oooo\\noo ooooo ooo\\nooo\\noo\\noooo\\noooo\\noooooooo\\nooo\\noo\\no\\nooooo\\noo\\noo\\nooo\\noooo\\noo ooo\\nooo\\noooooo\\nooo\\noooooo\\noo\\noooooo\\nooo\\noooo oo\\nooo\\noooo\\noo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\no o\\nooo\\nooooo\\nooo\\noooo\\nooo\\no\\nooo\\nooo\\nooo oooo\\nooooo\\noooooo\\noo oo\\nooooo\\nooo\\nooo\\noooo\\noooo\\noooo\\noooooo\\no\\nooo\\nooo\\nooo\\noooo\\noooo\\noo\\noooo\\nooooo\\nooooooo\\nooo\\noooooooo\\noo\\noo\\nooo\\no\\nooo0 10 20 30o\\noooo\\nooooooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\no\\nooooo\\noooo\\noo\\nooooooo\\nooo\\noooo\\noo\\noo\\nooooo\\noooo\\noo\\noooo\\noooooo\\nooo\\noo\\nooo\\nooooooooooooo\\no\\nooooooo\\noooo\\nooo\\noo\\nooooooooo\\nooooo\\noooooooo oo\\nooooooooo\\no o\\noooo\\noo\\noo\\noo\\nooooo\\nooooooooooooo\\nooooooo\\noo\\no oooooooooooooooo\\noooo\\noooooo\\noooo\\noooooooo\\noooo\\noooo\\noo\\nooooo\\noo\\noooooo\\nooooo\\nooo\\noooooo\\nooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oooo\\nooo\\noo\\nooooooooo\\nooooo\\noooooooo oo\\nooooooooo\\no o\\noooo\\noo\\noo\\noo\\nooooo\\nooooooooooooo\\nooooooo\\noo\\no oooooooooooooooo\\noooo\\noooooo\\noooo\\noooooooo\\noooo\\noooo\\noo\\nooooo\\noo\\noooooo\\nooooo\\nooo\\noooooo\\nooo\\noo\\noooo\\noo\\noo\\nooooooo\\noooooooooooooo\\no\\nooooooo\\no\\noooo\\nooo\\noooo\\no\\noooo\\nooo oo\\nooooo\\noooo\\nooooo\\nooooo\\nooo\\nooo\\nooooooo\\noo\\noooooo\\nooo\\noo\\noooooooo\\nooo\\no\\noooo\\nooooo\\noooo\\noo\\noo o\\nooo\\no\\no\\noo\\noo\\noo\\noo\\noooo\\noo\\no\\nooooo\\nooo oooooooo\\nooo\\noooooooooooooooo oo\\notobaccoo\\noooo\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noooooo\\noooo\\noo\\nooooooo\\nooo\\noooo\\noo\\noo\\nooooo\\noooo\\noo\\noooo\\noooooo\\nooooo\\nooo\\nooo oooooooooo\\no\\nooooooo\\noooo\\nooo\\noo\\noo\\nooooooo\\nooooo\\noooooooo oo\\nooo\\noooooo\\noo\\noooo\\noo\\noo\\noo\\nooooo\\nooooooooooooo\\nooooooo\\noo\\nooooo oooooooooooo\\noooo\\noooooo\\noooo\\noooooooo\\noooo\\noooo\\noo\\nooooo\\noo\\nooo\\nooo\\nooooo\\nooo\\noooooo\\nooo\\noo\\noooooo\\noo\\noooo\\nooo\\nooooooo ooooooo\\no\\nooooooo\\no\\noooo\\nooo\\noooo\\no\\noooo\\nooooo\\nooooo\\noooo\\nooooo\\noo ooo\\nooo\\nooo\\nooooooo\\noo\\nooooooooo\\noo\\noooo oo\\noo\\nooo\\no\\noo\\noo\\nooooo\\noooo\\noo\\nooo\\nooo\\no\\no\\noo\\noo\\noo\\noo\\noooo\\noo\\no\\nooooo\\nooooooooooo\\no oo\\noooooooooo\\noooooooo\\noo\\no ooo\\noo o\\noooo\\nooo\\nooo\\noo\\noo\\no oo\\nooo\\no\\nooooooooo\\noo\\no o ooooo\\nooo\\noooo\\noo\\noo\\no o o oo\\noooo\\noo\\noooo\\no ooo oo\\nooooo\\no oo\\noo ooooooo oooo\\no\\no o ooooo\\noooo\\nooo\\noooo\\noooooo o\\no oooo\\nooooooo o oo\\no oooo oooo\\noo\\noooo\\noo\\noo\\noo\\nooooo\\noooooo o oooooo\\no oooo\\noo\\noo\\no o oo oo o oo oooo ooo ooooo\\noooooo\\noooo\\no ooo oo oo\\noooo\\noooo\\no o\\nooo oooo\\no ooooo\\nooooo\\nooo\\noo oooo\\nooo\\noo\\noo oo\\noo\\noo\\nooooooo\\noooooooooooooo\\no\\nooo o o oo\\no\\noooo\\nooo\\noooo\\no\\noo oo\\noo o oo\\nooooo\\nooo o\\nooooo\\noo oo o\\nooo\\nooo\\nooooooo\\noooo oo oo\\nooo\\noo\\no ooo oooo\\no oo\\no\\noo\\noo\\nooooo\\noooo\\noo\\nooo\\nooo\\no\\nooo\\noo\\noo\\noo\\noooo\\noo\\noooo oo\\nooooo o o o ooo\\no oo\\noo oo oooooo\\no ooo oooo\\noo\\noooo\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\no\\nooooooooo\\noo\\nooooooo\\nooo\\noooo\\noo\\noo\\nooooo\\noooo\\noo\\noooo\\noooooo\\nooo\\noo\\nooo\\nooooo\\noooooooo\\no\\nooooooo\\noooo\\nooo\\noo\\nooooooooo\\nooooo\\noooooooooo\\nooooooooo\\noo\\noooo\\noo\\noo\\noo\\nooooo\\nooooooooooooo\\nooooo\\noo\\noo\\nooooooooooooooooo\\noooo\\noooooo\\noooo\\noooooooo\\noooo\\noooo\\noo\\nooooooo\\noooooo\\nooooo\\nooo\\nooo ooo\\nooo\\noo\\noooooo\\noo\\nooooooo\\noooooooooooooo\\no\\nooooooo\\no\\noooo\\nooooooo\\no\\noooo\\nooo oo\\nooooo\\noooo\\nooooo\\nooooo\\nooo\\nooo\\nooooooo\\noooooooo\\nooo\\noo\\noooooooo\\nooo\\no\\noo\\noo\\nooooo\\noooo\\noo\\nooo\\nooo\\no\\no\\noo\\noo\\noo\\noo\\noooo\\noo\\no\\nooooo\\nooooooo oooo\\nooo\\noooooooooo\\noooo oooo\\noo\\noooo\\nooo\\noooo\\no oo\\nooo\\noo\\noo\\nooo\\nooo\\no\\nooooooooo\\noo\\nooooooo\\nooooooo\\noo\\noo\\nooooo\\noooo\\noo\\noooo\\nooo ooo\\nooooo\\nooo\\nooooooooooooo\\no\\nooooooo\\noooo\\nooo\\noo\\nooooooooo\\nooooo\\nooooooo ooo\\nooo\\noooooo\\noo\\noooo\\noo\\noo\\noo\\no oooo\\nooooooooooooo\\nooooo\\noo\\noo\\noooooooooooooooo o\\noooo\\noooooo\\noooo\\noooooooo\\noooo\\noo oo\\noo\\nooooooo\\nooooo o\\nooooo\\nooo\\noooooo\\nooo\\noo\\noooo\\noo\\noo\\noooo\\nooo\\noooooooooooooo\\noooooooo\\no\\noooo\\nooo\\noooo\\no\\noooo\\nooooo\\nooooo\\noooo\\nooooo\\nooooo\\nooo\\nooo\\nooooooo\\nooooooooooo\\noo\\no ooo oo\\noo\\nooo\\no\\noo\\noo\\nooooo\\noooo\\noo\\nooo\\nooo\\no\\no\\noo\\noo\\noo\\noo\\noooo\\noo\\no\\nooooo\\no ooooooo ooo\\nooo\\noooooooooooooooooo\\noo\\noooo\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\no\\nooooo\\noooo\\noo\\nooooooo\\nooo\\noooo\\noo\\noo\\no o ooo\\noooo\\noo\\noooo\\noo oooo\\nooooo\\no oo\\nooo oooooo oooo\\no\\nooooooo\\noooo\\nooo\\noo\\noooo ooooo\\nooooo\\no ooooooo oo\\nooo\\noooooo\\noo\\noooo\\noo\\noo\\noo\\nooooo\\nooooooooooooo\\nooooo\\noo\\noo\\nooooooooooooooooooooo\\noooo oo\\noooo\\no ooooooo\\noooo\\noooo\\noo\\nooooo\\noo\\noooooo\\nooooo\\nooo\\nooo ooo\\nooo\\noo\\noooo\\noo\\noo\\noooo\\nooo\\nooooooooo\\nooooo\\nooooo ooo\\no\\noooo\\nooo\\noooo\\nooooo\\nooo oo\\no oooo\\noooo\\nooooo\\nooooo\\nooo\\nooo\\nooooooo\\nooooooooooo\\noo\\noooooooo\\nooo\\nooo\\noo\\nooooo\\noooo\\noo\\nooo\\nooo\\no\\no\\noo\\noo\\noo\\noo\\noooo\\noo\\no\\nooooo\\noooooooooooooo\\noooooooooo\\nooo o oooo\\no\\noooo\\noo\\noooo\\noo\\nooooo\\nooo\\nooo\\nooo\\no\\nooo\\nooooo\\noo\\no\\nooo\\no\\noooooo\\nooo\\noo\\nooo\\noooo\\noooo\\nooooo\\noooooo\\noooooo\\noo\\no\\nooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noooo\\nooo\\noo\\noo\\nooo\\noooo\\nooo\\nooo\\noo\\nooo\\noo\\nooooooooo\\noooooo\\noooo\\nooo\\nooo\\nooooooo\\nooo\\noo\\nooo\\nooo\\noooo\\noo\\nooooo\\noooo\\nooooo\\no\\nooooooooo\\noo\\no\\no\\nooo\\noooo\\noooo\\nooo\\nooo\\noo\\nooooooo\\nooooo\\no\\nooooooo\\noo\\nooo\\nooo\\nooooo\\nooooo\\noooo\\noooo\\noooooooooooo\\noo\\noo\\noooooooo\\noo\\nooooo oo\\noooo\\noooooooooooo\\nooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noooooo\\noo\\noooooo\\noooooooooo\\nooo\\noooooo\\nooo\\nooo\\noo\\nooooooooo\\noo\\noooooo\\no\\nooooo\\noo\\noo\\noo\\noooooooooo\\noooo\\nooooo ooooo\\noo\\noooo\\nooo\\noooo\\no\\noooo\\noo\\noo\\nooo\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oooo\\noooooooooooo\\nooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noooooo\\noo\\noooooo\\noooooooooo\\nooo\\noooooo\\nooo\\nooo\\noo\\nooooooooo\\noo\\noooooo\\no\\nooooo\\noo\\noo\\noo\\noooooooooo\\noooo\\nooooo ooooo\\noo\\noooo\\nooo\\noooo\\no\\noooo\\noo\\noo\\nooo\\noo\\noooo\\noo\\nooooo\\nooo\\nooo\\no oo\\no\\nooo\\nooooo\\noo\\no\\nooo\\no\\noooooo\\nooo\\noo\\nooo\\noooo\\noooo\\nooooo\\noooooo\\noooooo\\noo\\noooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noooo\\nooo\\noo\\noo\\nooo\\noooo\\nooo\\nooo\\nooooo\\noo\\nooooooooo\\noooooo\\noooo\\nooo\\nooo\\no oooooo\\nooo\\noo\\nooo\\nooo\\nooo\\no\\noo\\nooooo\\noo\\noo\\nooooo\\no\\nooo\\noooooo\\noo\\no\\no\\nooo\\noooo\\noooo\\nooo\\nooo\\nooo\\noooooo\\nooooo\\no\\noooo\\noo\\no\\noo\\nooo\\nooo\\nooooo\\nooooo\\noooo\\noooo\\noooooooo oooo\\noo\\noo\\noooooooo\\noo\\nooooooo\\noooo\\noooooooooooo\\nooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\nooo\\nooo\\noo\\noooooo\\noooooo\\noooo\\nooo\\noooooo\\nooo\\nooo\\noo\\no\\noooooooo\\noo\\noooooo\\no\\nooooo\\noo\\noo\\noo\\noooo oooooo\\noooo\\noooo\\noooooo\\noo\\no\\nooo\\nooo\\noooo\\no\\nooo\\no\\noo\\noldl\\noo oo\\noo\\noooo\\noo\\noo o oo\\nooo\\nooo\\nooo\\no\\no oo\\noo ooo\\noo\\noooo\\no\\noo oooo\\nooo\\noo\\no oo\\noooo\\nooo o\\no oo\\noo\\noooooo\\noo oo oo\\noo\\noooo oo\\nooo\\nooo\\noooo\\noo\\noo oo\\nooo o\\noo o\\noo\\noo\\nooo\\noooo\\nooo\\nooo\\noo\\nooo\\noo\\no o ooooooo\\nooo ooo\\nooooo ooooo\\nooooooo\\nooo\\noo\\no oo\\no oo\\nooo\\no\\noo\\nooooo\\noooo\\nooooo\\no\\nooo\\no oo ooo\\noo\\no\\no\\nooo\\noooo\\noooo\\nooo\\nooo\\noo\\no\\noooooo\\noo ooo\\no\\noooooo\\no\\noo\\nooo\\nooo\\no oooo\\nooooo\\no ooo\\noooo\\noooooooooooo\\noo\\noo\\nooooo o oo\\noo\\no oo\\noo oo\\no ooo\\nooooo ooooooo\\nooo\\no oo\\nooooo\\no\\noo\\noo\\nooo\\no\\nooo\\nooo\\noo\\noooo\\noo\\noooooo\\noooo\\no o o\\nooo ooo\\nooo\\nooo\\noo\\no\\noooooooo\\noo\\nooo ooo\\no\\nooooo\\noo\\noo\\noo\\noo oooooooo\\noo oo\\noooo\\no ooooo\\noo\\noooo\\nooo\\noo oo\\no\\nooo\\no\\noo\\nooooo\\noo\\noooo\\noo\\nooooo\\nooo\\nooo\\nooo\\no\\nooo\\nooooo\\noo\\no\\nooo\\no\\noo o ooo\\nooo\\noo\\nooo\\noooo\\noooo\\nooooo\\noooooo\\noooooo\\noo\\nooooooooo\\nooo\\noooo\\noo\\noooo\\noooo\\nooo\\noo\\noo\\nooo\\noooo\\nooo\\nooo\\noo\\nooo\\noo\\noo ooooooo\\noooooo\\noo\\noo\\nooo\\nooo\\nooooooo\\nooo\\noo\\nooo\\nooo\\noooo\\noo\\nooooo\\noooo\\nooo oo\\no\\nooooooooooo\\no\\no\\nooo\\noooo\\noooo\\nooo\\nooo\\nooooooooo\\nooooo\\no\\noooooo\\no\\noo\\nooo\\nooo\\nooooo\\nooooo\\noooo\\noooo\\noooooooooooo\\noo\\noo\\noooooooo\\noo\\nooooooo\\noooo\\noooooooooooo\\nooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noooooo\\noo\\noooooo\\noooooooooo\\nooo\\noooooo\\nooo\\nooo\\noo\\no\\noooooooo\\noo\\noooooo\\no\\nooooo\\noo\\noo\\noo\\noooooooooo\\nooo o\\noooooooooo\\noo\\noooo\\nooo\\noooo\\no\\nooo\\no\\noo\\nooooo\\noo\\noooo\\noo\\nooo oo\\nooo\\nooo\\nooo\\no\\nooo\\nooooo\\noo\\noooo\\no\\noooooo\\nooo\\noo\\nooo\\noooo\\noooo\\nooooo\\noooooo\\noooooo\\noo\\nooooooo\\noo\\nooo\\noooo\\noo\\noooo\\noooo\\nooo\\noo\\noo\\nooo\\noooo\\nooo\\nooo\\noo\\nooo\\noo\\nooooooooo\\noooooo\\noooo\\no oo\\nooo\\nooooooo\\nooo\\noo\\nooo\\nooo\\nooo\\no\\noo\\nooooo\\noooo\\nooooo\\no\\nooo\\noooooo\\noo\\no\\no\\nooo\\noo\\noo\\noooo\\nooo\\nooo\\noo\\noo ooooo\\nooooo\\no\\noooooo\\no\\noo\\nooo\\nooo\\nooooo\\nooooo\\noooo\\noooo\\noooooooooooo\\noo\\noo\\noooooooo\\noo\\nooooooo\\noooo\\noooooooooooo\\no oo\\nooo\\noo\\nooo\\no\\nooo\\no\\nooo\\no\\nooo\\nooo\\noo\\noooo\\noo\\noooooo\\nooooo oo\\noooooo\\nooo\\nooo\\noo\\no\\noooooooo\\noo\\noooooo\\no\\nooooo\\noo\\noo\\noo\\noooo oooooo\\noooo\\noooo\\noooooo\\noo\\noooo\\nooooooo\\no\\noooo\\noo\\no\\n2 6 10 14oooo\\noo\\noooo\\noo\\nooooo\\nooo\\nooo\\nooo\\no\\nooo\\nooooo\\noo\\no\\nooo\\no\\noo o ooo\\nooo\\noo\\nooo\\noooo\\noooo\\no oooo\\noooooo\\noooooo\\noo\\noooooo\\noooooo\\no ooo\\noo\\noooo\\noooo\\nooo\\noo\\noo\\nooo\\noo oo\\nooo\\nooo\\noo\\no oo\\noo\\nooo ooooo o\\noooooo\\no oooo oo\\nooo\\no ooo ooo\\nooo\\noo\\no oo\\nooo\\noooo\\noo\\nooooo\\noo\\noo\\nooooo\\no\\nooooooooo\\noo\\no\\no\\nooo\\noooo\\noooo\\nooo\\nooo\\noo\\nooooooo\\no oooo\\no\\noooooo\\no\\noo\\nooo\\nooo\\noo ooo\\nooooo\\noooo\\noooo\\noooooooooooo\\noo\\noo\\noooooooo\\noo\\nooo\\noooo\\noooo\\nooo ooooooooo\\nooo\\nooo\\nooooo\\no\\noo\\noo\\nooo\\no\\nooo\\no oo\\noo\\noooo\\noo\\noooooooooo\\noo o\\noooooo\\nooo\\nooo\\noo\\no\\noooooooo\\noo\\noooooo\\no\\nooooo\\noo\\noo\\noo\\noooooooooo\\noooo\\noooo\\noooooo\\noo\\noooo\\nooooooo\\no\\noooo\\noo\\no0.0 0.4 0.8o\\nooooo\\noooo\\noo\\noooo\\noooo\\nooo\\no oo\\nooo\\noooo\\noooooooo\\nooo\\noooo\\nooo\\nooo\\noooooo\\nooo\\noooo\\noo\\nooooo\\noooo\\nooo\\noo\\noo\\noooo\\nooooo ooooo\\noo\\noo\\nooooo\\noooo\\noo\\noo\\nooo\\nooooooo\\noo\\noo\\noooooo\\noo\\noo\\noo\\noooooo\\noooooooo\\nooooo o ooooooooo\\noooooo\\nooooooooo\\noooo\\nooo\\nooo\\nooo\\nooo\\nooo\\nooo\\noo\\noo\\nooooooo o oo\\noo\\nooo\\nooo o\\nooooooo\\noooo\\nooo\\nooo\\noo\\noooooooo\\noo\\nooooo\\nooooo o\\noo\\noooo\\noooooo oo\\nooooooooo\\nooooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noo\\nooo\\noo\\nooo\\noooooo\\noooo\\noo\\noooo\\nooo o\\noo\\nooooooo\\nooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\nooooo\\noooo\\nooooo\\nooooo\\noo\\nooooo\\noooo\\noooooo oo\\nooo\\no oooo\\nooo\\noo\\noooooo\\noo\\no oooo\\noooo\\noooo\\noooo\\nooo\\noo ooo o\\nooooo\\noooo\\noo\\noooo\\nooo o\\noo o\\no oo\\nooo\\noooo\\noooooooo\\nooo\\noooo\\nooo\\nooo\\noooooo\\nooo\\noooo\\noo\\nooooo\\noooo\\nooo\\noo\\noo\\noooo\\noooooooooo\\noo\\noo\\no oooo\\noooo\\noo\\noo\\nooo\\noo ooooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='o oooo\\noooo\\noooo\\noooo\\nooo\\noo ooo o\\nooooo\\noooo\\noo\\noooo\\nooo o\\noo o\\no oo\\nooo\\noooo\\noooooooo\\nooo\\noooo\\nooo\\nooo\\noooooo\\nooo\\noooo\\noo\\nooooo\\noooo\\nooo\\noo\\noo\\noooo\\noooooooooo\\noo\\noo\\no oooo\\noooo\\noo\\noo\\nooo\\noo ooooo\\noo\\noo\\noooooo\\noo\\noo\\noo\\noo oooo\\noooooooo\\nooooo o ooooooooo\\noo oooo\\nooooooooo\\no ooo\\nooo\\nooo\\nooo\\nooo\\nooo\\nooo\\noo\\noo\\noooooooooo\\noo\\nooo\\noooo\\noooo ooo\\noooo\\nooo\\nooo\\noo\\noooooooo\\noo\\nooooo\\nooooo o\\noo\\noooo\\noooooooo\\nooooooooo\\nooooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noo\\nooo\\noo\\nooo\\noooooo\\no ooo\\noo\\no ooo\\noooo\\noo\\nooooooo\\nooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\nooooo\\noooo\\nooooo\\nooo oo\\noo\\nooooo\\noo oo\\noo o ooooo\\nooo\\nooooo\\nooo\\noo\\noooooo\\noo\\nooooo\\no ooo\\noooo\\noooo\\nooo\\nooooo o\\nooooo\\noooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\noooo\\noooooooo\\nooo\\noooo\\nooo\\nooo\\noooooo\\nooo\\noooo\\noo\\nooooo\\noooo\\nooo\\noo\\noo\\noooo\\noooooo oooo\\noo\\noo\\nooooo\\noooo\\noo\\noo\\nooo\\nooooooo\\noo\\noo\\noooooo\\noo\\noo\\noo\\noooooo\\nooooooo o\\nooooooooooooooo\\noooooo\\nooooooooo\\noooo\\nooo\\nooo\\nooo\\nooo\\nooo\\nooo\\noo\\noo\\no ooo oooooo\\noo\\nooo\\noooo\\nooo oooo\\noooo\\nooo\\nooo\\noo\\noooooooo\\noo\\nooooo\\noooooo\\noo\\noooo\\noooooooo\\nooooooo oo\\nooooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noo\\nooo\\noo\\nooo\\noooooo\\noooo\\noo\\noooo\\noooo\\noo\\noo ooooo\\nooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\nooooo\\noooo\\nooooo\\nooooo\\noo\\nooooo\\noooo\\noooo oooo\\nooo\\nooooo\\nooo\\noo\\noooooo\\noo\\nooooo\\noooo\\noooo\\noooo\\nooo\\nooo oo\\nfamhisto\\nooooo\\noooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\noooo\\noooooooo\\nooo\\no ooo\\nooo\\nooo\\noo oooo\\noo o\\noooo\\noo\\noo ooo\\noooo\\nooo\\noo\\noo\\noooo\\noooooooooo\\noo\\noo\\nooooo\\noooo\\noo\\noo\\nooo\\nooo oooo\\noo\\noo\\noooooo\\noo\\noo\\noo\\noooooo\\noooooooo\\noo ooooooooooooo\\noooooo\\nooooooooo\\noooo\\nooo\\no oo\\nooo\\nooo\\nooo\\nooo\\noo\\noo\\noooooooooo\\noo\\nooo\\nooo o\\nooooooo\\noooo\\nooo\\nooo\\noo\\noooooooo\\noo\\nooooo\\noooooo\\noo\\noooo\\noooooooo\\noooooo ooo\\nooooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noo\\noo o\\noo\\nooo\\noooooo\\noooo\\noo\\noooo\\noooo\\noo\\nooooooo\\nooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\nooooo\\noooo\\nooooo\\nooooo\\noo\\nooooo\\noooo\\noooooooo\\noo o\\nooooo\\nooo\\noo\\noooooo\\noo\\nooooo\\noooo\\noooo\\noooo\\no oo\\nooooo o\\nooooo\\noooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\noooo\\nooooooo o\\nooo\\noooo\\nooo\\nooo\\noo oooo\\nooo\\noooo\\noo\\nooooo\\noooo\\nooo\\noo\\noo\\noooo\\noooooooooo\\noo\\noo\\noo o oo\\noooo\\noo\\noo\\nooo\\nooooooo\\noo\\noo\\noooooo\\noo\\noo\\noo\\noooooo\\noooooooo\\no ooooooo ooooooo\\noooooo\\nooooooooo\\noooo\\nooo\\nooo\\nooo\\nooo\\nooo\\nooo\\noo\\noo\\noooooooooo\\noo\\nooo\\nooo o\\nooo o ooo\\noooo\\nooo\\nooo\\noo\\no ooooooo\\noo\\nooooo\\no ooooo\\noo\\noooo\\noooooooo\\nooooooooo\\nooooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noo\\nooo\\noo\\nooo\\noooooo\\noooo\\noo\\nooo o\\noooo\\noo\\nooooooo\\nooo\\noo\\noooo\\noooo\\noo o\\noo o\\nooo\\nooooo\\noooo\\nooooo\\nooooo\\noo\\nooooo\\noooo\\no ooooooo\\nooo\\noo ooo\\nooo\\noo\\noooooo\\noo\\nooooo\\noooo\\nooo o\\noooo\\nooo\\nooooo o\\nooooo\\noo oo\\noo\\no ooo\\noooo\\nooo\\nooo\\nooo\\noooo\\noooooo oo\\nooo\\no ooo\\no oo\\nooo\\noooooo\\nooo\\noooo\\noo\\noo ooo\\noooo\\nooo\\noo\\noo\\noooo\\no o o ooooooo\\noo\\noo\\noo ooo\\noooo\\noo\\noo\\nooo\\no o ooooo\\noo\\noo\\noooooo\\noo\\noo\\noo\\noooooo\\noooooooo\\no o ooo o ooo oooooo\\noo oooo\\noo o oooooo\\noooo\\nooo\\nooo\\nooo\\no oo\\no oo\\noo o\\noo\\noo\\no oo oooo o oo\\noo\\nooo\\noooo\\noo ooooo\\noo oo\\nooo\\nooo\\noo\\no o oooooo\\noo\\noo ooo\\noo oooo\\noo\\noooo\\noooooooo\\nooooooo oo\\noooo ooo\\nooo\\noo o\\noooo\\noo\\no ooo\\noo\\nooo\\noo\\nooo\\nooo ooo\\noooo\\noo\\noooo\\no oo o\\noo\\nooooooo\\nooo\\noo\\noooo\\noooo\\nooo\\noo o\\nooo\\nooooo\\noooo\\nooooo\\nooooo\\noo\\nooooo\\noooo\\noo oooooo\\nooo\\noo ooo\\nooo\\noo\\noooooo\\noo\\nooooo\\noooo\\noooo\\no ooo\\nooo\\nooooo\\noooo\\noo\\nooooo\\nooooooo\\noo\\nooooo\\noo\\nooo\\nooooo\\nooo\\nooo\\noooo\\nooo\\nooooo\\nooooooooooo\\nooo\\noo\\noooo\\noo\\nooooooo\\noo\\noo\\noo\\noo\\nooo\\noo\\nooooooooooo\\noo\\noooo\\noo\\noo\\noooo\\noooooo\\noo\\noo\\noo\\nooooooooo\\nooo\\nooooo\\nooooooooo\\noooo\\noooooooo\\no\\nooo\\nooo\\nooooo\\no\\noo\\noo\\nooooo\\noo\\nooooo\\no ooo\\nooooo\\noo\\noo\\noo\\noo\\noooo\\noo\\noo\\nooooooooo\\nooooo\\noooooo\\no\\noooooo\\noooooo\\nooo\\noooo\\noooo\\noooo\\noooooo\\nooooooo\\nooooo\\nooooo\\nooooo\\nooo\\no\\nooo\\nooo\\noooo\\no\\nooooooooo\\nooo\\noooooo\\nooooooo\\noooooo\\no\\noooooo\\nooooo\\nooooooooo\\no\\nooooo\\nooooooo\\nooo\\nooo\\noooooo\\nooo\\noo\\nooooo\\nooo\\nooo\\noooo\\no\\noo\\nooo\\nooo oo\\nooooo\\noo\\noooo\\nooooooo\\noooo\\noo\\nooo\\nooooo\\noo\\noo\\noo\\nooooo\\noo\\noooo\\no\\nooooooooo\\nooooooo\\nooo\\nooooo\\nooo\\nooo\\noooo\\nooo\\nooooo\\nooooooooooo\\nooo\\noo\\noooo\\noo\\nooooooo\\noo\\noo\\noo\\noo\\nooo\\noo\\noooo\\nooooooo\\noo\\noooo\\noo\\noo\\noooo\\noooooo\\noo\\noo\\noo\\nooooooooo\\nooo\\nooooo\\nooooooooo\\noooo\\noooooooo\\no\\nooo\\nooo\\nooooo\\no\\noo\\nooooooo\\noo\\nooooooooo\\nooooooooo\\noo\\noo\\noooo\\noo\\noo\\noooooo\\nooo\\nooooo\\noooooo\\no\\noooooo\\noooooo\\nooo\\noooo\\noo oo\\noooo\\noooooo\\nooooooo\\nooooo\\nooooo\\nooooo\\nooo\\no\\nooo\\nooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='ooooooooo\\noooo\\noooooooo\\no\\nooo\\nooo\\nooooo\\no\\noo\\nooooooo\\noo\\nooooooooo\\nooooooooo\\noo\\noo\\noooo\\noo\\noo\\noooooo\\nooo\\nooooo\\noooooo\\no\\noooooo\\noooooo\\nooo\\noooo\\noo oo\\noooo\\noooooo\\nooooooo\\nooooo\\nooooo\\nooooo\\nooo\\no\\nooo\\nooo\\noooo\\no\\nooooooooo\\nooo\\noooooo\\nooooooo\\nooooooo\\noooooo\\nooooo\\nooooooooo\\no\\noooooooo\\noooo\\nooo\\nooo\\noooooo\\nooo\\noo\\nooooo\\nooo\\nooo\\noooo\\no\\noo\\nooo\\nooooo\\nooooo\\noo\\nooooooooooo\\noooo\\noo\\nooo\\nooooo\\noo\\noo\\noo\\nooooo\\noo\\noooooooooo oo\\noo\\nooooooo\\nooo\\nooooo\\nooo\\nooo\\noooo\\nooo\\nooooo\\nooooooooooo\\nooo\\noo\\noooo\\noo\\nooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\noo\\nooooo oooooo\\noo\\noooo\\noo\\noo\\noooo\\noooooo\\noo\\noo\\noo\\noo\\noo\\nooooo\\nooo\\nooooo\\nooooooooo\\noooo\\noooooooo\\no\\nooo\\nooo\\nooooo\\no\\noo\\noo\\nooooo\\noo\\noooo\\nooooo\\nooooooo\\noo\\noooo\\noooo\\noo\\noo\\nooooooooo\\nooooo\\noooooo\\no\\noooooo\\noooooo\\nooo\\noooo\\noooo\\noooo\\noooooo\\nooooooo\\nooooo\\nooooo\\nooooo\\nooo\\no\\nooo\\nooo\\noooo\\no\\nooooooooo\\nooo\\nooo\\nooo\\nooooooo\\nooooooo\\noooooo\\nooooo\\noooo\\nooooo\\no\\noooooooo\\noooo\\nooo\\nooo\\noooooo\\nooo\\noo\\nooooo\\nooo\\nooo\\noooo\\no\\noo\\nooo\\nooooo\\nooooo\\noo\\nooo oooooooo\\noooo\\noo\\nooo\\nooooo\\noo\\noo\\noo\\nooo oo\\noo\\nooooo\\nooo oo oooo\\nooooooo\\noo o\\nooooo\\nooo\\nooo\\noooo\\nooo\\noo ooo\\noooooo o ooo o\\nooo\\noo\\noooo\\noo\\nooooo\\noo\\no o\\noo\\noo\\noo\\nooo\\noo\\noooooooo ooo\\noo\\nooo o\\noo\\noo\\noooo\\noooooo\\noo\\noo\\noo\\nooooo o ooo\\nooo\\noo oo o\\nooooooo oo\\noooo\\noooooooo\\no\\nooo\\nooo\\nooooo\\no\\noo\\nooo oooo\\noo\\no ooo\\nooo oo\\no oooooo\\noo\\noo\\noo\\noooo\\noo\\noo\\nooo o ooooo\\nooooo\\noooo oo\\no\\no oo oo o\\noooooo\\no oo\\noooo\\no\\nooo\\noooo\\no o\\noooo\\nooooooo\\nooooo\\nooooo\\no\\noooo\\nooo\\no\\nooo\\nooo\\noooo\\no\\no oooo oo oo\\nooo\\nooo\\nooo\\noo ooooo\\nooooooo\\noooooo\\nooooo\\no oooooo oo\\no\\noo\\noooooooooo\\nooo\\nooo\\noooooo\\nooo\\noo\\noo ooo\\nooo\\nooo\\noooo\\no\\noo\\no oo\\nooooo\\nooooo\\noo\\nooo oooooooo\\noooo\\noo\\nooo\\nooooo\\noo\\noo\\noo\\noobesity\\noooo\\noo\\noooo\\noooooooooo\\nooooo\\noo\\nooo\\nooooo\\nooo\\nooo\\noooo\\nooo\\nooooo\\noooooooooo o\\nooo\\noo\\noooo\\noo\\nooooooo\\noo\\noo\\noo\\noo\\nooooo\\noooo\\nooooooooo\\noooo\\noo\\noo\\noooo\\noooooo\\noo\\noo\\noo\\noo\\nooooooo\\nooo\\nooooo\\nooooooooo\\noooo\\noooooooo\\no\\nooo\\nooo\\nooooo\\no\\noo\\nooooooo\\noo\\noooo\\nooooo\\nooooooo\\noo\\noo\\noo\\noooo\\noo\\noo\\nooooooooo\\nooooo\\noooooo\\no\\noooooo\\noooooo\\nooo\\noooo\\no\\nooo\\noooo\\nooooo\\no\\nooooooo\\nooooo\\nooooo\\nooooo\\nooo\\no\\nooo\\nooo\\noooo\\no\\nooooooooo\\nooo\\nooo\\nooo\\nooooooo\\nooooooo\\noooooo\\nooooo\\nooooooooo\\no\\noo\\nooooo\\nooooo\\nooo\\nooo\\noooooo\\nooo\\noo\\nooooo\\noooooo\\noooo\\no\\noo\\nooo\\nooo oo\\nooooo\\noo\\nooooooooo oo\\noooo\\noo\\nooo\\noooo\\no\\noo\\noo\\noo\\no\\n15 25 35 45oooo\\noo\\nooooo\\nooo oooooo\\nooooooo\\nooo\\nooooo\\nooo\\nooo\\noooo\\nooo\\nooooo\\noooooo ooooo\\nooo\\noo\\noooo\\noo\\nooooooo\\noo\\noo\\noo\\noo\\nooo\\noo\\nooooooooooo\\noo\\noooo\\noo\\noo\\noooo\\noooooo\\noo\\noo\\noo\\noooo\\nooooo\\nooo\\nooooo\\noooo\\nooo oo\\noooo\\nooo ooooo\\no\\nooo\\nooo\\nooooo\\no\\noo\\noo\\nooooo\\noo\\nooooo\\noooo\\nooooooo\\noo\\noo\\noo\\noooo\\noo\\noo\\nooooooooo\\nooooo\\noooooo\\no\\noooooo\\noooooo\\nooo\\noooo\\no\\nooo\\noooo\\noo\\nooo\\no\\nooooooo\\nooooo\\nooooo\\no\\noooo\\nooo\\no\\nooo\\nooo\\noooo\\no\\nooooooooo\\nooo\\nooo\\nooo\\nooooooo\\nooooooo\\noooooo\\nooooo\\nooooooooo\\no\\noo\\noooooo\\noooo\\nooo\\nooo\\noooooo\\nooo\\noo\\nooooo\\nooo\\nooo\\noooo\\no\\noo\\nooo\\nooooo\\nooooo\\noo\\noooo\\nooooooo\\noooo\\noo\\nooo\\nooooo\\noo\\noo\\noo\\no0 50 100o\\noooo\\noooooo\\nooo ooo\\noo\\nooo\\noooooo\\nooooooooo\\nooo\\nooooooooo\\nooo\\noooo\\noooooooooo\\nooooooo\\nooooooo\\noooooo\\nooooooooooo\\nooooooooo\\no\\noooooooo\\noo\\noo\\noooooo\\noooo\\nooo\\nooo\\nooo\\noooo\\noo\\nooooooooooo\\no\\noo\\noo\\noo\\nooooooooo oooooooooooooo\\nooooooooo\\nooooooo\\no\\nooo\\nooooo\\noooooooooooooooo\\noo\\nooo\\noooo\\noooooo\\noo\\noooooooo\\noooooo\\nooo\\noooo\\noooooo\\noooooooooooo\\nooo\\nooooo\\noooo\\noo\\nooo\\nooooooo\\nooo\\no\\noo\\nooo\\no\\noo\\nooo\\noo\\noooo oooooooooo\\nooo\\no\\noo\\nooo\\noooooo\\noo\\nooo\\nooooooo\\noo\\noooooo\\no\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\no\\nooooooo\\noo\\nooooo\\no ooo\\noooo\\noo\\noooo\\no\\noooooo ooooooooo\\noo\\nooooo\\no\\noo\\nooo\\nooooo\\noo\\noooo\\noooooo\\noo oooo\\noo\\nooo\\no ooooo\\nooooooooo\\nooo\\noooo\\nooooo\\nooo\\noooo\\noooooooooo\\nooooooo\\nooooooo\\noooooo\\nooooooooooo\\nooo\\noooooo\\nooooooooo\\noo\\noo\\noooooo\\noooo\\nooo\\nooo\\nooo\\noooo\\noo\\nooooooooooo\\no\\noo\\noo\\noo\\nooooooooooooooooooooooo\\noooooooooooooooo\\no\\nooo\\no oo\\noo\\noo\\noooooooooooooooo\\nooo\\noooo\\noooooo\\noo\\noooooooo\\noooooo\\nooo\\noooo\\noooooo\\noooooooooooo\\nooo\\nooooo\\noooo\\noo\\nooo\\noooooooooo\\no\\noo\\nooo\\no\\noo\\nooo\\noo\\nooooooooo\\nooooo\\nooo\\no\\noo\\nooo\\noooooooo\\nooo\\noo\\nooooo\\noo\\noooooo\\no\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\no\\nooooooooo\\noo ooo\\noooo\\noooo\\noo\\noooo\\no\\noo ooooo oooooooo\\noo\\nooooo\\no\\noo\\nooo\\nooooo\\noo\\noooo\\noooooo\\noooooo\\noo\\nooo\\noooooo\\nooooooooo\\nooo\\nooooo oooo\\nooo\\noo\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oo\\noooooo\\no\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\no\\nooooooooo\\noo ooo\\noooo\\noooo\\noo\\noooo\\no\\noo ooooo oooooooo\\noo\\nooooo\\no\\noo\\nooo\\nooooo\\noo\\noooo\\noooooo\\noooooo\\noo\\nooo\\noooooo\\nooooooooo\\nooo\\nooooo oooo\\nooo\\noo\\noo\\noooooooooo\\nooooooo\\nooooooo\\noooooo\\nooo oooooooo\\nooo\\noooooo\\nooooooooo\\noo\\noo\\noooooo\\noooo\\nooo\\nooo\\nooo\\noooooo\\nooooo\\noooooo\\no\\noo\\noo\\noo\\nooooooooooooooooooooooo\\noooooooooooooooo\\no\\nooo\\nooo\\noo\\noo\\noooooooooooooo\\noo\\nooo\\noooo\\noooooo\\noo\\no ooooooo\\noooooo\\nooo\\noooo\\noooooo\\noooooooooooo\\nooo\\nooooo\\noooo\\noo\\nooo\\noooooooooo\\no\\noo\\nooo\\no\\noo\\nooo\\noo\\noooooooooooooo\\nooo\\no\\noo\\nooo\\noooooo\\noo\\nooo\\nooooooo\\noo\\noooooo\\no\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\no\\nooooooooo\\nooooooooo\\noooo\\noo\\noooo\\no\\noooooooooo ooo oo\\noo\\nooooo\\no\\noo\\nooo\\noooo o\\noo\\no ooo\\noo oooo\\no oo oo o\\noo\\nooo\\no o o o oo\\no oooooooo\\nooo\\no ooooo ooo\\no oo\\noooo\\nooo o o ooooo\\nooooooo\\no ooo ooo\\no o oooo\\no oo oooooooo\\noooo o oooo\\noo ooo o o oo\\noo\\noo\\noooooo\\no ooo\\nooo\\nooo\\no oo\\noooo\\noo\\nooooo\\noooooo\\no\\noo\\noo\\noo\\nooooooooo ooo ooooooooooo\\no oooo o ooooooo ooo\\no\\nooo\\no oo\\noo\\nooooooooooo ooo oo\\noo\\nooo\\noooo\\noooooo\\noo\\no oo oo ooo\\noo oooo\\nooo\\noooo\\no ooooo\\no o ooo ooooooo\\nooo\\noo ooo\\noooo\\noo\\nooo\\noo oo o oo\\noooo\\noo\\nooo\\no\\noo\\nooo\\noo\\no oo o oooooooooo\\nooo\\no\\noo\\no oo\\noo o ooooo\\nooo\\noo\\noo ooo\\noo\\nooo o oo\\no\\noo\\noo\\noo\\nooo\\no oo\\noo ooo\\noooo\\noo oo\\no\\nooooo o ooo\\no oooooooo\\noooo\\noo\\no ooo\\no\\no ooooo o oooooo oo\\noo\\no oooo\\no\\noo\\nooo\\nooooo\\noo\\noooo\\noooooo\\noooooo\\noo\\nooo\\nooooooooooooooo\\nooo\\nooooo oooo\\nooo\\noooo\\nooooooo\\nooo\\nooooooo\\nooooooo\\noooooo\\nooooooooooo\\nooooooooo\\no\\noooooooo\\noo\\noo\\noooooo\\noooo\\nooo\\nooo\\nooo\\noooooo\\nooooo\\noo oooo\\no\\noo\\noo\\noo\\nooooooooo oooooooooooooo\\noooooo oooooooooo\\no\\nooo\\nooo\\noo\\nooooooooooo ooooo\\noo\\nooo\\noooo\\noooooo\\noo\\noooooooo\\noooooo\\nooo\\noooo\\noooooo\\noooooooooooo\\nooo\\nooooo\\noooo\\noo\\nooo\\nooooooo\\nooo\\no\\noo\\nooo\\no\\noo\\nooo\\noo\\noooooooooooooo\\nooo\\no\\noo\\nooo\\noooooooo\\nooo\\noo\\nooooo\\noo\\noooooo\\no\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\no\\nooooooooo\\nooooooooo\\noooo\\noo\\noooo\\no\\nooooooooooooooo\\noo\\nooooo\\no\\noo\\nooo\\nooooo\\noalcoholo\\noooo\\noooooo\\nooo ooo\\noo\\nooo\\noo oooo\\nooooooooo\\nooo\\nooooo oooo\\nooo\\noooo\\nooo o ooo\\nooo\\nooooooo\\nooooooo\\noooooo\\noooo o oooooo\\nooooooooo\\nooooo oooo\\noo\\noo\\noo oooo\\no ooo\\nooo\\nooo\\nooo\\noooo\\noo\\nooooo\\noooooo\\no\\noo\\noo\\noo\\nooooooooooooooooo o ooooo\\nooooooooo\\nooooooo\\no\\nooo\\nooooo\\noooo o oooooo ooooo\\noo\\nooo\\noo oo\\nooo o\\noo\\noo\\noooooooo\\no oooo\\no\\nooo\\noooo\\no oooo o\\noooooooooooo\\nooo\\nooooo\\noooo\\noo\\nooo\\nooooo oo\\noooo\\noo\\nooo\\no\\noo\\nooo\\noo\\noooo ooooo\\nooooo\\nooo\\nooo\\nooo\\noooooooo\\nooo\\noooo ooo\\noo\\noooooo\\no\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\no\\nooooooooo\\noo ooo\\noooo\\noooo\\noo\\noooo\\no\\no ooooo ooooooo oo\\noo\\nooooo\\no\\noo\\nooo\\no ooo o\\no\\n100 160 220oo\\noo\\noo\\noo\\nooo\\no\\nooo\\nooooo\\noooo\\no\\nooo\\noo\\nooo\\noo\\nooo\\noo\\no\\no\\noo\\nooo\\no\\noooo\\no\\nooo\\noo\\noo\\nooo\\no\\noo\\noooo\\nooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\nooo\\no\\nooo\\noo\\nooo\\noo\\noooooo\\noo\\noo\\noo\\nooo\\nooo\\no o\\noo\\noo\\noo\\noooo\\noo\\noo\\noo\\nooooo\\no\\noo\\no\\nooooo\\no\\noooooo\\noo\\noo\\nooo\\noo\\no\\noooooo\\no\\noooo\\no\\no\\nooooo\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\nooo\\noooo\\nooooo\\no\\noooooooooo\\no\\no\\noo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\no\\nooooo\\no\\nooooo\\noo\\nooooo\\noooo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\nooo\\nooooo\\nooo\\nooooo\\noo\\nooo\\noo\\no\\nooo\\noo\\nooo\\noo\\nooo\\no\\noooooo\\no\\noo\\noo\\noo\\noo\\no\\no\\no\\noooo\\no\\noo\\noooo\\no\\noo\\noo\\noooooo\\nooo\\no\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\noo\\noooooooooo\\no\\noo\\no\\nooo\\nooo\\noooo\\no\\nooooo\\noooo\\noo\\noo\\noo\\nooo\\no\\nooo\\nooooo\\noooo\\no\\nooo\\noo\\nooo\\noo\\nooo\\noo\\no\\no\\noo\\nooo\\no\\nooooo\\nooo\\noo\\noo\\nooo\\no\\noo\\noooo\\nooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\noo o\\no\\nooo\\noo\\nooo\\noo\\nooooo\\no\\noo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo\\noo\\no\\nooo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\no\\nooooo\\no\\noooo\\noo\\noo\\noo\\nooo\\noo\\no\\no\\nooooo\\nooooo\\no\\no\\nooooo\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\nooo\\noooo\\nooooo\\no\\noooooooooo\\no\\no\\noo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\no\\nooooo\\no\\nooooo\\noo\\nooooo\\noooo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\nooo\\noooo o\\nooo\\nooooo\\noo\\nooo\\noo\\no\\nooo\\noo\\nooo\\noo\\nooo\\no\\noooooo\\no\\noo\\noo\\noo\\noo\\no\\no\\no\\noooo\\no\\noo\\noooo\\no\\noo\\noo\\noooooo\\nooo\\no\\nooooo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\noo\\noooooooooo\\no\\noo\\no\\nooo\\nooo\\noooo\\no\\nooo\\noo\\noo\\n2 6 10 14oo\\noo\\noo\\noo\\nooo\\no\\nooo\\nooooo\\noooo\\no\\nooooo\\nooo\\noo\\nooo\\noo\\no\\no\\noo\\nooo\\no\\noooo\\no\\noo\\no\\noo\\noo\\nooo\\no\\noo\\noooo\\nooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\nooo\\no\\nooo\\noo\\nooo\\noo\\noooooo\\noo\\noo\\noo\\nooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='oo\\noo\\noo\\nooo\\no\\nooo\\nooooo\\noooo\\no\\nooooo\\nooo\\noo\\nooo\\noo\\no\\no\\noo\\nooo\\no\\noooo\\no\\noo\\no\\noo\\noo\\nooo\\no\\noo\\noooo\\nooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\nooo\\no\\nooo\\noo\\nooo\\noo\\noooooo\\noo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo\\noo\\noooo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\no\\nooooo\\no\\noooooo\\noo\\noo\\nooo\\noo\\no\\noooooo\\nooooo\\no\\no\\noooo o\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\nooo\\nooo\\no\\nooooo\\no\\nooo\\noooooooo\\no\\noo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\no\\nooooo\\no\\nooooo\\noo\\nooooo\\noooo\\noo\\noooo\\no\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\nooo\\nooooo\\nooo\\nooooo\\noo\\nooo\\noo\\no\\nooo\\noo\\nooo\\noo\\nooo\\no\\noooooo\\no\\noo\\noo\\noo\\noo\\no\\no\\no\\noooo\\no\\noo\\noooo\\no\\noo\\noo\\noooooo\\nooo\\no\\nooooo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\noo\\noooooooooo\\no\\noo\\no\\nooo\\nooo\\no\\nooo\\no\\nooo\\noo\\noooo\\noo\\noo\\noo\\nooo\\no\\nooo\\nooooo\\noooo\\no\\nooooo\\nooo\\noo\\nooo\\noo\\no\\no\\noo\\nooo\\no\\noooo\\no\\nooo\\noo\\noo\\nooo\\no\\noo\\no oo o\\nooo\\noooo o o\\noooo o\\nooo\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\noo o\\no\\nooo\\noo\\nooo\\noo\\noooo oo\\noo\\noo\\noo\\noo o\\nooo\\noo\\noo\\noo\\noo\\noooo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\no\\nooooo\\no\\noooooo\\noo\\noo\\nooo\\noo\\no\\noooooo\\nooo oo\\no\\no\\noo o o o\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\nooo\\noooo\\nooooo\\no\\nooo\\noooo oooo\\no\\noo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\no\\nooooo\\no\\nooooo\\noo\\nooooo\\noooo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\nooo\\nooooo\\noo o\\noooo o\\noo\\nooo\\noo\\no\\nooo\\noo\\noo o\\noo\\nooo\\no\\noooooo\\no\\noo\\noo\\noo\\noo\\no\\noo\\noooo\\no\\noo\\noooo\\no\\noo\\noo\\nooooo o\\noo o\\no\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\no oo\\no o\\nooooo ooooo\\no\\noo\\no\\nooo\\nooo\\no\\nooo\\no\\noo o\\noo\\noo\\n15 25 35 45oo\\noo\\no\\no\\noo\\nooo\\no\\nooo\\nooooo\\noooo\\no\\nooo\\noo\\nooo\\noo\\nooo\\noo\\no\\no\\noo\\nooo\\no\\noooo\\no\\nooo\\noo\\noo\\nooo\\no\\noo\\noooo\\nooo\\noooooo\\nooo oo\\nooo\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\nooo\\no\\nooo\\noo\\nooo\\noo\\noooooo\\noo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noooo\\noooo\\noo\\noo\\noo\\nooooo\\no\\noo\\no\\nooooo\\no\\noooooo\\noo\\noo\\nooo\\noo\\no\\noooooo\\noooooo\\no\\nooooo\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\nooo\\noooo\\nooooo\\no\\nooo\\nooooooo\\no\\no\\noo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\no\\nooooo\\no\\nooooo\\noo\\nooooo\\noooo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\nooo\\nooooo\\nooo\\nooooo\\noo\\nooo\\noo\\no\\nooo\\noo\\nooo\\noo\\nooo\\no\\noooooo\\no\\noo\\noo\\noo\\noo\\no\\noo\\noooo\\no\\noo\\noooo\\no\\noo\\noo\\nooooooooo\\no\\nooooo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\noo\\noooooooooo\\no\\noo\\no\\nooo\\nooo\\noooo\\no\\noo ooo\\noooo\\noo\\noo\\noo\\nooo\\no\\nooo\\nooooo\\noooo\\no\\nooooo\\nooo\\noo\\nooo\\noo\\no\\no\\noo\\nooo\\no\\noooo\\no\\noo\\no\\noo\\noo\\nooo\\no\\noo\\noooo\\nooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\nooo\\no\\nooo\\noo\\nooo\\noo\\noooooo\\noo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo\\noo\\noooo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\no\\nooooo\\no\\noooooo\\noo\\noo\\nooo\\noo\\no\\noooooo\\nooooo\\no\\no\\nooooo\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\nooo\\noooo\\no oooo\\no\\nooo\\nooooooo\\no\\no\\noo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\no\\nooooo\\no\\nooooo\\noo\\nooooo\\noooo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\nooo\\nooooo\\nooo\\nooooo\\noo\\nooo\\noo\\no\\nooo\\noo\\nooo\\noo\\nooo\\no\\noooooo\\no\\noo\\noo\\noo\\noo\\no\\noo\\noooo\\no\\noo\\noooo\\no\\noo\\noo\\noooooo\\nooo\\no\\nooooo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\noo\\noooooooooo\\no\\noo\\no\\nooo\\nooo\\no\\nooo\\no\\nooo\\noo\\noo\\n20 40 60\\n20 40 60age\\nFIGURE 4.12. A scatterplot matrix of the South African heart disease data.\\nEach plot shows a pair of risk factors, and the cases and contro ls are color coded\\n(red is a case). The variable family history of heart disease ( famhist)is binary\\n(yes or no).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 141}),\n",
       " Document(page_content='124 4. Linear Methods for Classiﬁcation\\nTABLE 4.3. Results from stepwise logistic regression ﬁt to South African heart\\ndisease data.\\nCoeﬃcient Std. Error Zscore\\n(Intercept) −4.204 0 .498−8.45\\ntobacco 0.081 0 .026 3.16\\nldl 0.168 0 .054 3.09\\nfamhist 0.924 0 .223 4.14\\nage 0.044 0 .010 4.52\\nother correlated variables, they are no longer needed (and c an even get a\\nnegative sign).\\nAt this stage the analyst might do some model selection; ﬁnd a subset\\nof the variables that are suﬃcient for explaining their join t eﬀect on the\\nprevalence of chd. One way to proceed by is to drop the least signiﬁcant co-\\neﬃcient, and reﬁt the model. This is done repeatedly until no further terms\\ncan be dropped from the model. This gave the model shown in Tab le 4.3.\\nA better but more time-consuming strategy is to reﬁt each of t he models\\nwith one variable removed, and then perform an analysis of deviance to\\ndecide which variable to exclude. The residual deviance of a ﬁtted model\\nis minus twice its log-likelihood, and the deviance between two models is\\nthe diﬀerence of their individual residual deviances (in an alogy to sums-of-\\nsquares). This strategy gave the same ﬁnal model as above.\\nHow does one interpret a coeﬃcient of 0 .081 (Std. Error = 0 .026) for\\ntobacco, for example? Tobacco is measured in total lifetime usage in kilo-\\ngrams, with a median of 1 .0kg for the controls and 4 .1kg for the cases. Thus\\nan increase of 1kg in lifetime tobacco usage accounts for an i ncrease in the\\nodds of coronary heart disease of exp(0 .081) = 1.084 or 8.4%. Incorporat-\\ning the standard error we get an approximate 95% conﬁdence in terval of\\nexp(0.081±2×0.026) = (1.03,1.14).\\nWe return to these data in Chapter 5, where we see that some of t he\\nvariables have nonlinear eﬀects, and when modeled appropri ately, are not\\nexcluded from the model.\\n4.4.3 Quadratic Approximations and Inference\\nThe maximum-likelihood parameter estimates ˆβsatisfy a self-consistency\\nrelationship: they are the coeﬃcients of a weighted least sq uares ﬁt, where\\nthe responses are\\nzi=xT\\niˆβ+(yi−ˆpi)\\nˆpi(1−ˆpi), (4.29)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 142}),\n",
       " Document(page_content='4.4 Logistic Regression 125\\nand the weights are wi= ˆpi(1−ˆpi), both depending on ˆβitself. Apart from\\nproviding a convenient algorithm, this connection with lea st squares has\\nmore to oﬀer:\\n•The weighted residual sum-of-squares is the familiar Pears on chi-\\nsquare statistic\\nN∑\\ni=1(yi−ˆpi)2\\nˆpi(1−ˆpi), (4.30)\\na quadratic approximation to the deviance.\\n•Asymptotic likelihood theory says that if the model is corre ct, then\\nˆβis consistent (i.e., converges to the trueβ).\\n•A central limit theorem then shows that the distribution of ˆβcon-\\nverges toN(β,(XTWX)−1). This and other asymptotics can be de-\\nriveddirectlyfromtheweightedleastsquaresﬁtbymimicki ngnormal\\ntheory inference.\\n•Model building can be costly for logistic regression models , because\\neach model ﬁtted requires iteration. Popular shortcuts are theRao\\nscore test which tests for inclusion of a term, and the Wald test which\\ncan be used to test for exclusion of a term. Neither of these re quire\\niterative ﬁtting, and are based on the maximum-likelihood ﬁ t of the\\ncurrent model. It turns out that both of these amount to addin g\\nor dropping a term from the weighted least squares ﬁt, using t he\\nsameweights. Such computations can be done eﬃciently, without\\nrecomputing the entire weighted least squares ﬁt.\\nSoftware implementations can take advantage of these conne ctions. For\\nexample, the generalized linear modeling software in R (whi ch includes lo-\\ngistic regression as part of the binomial family of models) e xploits them\\nfully.GLM(generalizedlinearmodel)objectscanbetreate daslinearmodel\\nobjects, and all the tools available for linear models can be applied auto-\\nmatically.\\n4.4.4L1Regularized Logistic Regression\\nTheL1penalty used in the lasso (Section 3.4.2) can be used for vari able\\nselection and shrinkage with any linear regression model. F or logistic re-\\ngression, we would maximize a penalized version of (4.20):\\nmax\\nβ0,β\\uf8f1\\n\\uf8f2\\n\\uf8f3N∑\\ni=1[\\nyi(β0+βTxi)−log(1+eβ0+βTxi)]\\n−λp∑\\nj=1|βj|\\uf8fc\\n\\uf8fd\\n\\uf8fe.(4.31)\\nAs with the lasso, we typically do not penalize the intercept term, and stan-\\ndardize the predictors for the penalty to be meaningful. Cri terion (4.31) is', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 143}),\n",
       " Document(page_content='126 4. Linear Methods for Classiﬁcation\\nconcave, and a solution can be found using nonlinear program ming meth-\\nods (Koh et al., 2007, for example). Alternatively, using th e same quadratic\\napproximations that were used in the Newton algorithm in Sec tion 4.4.1,\\nwe can solve (4.31) by repeated application of a weighted las so algorithm.\\nInterestingly, the score equations [see (4.24)] for the var iables with non-zero\\ncoeﬃcients have the form\\nxT\\nj(y−p) =λ·sign(βj), (4.32)\\nwhich generalizes (3.58) in Section 3.4.4; the active varia bles are tied in\\ntheirgeneralized correlation with the residuals.\\nPath algorithms such as LAR for lasso are more diﬃcult, becau se the\\ncoeﬃcient proﬁles are piecewise smooth rather than linear. Nevertheless,\\nprogress can be made using quadratic approximations.\\n******************************* ************** ****** *************************************************** ****************************************** **************************************************************************** *****************\\n0.0 0.5 1.0 1.5 2.00.0 0.2 0.4 0.6******************************* * ******************************************************************************************************************************************************************************************** *****************\\n******************************* ************** * ****************************************************************************************************************************************************************************** *****************\\n****************************** ********************************************************************************************************************************************************************************************** *****************\\n******************************* ************** ****** *************************************************** *************************************************** ************************ ******************************************* ************************************************ ************** ****** *************************************************** *************************************************** ************************ *************************** *************** * ********************************************************************************************************************************************************************************************************************************************* *****************\\nobesityalcoholsbptobaccoldlfamhistage1 2 4 5 6 7Coeﬃcients βj(λ)\\n||β(λ)||1\\nFIGURE 4.13. L1regularized logistic regression coeﬃcients for the South\\nAfrican heart disease data, plotted as a function of the L1norm. The variables\\nwere all standardized to have unit variance. The proﬁles are c omputed exactly at\\neach of the plotted points.\\nFigure 4.13 shows the L1regularization path for the South African\\nheart disease data of Section 4.4.2. This was produced using theRpackage\\nglmpath (Park and Hastie, 2007), which uses predictor–corrector methods\\nof convex optimization to identify the exact values of λat which the active\\nset of non-zero coeﬃcients changes (vertical lines in the ﬁg ure). Here the\\nproﬁles look almost linear; in other examples the curvature will be more\\nvisible.\\nCoordinate descent methods (Section 3.8.6) are very eﬃcien t for comput-\\ning the coeﬃcient proﬁles on a grid of values for λ. TheRpackageglmnet', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 144}),\n",
       " Document(page_content='4.4 Logistic Regression 127\\n(Friedman et al., 2010) can ﬁt coeﬃcient paths for very large logistic re-\\ngression problems eﬃciently (large in Norp). Their algorithms can exploit\\nsparsity in the predictor matrix X, which allows for even larger problems.\\nSee Section 18.4 for more details, and a discussion of L1-regularized multi-\\nnomial models.\\n4.4.5 Logistic Regression or LDA?\\nIn Section 4.3 we ﬁnd that the log-posterior odds between cla sskandK\\nare linear functions of x(4.9):\\nlogPr(G=k|X=x)\\nPr(G=K|X=x)= logπk\\nπK−1\\n2(µk+µK)TΣ−1(µk−µK)\\n+xTΣ−1(µk−µK)\\n=αk0+αT\\nkx. (4.33)\\nThis linearity is a consequence of the Gaussian assumption f or the class\\ndensities, as well as the assumption of a common covariance m atrix. The\\nlinear logistic model (4.17) by construction has linear log its:\\nlogPr(G=k|X=x)\\nPr(G=K|X=x)=βk0+βT\\nkx. (4.34)\\nItseemsthatthemodelsarethesame.Althoughtheyhaveexac tlythesame\\nform, the diﬀerence lies in the way the linear coeﬃcients are estimated. The\\nlogistic regression model is more general, in that it makes l ess assumptions.\\nWe can write the joint density ofXandGas\\nPr(X,G=k) = Pr(X)Pr(G=k|X), (4.35)\\nwhere Pr(X) denotes the marginal density of the inputs X. For both LDA\\nand logistic regression, the second term on the right has the logit-linear\\nform\\nPr(G=k|X=x) =eβk0+βT\\nkx\\n1+∑K−1\\nℓ=1eβℓ0+βT\\nℓx, (4.36)\\nwhere we have again arbitrarily chosen the last class as the r eference.\\nThe logistic regression model leaves the marginal density o fXas an arbi-\\ntrary density function Pr( X), and ﬁts the parameters of Pr( G|X) by max-\\nimizing the conditional likelihood —the multinomial likelihood with proba-\\nbilities the Pr( G=k|X). Although Pr( X) is totally ignored, we can think\\nof this marginal density as being estimated in a fully nonpar ametric and\\nunrestricted fashion, using the empirical distribution fu nction which places\\nmass 1/Nat each observation.\\nWith LDA we ﬁt the parameters by maximizing the full log-like lihood,\\nbased on the joint density\\nPr(X,G=k) =φ(X;µk,Σ)πk, (4.37)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 145}),\n",
       " Document(page_content='128 4. Linear Methods for Classiﬁcation\\nwhereφis the Gaussian density function. Standard normal theory le ads\\neasily to the estimates ˆ µk,ˆΣ, and ˆπkgiven in Section 4.3. Since the linear\\nparameters of the logistic form (4.33) are functions of the G aussian param-\\neters, we get their maximum-likelihood estimates by pluggi ng in the corre-\\nsponding estimates. However, unlike in the conditional cas e, the marginal\\ndensity Pr( X) does play a role here. It is a mixture density\\nPr(X) =K∑\\nk=1πkφ(X;µk,Σ), (4.38)\\nwhich also involves the parameters.\\nWhat role can this additional component/restriction play? By relying\\non the additional model assumptions, we have more informati on about the\\nparameters, and hence can estimate them more eﬃciently (low er variance).\\nIf in fact the true fk(x) are Gaussian, then in the worst case ignoring this\\nmarginal part of the likelihood constitutes a loss of eﬃcien cy of about 30%\\nasymptotically in the error rate (Efron, 1975). Paraphrasi ng: with 30%\\nmore data, the conditional likelihood will do as well.\\nFor example, observations far from the decision boundary (w hich are\\ndown-weighted by logistic regression) play a role in estima ting the common\\ncovariance matrix. This is not all good news, because it also means that\\nLDA is not robust to gross outliers.\\nFrom the mixture formulation, it is clear that even observat ions without\\nclass labels have information about the parameters. Often i t is expensive\\nto generate class labels, but unclassiﬁed observations com e cheaply. By\\nrelying on strong model assumptions, such as here, we can use both types\\nof information.\\nThe marginal likelihood can be thought of as a regularizer, r equiring\\nin some sense that class densities be visiblefrom this marginal view. For\\nexample, if the data in a two-class logistic regression mode l can be per-\\nfectly separated by a hyperplane, the maximum likelihood es timates of the\\nparameters are undeﬁned (i.e., inﬁnite; see Exercise 4.5). The LDA coeﬃ-\\ncients for the same data will be well deﬁned, since the margin al likelihood\\nwill not permit these degeneracies.\\nIn practice these assumptions are never correct, and often s ome of the\\ncomponents of Xare qualitative variables. It is generally felt that logist ic\\nregression is a safer, more robust bet than the LDA model, rel ying on fewer\\nassumptions. It is our experience that the models give very s imilar results,\\neven when LDA is used inappropriately, such as with qualitat ive predictors.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 146}),\n",
       " Document(page_content='4.5 Separating Hyperplanes 129\\nFIGURE 4.14. A toy example with two classes separable by a hyperplane. The\\norange line is the least squares solution, which misclassiﬁes on e of the training\\npoints. Also shown are two blue separating hyperplanes found by theperceptron\\nlearning algorithm with diﬀerent random starts.\\n4.5 Separating Hyperplanes\\nWe have seen that linear discriminant analysis and logistic regression both\\nestimate linear decision boundaries in similar but slightl y diﬀerent ways.\\nFor the rest of this chapter we describe separating hyperpla ne classiﬁers.\\nThese procedures construct linear decision boundaries tha t explicitly try\\nto separate the data into diﬀerent classes as well as possibl e. They provide\\nthe basis for support vector classiﬁers, discussed in Chapt er 12. The math-\\nematical level of this section is somewhat higher than that o f the previous\\nsections.\\nFigure 4.14 shows 20 data points in two classes in IR2. These data can be\\nseparated by a linear boundary. Included in the ﬁgure (blue l ines) are two\\nof the inﬁnitely many possible separating hyperplanes . The orange line is\\nthe least squares solution to the problem, obtained by regre ssing the−1/1\\nresponseYonX(with intercept); the line is given by\\n{x:ˆβ0+ˆβ1x1+ˆβ2x2= 0}. (4.39)\\nThis least squares solution does not do a perfect job in separ ating the\\npoints, and makes one error. This is the same boundary found b y LDA,\\nin light of its equivalence with linear regression in the two -class case (Sec-\\ntion 4.3 and Exercise 4.2).\\nClassiﬁers such as (4.39), that compute a linear combinatio n of the input\\nfeaturesandreturnthesign,werecalled perceptrons intheengineeringliter-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 147}),\n",
       " Document(page_content='130 4. Linear Methods for Classiﬁcation\\nx0x\\nβ∗β0+βTx= 0\\nFIGURE 4.15. The linear algebra of a hyperplane (aﬃne set).\\nature in the late 1950s (Rosenblatt, 1958). Perceptrons set the foundations\\nfor the neural network models of the 1980s and 1990s.\\nBeforewecontinue,letusdigressslightlyandreviewsomev ectoralgebra.\\nFigure 4.15 depicts a hyperplane or aﬃne setLdeﬁned by the equation\\nf(x) =β0+βTx= 0; since we are in IR2this is a line.\\nHere we list some properties:\\n1. For any two points x1andx2lying inL,βT(x1−x2) = 0, and hence\\nβ∗=β/||β||is the vector normal to the surface of L.\\n2. For any point x0inL,βTx0=−β0.\\n3. The signed distance of any point xtoLis given by\\nβ∗T(x−x0) =1\\n∥β∥(βTx+β0)\\n=1\\n||f′(x)||f(x). (4.40)\\nHencef(x) is proportional to the signed distance from xto the hyperplane\\ndeﬁned byf(x) = 0.\\n4.5.1 Rosenblatt’s Perceptron Learning Algorithm\\nTheperceptron learning algorithm tries to ﬁnd a separating hyperplane by\\nminimizing the distance of misclassiﬁed points to the decis ion boundary. If', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 148}),\n",
       " Document(page_content='4.5 Separating Hyperplanes 131\\na responseyi= 1 is misclassiﬁed, then xT\\niβ+β0<0, and the opposite for\\na misclassiﬁed response with yi=−1. The goal is to minimize\\nD(β,β0) =−∑\\ni∈Myi(xT\\niβ+β0), (4.41)\\nwhereMindexes the set of misclassiﬁed points. The quantity is non-\\nnegative and proportional to the distance of the misclassiﬁ ed points to\\nthe decision boundary deﬁned by βTx+β0= 0. The gradient (assuming\\nMis ﬁxed) is given by\\n∂D(β,β0)\\n∂β=−∑\\ni∈Myixi, (4.42)\\n∂D(β,β0)\\n∂β0=−∑\\ni∈Myi. (4.43)\\nThe algorithm in fact uses stochastic gradient descent to minimize this\\npiecewise linear criterion. This means that rather than com puting the sum\\nof the gradient contributions of each observation followed by a step in the\\nnegative gradient direction, a step is taken after each obse rvation is visited.\\nHence the misclassiﬁed observations are visited in some seq uence, and the\\nparameters βare updated via\\n(\\nβ\\nβ0)\\n←(\\nβ\\nβ0)\\n+ρ(\\nyixi\\nyi)\\n. (4.44)\\nHereρis the learning rate, which in this case can be taken to be 1 wit hout\\nloss in generality. If the classes are linearly separable, i t can be shown that\\nthe algorithm converges to a separating hyperplane in a ﬁnit e number of\\nsteps (Exercise 4.6). Figure 4.14 shows two solutions to a to y problem, each\\nstarted at a diﬀerent random guess.\\nThere are a number of problems with this algorithm, summariz ed in\\nRipley (1996):\\n•When the data are separable, there are many solutions, and wh ich\\none is found depends on the starting values.\\n•The “ﬁnite” number of steps can be very large. The smaller the gap,\\nthe longer the time to ﬁnd it.\\n•When the data are not separable, the algorithm will not conve rge,\\nand cycles develop. The cycles can be long and therefore hard to\\ndetect.\\nThe second problem can often be eliminated by seeking a hyper plane not\\nin the original space, but in a much enlarged space obtained b y creating', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 149}),\n",
       " Document(page_content='132 4. Linear Methods for Classiﬁcation\\nmany basis-function transformations of the original varia bles. This is anal-\\nogous to driving the residuals in a polynomial regression pr oblem down\\nto zero by making the degree suﬃciently large. Perfect separ ation cannot\\nalways be achieved: for example, if observations from two di ﬀerent classes\\nshare the same input. It may not be desirable either, since th e resulting\\nmodel is likely to be overﬁt and will not generalize well. We r eturn to this\\npoint at the end of the next section.\\nA rather elegant solution to the ﬁrst problem is to add additi onal con-\\nstraints to the separating hyperplane.\\n4.5.2 Optimal Separating Hyperplanes\\nTheoptimal separating hyperplane separates the two classes and maximizes\\nthe distance to the closest point from either class (Vapnik, 1996). Not only\\ndoes this provide a unique solution to the separating hyperp lane problem,\\nbutbymaximizingthemarginbetweenthetwoclassesonthetr ainingdata,\\nthis leads to better classiﬁcation performance on test data .\\nWeneedtogeneralizecriterion(4.41).Considertheoptimi zationproblem\\nmax\\nβ,β0,||β||=1M\\nsubject toyi(xT\\niβ+β0)≥M, i= 1,...,N.(4.45)\\nThe set of conditions ensure that all the points are at least a signed\\ndistanceMfrom the decision boundary deﬁned by βandβ0, and we seek\\nthe largest such Mand associated parameters. We can get rid of the ||β||=\\n1 constraint by replacing the conditions with\\n1\\n||β||yi(xT\\niβ+β0)≥M, (4.46)\\n(which redeﬁnes β0) or equivalently\\nyi(xT\\niβ+β0)≥M||β||. (4.47)\\nSince for any βandβ0satisfying these inequalities, any positively scaled\\nmultiple satisﬁes them too, we can arbitrarily set ||β||= 1/M. Thus (4.45)\\nis equivalent to\\nmin\\nβ,β01\\n2||β||2\\nsubject toyi(xT\\niβ+β0)≥1, i= 1,...,N.(4.48)\\nIn light of (4.40), the constraints deﬁne an empty slab or mar gin around the\\nlinear decision boundary of thickness 1 /||β||. Hence we choose βandβ0to\\nmaximize its thickness. This is a convex optimization probl em (quadratic', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 150}),\n",
       " Document(page_content='4.5 Separating Hyperplanes 133\\ncriterion with linear inequality constraints). The Lagran ge (primal) func-\\ntion, to be minimized w.r.t. βandβ0, is\\nLP=1\\n2||β||2−N∑\\ni=1αi[yi(xT\\niβ+β0)−1]. (4.49)\\nSetting the derivatives to zero, we obtain:\\nβ=N∑\\ni=1αiyixi, (4.50)\\n0 =N∑\\ni=1αiyi, (4.51)\\nand substituting these in (4.49) we obtain the so-called Wol fe dual\\nLD=N∑\\ni=1αi−1\\n2N∑\\ni=1N∑\\nk=1αiαkyiykxT\\nixk\\nsubject toαi≥0 andN∑\\ni=1αiyi= 0. (4.52)\\nThe solution is obtained by maximizing LDin the positive orthant, a sim-\\npler convex optimization problem, for which standard softw are can be used.\\nIn addition the solution must satisfy the Karush–Kuhn–Tuck er conditions,\\nwhich include (4.50), (4.51), (4.52) and\\nαi[yi(xT\\niβ+β0)−1] = 0∀i. (4.53)\\nFrom these we can see that\\n•ifαi>0, thenyi(xT\\niβ+β0) = 1, or in other words, xiis on the\\nboundary of the slab;\\n•ifyi(xT\\niβ+β0)>1,xiis not on the boundary of the slab, and αi= 0.\\nFrom (4.50) we see that the solution vector βis deﬁned in terms of a linear\\ncombination of the support points xi—those points deﬁned to be on the\\nboundary of the slab via αi>0. Figure 4.16 shows the optimal separating\\nhyperplane for our toy example; there are three support poin ts. Likewise,\\nβ0is obtained by solving (4.53) for any of the support points.\\nThe optimal separating hyperplane produces a function ˆf(x) =xTˆβ+ˆβ0\\nfor classifying new observations:\\nˆG(x) = signˆf(x). (4.54)\\nAlthough none of the training observations fall in the margi n (by con-\\nstruction), this will not necessarily be the case for test ob servations. The', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 151}),\n",
       " Document(page_content='134 4. Linear Methods for Classiﬁcation\\nFIGURE 4.16. The same data as in Figure 4.14. The shaded region delineates\\nthe maximum margin separating the two classes. There are thre e support points\\nindicated, which lie on the boundary of the margin, and the opt imal separating\\nhyperplane (blue line) bisects the slab. Included in the ﬁgure is t he boundary found\\nusing logistic regression (red line), which is very close to the optimal separating\\nhyperplane (see Section 12.3.3).\\nintuition is that a large margin on the training data will lea d to good\\nseparation on the test data.\\nThe description of the solution in terms of support points se ems to sug-\\ngest that the optimal hyperplane focuses more on the points t hat count,\\nand is more robust to model misspeciﬁcation. The LDA solutio n, on the\\nother hand, depends on all of the data, even points far away fr om the de-\\ncision boundary. Note, however, that the identiﬁcation of t hese support\\npoints required the use of all the data. Of course, if the clas ses are really\\nGaussian, then LDA is optimal, and separating hyperplanes w ill pay a price\\nfor focusing on the (noisier) data at the boundaries of the cl asses.\\nIncluded in Figure 4.16 is the logistic regression solution to this prob-\\nlem, ﬁt by maximum likelihood. Both solutions are similar in this case.\\nWhen a separating hyperplane exists, logistic regression w ill always ﬁnd\\nit, since the log-likelihood can be driven to 0 in this case (E xercise 4.5).\\nThe logistic regression solution shares some other qualita tive features with\\nthe separating hyperplane solution. The coeﬃcient vector i s deﬁned by a\\nweighted least squares ﬁt of a zero-mean linearized respons e on the input\\nfeatures, and the weights are larger for points near the deci sion boundary\\nthan for those further away.\\nWhen the data are not separable, there will be no feasible sol ution to\\nthis problem, and an alternative formulation is needed. Aga in one can en-\\nlarge the space using basis transformations, but this can le ad to artiﬁcial', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 152}),\n",
       " Document(page_content='Exercises 135\\nseparation through over-ﬁtting. In Chapter 12 we discuss a m ore attractive\\nalternative known as the support vector machine , which allows for overlap,\\nbut minimizes a measure of the extent of this overlap.\\nBibliographic Notes\\nGood general texts on classiﬁcation include Duda et al. (200 0), Hand\\n(1981), McLachlan (1992) and Ripley (1996). Mardia et al. (1 979) have\\na concise discussion of linear discriminant analysis. Mich ie et al. (1994)\\ncompare a large number of popular classiﬁers on benchmark da tasets. Lin-\\near separating hyperplanes are discussed in Vapnik (1996). Our account of\\nthe perceptron learning algorithm follows Ripley (1996).\\nExercises\\nEx. 4.1Show how to solve the generalized eigenvalue problem max aTBa\\nsubject toaTWa= 1 by transforming to a standard eigenvalue problem.\\nEx. 4.2Suppose we have features x∈IRp, a two-class response, with class\\nsizesN1,N2, and the target coded as −N/N1,N/N2.\\n(a) Show that the LDA rule classiﬁes to class 2 if\\nxTˆΣ−1(ˆµ2−ˆµ1)>1\\n2(ˆµ2+ ˆµ1)TˆΣ−1(ˆµ2−ˆµ1)−log(N2/N1),\\nand class 1 otherwise.\\n(b) Consider minimization of the least squares criterion\\nN∑\\ni=1(yi−β0−xT\\niβ)2. (4.55)\\nShow that the solution ˆβsatisﬁes\\n[\\n(N−2)ˆΣ+NˆΣB]\\nβ=N(ˆµ2−ˆµ1) (4.56)\\n(after simpliﬁcation), where ˆΣB=N1N2\\nN2(ˆµ2−ˆµ1)(ˆµ2−ˆµ1)T.\\n(c) Hence show that ˆΣBβis in the direction (ˆ µ2−ˆµ1) and thus\\nˆβ∝ˆΣ−1(ˆµ2−ˆµ1). (4.57)\\nTherefore the least-squares regression coeﬃcient is ident ical to the\\nLDA coeﬃcient, up to a scalar multiple.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 153}),\n",
       " Document(page_content='136 4. Linear Methods for Classiﬁcation\\n(d) Show that this result holds for any (distinct) coding of t he two classes.\\n(e) Find the solution ˆβ0(up to the same scalar multiple as in (c), and\\nhence the predicted value ˆf(x) =ˆβ0+xTˆβ. Consider the following\\nrule: classify to class 2 if ˆf(x)>0 and class 1 otherwise. Show this is\\nnot the same as the LDA rule unless the classes have equal numb ers\\nof observations.\\n(Fisher, 1936; Ripley, 1996)\\nEx. 4.3Suppose we transform the original predictors XtoˆYvia linear\\nregression. In detail, let ˆY=X(XTX)−1XTY=XˆB, where Yis the\\nindicator response matrix. Similarly for any input x∈IRp, we get a trans-\\nformed vector ˆ y=ˆBTx∈IRK. Show that LDA using ˆYis identical to\\nLDA in the original space.\\nEx. 4.4Consider the multilogit model with Kclasses (4.17). Let βbe the\\n(p+ 1)(K−1)-vector consisting of all the coeﬃcients. Deﬁne a suitabl y\\nenlarged version of the input vector xto accommodate this vectorized co-\\neﬃcient matrix. Derive the Newton-Raphson algorithm for ma ximizing the\\nmultinomial log-likelihood, and describe how you would imp lement this\\nalgorithm.\\nEx. 4.5Consider a two-class logistic regression problem with x∈IR. Char-\\nacterize the maximum-likelihood estimates of the slope and intercept pa-\\nrameterifthesample xiforthetwoclassesareseparatedbyapoint x0∈IR.\\nGeneralize this result to (a) x∈IRp(see Figure 4.16), and (b) more than\\ntwo classes.\\nEx. 4.6Suppose we have Npointsxiin IRpin general position, with class\\nlabelsyi∈{−1,1}. Prove that the perceptron learning algorithm converges\\nto a separating hyperplane in a ﬁnite number of steps:\\n(a) Denote a hyperplane by f(x) =βT\\n1x+β0= 0, or in more compact\\nnotationβTx∗= 0, where x∗= (x,1) andβ= (β1,β0). Letzi=\\nx∗\\ni/||x∗\\ni||. Show that separability implies the existence of a βsepsuch\\nthatyiβT\\nsepzi≥1∀i\\n(b)Givenacurrent βold,theperceptronalgorithmidentiﬁesapoint zithat\\nis misclassiﬁed, and produces the update βnew←βold+yizi. Show\\nthat||βnew−βsep||2≤||βold−βsep||2−1, and hence that the algorithm\\nconverges to a separating hyperplane in no more than ||βstart−βsep||2\\nsteps (Ripley, 1996).\\nEx. 4.7Consider the criterion\\nD∗(β,β0) =−N∑\\ni=1yi(xT\\niβ+β0), (4.58)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 154}),\n",
       " Document(page_content='Exercises 137\\na generalization of (4.41) where we sum over all the observat ions. Consider\\nminimizing D∗subject to||β||= 1. Describe this criterion in words. Does\\nit solve the optimal separating hyperplane problem?\\nEx. 4.8Consider the multivariate Gaussian model X|G=k∼N(µk,Σ),\\nwith the additional restriction that rank {µk}K\\n1=L <max(K−1,p).\\nDerive the constrained MLEs for the µkandΣ. Show that the Bayes clas-\\nsiﬁcation rule is equivalent to classifying in the reduced s ubspace computed\\nby LDA (Hastie and Tibshirani, 1996b).\\nEx. 4.9Write a computer program to perform a quadratic discriminan t\\nanalysis by ﬁtting a separate Gaussian model per class. Try i t out on the\\nvowel data, and compute the misclassiﬁcation error for the t est data. The\\ndatacanbefoundinthebookwebsite www-stat.stanford.edu/ElemStatLearn .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 155}),\n",
       " Document(page_content='138 4. Linear Methods for Classiﬁcation', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 156}),\n",
       " Document(page_content='This is page 139\\nPrinter: Opaque this\\n5\\nBasis Expansions and Regularization\\n5.1 Introduction\\nWe have already made use of models linear in the input feature s, both for\\nregression and classiﬁcation. Linear regression, linear d iscriminant analysis,\\nlogistic regression and separating hyperplanes all rely on a linear model.\\nIt is extremely unlikely that the true function f(X) is actually linear in\\nX. In regression problems, f(X) = E(Y|X) will typically be nonlinear and\\nnonadditive in X, and representing f(X) by a linear model is usually a con-\\nvenient, and sometimes a necessary, approximation. Conven ient because a\\nlinear model is easy to interpret, and is the ﬁrst-order Tayl or approxima-\\ntion tof(X). Sometimes necessary, because with Nsmall and/or plarge,\\na linear model might be all we are able to ﬁt to the data without overﬁt-\\nting. Likewise in classiﬁcation, a linear, Bayes-optimal d ecision boundary\\nimplies that some monotone transformation of Pr( Y= 1|X) is linear in X.\\nThis is inevitably an approximation.\\nIn this chapter and the next we discuss popular methods for mo ving\\nbeyond linearity. The core idea in this chapter is to augment /replace the\\nvector of inputs Xwith additional variables, which are transformations of\\nX, and then use linear models in this new space of derived input features.\\nDenote by hm(X) : IRp↦→IR themth transformation of X,m=\\n1,...,M. We then model\\nf(X) =M∑\\nm=1βmhm(X), (5.1)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 157}),\n",
       " Document(page_content='140 5. Basis Expansions and Regularization\\nalinear basis expansion inX. The beauty of this approach is that once the\\nbasis functions hmhave been determined, the models are linear in these\\nnew variables, and the ﬁtting proceeds as before.\\nSome simple and widely used examples of the hmare the following:\\n•hm(X) =Xm, m= 1,...,precovers the original linear model.\\n•hm(X) =X2\\njorhm(X) =XjXkallowsustoaugmenttheinputswith\\npolynomial terms to achieve higher-order Taylor expansion s. Note,\\nhowever, that the number of variables grows exponentially i n the de-\\ngree of the polynomial. A full quadratic model in pvariables requires\\nO(p2) square and cross-product terms, or more generally O(pd) for a\\ndegree-dpolynomial.\\n•hm(X) = log(Xj),√\\nXj,...permits other nonlinear transformations\\nof single inputs. More generally one can use similar functio ns involv-\\ning several inputs, such as hm(X) =||X||.\\n•hm(X) =I(Lm≤Xk<Um), an indicator for a region of Xk. By\\nbreaking the range of Xkup intoMksuch nonoverlapping regions\\nresults in a model with a piecewise constant contribution fo rXk.\\nSometimestheproblemathandwillcallforparticularbasis functionshm,\\nsuchaslogarithmsorpowerfunctions.Moreoften,however, weusethebasis\\nexpansions as a device to achieve more ﬂexible representati ons forf(X).\\nPolynomials are an example of the latter, although they are l imited by\\ntheir global nature—tweaking the coeﬃcients to achieve a fun ctional form\\nin one region can cause the function to ﬂap about madly in remo te regions.\\nIn this chapter we consider more useful families of piecewise-polynomials\\nandsplinesthat allow for local polynomial representations. We also di scuss\\nthewaveletbases, especially useful for modeling signals and images. T hese\\nmethods produce a dictionaryDconsisting of typically a very large number\\n|D|of basis functions, far more than we can aﬀord to ﬁt to our data . Along\\nwith the dictionary we require a method for controlling the c omplexity\\nof our model, using basis functions from the dictionary. The re are three\\ncommon approaches:\\n•Restriction methods, where we decide before-hand to limit t he class\\nof functions. Additivity is an example, where we assume that our\\nmodel has the form\\nf(X) =p∑\\nj=1fj(Xj)\\n=p∑\\nj=1Mj∑\\nm=1βjmhjm(Xj). (5.2)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 158}),\n",
       " Document(page_content='5.2 Piecewise Polynomials and Splines 141\\nThe size of the model is limited by the number of basis functio nsMj\\nused for each component function fj.\\n•Selection methods, which adaptively scan the dictionary an d include\\nonly those basis functions hmthat contribute signiﬁcantly to the ﬁt of\\nthe model. Here the variable selection techniques discusse d in Chap-\\nter 3 are useful. The stagewise greedy approaches such as CAR T,\\nMARS and boosting fall into this category as well.\\n•Regularization methods where we use the entire dictionary b ut re-\\nstrict the coeﬃcients. Ridge regression is a simple example of a regu-\\nlarization approach, while the lasso is both a regularizati on and selec-\\ntion method. Here we discuss these and more sophisticated me thods\\nfor regularization.\\n5.2 Piecewise Polynomials and Splines\\nWe assume until Section 5.7 that Xis one-dimensional. A piecewise poly-\\nnomial function f(X) is obtained by dividing the domain of Xinto contigu-\\nous intervals, and representing fby a separate polynomial in each interval.\\nFigure 5.1 shows two simple piecewise polynomials. The ﬁrst is piecewise\\nconstant, with three basis functions:\\nh1(X) =I(X <ξ 1), h2(X) =I(ξ1≤X <ξ 2), h3(X) =I(ξ2≤X).\\nSince these are positive over disjoint regions, the least sq uares estimate of\\nthe modelf(X) =∑3\\nm=1βmhm(X) amounts to ˆβm=¯Ym, the mean of Y\\nin themth region.\\nThe top right panel shows a piecewise linear ﬁt. Three additi onal basis\\nfunctions are needed: hm+3=hm(X)X, m= 1,...,3. Except in special\\ncases, we would typically prefer the third panel, which is al so piecewise\\nlinear, but restricted to be continuous at the two knots. The se continu-\\nity restrictions lead to linear constraints on the paramete rs; for example,\\nf(ξ−\\n1) =f(ξ+\\n1) implies that β1+ξ1β4=β2+ξ1β5. In this case, since there\\nare two restrictions, we expect to get backtwo parameters, leaving four free\\nparameters.\\nA more direct way to proceed in this case is to use a basis that i ncorpo-\\nrates the constraints:\\nh1(X) = 1, h2(X) =X, h 3(X) = (X−ξ1)+, h4(X) = (X−ξ2)+,\\nwheret+denotes the positive part. The function h3is shown in the lower\\nright panel of Figure 5.1. We often prefer smoother function s, and these\\ncan be achieved by increasing the order of the local polynomi al. Figure 5.2\\nshows a series of piecewise-cubic polynomials ﬁt to the same data, with', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 159}),\n",
       " Document(page_content='142 5. Basis Expansions and Regularization\\nOO\\nOOO\\nO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOO\\nO\\nO\\nOO\\nO\\nOOPiecewise Constant\\nOO\\nOOO\\nO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOO\\nO\\nO\\nOO\\nO\\nOOPiecewise Linear\\nOO\\nOOO\\nO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOO\\nO\\nO\\nOO\\nO\\nOOContinuous Piecewise Linear Piecewise-linear Basis Function\\n•\\n•••\\n••••\\n••\\n•• • ••\\n••••\\n••\\n•••\\n•\\n•••\\n•\\n•••\\n••\\n•••\\n• ••••\\n• ••\\n•\\n••\\n••\\nξ1 ξ1ξ1 ξ1\\nξ2 ξ2ξ2 ξ2\\n(X−ξ1)+\\nFIGURE 5.1. The top left panel shows a piecewise constant function ﬁt to so me\\nartiﬁcial data. The broken vertical lines indicate the posit ions of the two knots\\nξ1andξ2. The blue curve represents the true function, from which the d ata were\\ngenerated with Gaussian noise. The remaining two panels show p iecewise lin-\\near functions ﬁt to the same data—the top right unrestricted , and the lower left\\nrestricted to be continuous at the knots. The lower right pane l shows a piecewise–\\nlinear basis function, h3(X) = (X−ξ1)+, continuous at ξ1. The black points\\nindicate the sample evaluations h3(xi), i= 1,...,N.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 160}),\n",
       " Document(page_content='5.2 Piecewise Polynomials and Splines 143\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOOO\\nO\\nOO\\nOOODiscontinuous\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOOO\\nO\\nOO\\nOOOContinuous\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOOO\\nO\\nOO\\nOOOContinuous First Derivative\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOOO\\nO\\nOO\\nOOOContinuous Second DerivativePiecewise Cubic Polynomials\\nξ1 ξ1ξ1 ξ1\\nξ2 ξ2ξ2 ξ2\\nFIGURE 5.2. A series of piecewise-cubic polynomials, with increasing ord ers of\\ncontinuity.\\nincreasing orders of continuity at the knots. The function i n the lower\\nright panel is continuous, and has continuous ﬁrst and secon d derivatives\\nat the knots. It is known as a cubic spline . Enforcing one more order of\\ncontinuity would lead to a global cubic polynomial. It is not hard to show\\n(Exercise 5.1) that the following basis represents a cubic s pline with knots\\natξ1andξ2:\\nh1(X) = 1, h3(X) =X2, h5(X) = (X−ξ1)3\\n+,\\nh2(X) =X, h 4(X) =X3, h6(X) = (X−ξ2)3\\n+.(5.3)\\nTherearesixbasisfunctionscorrespondingtoasix-dimens ionallinearspace\\nof functions. A quick check conﬁrms the parameter count: (3 r egions)×(4\\nparameters per region) −(2 knots)×(3 constraints per knot)= 6.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 161}),\n",
       " Document(page_content='144 5. Basis Expansions and Regularization\\nMore generally, an order- Mspline with knots ξj, j= 1,...,Kis a\\npiecewise-polynomial of order M, and has continuous derivatives up to\\norderM−2. A cubic spline has M= 4. In fact the piecewise-constant\\nfunction in Figure 5.1 is an order-1 spline, while the contin uous piece-\\nwise linear function is an order-2 spline. Likewise the gene ral form for the\\ntruncated-power basis set would be\\nhj(X) =Xj−1, j= 1,...,M,\\nhM+ℓ(X) = (X−ξℓ)M−1\\n+, ℓ= 1,...,K.\\nIt is claimed that cubic splines are the lowest-order spline for which the\\nknot-discontinuity is not visible to the human eye. There is seldom any\\ngood reason to go beyond cubic-splines, unless one is intere sted in smooth\\nderivatives. In practice the most widely used orders are M= 1,2 and 4.\\nThese ﬁxed-knot splines are also known as regression splines . One needs\\nto select the order of the spline, the number of knots and thei r placement.\\nOne simple approach is to parameterize a family of splines by the number\\nof basis functions or degrees of freedom, and have the observ ationsxide-\\ntermine the positions of the knots. For example, the express ionbs(x,df=7)\\ninRgenerates a basis matrix of cubic-spline functions evaluat ed at theN\\nobservations in x, with the 7−3 = 41interior knots at the appropriate per-\\ncentilesof x(20,40,60and80th.)Onecanbemoreexplicit,however; bs(x,\\ndegree=1, knots = c(0.2, 0.4, 0.6)) generates a basis for linear splines,\\nwith three interior knots, and returns an N×4 matrix.\\nSincethespaceofsplinefunctionsofaparticularorderand knotsequence\\nis a vector space, there are many equivalent bases for repres enting them\\n(just as there are for ordinary polynomials.) While the trun cated power\\nbasis is conceptually simple, it is not too attractive numer ically: powers of\\nlarge numbers can lead to severe rounding problems. The B-spline basis,\\ndescribed in the Appendix to this chapter, allows for eﬃcien t computations\\neven when the number of knots Kis large.\\n5.2.1 Natural Cubic Splines\\nWe know that the behavior of polynomials ﬁt to data tends to be erratic\\nnear the boundaries, and extrapolation can be dangerous. Th ese problems\\nare exacerbated with splines. The polynomials ﬁt beyond the boundary\\nknots behave even more wildly than the corresponding global polynomials\\nin that region. This can be conveniently summarized in terms of the point-\\nwise variance of spline functions ﬁt by least squares (see th e example in the\\nnext section for details on these variance calculations). F igure 5.3 compares\\n1A cubic spline with four knots is eight-dimensional. The bs()function omits by\\ndefault the constant term in the basis, since terms like this are typical ly included with\\nother terms in the model.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 162}),\n",
       " Document(page_content='5.2 Piecewise Polynomials and Splines 145\\nXPointwise Variances\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5 0.6•\\n••\\n•\\n••\\n•••••••••••••• •••• ••••••••••••••••••••••••••\\n•••••\\n•••••••••••••••••••••••••••••••••••••••••••\\n•\\n••\\n•\\n••••••••••••••••••••••••••\\n•\\n••••••••••••••••••\\n•••••••••••••••••••• ••••••••••••••••••••••••• ••••Global Linear\\nGlobal Cubic Polynomial\\nCubic Spline - 2 knots\\nNatural Cubic Spline - 6 knots\\nFIGURE 5.3. Pointwise variance curves for four diﬀerent models, with Xcon-\\nsisting of 50points drawn at random from U[0,1], and an assumed error model\\nwith constant variance. The linear and cubic polynomial ﬁts ha ve two and four\\ndegrees of freedom, respectively, while the cubic spline and na tural cubic spline\\neach have six degrees of freedom. The cubic spline has two knot s at0.33and0.66,\\nwhile the natural spline has boundary knots at 0.1and0.9, and four interior knots\\nuniformly spaced between them.\\nthe pointwise variances for a variety of diﬀerent models. Th e explosion of\\nthe variance near the boundaries is clear, and inevitably is worst for cubic\\nsplines.\\nAnatural cubic spline adds additional constraints, namely that the func-\\ntion is linear beyond the boundary knots. This frees up four d egrees of\\nfreedom (two constraints each in both boundary regions), wh ich can be\\nspent more proﬁtably by sprinkling more knots in the interio r region. This\\ntradeoﬀ is illustrated in terms of variance in Figure 5.3. Th ere will be a\\nprice paid in bias near the boundaries, but assuming the func tion is lin-\\near near the boundaries (where we have less information anyw ay) is often\\nconsidered reasonable.\\nA natural cubic spline with Kknots is represented by Kbasis functions.\\nOne can start from a basis for cubic splines, and derive the re duced ba-\\nsis by imposing the boundary constraints. For example, star ting from the\\ntruncated power series basis described in Section 5.2, we ar rive at (Exer-\\ncise 5.4):\\nN1(X) = 1, N2(X) =X, N k+2(X) =dk(X)−dK−1(X),(5.4)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 163}),\n",
       " Document(page_content='146 5. Basis Expansions and Regularization\\nwhere\\ndk(X) =(X−ξk)3\\n+−(X−ξK)3\\n+\\nξK−ξk. (5.5)\\nEach of these basis functions can be seen to have zero second a nd third\\nderivative for X≥ξK.\\n5.2.2 Example: South African Heart Disease (Continued)\\nIn Section 4.4.2 we ﬁt linear logistic regression models to t he South African\\nheart disease data. Here we explore nonlinearities in the fu nctions using\\nnatural splines. The functional form of the model is\\nlogit[Pr(chd|X)] =θ0+h1(X1)Tθ1+h2(X2)Tθ2+···+hp(Xp)Tθp,(5.6)\\nwhere each of the θjare vectors of coeﬃcients multiplying their associated\\nvector of natural spline basis functions hj.\\nWe use four natural spline bases for each term in the model. Fo r example,\\nwithX1representing sbp,h1(X1) is a basis consisting of four basis func-\\ntions. This actually implies three rather than two interior knots (chosen at\\nuniform quantiles of sbp), plus two boundary knots at the extremes of the\\ndata, since we exclude the constant term from each of the hj.\\nSincefamhist is a two-level factor, it is coded by a simple binary or\\ndummy variable, and is associated with a single coeﬃcient in the ﬁt of the\\nmodel.\\nMore compactly we can combine all pvectors of basis functions (and\\nthe constant term) into one big vector h(X), and then the model is simply\\nh(X)Tθ, with total number of parameters df = 1 +∑p\\nj=1dfj, the sum of\\nthe parameters in each component term. Each basis function i s evaluated\\nat each of the Nsamples, resulting in a N×df basis matrix H. At this\\npoint the model is like any other linear logistic model, and t he algorithms\\ndescribed in Section 4.4.1 apply.\\nWe carried out a backward stepwise deletion process, droppi ng terms\\nfrom this model while preserving the group structure of each term, rather\\nthan dropping one coeﬃcient at a time. The AIC statistic (Sec tion 7.5) was\\nused to drop terms, and all the terms remaining in the ﬁnal mod el would\\ncause AIC to increase if deleted from the model (see Table 5.1 ). Figure 5.4\\nshows a plot of the ﬁnal model selected by the stepwise regres sion. The\\nfunctions displayed are ˆfj(Xj) =hj(Xj)Tˆθjfor each variable Xj. The\\ncovariance matrix Cov( ˆθ) =Σis estimated by ˆΣ= (HTWH)−1, whereW\\nis the diagonal weight matrix from the logistic regression. Hencevj(Xj) =\\nVar[ˆfj(Xj)] =hj(Xj)TˆΣjjhj(Xj) is the pointwise variance function of ˆfj,\\nwhereCov( ˆθj) =ˆΣjjistheappropriatesub-matrixof ˆΣ.Theshadedregion\\nin each panel is deﬁned by ˆfj(Xj)±2√\\nvj(Xj).\\nThe AIC statistic is slightly more generous than the likelih ood-ratio test\\n(deviance test). Both sbpandobesityare included in this model, while', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 164}),\n",
       " Document(page_content='5.2 Piecewise Polynomials and Splines 147\\n100 120 140 160 180 200 220-2 0 2 4\\n0 5 10 15 20 25 300 2 4 6 8\\n2 4 6 8 10 12 14-4 -2 0 2 4\\n-4 -2 0 2 4\\nAbsent Present\\n15 20 25 30 35 40 45-2 0 2 4 6\\n20 30 40 50 60-6 -4 -2 0 2ˆf(sbp)\\nsbp\\nˆf(tobacco)\\ntobaccoˆf(ldl)\\nldlˆf(obesity)\\nobesity\\nˆf(age)\\nageˆf(famhist)\\nfamhist\\nFIGURE 5.4. Fitted natural-spline functions for each of the terms in the ﬁn al\\nmodel selected by the stepwise procedure. Included are pointw ise standard-error\\nbands. The rug plot at the base of each ﬁgure indicates the location of each of the\\nsample values for that variable (jittered to break ties).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 165}),\n",
       " Document(page_content='148 5. Basis Expansions and Regularization\\nTABLE 5.1. Final logistic regression model, after stepwise deletion of na tural\\nsplines terms. The column labeled “LRT” is the likelihood-ratio te st statistic when\\nthat term is deleted from the model, and is the change in devianc e from the full\\nmodel (labeled “none”).\\nTerms Df Deviance AIC LRT P-value\\nnone 458.09 502.09\\nsbp4 467.16 503.16 9.076 0.059\\ntobacco 4 470.48 506.48 12.387 0.015\\nldl4 472.39 508.39 14.307 0.006\\nfamhist 1 479.44 521.44 21.356 0.000\\nobesity 4 466.24 502.24 8.147 0.086\\nage4 481.86 517.86 23.768 0.000\\nthey were not in the linear model. The ﬁgure explains why, sin ce their\\ncontributions are inherently nonlinear. These eﬀects at ﬁr st may come as\\na surprise, but an explanation lies in the nature of the retro spective data.\\nThese measurements were made sometime after the patients su ﬀered a\\nheart attack, and in many cases they had already beneﬁted fro m a healthier\\ndiet and lifestyle, hence the apparent increase in risk at low values for\\nobesityandsbp. Table 5.1 shows a summary of the selected model.\\n5.2.3 Example: Phoneme Recognition\\nIn this example we use splines to reduce ﬂexibility rather th an increase it;\\nthe application comes under the general heading of functional modeling. In\\nthe top panel of Figure 5.5 are displayed a sample of 15 log-pe riodograms\\nfor each of the two phonemes “aa” and “ao” measured at 256 freq uencies.\\nThe goal is to use such data to classify a spoken phoneme. Thes e two\\nphonemes were chosen because they are diﬃcult to separate.\\nThe input feature is a vector xof length 256, which we can think of as\\na vector of evaluations of a function X(f) over a grid of frequencies f. In\\nreality there is a continuous analog signal which is a functi on of frequency,\\nand we have a sampled version of it.\\nThe gray lines in the lower panel of Figure 5.5 show the coeﬃci ents of\\na linear logistic regression model ﬁt by maximum likelihood to a training\\nsample of 1000 drawn from the total of 695 “aa”s and 1022 “ao”s . The\\ncoeﬃcients are also plotted as a function of frequency, and i n fact we can\\nthink of the model in terms of its continuous counterpart\\nlogPr(aa|X)\\nPr(ao|X)=∫\\nX(f)β(f)df, (5.7)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 166}),\n",
       " Document(page_content='5.2 Piecewise Polynomials and Splines 149\\nFrequencyLog-periodogram\\n0 50 100 150 200 2500 5 10 15 20 25Phoneme Examples\\naa\\nao\\nFrequencyLogistic Regression Coefficients\\n0 50 100 150 200 250-0.4 -0.2 0.0 0.2 0.4Phoneme Classification: Raw and Restricted Logistic Regression\\nFIGURE 5.5. The top panel displays the log-periodogram as a function of fre -\\nquency for 15examples each of the phonemes “aa” and “ao” sampled from a total\\nof695“aa”s and 1022“ao”s. Each log-periodogram is measured at 256uniformly\\nspaced frequencies. The lower panel shows the coeﬃcients (as a function of fre-\\nquency) of a logistic regression ﬁt to the data by maximum likeli hood, using the\\n256log-periodogram values as inputs. The coeﬃcients are restric ted to be smooth\\nin the red curve, and are unrestricted in the jagged gray curv e.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 167}),\n",
       " Document(page_content='150 5. Basis Expansions and Regularization\\nwhich we approximate by\\n256∑\\nj=1X(fj)β(fj) =256∑\\nj=1xjβj. (5.8)\\nThe coeﬃcients compute a contrast functional, and will have appreciable\\nvalues in regions of frequency where the log-periodograms d iﬀer between\\nthe two classes.\\nThe gray curves are very rough. Since the input signals have f airly strong\\npositive autocorrelation, this results in negative autoco rrelation in the co-\\neﬃcients. In addition the sample size eﬀectively provides o nly four obser-\\nvations per coeﬃcient.\\nApplications such as this permit a natural regularization. We force the\\ncoeﬃcientstovarysmoothlyasafunctionoffrequency.Ther edcurveinthe\\nlower panel of Figure 5.5 shows such a smooth coeﬃcient curve ﬁt to these\\ndata.Weseethatthelowerfrequenciesoﬀerthemostdiscrim inatorypower.\\nNot only does the smoothing allow easier interpretation of t he contrast, it\\nalso produces a more accurate classiﬁer:\\nRawRegularized\\nTraining error 0.080 0.185\\nTest error 0.255 0.158\\nThe smooth red curve was obtained through a very simple use of natural\\ncubic splines. We can represent the coeﬃcient function as an expansion of\\nsplinesβ(f) =∑M\\nm=1hm(f)θm. In practice this means that β=Hθwhere,\\nHis ap×Mbasis matrix of natural cubic splines, deﬁned on the set of\\nfrequencies. Here we used M= 12 basis functions, with knots uniformly\\nplaced over the integers 1 ,2,...,256 representing the frequencies. Since\\nxTβ=xTHθ, we can simply replace the input features xby their ﬁltered\\nversionsx∗=HTx, and ﬁtθby linear logistic regression on the x∗. The\\nred curve is thus ˆβ(f) =h(f)Tˆθ.\\n5.3 Filtering and Feature Extraction\\nIn the previous example, we constructed a p×Mbasis matrix H, and then\\ntransformed our features xinto new features x∗=HTx. These ﬁltered\\nversions of the features were then used as inputs into a learn ing procedure:\\nin the previous example, this was linear logistic regressio n.\\nPreprocessing of high-dimensional features is a very gener al and pow-\\nerful method for improving the performance of a learning alg orithm. The\\npreprocessing need not be linear as it was above, but can be a g eneral', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 168}),\n",
       " Document(page_content='5.4 Smoothing Splines 151\\n(nonlinear) function of the form x∗=g(x). The derived features x∗can\\nthen be used as inputs into any (linear or nonlinear) learnin g procedure.\\nForexample,forsignalorimagerecognitionapopularappro achistoﬁrst\\ntransform the raw features via a wavelet transform x∗=HTx(Section 5.9)\\nand then use the features x∗as inputs into a neural network (Chapter 11).\\nWavelets are eﬀective in capturing discrete jumps or edges, and the neural\\nnetwork is a powerful tool for constructing nonlinear funct ions of these\\nfeatures for predicting the target variable. By using domai n knowledge\\nto construct appropriate features, one can often improve up on a learning\\nmethod that has only the raw features xat its disposal.\\n5.4 Smoothing Splines\\nHere we discuss a spline basis method that avoids the knot sel ection prob-\\nlem completely by using a maximal set of knots. The complexit y of the ﬁt\\nis controlled by regularization. Consider the following pr oblem: among all\\nfunctionsf(x) with two continuous derivatives, ﬁnd one that minimizes th e\\npenalized residual sum of squares\\nRSS(f,λ) =N∑\\ni=1{yi−f(xi)}2+λ∫\\n{f′′(t)}2dt, (5.9)\\nwhereλis a ﬁxed smoothing parameter . The ﬁrst term measures closeness\\nto the data, while the second term penalizes curvature in the function, and\\nλestablishes a tradeoﬀ between the two. Two special cases are :\\nλ= 0 :fcan be any function that interpolates the data.\\nλ=∞:the simple least squares line ﬁt, since no second derivative can\\nbe tolerated.\\nThesevaryfromveryroughtoverysmooth,andthehopeisthat λ∈(0,∞)\\nindexes an interesting class of functions in between.\\nThe criterion (5.9) is deﬁned on an inﬁnite-dimensional fun ction space—\\nin fact, a Sobolev space of functions for which the second ter m is deﬁned.\\nRemarkably, it can be shown that (5.9) has an explicit, ﬁnite -dimensional,\\nunique minimizer which is a natural cubic spline with knots a t the unique\\nvalues of the xi, i= 1,...,N(Exercise 5.7). At face value it seems that\\nthe family is still over-parametrized, since there are as ma ny asNknots,\\nwhich implies Ndegrees of freedom. However, the penalty term translates\\nto a penalty on the spline coeﬃcients, which are shrunk some o f the way\\ntoward the linear ﬁt.\\nSince the solution is a natural spline, we can write it as\\nf(x) =N∑\\nj=1Nj(x)θj, (5.10)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 169}),\n",
       " Document(page_content='152 5. Basis Expansions and Regularization\\nAgeRelative Change in Spinal BMD\\n10 15 20 25-0.05 0.0 0.05 0.10 0.15 0.20••\\n••\\n••\\n••\\n•\\n••\\n••\\n••\\n••\\n•\\n•\\n•\\n••••\\n••\\n•\\n••\\n•\\n•\\n••\\n•••\\n••\\n•••\\n•••\\n•\\n•\\n•\\n••\\n•\\n••••\\n•••••\\n••\\n•••••\\n•\\n•\\n••\\n•••\\n••\\n•\\n••\\n•••\\n•\\n••\\n••••\\n•\\n•••\\n••\\n•••••\\n•\\n••\\n••\\n•\\n•\\n••••\\n• ••• ••••\\n••\\n••\\n•••\\n•\\n••\\n•••••\\n•\\n•••••\\n••\\n•••\\n•\\n• •••\\n•••\\n••••\\n••\\n••••\\n• •\\n••••\\n••• •\\n••••\\n•\\n•••\\n•\\n••••••\\n••\\n•\\n••\\n•\\n•\\n••\\n••\\n•\\n••\\n•\\n• •••\\n•\\n•\\n••••••\\n•\\n••\\n••\\n••••\\n•\\n••\\n••\\n•\\n••••\\n••\\n•\\n•••\\n••\\n•\\n•••\\n•\\n• •••\\n• ••\\n•••••\\n•\\n••\\n•••\\n•\\n••••\\n••\\n•\\n••\\n•••\\n•\\n•••\\n•\\n••\\n•••••\\n••\\n•\\n••••••\\n•\\n•\\n•••\\n• ••\\n•••••••\\n•\\n•••\\n••\\n•\\n•••\\n•\\n•••\\n••\\n••\\n••\\n••\\n•• ••\\n•\\n••••\\n•\\n•\\n•••\\n••\\n• •\\n•••\\n•\\n•••\\n• •\\n••\\n•\\n•••\\n••••\\n•\\n••••\\n••\\n•\\n••\\n••••\\n••••\\n•\\n••\\n•\\n•\\n••••\\n•\\n••\\n•\\n••••\\n• ••\\n•\\n••••\\n•\\n••\\n•••\\n•\\n••\\n•••\\n•\\n•\\n••\\n••\\n•\\n•\\n••\\n••\\n••\\n•••\\n•••\\n•\\n••\\n•\\n•••\\n•\\n•\\n•••\\n••\\n••\\n••\\n•\\n•••\\n•••\\n• ••••\\n••\\n••\\n••\\n•\\n•Male\\nFemale\\nFIGURE 5.6. The response is the relative change in bone mineral density me a-\\nsured at the spine in adolescents, as a function of age. A separ ate smoothing spline\\nwas ﬁt to the males and females, with λ≈0.00022. This choice corresponds to\\nabout12degrees of freedom.\\nwhere the Nj(x) are anN-dimensional set of basis functions for repre-\\nsenting this family of natural splines (Section 5.2.1 and Ex ercise 5.4). The\\ncriterion thus reduces to\\nRSS(θ,λ) = (y−Nθ)T(y−Nθ)+λθTΩNθ, (5.11)\\nwhere{N}ij=Nj(xi) and{ΩN}jk=∫\\nN′′\\nj(t)N′′\\nk(t)dt. The solution is\\neasily seen to be\\nˆθ= (NTN+λΩN)−1NTy, (5.12)\\na generalized ridge regression. The ﬁtted smoothing spline is given by\\nˆf(x) =N∑\\nj=1Nj(x)ˆθj. (5.13)\\nEﬃcient computational techniques for smoothing splines ar e discussed in\\nthe Appendix to this chapter.\\nFigure 5.6 shows a smoothing spline ﬁt to some data on bone min eral\\ndensity (BMD) in adolescents. The response is relative chan ge in spinal\\nBMD over two consecutive visits, typically about one year ap art. The data\\nare color coded by gender, and two separate curves were ﬁt. Th is simple', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 170}),\n",
       " Document(page_content='5.4 Smoothing Splines 153\\nsummary reinforces the evidence in the data that the growth s purt for\\nfemales precedes that for males by about two years. In both ca ses the\\nsmoothing parameter λwas approximately 0 .00022; this choice is discussed\\nin the next section.\\n5.4.1 Degrees of Freedom and Smoother Matrices\\nWe have not yet indicated how λis chosen for the smoothing spline. Later\\nin this chapter we describe automatic methods using techniq ues such as\\ncross-validation. In this section we discuss intuitive way s of prespecifying\\nthe amount of smoothing.\\nA smoothing spline with prechosen λis an example of a linear smoother\\n(as in linear operator). This is because the estimated param eters in (5.12)\\nare a linear combination of the yi. Denote by ˆftheN-vector of ﬁtted values\\nˆf(xi) at the training predictors xi. Then\\nˆf=N(NTN+λΩN)−1NTy\\n=Sλy. (5.14)\\nAgain the ﬁt is linear in y, and the ﬁnite linear operator Sλis known as\\nthesmoother matrix . One consequence of this linearity is that the recipe\\nfor producing ˆffromydoes not depend on yitself;Sλdepends only on\\nthexiandλ.\\nLinear operators are familiar in more traditional least squ ares ﬁtting as\\nwell. Suppose Bξis aN×Mmatrix ofMcubic-spline basis functions\\nevaluated at the Ntraining points xi, with knot sequence ξ, andM≪N.\\nThen the vector of ﬁtted spline values is given by\\nˆf=Bξ(BT\\nξBξ)−1BT\\nξy\\n=Hξy. (5.15)\\nHere the linear operator Hξis a projection operator, also known as the hat\\nmatrix in statistics. There are some important similaritie s and diﬀerences\\nbetween HξandSλ:\\n•Both are symmetric, positive semideﬁnite matrices.\\n•HξHξ=Hξ(idempotent), while SλSλ⪯Sλ, meaning that the right-\\nhand side exceeds the left-hand side by a positive semideﬁni te matrix.\\nThis is a consequence of the shrinking nature of Sλ, which we discuss\\nfurther below.\\n•Hξhas rankM, whileSλhas rankN.\\nThe expression M= trace(Hξ) gives the dimension of the projection space,\\nwhich is also the number of basis functions, and hence the num ber of pa-\\nrameters involved in the ﬁt. By analogy we deﬁne the eﬀective degrees of', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 171}),\n",
       " Document(page_content='154 5. Basis Expansions and Regularization\\nfreedomof a smoothing spline to be\\ndfλ= trace(Sλ), (5.16)\\nthe sum of the diagonal elements of Sλ. This very useful deﬁnition allows\\nus a more intuitive way to parameterize the smoothing spline , and indeed\\nmany other smoothers as well, in a consistent fashion. For ex ample, in Fig-\\nure 5.6 we speciﬁed df λ= 12 for each of the curves, and the corresponding\\nλ≈0.00022 was derived numerically by solving trace( Sλ) = 12. There are\\nmany arguments supporting this deﬁnition of degrees of free dom, and we\\ncover some of them here.\\nSinceSλis symmetric (and positive semideﬁnite), it has a real eigen -\\ndecomposition. Before we proceed, it is convenient to rewri teSλin the\\nReinschform\\nSλ= (I+λK)−1, (5.17)\\nwhereKdoes not depend on λ(Exercise 5.9). Since ˆf=Sλysolves\\nmin\\nf(y−f)T(y−f)+λfTKf, (5.18)\\nKis known as the penalty matrix , and indeed a quadratic form in Khas\\na representation in terms of a weighted sum of squared (divid ed) second\\ndiﬀerences. The eigen-decomposition of Sλis\\nSλ=N∑\\nk=1ρk(λ)ukuT\\nk (5.19)\\nwith\\nρk(λ) =1\\n1+λdk, (5.20)\\nanddkthe corresponding eigenvalue of K. Figure 5.7 (top) shows the re-\\nsults of applying a cubic smoothing spline to some air pollut ion data (128\\nobservations). Two ﬁts are given: a smoother ﬁt corresponding to a larger\\npenaltyλand arougherﬁt for a smaller penalty. The lower panels repre-\\nsent the eigenvalues (lower left) and some eigenvectors (lo wer right) of the\\ncorresponding smoother matrices. Some of the highlights of the eigenrep-\\nresentation are the following:\\n•Theeigenvectorsarenotaﬀectedbychangesin λ,andhencethewhole\\nfamily of smoothing splines (for a particular sequence x) indexed by\\nλhave the same eigenvectors.\\n•Sλy=∑N\\nk=1ukρk(λ)⟨uk,y⟩, and hence the smoothing spline oper-\\nates by decomposing yw.r.t. the (complete) basis {uk}, and diﬀer-\\nentially shrinking the contributions using ρk(λ). This is to be con-\\ntrasted with a basis-regression method, where the componen ts are', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 172}),\n",
       " Document(page_content='5.4 Smoothing Splines 155\\nDaggot Pressure GradientOzone Concentration\\n-50 0 50 1000 10 20 30•••\\n••••••\\n••\\n••••••\\n•\\n•••\\n•\\n•••\\n••\\n•••\\n•\\n••\\n•\\n••\\n•••\\n•\\n••\\n••\\n••\\n••\\n••••\\n•••••\\n•\\n•••\\n•••\\n••\\n•\\n••\\n•••\\n••\\n••\\n•••\\n••••\\n••\\n••\\n•••\\n•\\n••••\\n••\\n••\\n••\\n•••\\n•••\\n•••\\n••\\n•\\n••\\n••••\\n••\\n•••\\n•\\n•\\n•\\n•\\nOrderEigenvalues\\n5 10 15 20 25-0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2•••\\n•\\n•\\n•\\n••••••••••• •• • • •• • ••••••••\\n•\\n•\\n•\\n•\\n•\\n•••••••••••••df=5\\ndf=11\\n-50 0 50 100 -50 0 50 100\\nFIGURE 5.7. (Top:) Smoothing spline ﬁt of ozone concentration versus Dag got\\npressure gradient. The two ﬁts correspond to diﬀerent values of the smoothing\\nparameter, chosen to achieve ﬁve and eleven eﬀective degrees of freedom, deﬁned\\nby dfλ=trace(Sλ). (Lower left:) First 25eigenvalues for the two smoothing-spline\\nmatrices. The ﬁrst two are exactly 1, and all are ≥0. (Lower right:) Third to\\nsixth eigenvectors of the spline smoother matrices. In each c ase,ukis plotted\\nagainstx, and as such is viewed as a function of x. Therugat the base of the\\nplots indicate the occurrence of data points. The damped func tions represent the\\nsmoothed versions of these functions (using the 5df smoother).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 173}),\n",
       " Document(page_content='156 5. Basis Expansions and Regularization\\neither left alone, or shrunk to zero—that is, a projection mat rix such\\nasHξabove hasMeigenvalues equal to 1, and the rest are 0. For\\nthis reason smoothing splines are referred to as shrinking smoothers,\\nwhile regression splines are projection smoothers (see Figure 3.17 on\\npage 80).\\n•The sequence of uk, ordered by decreasing ρk(λ), appear to increase\\nincomplexity. Indeed,theyhave thezero-crossingbehavio r ofpolyno-\\nmials of increasing degree. Since Sλuk=ρk(λ)uk, we see how each of\\nthe eigenvectors themselves are shrunk by the smoothing spl ine: the\\nhigher the complexity, the more they are shrunk. If the domai n ofX\\nis periodic, then the ukare sines and cosines at diﬀerent frequencies.\\n•The ﬁrst two eigenvalues are alwaysone, and they correspond to the\\ntwo-dimensional eigenspace of functions linear in x(Exercise 5.11),\\nwhich are never shrunk.\\n•The eigenvalues ρk(λ) = 1/(1 +λdk) are an inverse function of the\\neigenvalues dkof the penalty matrix K, moderated by λ;λcontrols\\nthe rate at which the ρk(λ) decrease to zero. d1=d2= 0 and again\\nlinear functions are not penalized.\\n•One can reparametrize the smoothing spline using the basis v ectors\\nuk(theDemmler–Reinsch basis). In this case the smoothing spline\\nsolves\\nmin\\nθ∥y−Uθ∥2+λθTDθ, (5.21)\\nwhereUhas columns ukandDis a diagonal matrix with elements\\ndk.\\n•dfλ= trace( Sλ) =∑N\\nk=1ρk(λ). For projection smoothers, all the\\neigenvalues are 1, each one corresponding to a dimension of t he pro-\\njection subspace.\\nFigure 5.8 depicts a smoothing spline matrix, with the rows o rdered with\\nx. The banded nature of this representation suggests that a sm oothing\\nspline is a local ﬁtting method, much like the locally weight ed regression\\nprocedures in Chapter 6. The right panel shows in detail sele cted rows of\\nS, which we call the equivalent kernels . Asλ→0, dfλ→N, andSλ→I,\\ntheN-dimensional identity matrix. As λ→∞, dfλ→2, andSλ→H, the\\nhat matrix for linear regression on x.\\n5.5 Automatic Selection of the Smoothing\\nParameters\\nThe smoothing parameters for regression splines encompass the degree of\\nthe splines, and the number and placement of the knots. For sm oothing', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 174}),\n",
       " Document(page_content='5.5 Automatic Selection of the Smoothing Parameters 157\\n11510075502512Smoother Matrix\\n••••• • •••••• •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\\n•Row 115••••• ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\\n•Row 100••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••Row 75•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• • •Row 50•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• • •Row 25•••••• •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• • •Row 12Equivalent Kernels\\nFIGURE 5.8. The smoother matrix for a smoothing spline is nearly banded,\\nindicating an equivalent kernel with local support. The left pa nel represents the\\nelements of Sas an image. The right panel shows the equivalent kernel or wei ght-\\ning function in detail for the indicated rows.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 175}),\n",
       " Document(page_content='158 5. Basis Expansions and Regularization\\nsplines, we have only the penalty parameter λto select, since the knots are\\nat all the unique training X’s, and cubic degree is almost always used in\\npractice.\\nSelectingtheplacementandnumberofknotsforregressions plinescanbe\\na combinatorially complex task, unless some simpliﬁcation s are enforced.\\nThe MARS procedure in Chapter 9 uses a greedy algorithm with s ome\\nadditional approximations to achieve a practical compromi se. We will not\\ndiscuss this further here.\\n5.5.1 Fixing the Degrees of Freedom\\nSince df λ= trace(Sλ) is monotone in λfor smoothing splines, we can in-\\nvert the relationship and specify λby ﬁxing df. In practice this can be\\nachieved by simple numerical methods. So, for example, in Rone can use\\nsmooth.spline(x,y,df=6) to specify the amount of smoothing. This encour-\\nages a more traditional mode of model selection, where we mig ht try a cou-\\nple of diﬀerent values of df, and select one based on approxim ateF-tests,\\nresidual plots and other more subjective criteria. Using df in this way pro-\\nvides a uniform approach to compare many diﬀerent smoothing methods.\\nIt is particularly useful in generalized additive models (Chapter 9), where\\nseveral smoothing methods can be simultaneously used in one model.\\n5.5.2 The Bias–Variance Tradeoﬀ\\nFigure 5.9 shows the eﬀect of the choice of df λwhen using a smoothing\\nspline on a simple example:\\nY=f(X)+ε,\\nf(X) =sin(12(X+0.2))\\nX+0.2,(5.22)\\nwithX∼U[0,1] andε∼N(0,1). Our training sample consists of N= 100\\npairsxi,yidrawn independently from this model.\\nThe ﬁtted splines for three diﬀerent values of df λare shown. The yellow\\nshaded region in the ﬁgure represents the pointwise standar d error of ˆfλ,\\nthat is, we have shaded the region between ˆfλ(x)±2·se(ˆfλ(x)). Since\\nˆf=Sλy,\\nCov(ˆf) =SλCov(y)ST\\nλ\\n=SλST\\nλ. (5.23)\\nThe diagonal contains the pointwise variances at the traini ngxi. The bias\\nis given by\\nBias(ˆf) =f−E(ˆf)\\n=f−Sλf, (5.24)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 176}),\n",
       " Document(page_content='5.5 Automatic Selection of the Smoothing Parameters 159\\n6 8 10 12 141.0 1.1 1.2 1.3 1.4 1.5\\n0.0 0.2 0.4 0.6 0.8 1.0−4 −2 0 2yO\\nO\\nO\\nOO\\nOO\\nO\\nOOO\\nOO\\nO\\nOO\\nOOOO\\nOO\\nOOOOO\\nOOO\\nO\\nOO\\nOO\\nOO\\nOO\\nO\\nO\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOOO\\nOOOO\\nOO\\nO\\nOO\\nOOOOOO\\nO\\nOOO\\nOOO\\nOO\\nOO\\nOOOOO\\nOOOO\\nOO\\n0.0 0.2 0.4 0.6 0.8 1.0−4 −2 0 2yO\\nO\\nO\\nOO\\nOO\\nO\\nOOO\\nOO\\nO\\nOO\\nOOOO\\nOO\\nOOOOO\\nOOO\\nO\\nOO\\nOO\\nOO\\nOO\\nO\\nO\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOOO\\nOOOO\\nOO\\nO\\nOO\\nOOOOOO\\nO\\nOOO\\nOOO\\nOO\\nOO\\nOOOOO\\nOOOO\\nOO\\n0.0 0.2 0.4 0.6 0.8 1.0−4 −2 0 2yO\\nO\\nO\\nOO\\nOO\\nO\\nOOO\\nOO\\nO\\nOO\\nOOOO\\nOO\\nOOOOO\\nOOO\\nO\\nOO\\nOO\\nOO\\nOO\\nO\\nO\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOOO\\nOOOO\\nOO\\nO\\nOO\\nOOOOOO\\nO\\nOOO\\nOOO\\nOO\\nOO\\nOOOOO\\nOOOO\\nOOEPECV\\nX XXdfλ= 5\\ndfλ= 9 dfλ= 15dfλCross-ValidationEPE(λ) and CV( λ)\\nFIGURE 5.9. The top left panel shows the EPE(λ)andCV(λ)curves for a\\nrealization from a nonlinear additive error model (5.22). The remaining panels\\nshow the data, the true functions (in purple), and the ﬁtted cu rves (in green) with\\nyellow shaded ±2×standard error bands, for three diﬀerent values of dfλ.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 177}),\n",
       " Document(page_content='160 5. Basis Expansions and Regularization\\nwherefis the (unknown) vector of evaluations of the true fat the training\\nX’s. The expectations and variances are with respect to repea ted draws\\nof samples of size N= 100 from the model (5.22). In a similar fashion\\nVar(ˆfλ(x0)) and Bias( ˆfλ(x0)) can be computed at any point x0(Exer-\\ncise 5.10). The three ﬁts displayed in the ﬁgure give a visual demonstration\\nof the bias-variance tradeoﬀ associated with selecting the smoothing\\nparameter.\\ndfλ= 5:The spline under ﬁts, and clearly trims down the hills and ﬁlls in\\nthe valleys . This leads to a bias that is most dramatic in regions of\\nhigh curvature. The standard error band is very narrow, so we esti-\\nmate a badly biased version of the true function with great re liability!\\ndfλ= 9:Here the ﬁtted function is close to the true function, althou gh a\\nslight amount of bias seems evident. The variance has not inc reased\\nappreciably.\\ndfλ= 15:The ﬁtted function is somewhat wiggly, but close to the true\\nfunction. The wiggliness also accounts for the increased wi dth of the\\nstandard error bands—the curve is starting to follow some ind ividual\\npoints too closely.\\nNote that in these ﬁgures we are seeing a single realization o f data and\\nhence ﬁtted spline ˆfin each case, while the bias involves an expectation\\nE(ˆf). We leave it as an exercise (5.10) to compute similar ﬁgures where the\\nbias is shown as well. The middle curve seems “just right,” in that it has\\nachieved a good compromise between bias and variance.\\nThe integrated squared prediction error (EPE) combines bot h bias and\\nvariance in a single summary:\\nEPE(ˆfλ) = E(Y−ˆfλ(X))2\\n= Var(Y)+E[\\nBias2(ˆfλ(X))+Var( ˆfλ(X))]\\n=σ2+MSE( ˆfλ). (5.25)\\nNote that this is averaged both over the training sample (giv ing rise to ˆfλ),\\nand the values of the (independently chosen) prediction poi nts (X,Y). EPE\\nis a natural quantity of interest, and does create a tradeoﬀ b etween bias\\nand variance. The blue points in the top left panel of Figure 5 .9 suggest\\nthat df λ= 9 is spot on!\\nSincewedon’tknowthetruefunction,wedonothaveaccessto EPE,and\\nneed an estimate. This topic is discussed in some detail in Ch apter 7, and\\ntechniques such as K-fold cross-validation, GCV and Cpare all in common\\nuse. In Figure 5.9 we include the N-fold (leave-one-out) cross-validation\\ncurve:', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 178}),\n",
       " Document(page_content='5.6 Nonparametric Logistic Regression 161\\nCV(ˆfλ) =1\\nNN∑\\ni=1(yi−ˆf(−i)\\nλ(xi))2(5.26)\\n=1\\nNN∑\\ni=1(\\nyi−ˆfλ(xi)\\n1−Sλ(i,i))2\\n, (5.27)\\nwhich can (remarkably) be computed for each value of λfrom the original\\nﬁtted values and the diagonal elements Sλ(i,i) ofSλ(Exercise 5.13).\\nThe EPE and CV curves have a similar shape, but the entire CV cu rve\\nis above the EPE curve. For some realizations this is reverse d, and overall\\nthe CV curve is approximately unbiased as an estimate of the E PE curve.\\n5.6 Nonparametric Logistic Regression\\nThe smoothing spline problem (5.9) in Section 5.4 is posed in a regression\\nsetting. It is typically straightforward to transfer this t echnology to other\\ndomains. Here we consider logistic regression with a single quantitative\\ninputX. The model is\\nlogPr(Y= 1|X=x)\\nPr(Y= 0|X=x)=f(x), (5.28)\\nwhich implies\\nPr(Y= 1|X=x) =ef(x)\\n1+ef(x). (5.29)\\nFittingf(x) in a smooth fashion leads to a smooth estimate of the condi-\\ntional probability Pr( Y= 1|x), which can be used for classiﬁcation or risk\\nscoring.\\nWe construct the penalized log-likelihood criterion\\nℓ(f;λ) =N∑\\ni=1[yilogp(xi)+(1−yi)log(1−p(xi))]−1\\n2λ∫\\n{f′′(t)}2dt\\n=N∑\\ni=1[\\nyif(xi)−log(1+ef(xi))]\\n−1\\n2λ∫\\n{f′′(t)}2dt,(5.30)\\nwhere we have abbreviated p(x) = Pr(Y= 1|x). The ﬁrst term in this ex-\\npression is the log-likelihood based on the binomial distri bution (c.f. Chap-\\nter 4, page 120). Arguments similar to those used in Section 5 .4 show that\\nthe optimal fis aﬁnite-dimensional natural spline with knots at the uniq ue', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 179}),\n",
       " Document(page_content='162 5. Basis Expansions and Regularization\\nvalues ofx. This means that we can represent f(x) =∑N\\nj=1Nj(x)θj. We\\ncompute the ﬁrst and second derivatives\\n∂ℓ(θ)\\n∂θ=NT(y−p)−λΩθ, (5.31)\\n∂2ℓ(θ)\\n∂θ∂θT=−NTWN−λΩ, (5.32)\\nwherepis theN-vector with elements p(xi), andWis a diagonal matrix\\nof weightsp(xi)(1−p(xi)). The ﬁrst derivative (5.31) is nonlinear in θ, so\\nwe need to use an iterative algorithm as in Section 4.4.1. Usi ng Newton–\\nRaphson as in (4.23) and (4.26) for linear logistic regressi on, the update\\nequation can be written\\nθnew= (NTWN+λΩ)−1NTW(\\nNθold+W−1(y−p))\\n= (NTWN+λΩ)−1NTWz. (5.33)\\nWe can also express this update in terms of the ﬁtted values\\nfnew=N(NTWN+λΩ)−1NTW(\\nfold+W−1(y−p))\\n=Sλ,wz. (5.34)\\nReferring back to (5.12) and (5.14), we see that the update ﬁt s a weighted\\nsmoothing spline to the working response z(Exercise 5.12).\\nThe form of (5.34) is suggestive. It is tempting to replace Sλ,wby any\\nnonparametric (weighted) regression operator, and obtain general fami-\\nlies of nonparametric logistic regression models. Althoug h herexis one-\\ndimensional, this procedure generalizes naturally to high er-dimensional x.\\nThese extensions are at the heart of generalized additive models , which we\\npursue in Chapter 9.\\n5.7 Multidimensional Splines\\nSo far we have focused on one-dimensional spline models. Eac h of the ap-\\nproaches have multidimensional analogs. Suppose X∈IR2, and we have\\na basis of functions h1k(X1), k= 1,...,M 1for representing functions of\\ncoordinate X1, and likewise a set of M2functionsh2k(X2) for coordinate\\nX2. Then the M1×M2dimensional tensor product basis deﬁned by\\ngjk(X) =h1j(X1)h2k(X2), j= 1,...,M 1, k= 1,...,M 2(5.35)\\ncan be used for representing a two-dimensional function:\\ng(X) =M1∑\\nj=1M2∑\\nk=1θjkgjk(X). (5.36)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 180}),\n",
       " Document(page_content='5.7 Multidimensional Splines 163\\nFIGURE 5.10. A tensor product basis of B-splines, showing some selected pai rs.\\nEach two-dimensional function is the tensor product of the c orresponding one\\ndimensional marginals.\\nFigure 5.10 illustrates a tensor product basis using B-spli nes. The coeﬃ-\\ncients can be ﬁt by least squares, as before. This can be gener alized tod\\ndimensions, but note that the dimension of the basis grows ex ponentially\\nfast—yet another manifestation of the curse of dimensionali ty. The MARS\\nprocedure discussed in Chapter 9 is a greedy forward algorit hm for includ-\\ning only those tensor products that are deemed necessary by l east squares.\\nFigure5.11 illustrates thediﬀerencebetween additive and tensor product\\n(natural) splines on the simulated classiﬁcation example f rom Chapter 2.\\nA logistic regression model logit[Pr( T|x)] =h(x)Tθis ﬁt to the binary re-\\nsponse, and the estimated decision boundary is the contour h(x)Tˆθ= 0.\\nThe tensor product basis can achieve more ﬂexibility at the d ecision bound-\\nary, but introduces some spurious structure along the way.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 181}),\n",
       " Document(page_content='164 5. Basis Expansions and Regularization\\nAdditive Natural Cubic Splines - 4 df each', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 182}),\n",
       " Document(page_content='.. .. . .. . . .. . . . .. . . . . .. . . . . . .. . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 182}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 182}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 182}),\n",
       " Document(page_content='. . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . .. . . . . . . .. . . . . . . .. . . . . . .. . . . . . .. . . . . . .. . . . . .. . . . . .. . . . .. . . . .. . . . .. . . .. . . .. . . .. . .. . .. . .. .. .. ...', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 182}),\n",
       " Document(page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\nooooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.23\\nTest Error:       0.28\\nBayes Error:    0.21\\nNatural Cubic Splines - Tensor Product - 4 df each', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 182}),\n",
       " Document(page_content='. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . .. . . . . . . . .. . . . . . . .. . . . ... . . . . .. . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 182}),\n",
       " Document(page_content='. . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 182}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 182}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 182}),\n",
       " Document(page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\nooooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.230\\nTest Error:       0.282\\nBayes Error:    0.210\\nFIGURE 5.11. The simulation example of Figure 2.1. The upper panel shows the\\ndecision boundary of an additive logistic regression model, u sing natural splines\\nin each of the two coordinates (total df = 1+(4 −1)+(4−1) = 7). The lower\\npanel shows the results of using a tensor product of natural sp line bases in each\\ncoordinate (total df = 4×4 = 16). The broken purple boundary is the Bayes\\ndecision boundary for this problem.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 182}),\n",
       " Document(page_content='5.7 Multidimensional Splines 165\\nOne-dimensionalsmoothingsplines(viaregularization)g eneralizetohigh-\\ner dimensions as well. Suppose we have pairs yi,xiwithxi∈IRd, and we\\nseek ad-dimensional regression function f(x). The idea is to set up the\\nproblem\\nmin\\nfN∑\\ni=1{yi−f(xi)}2+λJ[f], (5.37)\\nwhereJis an appropriate penalty functional for stabilizing a func tionfin\\nIRd.Forexample,anaturalgeneralizationoftheone-dimensio nalroughness\\npenalty (5.9) for functions on IR2is\\nJ[f] =∫ ∫\\nIR2[(∂2f(x)\\n∂x2\\n1)2\\n+2(∂2f(x)\\n∂x1∂x2)2\\n+(∂2f(x)\\n∂x2\\n2)2]\\ndx1dx2.(5.38)\\nOptimizing (5.37) with this penalty leads to a smooth two-di mensional\\nsurface, known as a thin-plate spline. It shares many proper ties with the\\none-dimensional cubic smoothing spline:\\n•asλ→0, the solution approaches an interpolating function [the o ne\\nwith smallest penalty (5.38)];\\n•asλ→∞, the solution approaches the least squares plane;\\n•for intermediate values of λ, the solution can be represented as a\\nlinear expansion of basis functions, whose coeﬃcients are o btained\\nby a form of generalized ridge regression.\\nThe solution has the form\\nf(x) =β0+βTx+N∑\\nj=1αjhj(x), (5.39)\\nwherehj(x) =||x−xj||2log||x−xj||. Thesehjare examples of radial\\nbasis functions , which are discussed in more detail in the next section. The\\ncoeﬃcients are found by plugging (5.39) into (5.37), which r educes to a\\nﬁnite-dimensional penalized least squares problem. For th e penalty to be\\nﬁnite, the coeﬃcients αjhave to satisfy a set of linear constraints; see\\nExercise 5.14.\\nThin-plate splines are deﬁned more generally for arbitrary dimensiond,\\nfor which an appropriately more general Jis used.\\nThere are a number of hybrid approaches that are popular in pr actice,\\nboth for computational and conceptual simplicity. Unlike o ne-dimensional\\nsmoothing splines, the computational complexity for thin- plate splines is\\nO(N3), since there is not in general any sparse structure that can be ex-\\nploited. However, as with univariate smoothing splines, we can get away\\nwith substantially less than the Nknots prescribed by the solution (5.39).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 183}),\n",
       " Document(page_content='166 5. Basis Expansions and Regularization\\n125\\n130135140145150155\\n15202530354045\\n20 30 40 50 60\\nAgeObesitySystolic Blood Pressure\\n120125130135140145150155160\\n•• ••\\n••\\n••••\\n•\\n•\\n•• ••••\\n••\\n••\\n••\\n•\\n••\\n••\\n•\\n•••••\\n•••\\n•••\\n•\\n••\\n•••\\n••\\n•\\n••\\n•••\\n••\\n••••••\\n•••\\n••\\n•\\n•\\n••\\n••\\n••\\n•\\n••\\n••\\n••\\n••\\n••\\n••\\n••\\n•\\n••\\n••••\\n••••\\n•\\n•\\n•\\n••\\n•\\n•••\\n••\\n••\\n•\\n•••\\n•••\\n•••\\n••\\n••\\n••\\n••\\n•\\n•\\n•••••\\n•••\\n•\\n• •••\\n••••\\n•••\\n••\\n••••\\n•\\n••••\\n• ••\\n•\\n•••\\n•••\\n•••••\\n•\\n••\\n••\\n•••\\n••\\n••\\n••••\\n•\\n••••\\n•••\\n••\\n••\\n••\\n••\\n••\\n•\\n•••\\n••\\n•\\n•\\n••••\\n••\\n•••\\n••••\\n•\\n•••\\n•••\\n•\\n•••••\\n•\\n•\\n•••\\n••\\n•••\\n••\\n•\\n••\\n•\\n•••\\n••••\\n• •\\n•••\\n•\\n•••\\n••\\n••\\n••\\n•\\n••\\n•\\n••••\\n•\\n••••\\n•••\\n•\\n•••\\n•••\\n••\\n••\\n•\\n••••\\n••\\n• ••\\n•••\\n•••\\n•••\\n••• •••\\n•\\n•\\n•\\n••\\n••\\n•\\n••\\n••\\n••\\n•••••\\n•••\\n•\\n••\\n•••\\n•\\n••\\n•\\n••\\n••\\n•\\n••• •\\n••\\n•\\n•••\\n••\\n••\\n••\\n•••\\n••\\n•••\\n••\\n•••\\n•••\\n•\\n•••\\n•\\n••\\n•••\\n••\\n•••\\n••••\\n•\\n••\\n••••\\n•••\\n•\\n•••\\n••\\n••\\n••\\n•••\\n••••\\n•\\n••\\n••\\n••\\n•• • • • • •• • • • • • •• • • • • • •• • • • • •• • • •• •\\n• • • • • • • •• •••• •• • • •• • • • • •• • • • • • • •\\nFIGURE 5.12. A thin-plate spline ﬁt to the heart disease data, displayed as a\\ncontour plot. The response is systolic blood pressure , modeled as a function\\nofageandobesity. The data points are indicated, as well as the lattice of point s\\nused as knots. Care should be taken to use knots from the lattice inside the convex\\nhull of the data (red), and ignore those outside (green).\\nIn practice, it is usually suﬃcient to work with a lattice of k nots covering\\nthe domain. The penalty is computed for the reduced expansio n just as\\nbefore. Using Kknots reduces the computations to O(NK2+K3). Fig-\\nure 5.12 shows the result of ﬁtting a thin-plate spline to som e heart disease\\nrisk factors, representing the surface as a contour plot. In dicated are the\\nlocation of the input features, as well as the knots used in th e ﬁt. Note that\\nλwas speciﬁed via df λ= trace(Sλ) = 15.\\nMore generally one can represent f∈IRdas an expansion in any arbi-\\ntrarily large collection of basis functions, and control th e complexity by ap-\\nplying a regularizer such as (5.38). For example, we could co nstruct a basis\\nby forming the tensor products of all pairs of univariate smo othing-spline\\nbasis functions as in (5.35), using, for example, the univar iateB-splines\\nrecommended in Section 5.9.2 as ingredients. This leads to a n exponential', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 184}),\n",
       " Document(page_content='5.8 Regularization and Reproducing Kernel Hilbert Spaces 16 7\\ngrowth in basis functions as the dimension increases, and ty pically we have\\nto reduce the number of functions per coordinate accordingl y.\\nThe additive spline models discussed in Chapter 9 are a restr icted class\\nof multidimensional splines. They can be represented in thi s general formu-\\nlation as well; that is, there exists a penalty J[f] that guarantees that the\\nsolution has the form f(X) =α+f1(X1)+···+fd(Xd) and that each of\\nthe functions fjare univariate splines. In this case the penalty is somewhat\\ndegenerate, and it is more natural to assumethatfis additive, and then\\nsimply impose an additional penalty on each of the component functions:\\nJ[f] =J(f1+f2+···+fd)\\n=d∑\\nj=1∫\\nf′′\\nj(tj)2dtj. (5.40)\\nThese are naturally extended to ANOVA spline decomposition s,\\nf(X) =α+∑\\njfj(Xj)+∑\\nj<kfjk(Xj,Xk)+···,(5.41)\\nwhere each of the components are splines of the required dime nsion. There\\nare many choices to be made:\\n•The maximum order of interaction—we have shown up to order 2\\nabove.\\n•Which terms to include—not all main eﬀects and interactions a re\\nnecessarily needed.\\n•What representation to use—some choices are:\\n–regression splines with a relatively small number of basis f unc-\\ntions per coordinate, and their tensor products for interac tions;\\n–a complete basis as in smoothing splines, and include approp ri-\\nate regularizers for each term in the expansion.\\nIn many cases when the number of potential dimensions (featu res) is large,\\nautomatic methods are more desirable. The MARS and MART proc edures\\n(Chapters 9 and 10, respectively), both fall into this categ ory.\\n5.8 Regularization and Reproducing Kernel\\nHilbert Spaces\\nInthissectionwecastsplinesintothelargercontextofreg ularizationmeth-\\nods and reproducing kernel Hilbert spaces. This section is q uite technical\\nand can be skipped by the disinterested or intimidated reade r.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 185}),\n",
       " Document(page_content='168 5. Basis Expansions and Regularization\\nA general class of regularization problems has the form\\nmin\\nf∈H[N∑\\ni=1L(yi,f(xi))+λJ(f)]\\n(5.42)\\nwhereL(y,f(x)) is a loss function, J(f) is a penalty functional, and His\\na space of functions on which J(f) is deﬁned. Girosi et al. (1995) describe\\nquite general penalty functionals of the form\\nJ(f) =∫\\nIRd|˜f(s)|2\\n˜G(s)ds, (5.43)\\nwhere˜fdenotes the Fourier transform of f, and˜Gis some positive function\\nthatfalls oﬀtozeroas ||s||→∞.Theideaisthat1 /˜Gincreasesthepenalty\\nfor high-frequency components of f. Under some additional assumptions\\nthey show that the solutions have the form\\nf(X) =K∑\\nk=1αkφk(X)+N∑\\ni=1θiG(X−xi), (5.44)\\nwhere theφkspan the null space of the penalty functional J, andGis the\\ninverse Fourier transform of ˜G. Smoothing splines and thin-plate splines\\nfall into this framework. The remarkable feature of this sol ution is that\\nwhile the criterion (5.42) is deﬁned over an inﬁnite-dimens ional space, the\\nsolution is ﬁnite-dimensional. In the next sections we look at some speciﬁc\\nexamples.\\n5.8.1 Spaces of Functions Generated by Kernels\\nAn important subclass of problems of the form (5.42) are gene rated by\\na positive deﬁnite kernel K(x,y), and the corresponding space of func-\\ntionsHKis called a reproducing kernel Hilbert space (RKHS). The penalty\\nfunctionalJis deﬁned in terms of the kernel as well. We give a brief and\\nsimpliﬁed introduction to this class of models, adapted fro m Wahba (1990)\\nand Girosi et al. (1995), and nicely summarized in Evgeniou e t al. (2000).\\nLetx,y∈IRp. We consider the space of functions generated by the linear\\nspan of{K(·,y), y∈IRp)}; i.e arbitrary linear combinations of the form\\nf(x) =∑\\nmαmK(x,ym), where each kernel term is viewed as a function\\nof the ﬁrst argument, and indexed by the second. Suppose that Khas an\\neigen-expansion\\nK(x,y) =∞∑\\ni=1γiφi(x)φi(y) (5.45)\\nwithγi≥0,∑∞\\ni=1γ2\\ni<∞. Elements ofHKhave an expansion in terms of\\nthese eigen-functions,\\nf(x) =∞∑\\ni=1ciφi(x), (5.46)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 186}),\n",
       " Document(page_content='5.8 Regularization and Reproducing Kernel Hilbert Spaces 16 9\\nwith the constraint that\\n||f||2\\nHKdef=∞∑\\ni=1c2\\ni/γi<∞, (5.47)\\nwhere||f||HKis the norm induced by K. The penalty functional in (5.42)\\nfor the spaceHKis deﬁned to be the squared norm J(f) =||f||2\\nHK. The\\nquantityJ(f) can be interpreted as a generalized ridge penalty, where\\nfunctions with large eigenvalues in the expansion (5.45) ge t penalized less,\\nand vice versa.\\nRewriting (5.42) we have\\nmin\\nf∈HK[N∑\\ni=1L(yi,f(xi))+λ||f||2\\nHK]\\n(5.48)\\nor equivalently\\nmin\\n{cj}∞\\n1\\uf8ee\\n\\uf8f0N∑\\ni=1L(yi,∞∑\\nj=1cjφj(xi))+λ∞∑\\nj=1c2\\nj/γj\\uf8f9\\n\\uf8fb. (5.49)\\nIt can be shown (Wahba, 1990, see also Exercise 5.15) that the solution\\nto (5.48) is ﬁnite-dimensional, and has the form\\nf(x) =N∑\\ni=1αiK(x,xi). (5.50)\\nThe basis function hi(x) =K(x,xi) (as a function of the ﬁrst argument) is\\nknown as the representer of evaluation atxiinHK, since forf∈HK, it is\\neasily seen that⟨K(·,xi),f⟩HK=f(xi). Similarly⟨K(·,xi),K(·,xj)⟩HK=\\nK(xi,xj) (thereproducing property ofHK), and hence\\nJ(f) =N∑\\ni=1N∑\\nj=1K(xi,xj)αiαj (5.51)\\nforf(x) =∑N\\ni=1αiK(x,xi).\\nIn light of (5.50) and (5.51), (5.48) reduces to a ﬁnite-dime nsional crite-\\nrion\\nmin\\nαL(y,Kα)+λαTKα. (5.52)\\nWe are using a vector notation, in which Kis theN×Nmatrix with ijth\\nentryK(xi,xj) and so on. Simple numerical algorithms can be used to\\noptimize (5.52). This phenomenon, whereby the inﬁnite-dim ensional prob-\\nlem (5.48) or (5.49) reduces to a ﬁnite dimensional optimiza tion problem,\\nhas been dubbed the kernel property in the literature on support-vector\\nmachines (see Chapter 12).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 187}),\n",
       " Document(page_content='170 5. Basis Expansions and Regularization\\nThere is a Bayesian interpretation of this class of models, i n whichf\\nis interpreted as a realization of a zero-mean stationary Ga ussian process,\\nwith prior covariance function K. The eigen-decomposition produces a se-\\nries of orthogonal eigen-functions φj(x) with associated variances γj. The\\ntypical scenario is that “smooth” functions φjhave large prior variance,\\nwhile “rough” φjhave small prior variances. The penalty in (5.48) is the\\ncontribution of the prior to the joint likelihood, and penal izes more those\\ncomponents with smaller prior variance (compare with (5.43 )).\\nFor simplicity we have dealt with the case here where all memb ers ofH\\nare penalized, as in (5.48). More generally, there may be som e components\\ninHthat we wish to leave alone, such as the linear functions for c ubic\\nsmoothing splines in Section 5.4. The multidimensional thi n-plate splines\\nof Section 5.7 and tensor product splines fall into this cate gory as well.\\nIn these cases there is a more convenient representation H=H0⊕H1,\\nwith the null spaceH0consisting of, for example, low degree polynomi-\\nals inxthat do not get penalized. The penalty becomes J(f) =∥P1f∥,\\nwhereP1is the orthogonal projection of fontoH1. The solution has the\\nformf(x) =∑M\\nj=1βjhj(x)+∑N\\ni=1αiK(x,xi), where the ﬁrst term repre-\\nsents an expansion in H0. From a Bayesian perspective, the coeﬃcients of\\ncomponents inH0have improper priors, with inﬁnite variance.\\n5.8.2 Examples of RKHS\\nThe machinery above is driven by the choice of the kernel Kand the loss\\nfunctionL. We consider ﬁrst regression using squared-error loss. In t his\\ncase (5.48) specializes to penalized least squares, and the solution can be\\ncharacterized in two equivalent ways corresponding to (5.4 9) or (5.52):\\nmin\\n{cj}∞\\n1N∑\\ni=1\\uf8eb\\n\\uf8edyi−∞∑\\nj=1cjφj(xi)\\uf8f6\\n\\uf8f82\\n+λ∞∑\\nj=1c2\\nj\\nγj(5.53)\\nan inﬁnite-dimensional, generalized ridge regression pro blem, or\\nmin\\nα(y−Kα)T(y−Kα)+λαTKα. (5.54)\\nThe solution for αis obtained simply as\\nˆα= (K+λI)−1y, (5.55)\\nand\\nˆf(x) =N∑\\nj=1ˆαjK(x,xj). (5.56)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 188}),\n",
       " Document(page_content='5.8 Regularization and Reproducing Kernel Hilbert Spaces 17 1\\nThe vector of Nﬁtted values is given by\\nˆf=Kˆα\\n=K(K+λI)−1y (5.57)\\n= (I+λK−1)−1y. (5.58)\\nThe estimate (5.57) also arises as the krigingestimate of a Gaussian ran-\\ndom ﬁeld in spatial statistics (Cressie, 1993). Compare als o (5.58) with the\\nsmoothing spline ﬁt (5.17) on page 154.\\nPenalized Polynomial Regression\\nThe kernel K(x,y) = (⟨x,y⟩+ 1)d(Vapnik, 1996), for x,y∈IRp, has\\nM=(p+d\\nd)\\neigen-functions that span the space of polynomials in IRpof\\ntotal degree d. For example, with p= 2 andd= 2,M= 6 and\\nK(x,y) = 1+2 x1y1+2x2y2+x2\\n1y2\\n1+x2\\n2y2\\n2+2x1x2y1y2(5.59)\\n=M∑\\nm=1hm(x)hm(y) (5.60)\\nwith\\nh(x)T= (1,√\\n2x1,√\\n2x2,x2\\n1,x2\\n2,√\\n2x1x2). (5.61)\\nOne can represent hin terms of the Morthogonal eigen-functions and\\neigenvalues of K,\\nh(x) =VD1\\n2γφ(x), (5.62)\\nwhereDγ= diag(γ1,γ2,...,γ M), andVisM×Mand orthogonal.\\nSuppose we wish to solve the penalized polynomial regressio n problem\\nmin\\n{βm}M\\n1N∑\\ni=1(\\nyi−M∑\\nm=1βmhm(xi))2\\n+λM∑\\nm=1β2\\nm. (5.63)\\nSubstituting (5.62) into (5.63), we get an expression of the form (5.53) to\\noptimize (Exercise 5.16).\\nThe number of basis functions M=(p+d\\nd)\\ncan be very large, often much\\nlarger than N. Equation (5.55) tells us that if we use the kernel represen-\\ntation for the solution function, we have only to evaluate th e kernelN2\\ntimes, and can compute the solution in O(N3) operations.\\nThis simplicity is not without implications. Each of the pol ynomialshm\\nin (5.61) inherits a scaling factor from the particular form ofK, which has\\na bearing on the impact of the penalty in (5.63). We elaborate on this in\\nthe next section.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 189}),\n",
       " Document(page_content='172 5. Basis Expansions and Regularization\\n−2 −1 0 1 2 3 40.0 0.4 0.8\\nXRadial Kernel in IR1K(·,xm)\\nFIGURE 5.13. Radial kernels kk(x)for the mixture data, with scale parameter\\nν= 1. The kernels are centered at ﬁve points xmchosen at random from the 200.\\nGaussian Radial Basis Functions\\nIn the preceding example, the kernel is chosen because it rep resents an\\nexpansion of polynomials and can conveniently compute high -dimensional\\ninnerproducts.Inthisexamplethekernelischosenbecause ofitsfunctional\\nform in the representation (5.50).\\nThe Gaussian kernel K(x,y) =e−ν||x−y||2along with squared-error loss,\\nfor example, leads to a regression model that is an expansion in Gaussian\\nradial basis functions,\\nkm(x) =e−ν||x−xm||2, m= 1,...,N, (5.64)\\neach one centered at one of the training feature vectors xm. The coeﬃcients\\nare estimated using (5.54).\\nFigure 5.13 illustrates radial kernels in IR1using the ﬁrst coordinate of\\nthe mixture example from Chapter 2. We show ﬁve of the 200 kern el basis\\nfunctionskm(x) =K(x,xm).\\nFigure 5.14 illustrates the implicit feature space for the r adial kernel\\nwithx∈IR1. We computed the 200 ×200 kernel matrix K, and its eigen-\\ndecomposition ΦDγΦT. We can think of the columns of Φand the corre-\\nsponding eigenvalues in Dγas empirical estimates of the eigen expansion\\n(5.45)2. Although the eigenvectors are discrete, we can represent t hem as\\nfunctions on IR1(Exercise 5.17). Figure 5.15 shows the largest 50 eigenval-\\nues ofK. The leading eigenfunctions are smooth, and they are succes sively\\nmore wiggly as the order increases. This brings to life the pe nalty in (5.49),\\nwhere we see the coeﬃcients of higher-order functions get pe nalized more\\nthanlower-order ones.Theright panelinFigure5.14shows t hecorrespond-\\n2Theℓth column of Φis an estimate of φℓ, evaluated at each of the Nobservations.\\nAlternatively, the ith row of Φis the estimated vector of basis functions φ(xi), evaluated\\nat the point xi. Although in principle, there can be inﬁnitely many elements in φ, our\\nestimate has at most Nelements.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 190}),\n",
       " Document(page_content='5.8 Regularization and Reproducing Kernel Hilbert Spaces 17 3\\n*\\n******* ** ** ******\\n******\\n*******\\n******\\n**********\\n****\\n** *******\\n*****\\n**\\n***\\n**********\\n*****\\n*****\\n***\\n**\\n***\\n*****\\n*************\\n******\\n***\\n**\\n***\\n**\\n***********\\n**\\n****\\n**** *\\n** ******* * **\\n**************\\n********* ***\\n**\\n*******\\n***\\n**\\n****\\n***\\n***\\n**\\n***\\n*****\\n*******\\n**\\n**\\n**\\n**\\n***\\n**\\n*****\\n**\\n******\\n*\\n***\\n**\\n**\\n*********\\n**\\n**\\n****\\n**\\n**\\n**\\n****\\n****\\n***\\n**\\n**\\n*******\\n**\\n***\\n**\\n*\\n****\\n**\\n****\\n**\\n**\\n****\\n****\\n**\\n**\\n*****\\n****\\n**\\n****\\n*\\n**\\n***\\n**\\n**\\n****\\n****\\n**\\n**\\n*\\n***\\n**\\n*********\\n******\\n*\\n***\\n** **\\n****\\n*\\n***\\n***\\n***\\n***\\n*******\\n**\\n**\\n**\\n*\\n*******\\n**\\n***\\n***\\n******\\n**\\n***\\n**\\n******\\n**\\n**\\n*\\n**\\n**\\n**\\n*\\n***\\n**\\n**\\n****\\n*** **\\n*********\\n****\\n**\\n****\\n** **\\n** ****\\n****\\n***\\n***\\n**\\n** ***\\n**\\n**\\n*\\n***\\n***\\n* * * **\\n****\\n**\\n**\\n****\\n*****\\n****\\n* **** ***\\n**\\n*** **** * *\\n**\\n* *\\n**\\n**\\n******\\n***\\n***\\n****\\n***\\n***\\n*\\n**\\n**\\n**\\n**\\n***\\n***\\n***\\n****\\n***\\n********\\n*******\\n** ** **\\n**\\n***\\n***\\n**\\n*\\n*********\\n**********\\n*\\n****\\n***\\n***\\n**\\n**\\n***\\n*****\\n****\\n**\\n**\\n****\\n***\\n***\\n*\\n**\\n**\\n***\\n**\\n**\\n******\\n*******\\n***\\n***\\n****\\n****\\n***\\n****\\n* ***\\n* * ** *\\n* ***\\n**\\n*\\n**\\n***\\n***\\n**\\n***\\n**\\n*******\\n**\\n**\\n*******\\n**\\n*\\n* *********\\n***\\n******\\n******\\n** ***\\n*\\n**\\n*\\n**\\n**\\n***\\n* **\\n***\\n*\\n***\\n**\\n***********\\n**\\n**\\n**\\n***\\n**\\n***\\n**\\n**\\n*********\\n***\\n***\\n**\\n**\\n*\\n**\\n*\\n**\\n***\\n* * **\\n*****\\n**\\n**\\n*****\\n**\\n**\\n** ******\\n**\\n**\\n***** ****\\n***\\n****\\n* *****\\n****\\n****\\n***\\n**\\n**\\n**\\n* *******\\n****\\n****\\n***\\n**\\n****\\n*****\\n******\\n*****\\n*\\n*****\\n***\\n*\\n***\\n******\\n*****\\n***\\n***********\\n***\\n***** **\\n* *\\n***\\n*****\\n*******\\n*****\\n**\\n****\\n**\\n*\\n**\\n*\\n*****\\n***\\n*******\\n******* *\\n*****\\n*\\n**\\n***\\n*\\n*****\\n**\\n*****\\n**\\n*** *\\n***\\n***\\n******\\n**\\n***\\n********\\n***\\n**\\n**\\n***\\n***** ****\\n***\\n**\\n****\\n********\\n*******\\n***\\n**\\n***\\n**\\n***\\n**\\n*********\\n*\\n*******\\n*****\\n*\\n**\\n**\\n***\\n***\\n*\\n**\\n*\\n**\\n**\\n******\\n**\\n*\\n*****\\n*\\n***\\n** **\\n*\\n***\\n** ***\\n****\\n**\\n*******\\n****\\n**\\n****\\n*****\\n******\\n**\\n****\\n**\\n**\\n** ***\\n*****\\n*\\n**\\n**\\n***\\n**\\n****\\n**\\n*****\\n***\\n**\\n****\\n***\\n**\\n**\\n*\\n**\\n***\\n*\\n*****\\n***\\n******* ***\\n**\\n****\\n* ***\\n******\\n*****\\n*****\\n***\\n**\\n* ** ***\\n***\\n***\\n*\\n***\\n***\\n****\\n****\\n***\\n*****\\n*****\\n***\\n****\\n**\\n**\\n**\\n*\\n*****\\n**\\n******\\n********* *\\n**\\n*\\n****\\n**\\n*\\n* *\\n***\\n**\\n****\\n***\\n****\\n*** ****\\n****\\n**\\n*****\\n***\\n**\\n**\\n***\\n*****\\n*\\n***\\n*** *\\n******\\n*\\n***********\\n**\\n******\\n**\\n*******\\n**\\n****\\n*\\n**********\\n*\\n**** *\\n**\\n******\\n*\\n* **\\n***\\n*\\n**\\n*****\\n*\\n**\\n*\\n****\\n**\\n*\\n***\\n** *\\n****\\n******\\n***\\n**\\n*\\n**\\n**** ****\\n**\\n***\\n**\\n***\\n***\\n*****\\n**** * *\\n**\\n**\\n******\\n*****\\n*****\\n***\\n** *\\n****\\n*\\n****\\n** *\\n*****\\n****\\n**** *\\n*****\\n**\\n****\\n*****\\n** **\\n***\\n****\\n****\\n**\\n** *** **\\n**\\n*****\\n***\\n**\\n****\\n*\\n****\\n****\\n**\\n*****\\n***\\n********\\n*\\n****\\n***\\n**\\n***\\n**\\n*****\\n**\\n***\\n*****\\n********\\n****\\n* *******\\n******* * *\\n***\\n****\\n**** *\\n**\\n**\\n***\\n****\\n**\\n**\\n**\\n******\\n**\\n**\\n**\\n****\\n*****\\n***\\n****\\n*****\\n*\\n***********\\n**\\n**\\n**\\n**\\n**\\n*******\\n******\\n***********\\n***\\n***\\n*****\\n****\\n***\\n* ***\\n*****\\n**\\n*********\\n*\\n**\\n***\\n** ****\\n** * *\\n**\\n***\\n***\\n**\\n*****\\n***\\n*****\\n****\\n**\\n**\\n***\\n*****\\n* * **\\n**\\n**\\n* **\\n***\\n**\\n*****\\n***\\n***\\n***\\n*******\\n**\\n*** *\\n****\\n*\\n* ***\\n****\\n***\\n***\\n***\\n***\\n*****\\n* ***\\n****\\n****\\n***\\n****\\n*****\\n*******\\n***\\n**\\n***\\n****\\n*\\n***\\n***\\n*\\n**\\n*\\n*****\\n**\\n***\\n**\\n**\\n***\\n**\\n*** *\\n******\\n**\\n* **\\n****\\n**\\n***\\n**\\n***\\n*******\\n**\\n***\\n**\\n****\\n***\\n****\\n** *\\n****\\n* ***\\n***\\n**\\n**\\n******\\n***\\n**\\n*****\\n**\\n**\\n*\\n***\\n***\\n**\\n** *\\n******\\n*****\\n***\\n*\\n***\\n****\\n**\\n* **\\n*******\\n****\\n*******\\n**\\n***\\n***\\n*\\n*****\\n*\\n********\\n***\\n*\\n***\\n***\\n**\\n*******\\n**\\n*\\n*****\\n***\\n**\\n***\\n***\\n****\\n* ***\\n******\\n***\\n***\\n*****\\n*** **\\n**\\n**\\n***\\n***\\n********\\n**\\n****\\n**** *****\\n****\\n****\\n**\\n***\\n**\\n****\\n** *****\\n**\\n**\\n*****\\n**\\n**\\n**\\n***\\n****\\n*****\\n*** *\\n*******\\n****\\n********\\n**\\n** **\\n*****\\n**\\n***\\n*\\n******\\n**\\n***\\n***\\n*\\n****\\n**\\n***\\n**\\n******\\n***\\n***\\n*\\n**\\n*******\\n**\\n****\\n****\\n*\\n*******\\n**** ******\\n*\\n****\\n****\\n***\\n*\\n**\\n****\\n** **\\n** *\\n*** *\\n**\\n****\\n***************\\n***\\n***\\n*****\\n*****\\n* *****\\n***\\n****\\n*****\\n**\\n***\\n**\\n****\\n**\\n***\\n*\\n*****\\n***\\n**\\n****\\n***\\n**\\n**\\n*******\\n*****\\n***\\n*\\n***\\n*******\\n*\\n**\\n**** ***\\n****\\n*\\n****\\n****\\n*****\\n*****\\n***\\n***\\n*****\\n**\\n***\\n**\\n*\\n**', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 191}),\n",
       " Document(page_content='***\\n***\\n*****\\n*****\\n* *****\\n***\\n****\\n*****\\n**\\n***\\n**\\n****\\n**\\n***\\n*\\n*****\\n***\\n**\\n****\\n***\\n**\\n**\\n*******\\n*****\\n***\\n*\\n***\\n*******\\n*\\n**\\n**** ***\\n****\\n*\\n****\\n****\\n*****\\n*****\\n***\\n***\\n*****\\n**\\n***\\n**\\n*\\n**\\n**\\n*****\\n**\\n**\\n*\\n**\\n**\\n**\\n****\\n**** *\\n*\\n**\\n******\\n****\\n**\\n****\\n***\\n*\\n*\\n****\\n****\\n****\\n***\\n***\\n******\\n***\\n****\\n*\\n***\\n**\\n*****\\n*\\n* ****\\n****\\n**\\n*\\n**\\n*\\n***\\n****\\n**\\n**\\n***\\n****\\n***\\n**\\n*\\n**\\n**\\n********* *\\n**\\n******\\n**\\n***\\n**\\n**\\n****\\n***\\n***\\n**\\n**\\n*******\\n***\\n**\\n**\\n***\\n**\\n*\\n**\\n*\\n**\\n**Orthonormal Basis Φ\\n*\\n*****\\n** *\\n* ***\\n*\\n****\\n***\\n*\\n**\\n*\\n***\\n***\\n******\\n**\\n**\\n***\\n***\\n****\\n**\\n*\\n**\\n****\\n*****\\n**\\n***\\n**\\n******\\n**\\n**\\n*\\n**\\n**\\n***\\n***\\n**\\n***\\n*****\\n*\\n***\\n*\\n***\\n*****\\n***\\n***\\n***\\n**\\n***\\n**\\n*\\n**\\n********\\n**\\n****\\n*\\n***\\n*\\n** *\\n*\\n***\\n** ***\\n**\\n**\\n**\\n**\\n***\\n***\\n***\\n***\\n***\\n***\\n**\\n***\\n****\\n***\\n**\\n****\\n***\\n***\\n**\\n***\\n****\\n*\\n**\\n**\\n***\\n**\\n**\\n**\\n**\\n**\\n*\\n**\\n****\\n*\\n**\\n******\\n*\\n***\\n**\\n**\\n*\\n**\\n**\\n*\\n*\\n**\\n**\\n**\\n****\\n*\\n*\\n**\\n**\\n**\\n**\\n****\\n***\\n**\\n**\\n***\\n***\\n*\\n**\\n***\\n**\\n*\\n****\\n**\\n*\\n***\\n**\\n**\\n****\\n****\\n**\\n**\\n****\\n*\\n****\\n**\\n****\\n*\\n**\\n***\\n**\\n**\\n***\\n*\\n****\\n**\\n**\\n*\\n***\\n**\\n***\\n**\\n*\\n**\\n*\\n******\\n*\\n**\\n*\\n***\\n*\\n***\\n*\\n*\\n***\\n***\\n***\\n*\\n**\\n****\\n**\\n*\\n**\\n**\\n**\\n*\\n***\\n***\\n*\\n**\\n***\\n**\\n*\\n****\\n**\\n**\\n***\\n**\\n*****\\n*\\n**\\n**\\n*\\n**\\n**\\n**\\n*\\n***\\n**\\n**\\n***\\n*\\n*****\\n*****\\n****\\n****\\n**\\n*\\n***\\n** **\\n***\\n***\\n***\\n*\\n**\\n*\\n***\\n**\\n** ***\\n**\\n**\\n*\\n***\\n***\\n** ***\\n****\\n**\\n**\\n***\\n*\\n**\\n***\\n*\\n***\\n*******\\n*\\n**\\n*\\n***\\n*** **\\n**\\n* *\\n**\\n**\\n***\\n***\\n***\\n***\\n****\\n***\\n***\\n*\\n**\\n**\\n**\\n*\\n*\\n***\\n**\\n*\\n*\\n**\\n**\\n**\\n***\\n***\\n*****\\n*\\n******\\n** ****\\n**\\n***\\n**\\n*\\n**\\n*\\n****\\n*\\n****\\n**\\n********\\n*\\n****\\n**\\n*\\n***\\n**\\n**\\n***\\n*****\\n****\\n**\\n**\\n****\\n***\\n*\\n**\\n*\\n**\\n**\\n***\\n**\\n**\\n******\\n*****\\n**\\n***\\n**\\n*\\n****\\n****\\n***\\n*\\n**\\n*\\n* ***\\n* *** *\\n* ***\\n**\\n*\\n**\\n***\\n***\\n**\\n*\\n**\\n**\\n*******\\n**\\n**\\n**\\n*****\\n**\\n*\\n* ****\\n***\\n**\\n***\\n*\\n*****\\n******\\n** ***\\n*\\n**\\n*\\n**\\n**\\n*\\n**\\n* **\\n***\\n*\\n***\\n**\\n*****\\n***\\n***\\n**\\n**\\n**\\n***\\n**\\n***\\n**\\n**\\n**\\n*******\\n*\\n**\\n***\\n**\\n**\\n*\\n**\\n*\\n**\\n***\\n* ***\\n*****\\n**\\n**\\n*****\\n**\\n**\\n** ***\\n*\\n**\\n**\\n**\\n***** ****\\n***\\n****\\n******\\n****\\n****\\n***\\n**\\n**\\n**\\n* *******\\n**\\n**\\n****\\n*\\n**\\n**\\n****\\n*****\\n******\\n*****\\n*\\n*****\\n***\\n*\\n***\\n******\\n*****\\n***\\n****\\n*****\\n**\\n***\\n***** **\\n* *\\n***\\n*****\\n*******\\n****\\n*\\n**\\n****\\n**\\n*\\n**\\n*\\n***\\n**\\n***\\n***\\n****\\n******* *\\n*****\\n*\\n**\\n***\\n*\\n*****\\n**\\n*****\\n**\\n*** *\\n***\\n***\\n******\\n**\\n***\\n********\\n***\\n**\\n**\\n***\\n***** ****\\n***\\n**\\n****\\n********\\n*******\\n***\\n**\\n***\\n**\\n***\\n**\\n*********\\n*\\n*******\\n*****\\n*\\n**\\n**\\n***\\n***\\n*\\n**\\n*\\n**\\n**\\n******\\n**\\n*\\n*****\\n*\\n***\\n** **\\n*\\n***\\n** ***\\n****\\n**\\n*******\\n****\\n**\\n****\\n*****\\n******\\n**\\n****\\n****\\n** ********\\n***\\n*****\\n**\\n****\\n**\\n**********\\n****\\n***\\n**\\n***\\n**\\n****\\n*****\\n***\\n******* ***\\n**\\n****\\n* ***\\n******\\n*****\\n*****\\n****** ** ***\\n***\\n***\\n*\\n***\\n***\\n****\\n*******\\n*****\\n*****\\n***\\n* *****\\n**\\n**\\n*\\n*****\\n**\\n******\\n********* ****\\n****\\n***\\n* ****\\n**\\n****\\n***\\n******* ** ********\\n********\\n**\\n**\\n********\\n****\\n*** *\\n***** ***************\\n******\\n**\\n*** ****\\n******************** ** ****** ***\\n** **\\n***\\n*\\n**********\\n*****\\n******** *\\n****\\n** * ******\\n********* * ***\\n**\\n********\\n******* ***** * *\\n****** ***** *** *********** ************ ** ***** ******* **\\n**** ************** *** ********* **** * *** **************** ********** ***************** *******\\n*****\\n***\\n******** *************** ******* ** * ****\\n******* * ******* ** ** * ***** *** ** ************* ******** ** ***** ** ***** **\\n**** * ************************* ******************************** ** * ** *** *********** * ************** * ****** * *************** ***************\\n****** * **** * ** ** *** ***** ***** ************* ******** ** ***** ** ************ ************* * ********** * **** ********* ********************* ** * *** *********** ** ********** ** * * ****** * **************** ****** *** ***********', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 191}),\n",
       " Document(page_content='* **** * ** ** ** * **** * *** ** ************* ******** ********* ***** ****** * ************* * ** ******** * **** ********* *********** ** ****** *** ** *** *********** * * ***** * **** ** * * ****** * * ******* ******* * ****** ** * *********** * **** * ** ** ** * **** * *** ** *** ********** ******** ** ***** ** ***** ** **** * **** ********* * ** ******** * **** ********* *********** ** ****** ** * * * *** *********** * * ***** * **** ** ** ****** * * ************** * ****** ** * **** * ****** * ** ** * ** ** ** * **** * *** ** ************* ******** ** **** *** ***** ** **** * ************* * ** ******** * **** ********* *********** ** ****** ** * * * *** *********** * * *** ** * **** ** * * ****** * * ************** * ****** ** * **** * ****** * **** * ** ** ** * **** * *** ** **** ***** **** ******** ** ***** ** ***** ** **** * ************* * ** ******** * **** ********* *** * ******* ** ****** ** * * * *** *********** ** ***** * **** ** * * * * **** * * ************** * ****** ** * **** * ******Feature Space H\\nFIGURE 5.14. (Left panel) The ﬁrst 16normalized eigenvectors of K, the\\n200×200kernel matrix for the ﬁrst coordinate of the mixture data. Th ese are\\nviewed as estimates ˆφℓof the eigenfunctions in (5.45), and are represented as\\nfunctions in IR1with the observed values superimposed in color. They are arran ged\\nin rows, starting at the top left. (Right panel) Rescaled versio nshℓ=√ˆγℓˆφℓof\\nthe functions in the left panel, for which the kernel computes t he “inner product.”\\n0 10 20 30 40 501e−15 1e−11 1e−07 1e−03 1e+01Eigenvalue\\nFIGURE 5.15. The largest 50eigenvalues of K; all those beyond the 30th are\\neﬀectively zero.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 191}),\n",
       " Document(page_content='174 5. Basis Expansions and Regularization\\ningfeature space representation of the eigenfunctions\\nhℓ(x) =√\\nˆγℓˆφℓ(x), ℓ= 1,...,N. (5.65)\\nNotethat⟨h(xi),h(xi′)⟩=K(xi,xi′).Thescalingbytheeigenvaluesquickly\\nshrinks most of the functions down to zero, leaving an eﬀecti ve dimension\\nof about 12 in this case. The corresponding optimization pro blem is a stan-\\ndard ridge regression, as in (5.63). So although in principl e the implicit\\nfeature space is inﬁnite dimensional, the eﬀective dimensi on is dramat-\\nically lower because of the relative amounts of shrinkage ap plied to each\\nbasis function. The kernel scale parameter νplays a role here as well; larger\\nνimplies more local kmfunctions, and increases the eﬀective dimension of\\nthe feature space. See Hastie and Zhu (2006) for more details .\\nIt is also known (Girosi et al., 1995) that a thin-plate splin e (Section 5.7)\\nis an expansion in radial basis functions, generated by the k ernel\\nK(x,y) =∥x−y∥2log(∥x−y∥). (5.66)\\nRadial basis functions are discussed in more detail in Secti on 6.7.\\nSupport Vector Classiﬁers\\nThe support vector machines of Chapter 12 for a two-class cla ssiﬁcation\\nproblem have the form f(x) =α0+∑N\\ni=1αiK(x,xi), where the parameters\\nare chosen to minimize\\nmin\\nα0,α{N∑\\ni=1[1−yif(xi)]++λ\\n2αTKα}\\n, (5.67)\\nwhereyi∈{−1,1}, and [z]+denotes the positive part of z. This can be\\nviewed as a quadratic optimization problem with linear cons traints, and\\nrequires a quadratic programming algorithm for its solutio n. The name\\nsupport vector arises from the fact that typically many of the ˆ αi= 0 [due\\nto the piecewise-zero nature of the loss function in (5.67)] , and so ˆfis an\\nexpansion in a subset of the K(·,xi). See Section 12.3.3 for more details.\\n5.9 Wavelet Smoothing\\nWe have seen two diﬀerent modes of operation with dictionari es of basis\\nfunctions. With regression splines, we select a subset of th e bases, using\\neither subject-matter knowledge, or else automatically. T he more adaptive\\nprocedures such as MARS (Chapter 9) can capture both smooth a nd non-\\nsmooth behavior. With smoothing splines, we use a complete b asis, but\\nthen shrink the coeﬃcients toward smoothness.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 192}),\n",
       " Document(page_content='5.9 Wavelet Smoothing 175\\nTime0.0 0.2 0.4 0.6 0.8 1.0Haar Wavelets\\nTime0.0 0.2 0.4 0.6 0.8 1.0Symmlet-8 Wavelets\\nψ1,0ψ2,1ψ2,3ψ3,2ψ3,5ψ4,4ψ4,9ψ5,1ψ5,15ψ6,15ψ6,35\\nFIGURE 5.16. Some selected wavelets at diﬀerent translations and dilations\\nfor the Haar and symmlet families. The functions have been scale d to suit the\\ndisplay.\\nWavelets typically use a complete orthonormal basis to repr esent func-\\ntions, but then shrink and select the coeﬃcients toward a sparserepresen-\\ntation. Just as a smooth function can be represented by a few s pline basis\\nfunctions, a mostly ﬂat function with a few isolated bumps ca n be repre-\\nsented with a few (bumpy) basis functions. Wavelets bases ar e very popular\\nin signal processing and compression, since they are able to represent both\\nsmooth and/or locally bumpy functions in an eﬃcient way—a phe nomenon\\ndubbedtime and frequency localization . In contrast, the traditional Fourier\\nbasis allows only frequency localization.\\nBefore we give details, let’s look at the Haar wavelets in the left panel\\nof Figure 5.16 to get an intuitive idea of how wavelet smoothi ng works.\\nThe vertical axis indicates the scale (frequency) of the wav elets, from low\\nscale at the bottom to high scale at the top. At each scale the w avelets are\\n“packedin”side-by-sidetocompletelyﬁllthetimeaxis:we haveonlyshown', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 193}),\n",
       " Document(page_content='176 5. Basis Expansions and Regularization\\na selected subset. Wavelet smoothing ﬁts the coeﬃcients for this basis by\\nleast squares, and then thresholds (discards, ﬁlters) the s maller coeﬃcients.\\nSince there are many basis functions at each scale, it can use bases where\\nit needs them and discard the ones it does not need, to achieve time and\\nfrequencylocalization.TheHaarwaveletsaresimpletound erstand,butnot\\nsmooth enough for most purposes. The symmlet wavelets in the right panel\\nof Figure 5.16 have the same orthonormal properties, but are smoother.\\nFigure 5.17 displays an NMR (nuclear magnetic resonance) si gnal, which\\nappears to be composed of smooth components and isolated spi kes, plus\\nsome noise. The wavelet transform, using a symmlet basis, is shown in the\\nlower left panel. The wavelet coeﬃcients are arranged in row s, from lowest\\nscale at the bottom, to highest scale at the top. The length of each line\\nsegment indicates the size of the coeﬃcient. The bottom righ t panel shows\\nthe wavelet coeﬃcients after they have been thresholded. Th e threshold\\nprocedure, given below in equation (5.69), is the same soft- thresholding\\nrule that arises in the lasso procedure for linear regressio n (Section 3.4.2).\\nNotice that many of the smaller coeﬃcients have been set to ze ro. The\\ngreen curve in the top panel shows the back-transform of the t hresholded\\ncoeﬃcients: this is the smoothed version of the original sig nal. In the next\\nsection we give the details of this process, including the co nstruction of\\nwavelets and the thresholding rule.\\n5.9.1 Wavelet Bases and the Wavelet Transform\\nIn this section we give details on the construction and ﬁlter ing of wavelets.\\nWavelet bases are generated by translations and dilations o f a single scal-\\ning function φ(x) (also known as the father). The red curves in Figure 5.18\\nare theHaarandsymmlet-8 scaling functions. The Haar basis is particu-\\nlarly easy to understand, especially for anyone with experi ence in analysis\\nof variance or trees, since it produces a piecewise-constan t representation.\\nThus ifφ(x) =I(x∈[0,1]), thenφ0,k(x) =φ(x−k),kan integer, generates\\nan orthonormal basis for functions with jumps at the integer s. Call this ref-\\nerencespaceV0. The dilations φ1,k(x) =√\\n2φ(2x−k) form an orthonormal\\nbasis for a space V1⊃V0of functions piecewise constant on intervals of\\nlength1\\n2. In fact, more generally we have ···⊃V1⊃V0⊃V−1⊃···where\\neachVjis spanned by φj,k= 2j/2φ(2jx−k).\\nNow to the deﬁnition of wavelets. In analysis of variance, we often rep-\\nresent a pair of means µ1andµ2by their grand mean µ=1\\n2(µ1+µ2), and\\nthen a contrast α=1\\n2(µ1−µ2). A simpliﬁcation occurs if the contrast αis\\nvery small, because then we can set it to zero. In a similar man ner we might\\nrepresent a function in Vj+1by a component in Vjplus the component in\\nthe orthogonal complement WjofVjtoVj+1, written as Vj+1=Vj⊕Wj.\\nThe component in Wjrepresents detail, and we might wish to set some ele-\\nments of this component to zero. It is easy to see that the func tionsψ(x−k)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 194}),\n",
       " Document(page_content='5.9 Wavelet Smoothing 177\\nNMR Signal\\n0 200 400 600 800 10000 20 40 60\\n0 200 400 600 800 1000Wavelet Transform - Original Signal\\n0 200 400 600 800 1000Wavelet Transform - WaveShrunk Signal\\nSignal Signal\\nW9 W9\\nW8 W8\\nW7 W7\\nW6 W6\\nW5 W5\\nW4 W4\\nV4 V4\\nFIGURE 5.17. The top panel shows an NMR signal, with the wavelet-shrunk\\nversion superimposed in green. The lower left panel represent s the wavelet trans-\\nform of the original signal, down to V4, using the symmlet-8 basis. Each coeﬃ-\\ncient is represented by the height (positive or negative) of the vertical bar. The\\nlower right panel represents the wavelet coeﬃcients after bei ng shrunken using\\nthewaveshrink function in S-PLUS, which implements the SureShrink method\\nof wavelet adaptation of Donoho and Johnstone.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 195}),\n",
       " Document(page_content='178 5. Basis Expansions and Regularization\\nHaar Basis Symmlet Basis\\nφ(x) φ(x)\\nψ(x) ψ(x)\\nFIGURE 5.18. TheHaarandsymmlet father (scaling) wavelet φ(x)and mother\\nwaveletψ(x).\\ngeneratedbythe mother wavelet ψ(x) =φ(2x)−φ(2x−1)formanorthonor-\\nmal basis for W0for the Haar family. Likewise ψj,k= 2j/2ψ(2jx−k) form\\na basis forWj.\\nNowVj+1=Vj⊕Wj=Vj−1⊕Wj−1⊕Wj, so besides representing a\\nfunction by its level- jdetail and level- jrough components, the latter can\\nbe broken down to level-( j−1) detail and rough, and so on. Finally we get\\na representation of the form VJ=V0⊕W0⊕W1···⊕WJ−1. Figure 5.16\\non page 175 shows particular wavelets ψj,k(x).\\nNotice that since these spaces are orthogonal, all the basis functions are\\northonormal. In fact, if the domain is discrete with N= 2J(time) points,\\nthis is as far as we can go. There are 2jbasis elements at level j, and\\nadding up, we have a total of 2J−1 elements in the Wj, and one in V0.\\nThis structured orthonormal basis allows for a multiresolution analysis ,\\nwhich we illustrate in the next section.\\nWhile helpful for understanding the construction above, th e Haar basis\\nis often too coarse for practical purposes. Fortunately, ma ny clever wavelet\\nbases have been invented. Figures 5.16 and 5.18 include the Daubechies\\nsymmlet-8 basis. This basis has smoother elements than the correspond ing\\nHaar basis, but there is a tradeoﬀ:\\n•Each wavelet has a support covering 15 consecutive time inte rvals,\\nrather than one for the Haar basis. More generally, the symml et-p\\nfamily has a support of 2 p−1 consecutive intervals. The wider the\\nsupport, the more time the wavelet has to die to zero, and so it can', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 196}),\n",
       " Document(page_content='5.9 Wavelet Smoothing 179\\nachieve this more smoothly. Note that the eﬀective support s eems to\\nbe much narrower.\\n•The symmlet- pwaveletψ(x) haspvanishing moments; that is,\\n∫\\nψ(x)xjdx= 0, j= 0,...,p−1.\\nOneimplicationisthatanyorder- ppolynomialoverthe N= 2Jtimes\\npoints is reproduced exactly in V0(Exercise 5.18). In this sense V0\\nis equivalent to the null space of the smoothing-spline pena lty. The\\nHaar wavelets have one vanishing moment, and V0can reproduce any\\nconstant function.\\nThe symmlet- pscaling functions are one of many families of wavelet\\ngenerators. The operations are similar to those for the Haar basis:\\n•IfV0is spanned by φ(x−k), thenV1⊃V0is spanned by φ1,k(x) =√\\n2φ(2x−k)andφ(x) =∑\\nk∈Zh(k)φ1,k(x), for some ﬁlter coeﬃcients\\nh(k).\\n•W0is spanned by ψ(x) =∑\\nk∈Zg(k)φ1,k(x), with ﬁlter coeﬃcients\\ng(k) = (−1)1−kh(1−k).\\n5.9.2 Adaptive Wavelet Filtering\\nWavelets are particularly useful when the data are measured on a uniform\\nlattice, such as a discretized signal, image, or a time serie s. We will focus on\\nthe one-dimensional case, and having N= 2Jlattice-points is convenient.\\nSuppose yis the response vector, and Wis theN×Northonormal wavelet\\nbasis matrix evaluated at the Nuniformly spaced observations. Then y∗=\\nWTyis called the wavelet transform ofy(and is the full least squares\\nregression coeﬃcient). A popular method for adaptive wavel et ﬁtting is\\nknown as SURE shrinkage (Stein Unbiased Risk Estimation, Donoho and\\nJohnstone (1994)). We start with the criterion\\nmin\\nθ||y−Wθ||2\\n2+2λ||θ||1, (5.68)\\nwhich is the same as the lasso criterion in Chapter 3. Because Wis or-\\nthonormal, this leads to the simple solution:\\nˆθj= sign(y∗\\nj)(|y∗\\nj|−λ)+. (5.69)\\nThe least squares coeﬃcients are translated toward zero, an d truncated\\nat zero. The ﬁtted function (vector) is then given by the inverse wavelet\\ntransform ˆf=Wˆθ.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 197}),\n",
       " Document(page_content='180 5. Basis Expansions and Regularization\\nA simple choice for λisλ=σ√2logN, whereσis an estimate of the\\nstandarddeviationofthenoise.Wecangivesomemotivation forthischoice.\\nSinceWis an orthonormal transformation, if the elements of yare white\\nnoise (independent Gaussian variates with mean 0 and varian ceσ2), then\\nso arey∗. Furthermore if random variables Z1,Z2,...,Z Nare white noise,\\nthe expected maximum of |Zj|,j= 1,...,Nis approximately σ√2logN.\\nHence all coeﬃcients below σ√2logNare likely to be noise and are set to\\nzero.\\nThe space Wcould be any basis of orthonormal functions: polynomials,\\nnaturalsplinesorcosinusoids.Whatmakeswaveletsspecia listheparticular\\nform of basis functions used, which allows for a representat ionlocalized in\\ntime and in frequency .\\nLet’s look again at the NMR signal of Figure 5.17. The wavelet transform\\nwas computed using a symmlet−8 basis. Notice that the coeﬃcients do not\\ndescend all the way to V0, but stop at V4which has 16 basis functions.\\nAs we ascend to each level of detail, the coeﬃcients get small er, except in\\nlocationswherespikybehaviorispresent.Thewaveletcoeﬃ cientsrepresent\\ncharacteristics of the signal localized in time (the basis f unctions at each\\nlevelaretranslationsofeachother)andlocalizedinfrequ ency.Eachdilation\\nincreases the detail by a factor of two, and in this sense corr esponds to\\ndoubling the frequency in a traditional Fourier representa tion. In fact, a\\nmore mathematical understanding of wavelets reveals that t he wavelets at\\na particular scale have a Fourier transform that is restrict ed to a limited\\nrange or octave of frequencies.\\nTheshrinking/truncationintherightpanelwasachievedus ingtheSURE\\napproach described in the introduction to this section. The orthonormal\\nN×Nbasis matrix Whas columns which are the wavelet basis functions\\nevaluated at the Ntime points. In particular, in this case there will be 16\\ncolumns corresponding to the φ4,k(x), and the remainder devoted to the\\nψj,k(x), j= 4,...,11. In practice λdepends on the noise variance, and has\\nto be estimated from the data (such as the variance of the coeﬃ cients at\\nthe highest level).\\nNotice the similarity between the SURE criterion (5.68) on p age 179,\\nand the smoothing spline criterion (5.21) on page 156:\\n•Both are hierarchically structured from coarse to ﬁne detai l, although\\nwavelets are also localized in time within each resolution l evel.\\n•The splines build in a bias toward smooth functions by imposi ng\\ndiﬀerential shrinking constants dk. Early versions of SURE shrinkage\\ntreated all scales equally. The S+wavelets function waveshrink() has\\nmany options, some of which allow for diﬀerential shrinkage .\\n•The spline L2penalty cause pure shrinkage, while the SURE L1\\npenalty does shrinkage and selection.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 198}),\n",
       " Document(page_content='Exercises 181\\nMore generally smoothing splines achieve compression of th e original signal\\nby imposing smoothness, while wavelets impose sparsity. Fi gure 5.19 com-\\npares a wavelet ﬁt (using SURE shrinkage) to a smoothing spli ne ﬁt (using\\ncross-validation) on two examples diﬀerent in nature. For t he NMR data in\\nthe upper panel, the smoothing spline introduces detail eve rywhere in order\\nto capture the detail in the isolated spikes; the wavelet ﬁt n icely localizes\\nthe spikes. In the lower panel, the true function is smooth, a nd the noise is\\nrelatively high. The wavelet ﬁt has let in some additional an d unnecessary\\nwiggles—a price it pays in variance for the additional adapti vity.\\nThe wavelet transform is not performed by matrix multiplica tion as in\\ny∗=WTy. In fact, using clever pyramidal schemes y∗can be obtained\\ninO(N) computations, which is even faster than the Nlog(N) of the fast\\nFourier transform (FFT). While the general construction is beyond the\\nscope of this book, it is easy to see for the Haar basis (Exerci se 5.19).\\nLikewise, the inverse wavelet transform Wˆθis alsoO(N).\\nThis has been a very brief glimpse of this vast and growing ﬁel d. There is\\na very large mathematical and computational base built on wa velets. Mod-\\nern image compression is often performed using two-dimensi onal wavelet\\nrepresentations.\\nBibliographic Notes\\nSplines and B-splines are discussed in detail in de Boor (1978). Green\\nand Silverman (1994) and Wahba (1990) give a thorough treatm ent of\\nsmoothing splines and thin-plate splines; the latter also c overs reproducing\\nkernel Hilbert spaces. See also Girosi et al. (1995) and Evge niou et al.\\n(2000) for connections between many nonparametric regress ion techniques\\nusing RKHS approaches. Modeling functional data, as in Sect ion 5.2.3, is\\ncovered in detail in Ramsay and Silverman (1997).\\nDaubechies (1992) is a classic and mathematical treatment o f wavelets.\\nOtherusefulsourcesareChui(1992)andWickerhauser(1994 ).Donohoand\\nJohnstone (1994) developed the SURE shrinkage and selectio n technology\\nfrom a statistical estimation framework; see also Vidakovi c (1999). Bruce\\nand Gao (1996) is a useful applied introduction, which also d escribes the\\nwavelet software in S-PLUS.\\nExercises\\nEx. 5.1Show that the truncated power basis functions in (5.3) repre sent a\\nbasis for a cubic spline with the two knots as indicated.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 199}),\n",
       " Document(page_content='182 5. Basis Expansions and Regularization\\nNMR Signal0 200 400 600 800 10000 20 40 60spline\\nwavelet\\nSmooth Function (Simulated)n\\n0.0 0.2 0.4 0.6 0.8 1.0-4 -2 0 2 4spline\\nwavelet\\ntrue•\\n••\\n•\\n•••\\n•\\n•\\n•\\n••••\\n•••\\n••••\\n••\\n••\\n••\\n••••••\\n•••\\n•\\n••\\n••\\n•••••\\n•••••••\\n••••\\n•\\n•••\\n••\\n••\\n••\\n•••\\n••••\\n••••\\n••\\n••\\n•••\\n•••\\n•••\\n••\\n•\\n•••\\n•••\\n•\\n••••••\\n••••\\n••\\n•\\n•••••\\n••••\\n••\\n•••\\nFIGURE 5.19. Wavelet smoothing compared with smoothing splines on two\\nexamples. Each panel compares the SURE-shrunk wavelet ﬁt to th e cross-validated\\nsmoothing spline ﬁt.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 200}),\n",
       " Document(page_content='Exercises 183\\nEx. 5.2Suppose that Bi,M(x) is an order- M B-spline deﬁned in the Ap-\\npendix on page 186 through the sequence (5.77)–(5.78).\\n(a) Show by induction that Bi,M(x) = 0 forx̸∈[τi,τi+M]. This shows, for\\nexample, that the support of cubic B-splines is at most 5 knots.\\n(b) Show by induction that Bi,M(x)>0 forx∈(τi,τi+M). TheB-splines\\nare positive in the interior of their support.\\n(c) Show by induction that∑K+M\\ni=1Bi,M(x) = 1∀x∈[ξ0,ξK+1].\\n(d) Show that Bi,Mis a piecewise polynomial of order M(degreeM−1)\\non [ξ0,ξK+1], with breaks only at the knots ξ1,...,ξ K.\\n(e) Show that an order- M B-spline basis function is the density function\\nof a convolution of Muniform random variables.\\nEx. 5.3Write a program to reproduce Figure 5.3 on page 145.\\nEx.5.4Considerthetruncatedpowerseriesrepresentationforcub icsplines\\nwithKinterior knots. Let\\nf(X) =3∑\\nj=0βjXj+K∑\\nk=1θk(X−ξk)3\\n+. (5.70)\\nProve that the natural boundary conditions for natural cubi c splines (Sec-\\ntion 5.2.1) imply the following linear constraints on the co eﬃcients:\\nβ2= 0,∑K\\nk=1θk= 0,\\nβ3= 0,∑K\\nk=1ξkθk= 0.(5.71)\\nHence derive the basis (5.4) and (5.5).\\nEx. 5.5Write a program to classify the phonemedata using a quadratic dis-\\ncriminant analysis (Section 4.3). Since there are many corr elated features,\\nyou should ﬁlter them using a smooth basis of natural cubic sp lines (Sec-\\ntion 5.2.3). Decide beforehand on a series of ﬁve diﬀerent ch oices for the\\nnumber and position of the knots, and use tenfold cross-vali dation to make\\nthe ﬁnal selection. The phonemedata are available from the book website\\nwww-stat.stanford.edu/ElemStatLearn .\\nEx. 5.6Suppose you wish to ﬁt a periodic function, with a known perio dT.\\nDescribe how you could modify the truncated power series bas is to achieve\\nthis goal.\\nEx. 5.7Derivation of smoothing splines (Green and Silverman, 1994). Sup-\\npose thatN≥2, and that gis the natural cubic spline interpolant to the\\npairs{xi,zi}N\\n1, witha < x 1<···< xN< b. This is a natural spline', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 201}),\n",
       " Document(page_content='184 5. Basis Expansions and Regularization\\nwith a knot at every xi; being anN-dimensional space of functions, we can\\ndetermine the coeﬃcients such that it interpolates the sequ enceziexactly.\\nLet ˜gbe any other diﬀerentiable function on [ a,b] that interpolates the N\\npairs.\\n(a) Leth(x) = ˜g(x)−g(x). Use integration by parts and the fact that gis\\na natural cubic spline to show that\\n∫b\\nag′′(x)h′′(x)dx=−N−1∑\\nj=1g′′′(x+\\nj){h(xj+1)−h(xj)}(5.72)\\n= 0.\\n(b) Hence show that∫b\\na˜g′′(t)2dt≥∫b\\nag′′(t)2dt,\\nand that equality can only hold if his identically zero in [ a,b].\\n(c) Consider the penalized least squares problem\\nmin\\nf[N∑\\ni=1(yi−f(xi))2+λ∫b\\naf′′(t)2dt]\\n.\\nUse (b) to argue that the minimizer must be a cubic spline with knots\\nat each of the xi.\\nEx. 5.8In the appendix to this chapter we show how the smoothing spli ne\\ncomputations could be more eﬃciently carried out using a ( N+4) dimen-\\nsional basis of B-splines. Describe a slightly simpler scheme using a ( N+2)\\ndimensional B-spline basis deﬁned on the N−2 interior knots.\\nEx. 5.9Derive the Reinsch form Sλ= (I+λK)−1for the smoothing spline.\\nEx. 5.10 Derive an expression for Var( ˆfλ(x0)) and bias( ˆfλ(x0)). Using the\\nexample (5.22), create a version of Figure 5.9 where the mean and several\\n(pointwise) quantiles of ˆfλ(x) are shown.\\nEx. 5.11 Prove that for a smoothing spline the null space of Kis spanned\\nby functions linear in X.\\nEx. 5.12 Characterize the solution to the following problem,\\nmin\\nfRSS(f,λ) =N∑\\ni=1wi{yi−f(xi)}2+λ∫\\n{f′′(t)}2dt, (5.73)\\nwhere thewi≥0 are observation weights.\\nCharacterize the solution to the smoothing spline problem ( 5.9) when\\nthe training data have ties in X.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 202}),\n",
       " Document(page_content='Exercises 185\\nEx. 5.13 You have ﬁtted a smoothing spline ˆfλto a sample of Npairs\\n(xi,yi).Supposeyouaugmentyouroriginalsamplewiththepair x0,ˆfλ(x0),\\nand reﬁt; describe the result. Use this to derive the N-fold cross-validation\\nformula (5.26).\\nEx. 5.14 Derive the constraints on the αjin the thin-plate spline expan-\\nsion (5.39) to guarantee that the penalty J(f) is ﬁnite. How else could one\\nensure that the penalty was ﬁnite?\\nEx. 5.15 This exercise derives some of the results quoted in Section 5 .8.1.\\nSupposeK(x,y) satisfying the conditions (5.45) and let f(x)∈HK. Show\\nthat\\n(a)⟨K(·,xi),f⟩HK=f(xi).\\n(b)⟨K(·,xi),K(·,xj)⟩HK=K(xi,xj).\\n(c) Ifg(x) =∑N\\ni=1αiK(x,xi), then\\nJ(g) =N∑\\ni=1N∑\\nj=1K(xi,xj)αiαj.\\nSuppose that ˜ g(x) =g(x)+ρ(x), withρ(x)∈HK, and orthogonal in HK\\nto each ofK(x,xi), i= 1,...,N. Show that\\n(d)\\nN∑\\ni=1L(yi,˜g(xi))+λJ(˜g)≥N∑\\ni=1L(yi,g(xi))+λJ(g) (5.74)\\nwith equality iﬀ ρ(x) = 0.\\nEx. 5.16 Consider the ridge regression problem (5.53), and assume M≥N.\\nAssume you have a kernel Kthat computes the inner product K(x,y) =∑M\\nm=1hm(x)hm(y).\\n(a) Derive (5.62) on page 171 in the text. How would you comput e the\\nmatrices VandDγ, givenK? Hence show that (5.63) is equivalent\\nto (5.53).\\n(b) Show that\\nˆf=Hˆβ\\n=K(K+λI)−1y, (5.75)\\nwhereHis theN×Mmatrix of evaluations hm(xi), andK=HHT\\ntheN×Nmatrix of inner-products h(xi)Th(xj).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 203}),\n",
       " Document(page_content='186 5. Basis Expansions and Regularization\\n(c) Show that\\nˆf(x) =h(x)Tˆβ\\n=N∑\\ni=1K(x,xi)ˆαi (5.76)\\nandˆα= (K+λI)−1y.\\n(d) How would you modify your solution if M <N?\\nEx. 5.17 Show how to convert the discrete eigen-decomposition of Kin\\nSection 5.8.2 to estimates of the eigenfunctions of K.\\nEx. 5.18 The wavelet function ψ(x) of the symmlet- pwavelet basis has\\nvanishing moments up to order p. Show that this implies that polynomials\\nof orderpare represented exactly in V0, deﬁned on page 176.\\nEx. 5.19 Show that the Haar wavelet transform of a signal of length N= 2J\\ncan be computed in O(N) computations.\\nAppendix: Computations for Splines\\nIn this Appendix, we describe the B-spline basis for representing polyno-\\nmial splines. We also discuss their use in the computations o f smoothing\\nsplines.\\nB-splines\\nBefore we can get started, we need to augment the knot sequenc e deﬁned\\nin Section 5.2. Let ξ0<ξ1andξK<ξK+1be twoboundary knots, which\\ntypically deﬁne the domain over which we wish to evaluate our spline. We\\nnow deﬁne the augmented knot sequence τsuch that\\n•τ1≤τ2≤···≤τM≤ξ0;\\n•τj+M=ξj, j= 1,···,K;\\n•ξK+1≤τK+M+1≤τK+M+2≤···≤τK+2M.\\nThe actual values of these additional knots beyond the bound ary are arbi-\\ntrary, and it is customary to make them all the same and equal t oξ0and\\nξK+1, respectively.\\nDenote by Bi,m(x) theithB-spline basis function of order mfor the\\nknot-sequence τ,m≤M. They are deﬁned recursively in terms of divided', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 204}),\n",
       " Document(page_content='Appendix: Computations for Splines 187\\ndiﬀerences as follows:\\nBi,1(x) ={\\n1 ifτi≤x<τi+1\\n0 otherwise(5.77)\\nfori= 1,...,K+2M−1. These are also known as Haar basis functions.\\nBi,m(x) =x−τi\\nτi+m−1−τiBi,m−1(x)+τi+m−x\\nτi+m−τi+1Bi+1,m−1(x)\\nfori= 1,...,K+2M−m.\\n(5.78)\\nThus with M= 4,Bi,4, i= 1,···,K+ 4 are the K+ 4 cubicB-spline\\nbasis functions for the knot sequence ξ. This recursion can be contin-\\nued and will generate the B-spline basis for any order spline. Figure 5.20\\nshows the sequence of B-splines up to order four with knots at the points\\n0.0,0.1,...,1.0. Since we have created some duplicate knots, some care\\nhas to be taken to avoid division by zero. If we adopt the conve ntion\\nthatBi,1= 0 ifτi=τi+1, then by induction Bi,m= 0 ifτi=τi+1=\\n...=τi+m. Note also that in the construction above, only the subset\\nBi,m, i=M−m+ 1,...,M+Kare required for the B-spline basis\\nof orderm<Mwith knots ξ.\\nTo fully understand the properties of these functions, and t o show that\\nthey do indeed span the space of cubic splines for the knot seq uence, re-\\nquires additional mathematical machinery, including the p roperties of di-\\nvided diﬀerences. Exercise 5.2 explores these issues.\\nThe scope of B-splines is in fact bigger than advertised here, and has to\\ndo with knot duplication. If we duplicate an interior knot in the construc-\\ntion of the τsequence above, and then generate the B-spline sequence as\\nbefore, the resulting basis spans the space of piecewise pol ynomials with\\none less continuous derivative at the duplicated knot. In ge neral, if in ad-\\ndition to the repeated boundary knots, we include the interi or knotξj\\n1≤rj≤Mtimes, then the lowest-order derivative to be discontinuou s\\natx=ξjwill be order M−rj. Thus for cubic splines with no repeats,\\nrj= 1, j= 1,...,K, and at each interior knot the third derivatives (4 −1)\\nare discontinuous. Repeating the jth knot three times leads to a discontin-\\nuous 1st derivative; repeating it four times leads to a disco ntinuous zeroth\\nderivative, i.e., the function is discontinuous at x=ξj. This is exactly what\\nhappens at the boundary knots; we repeat the knots Mtimes, so the spline\\nbecomes discontinuous at the boundary knots (i.e., undeﬁne d beyond the\\nboundary).\\nThe local support of B-splines has important computational implica-\\ntions, especially when the number of knots Kis large. Least squares com-\\nputations with Nobservations and K+Mvariables (basis functions) take\\nO(N(K+M)2+(K+M)3) ﬂops (ﬂoating point operations.) If Kis some\\nappreciable fraction of N, this leads to O(N3) algorithms which becomes', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 205}),\n",
       " Document(page_content='188 5. Basis Expansions and Regularization\\nB-splines of Order 1\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.4 0.8 1.2\\nB-splines of Order 2\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.4 0.8 1.2\\nB-splines of Order 3\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.4 0.8 1.2\\nB-splines of Order 4\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.4 0.8 1.2\\nFIGURE 5.20. The sequence of B-splines up to order four with ten knots evenly\\nspaced from 0to1. TheB-splines have local support ; they are nonzero on an\\ninterval spanned by M+1knots.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 206}),\n",
       " Document(page_content='Appendix: Computations for Splines 189\\nunacceptable for large N. IftheNobservations aresorted, the N×(K+M)\\nregression matrix consisting of the K+M B-spline basis functions evalu-\\nated at the Npoints has many zeros, which can be exploited to reduce the\\ncomputational complexity back to O(N). We take this up further in the\\nnext section.\\nComputations for Smoothing Splines\\nAlthough natural splines (Section 5.2.1) provide a basis fo r smoothing\\nsplines, it is computationally more convenient to operate i n the larger space\\nof unconstrained B-splines. We write f(x) =∑N+4\\n1γjBj(x), whereγjare\\ncoeﬃcients and the Bjare the cubic B-spline basis functions. The solution\\nlooks the same as before,\\nˆγ= (BTB+λΩB)−1BTy, (5.79)\\nexcept now the N×NmatrixNis replaced by the N×(N+ 4) matrix\\nB, and similarly the ( N+ 4)×(N+ 4) penalty matrix ΩBreplaces the\\nN×Ndimensional ΩN. Although at face value it seems that there are\\nno boundary derivative constraints, it turns out that the pe nalty term\\nautomatically imposes them by giving eﬀectively inﬁnite we ight to any non\\nzero derivative beyond the boundary. In practice, ˆ γis restricted to a linear\\nsubspace for which the penalty is always ﬁnite.\\nSince the columns of Bare the evaluated B-splines, in order from left\\nto right and evaluated at the sortedvalues ofX, and the cubic B-splines\\nhave local support, Bis lower 4-banded. Consequently the matrix M=\\n(BTB+λΩ) is 4-banded and hence its Cholesky decomposition M=LLT\\ncanbecomputedeasily.Onethensolves LLTγ=BTybyback-substitution\\nto giveγand hence the solution ˆfinO(N) operations.\\nIn practice, when Nis large, it is unnecessary to use all Ninterior knots,\\nand any reasonable thinning strategy will save in computations and have\\nnegligible eﬀect on the ﬁt. For example, the smooth.spline function in S-\\nPLUS uses an approximately logarithmic strategy: if N <50 all knots are\\nincluded, but even at N= 5,000 only 204 knots are used.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 207}),\n",
       " Document(page_content='190 5. Basis Expansions and Regularization', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 208}),\n",
       " Document(page_content='This is page 191\\nPrinter: Opaque this\\n6\\nKernel Smoothing Methods\\nIn this chapter we describe a class of regression techniques that achieve\\nﬂexibility in estimating the regression function f(X) over the domain IRp\\nby ﬁtting a diﬀerent but simple model separately at each quer y pointx0.\\nThis is done by using only those observations close to the tar get pointx0to\\nﬁtthesimplemodel,andinsuchawaythattheresultingestim atedfunction\\nˆf(X) issmoothin IRp. This localization is achieved via a weighting function\\norkernelKλ(x0,xi), which assigns a weight to xibased on its distance from\\nx0. The kernels Kλare typically indexed by a parameter λthat dictates\\nthe width of the neighborhood. These memory-based methods require in\\nprinciple little or no training; all the work gets done at eva luation time.\\nThe only parameter that needs to be determined from the train ing data is\\nλ. The model, however, is the entire training data set.\\nWe also discuss more general classes of kernel-based techni ques , which\\ntie in with structured methods in other chapters, and are use ful for density\\nestimation and classiﬁcation.\\nThe techniques in this chapter should not be confused with th ose asso-\\nciated with the more recent usage of the phrase “kernel metho ds”. In this\\nchapter kernels are mostly used as a device for localization . We discuss ker-\\nnel methods in Sections 5.8, 14.5.4, 18.5 and Chapter 12; in t hose contexts\\nthe kernel computes an inner product in a high-dimensional ( implicit) fea-\\nture space, and is used for regularized nonlinear modeling. We make some\\nconnections to the methodology in this chapter at the end of S ection 6.7.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 209}),\n",
       " Document(page_content='192 6. Kernel Smoothing Methods\\nNearest-Neighbor Kernel\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OO\\nOOOO\\nOO\\nOO\\nOOOO\\nOO\\nOO\\nOOOOO\\nOO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nO\\nO\\nOO\\nO\\nOO\\nOO\\nO\\nOO\\nO\\nOOO\\nOO\\nOOOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOOO\\nO\\nOO\\nOOO\\nOOO\\nO\\nOO\\nO\\nOO\\nO\\nOOO\\nO\\nO\\nOO\\nOO\\nO\\nOOOO\\nO\\nOO\\nOO\\nO\\nOO\\nO\\nOOO\\nOO\\nOOOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOO•\\nx0ˆf(x0)Epanechnikov Kernel\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OO\\nOOOO\\nOO\\nOO\\nOOOO\\nOO\\nOO\\nOOOOO\\nOO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nO\\nO\\nOO\\nO\\nOO\\nOO\\nO\\nOO\\nO\\nOOO\\nOO\\nOOOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOOO\\nO\\nOO\\nOOO\\nOOO\\nO\\nOO\\nO\\nOO\\nO\\nOOO\\nO\\nO\\nOO\\nOO\\nO\\nOOO\\nOO\\nOO\\nO\\nO\\nOO\\nO\\nOO\\nOO\\nO\\nOO\\nO\\nOOO\\nOO\\nOOOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOOO\\nO\\nOO•\\nx0ˆf(x0)\\nFIGURE 6.1. In each panel 100pairsxi, yiare generated at random from the\\nblue curve with Gaussian errors: Y= sin(4X)+ε,X∼U[0,1],ε∼N(0,1/3). In\\nthe left panel the green curve is the result of a 30-nearest-neighbor running-mean\\nsmoother. The red point is the ﬁtted constant ˆf(x0), and the red circles indicate\\nthose observations contributing to the ﬁt at x0. The solid yellow region indicates\\nthe weights assigned to observations. In the right panel, the green curve is the\\nkernel-weighted average, using an Epanechnikov kernel with (half) window width\\nλ= 0.2.\\n6.1 One-Dimensional Kernel Smoothers\\nIn Chapter 2, we motivated the k–nearest-neighbor average\\nˆf(x) = Ave(yi|xi∈Nk(x)) (6.1)\\nas an estimate of the regression function E( Y|X=x). HereNk(x) is the set\\nofkpoints nearest to xin squared distance, and Ave denotes the average\\n(mean). The idea is to relax the deﬁnition of conditional exp ectation, as\\nillustrated in the left panel of Figure 6.1, and compute an av erage in a\\nneighborhood of the target point. In this case we have used th e 30-nearest\\nneighborhood—the ﬁt at x0is the average of the 30 pairs whose xivalues\\nare closest to x0. The green curve is traced out as we apply this deﬁnition\\nat diﬀerent values x0. The green curve is bumpy, since ˆf(x) is discontinuous\\ninx. As we move x0from left to right, the k-nearest neighborhood remains\\nconstant, until a point xito the right of x0becomes closer than the furthest\\npointxi′in the neighborhood to the left of x0, at which time xireplacesxi′.\\nThe average in (6.1) changes in a discrete way, leading to a di scontinuous\\nˆf(x).\\nThis discontinuity is ugly and unnecessary. Rather than giv e all the\\npoints in the neighborhood equal weight, we can assign weigh ts that die\\noﬀ smoothly with distance from the target point. The right pa nel shows\\nan example of this, using the so-called Nadaraya–Watson ker nel-weighted', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 210}),\n",
       " Document(page_content='6.1 One-Dimensional Kernel Smoothers 193\\naverage\\nˆf(x0) =∑N\\ni=1Kλ(x0,xi)yi∑N\\ni=1Kλ(x0,xi), (6.2)\\nwith the Epanechnikov quadratic kernel\\nKλ(x0,x) =D(|x−x0|\\nλ)\\n, (6.3)\\nwith\\nD(t) ={3\\n4(1−t2) if|t|≤1;\\n0 otherwise .(6.4)\\nThe ﬁtted function is now continuous, and quite smooth in the right panel\\nof Figure 6.1. As we move the target from left to right, points enter the\\nneighborhood initially with weight zero, and then their con tribution slowly\\nincreases (see Exercise 6.1).\\nIn the right panel we used a metric window size λ= 0.2 for the kernel\\nﬁt, which does not change as we move the target point x0, while the size\\nof the 30-nearest-neighbor smoothing window adapts to the l ocal density\\nof thexi. One can, however, also use such adaptive neighborhoods wit h\\nkernels, but we need to use a more general notation. Let hλ(x0) be a width\\nfunction (indexed by λ) that determines the width of the neighborhood at\\nx0. Then more generally we have\\nKλ(x0,x) =D(|x−x0|\\nhλ(x0))\\n. (6.5)\\nIn (6.3),hλ(x0) =λis constant. For k-nearest neighborhoods, the neigh-\\nborhood size kreplacesλ, and we have hk(x0) =|x0−x[k]|wherex[k]is\\nthekth closestxitox0.\\nThere are a number of details that one has to attend to in pract ice:\\n•The smoothing parameter λ, which determines the width of the local\\nneighborhood, has to be determined. Large λimplies lower variance\\n(averages over more observations) but higher bias (we essen tially as-\\nsume the true function is constant within the window).\\n•Metric window widths (constant hλ(x)) tend to keep the bias of the\\nestimate constant, but the variance is inversely proportio nal to the\\nlocal density. Nearest-neighbor window widths exhibit the opposite\\nbehavior; the variance stays constant and the absolute bias varies\\ninversely with local density.\\n•Issues arise with nearest-neighbors when there are ties in t hexi. With\\nmost smoothing techniques one can simply reduce the data set by\\naveraging the yiat tied values of X, and supplementing these new\\nobservations at the unique values of xiwith an additional weight wi\\n(which multiples the kernel weight).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 211}),\n",
       " Document(page_content='194 6. Kernel Smoothing Methods\\n-3 -2 -1 0 1 2 30.0 0.4 0.8Epanechnikov\\nTri-cube\\nGaussianKλ(x0,x)\\nFIGURE 6.2. A comparison of three popular kernels for local smoothing. Each\\nhas been calibrated to integrate to 1. The tri-cube kernel is compact and has two\\ncontinuous derivatives at the boundary of its support, while the Epanechnikov ker-\\nnel has none. The Gaussian kernel is continuously diﬀerentiab le, but has inﬁnite\\nsupport.\\n•This leaves a more general problem to deal with: observation weights\\nwi. Operationally we simply multiply them by the kernel weight s be-\\nfore computing the weighted average. With nearest neighbor hoods, it\\nis now natural to insist on neighborhoods with a total weight content\\nk(relative to∑wi). In the event of overﬂow (the last observation\\nneeded in a neighborhood has a weight wjwhich causes the sum of\\nweights to exceed the budget k), then fractional parts can be used.\\n•Boundary issues arise. The metric neighborhoods tend to con tain less\\npoints on the boundaries, while the nearest-neighborhoods get wider.\\n•The Epanechnikov kernel has compact support (needed when us ed\\nwith nearest-neighbor window size). Another popular compa ct kernel\\nis based on the tri-cube function\\nD(t) ={\\n(1−|t|3)3if|t|≤1;\\n0 otherwise(6.6)\\nThis is ﬂatter on the top (like the nearest-neighbor box) and is diﬀer-\\nentiable at the boundary of its support. The Gaussian densit y func-\\ntionD(t) =φ(t) is a popular noncompact kernel, with the standard-\\ndeviation playing the role of the window size. Figure 6.2 com pares\\nthe three.\\n6.1.1 Local Linear Regression\\nWe have progressed from the raw moving average to a smoothly v arying\\nlocally weighted average by using kernel weighting. The smo oth kernel ﬁt\\nstill has problems, however, as exhibited in Figure 6.3 (lef t panel). Locally-\\nweighted averages can be badly biased on the boundaries of th e domain,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 212}),\n",
       " Document(page_content='6.1 One-Dimensional Kernel Smoothers 195\\nN-W Kernel at Boundary\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5O\\nOO\\nOO\\nOO\\nOO\\nOO\\nOOOOO\\nOOO\\nOOO\\nOOO\\nO\\nOO\\nOO\\nO\\nOO\\nOOO\\nOOOO\\nO\\nOO\\nOO\\nO\\nOO\\nOO\\nOOOO\\nOO\\nOOOO\\nOOO\\nOO\\nOO\\nO\\nOOO\\nOOOO\\nO\\nO\\nOO\\nOOO\\nOO\\nO\\nOOO\\nOO\\nO\\nOO\\nO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOOOOO\\nOOO\\nOOO\\nOOO\\nO\\nOO\\nOO\\n•\\nx0ˆf(x0)Local Linear Regression at Boundary\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5O\\nOO\\nOO\\nOO\\nOO\\nOO\\nOOOOO\\nOOO\\nOOO\\nOOO\\nO\\nOO\\nOO\\nO\\nOO\\nOOO\\nOOOO\\nO\\nOO\\nOO\\nO\\nOO\\nOO\\nOOOO\\nOO\\nOOOO\\nOOO\\nOO\\nOO\\nO\\nOOO\\nOOOO\\nO\\nO\\nOO\\nOOO\\nOO\\nO\\nOOO\\nOO\\nO\\nOO\\nO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOOOOO\\nOOO\\nOOO\\nOOO\\nO\\nOO\\nOO\\n•\\nx0ˆf(x0)\\nFIGURE 6.3. The locally weighted average has bias problems at or near the\\nboundaries of the domain. The true function is approximately linear here, but\\nmost of the observations in the neighborhood have a higher me an than the target\\npoint, so despite weighting, their mean will be biased upwar ds. By ﬁtting a locally\\nweighted linear regression (right panel), this bias is remove d to ﬁrst order.\\nbecause of the asymmetry of the kernel in that region. By ﬁtti ng straight\\nlines rather than constants locally, we can remove this bias exactly to ﬁrst\\norder; see Figure 6.3 (right panel). Actually, this bias can be present in the\\ninterior of the domain as well, if the Xvalues are not equally spaced (for\\nthe same reasons, but usually less severe). Again locally we ighted linear\\nregression will make a ﬁrst-order correction.\\nLocallyweightedregressionsolvesaseparateweightedlea stsquaresprob-\\nlem at each target point x0:\\nmin\\nα(x0),β(x0)N∑\\ni=1Kλ(x0,xi)[yi−α(x0)−β(x0)xi]2. (6.7)\\nThe estimate is then ˆf(x0) = ˆα(x0)+ˆβ(x0)x0. Notice that although we ﬁt\\nan entire linear model to the data in the region, we only use it to evaluate\\nthe ﬁt at the single point x0.\\nDeﬁne the vector-valued function b(x)T= (1,x). LetBbe theN×2\\nregression matrix with ith rowb(xi)T, andW(x0) theN×Ndiagonal\\nmatrix with ith diagonal element Kλ(x0,xi). Then\\nˆf(x0) =b(x0)T(BTW(x0)B)−1BTW(x0)y (6.8)\\n=N∑\\ni=1li(x0)yi. (6.9)\\nEquation (6.8) gives an explicit expression for the local li near regression\\nestimate, and (6.9) highlights the fact that the estimate is linearin the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 213}),\n",
       " Document(page_content='196 6. Kernel Smoothing Methods\\nLocal Linear Equivalent Kernel at Boundary\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OO\\nOO\\nOO\\nOOO\\nOO\\nOOO\\nOOO\\nO\\nOOOOOOO\\nOO\\nOO\\nOO\\nOO\\nO\\nOO\\nOOOOO\\nOO\\nOO\\nOOOO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOO\\nOOO\\nOOO\\nOOOO\\nOO\\nO\\nOOO\\nOOOOO\\nO\\nOO\\nOO\\nOOOO\\nOO\\nOOO\\nOOO\\nOO\\nOO\\nOOO\\nOO\\nOOO\\nOOO\\nO\\nOOOOOO\\n•••••••••\\n•\\n••\\n••\\n•••••••••••••• • ••••• •• • • • ••• • ••• • •• ••• • •• ••••• ••• • ••• •••• •• • •• • • •• • •• •••• ••• • • • ••••••\\nx0ˆf(x0)Local Linear Equivalent Kernel in Interior\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OO\\nOO\\nOO\\nOOO\\nOO\\nOOO\\nOOO\\nO\\nOOOOOOO\\nOO\\nOO\\nOO\\nOO\\nO\\nOO\\nOOOOO\\nOO\\nOO\\nOOOO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOO\\nOOO\\nOOO\\nOOOO\\nOO\\nO\\nOOO\\nOOOOO\\nO\\nOO\\nOO\\nOOOO\\nOO\\nOOO\\nOO\\nOO\\nOO\\nOO\\nO\\nOO\\nOOOOO\\nOO\\nOO\\nOOOO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOO\\nOOO\\nOOO•\\n•• • ••• •••• •••• • •••••••• ••• ••••••••••••••••••••••••••••••••••••••\\n•••\\n••\\n• •• • ••• • •• • •• •••• ••• • • • ••••••\\nx0ˆf(x0)\\nFIGURE 6.4. The green points show the equivalent kernel li(x0)for local re-\\ngression. These are the weights in ˆf(x0) =∑N\\ni=1li(x0)yi, plotted against their\\ncorresponding xi. For display purposes, these have been rescaled, since in fact\\nthey sum to 1. Since the yellow shaded region is the (rescaled) equivalent ke rnel\\nfor the Nadaraya–Watson local average, we see how local regres sion automati-\\ncally modiﬁes the weighting kernel to correct for biases due t o asymmetry in the\\nsmoothing window.\\nyi(theli(x0) do not involve y). These weights li(x0) combine the weight-\\ning kernelKλ(x0,·) and the least squares operations, and are sometimes\\nreferred to as the equivalent kernel . Figure 6.4 illustrates the eﬀect of lo-\\ncal linear regression on the equivalent kernel. Historical ly, the bias in the\\nNadaraya–Watson and other local average kernel methods wer e corrected\\nby modifying the kernel. These modiﬁcations were based on th eoretical\\nasymptotic mean-square-error considerations, and beside s being tedious to\\nimplement, are only approximate for ﬁnite sample sizes. Loc al linear re-\\ngression automatically modiﬁes the kernel to correct the bias exactlyto\\nﬁrst order, a phenomenon dubbed as automatic kernel carpentry . Consider\\nthe following expansion for E ˆf(x0), using the linearity of local regression\\nand a series expansion of the true function faroundx0,\\nEˆf(x0) =N∑\\ni=1li(x0)f(xi)\\n=f(x0)N∑\\ni=1li(x0)+f′(x0)N∑\\ni=1(xi−x0)li(x0)\\n+f′′(x0)\\n2N∑\\ni=1(xi−x0)2li(x0)+R, (6.10)\\nwhere the remainder term Rinvolves third- and higher-order derivatives of\\nf, and is typically small under suitable smoothness assumpti ons. It can be', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 214}),\n",
       " Document(page_content='6.1 One-Dimensional Kernel Smoothers 197\\nLocal Linear in Interior\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OOO\\nO\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOO\\nOO\\nO\\nOO\\nOOO\\nOO\\nOO\\nO\\nOOO\\nOO\\nOOO\\nOO\\nOO\\nO\\nOO\\nO\\nOOOOO\\nOOO\\nO\\nO\\nO\\nOOO\\nOOOO\\nOOOO\\nOOOOO\\nOO\\nOO\\nOO\\nO\\nOO\\nO\\nO\\nOO\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOO\\nOO\\nO\\nOO\\nOOO\\nOO\\nOO\\nO\\nOOO\\nOO\\nOOO\\nOO\\nOO\\nO\\nOO\\nO\\nOOOOO\\nO•ˆf(x0)Local Quadratic in Interior\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OOO\\nO\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOO\\nOO\\nO\\nOO\\nOOO\\nOO\\nOO\\nO\\nOOO\\nOO\\nOOO\\nOO\\nOO\\nO\\nOO\\nO\\nOOOOO\\nOOO\\nO\\nO\\nO\\nOOO\\nOOOO\\nOOOO\\nOOOOO\\nOO\\nOO\\nOO\\nO\\nOO\\nO\\nO\\nOO\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOO\\nOO\\nO\\nOO\\nOOO\\nOO\\nOO\\nO\\nOOO\\nOO\\nOOO\\nOO\\nOO\\nO\\nOO\\nO\\nOOOOO\\nO•ˆf(x0)\\nFIGURE 6.5. Local linear ﬁts exhibit bias in regions of curvature of the tr ue\\nfunction. Local quadratic ﬁts tend to eliminate this bias.\\nshown (Exercise 6.2) that for local linear regression,∑N\\ni=1li(x0) = 1 and∑N\\ni=1(xi−x0)li(x0) = 0. Hence the middle term equals f(x0), and since\\nthe bias is E ˆf(x0)−f(x0), we see that it depends only on quadratic and\\nhigher–order terms in the expansion of f.\\n6.1.2 Local Polynomial Regression\\nWhy stop at local linear ﬁts? We can ﬁt local polynomial ﬁts of any de-\\ngreed,\\nmin\\nα(x0),βj(x0), j=1,...,dN∑\\ni=1Kλ(x0,xi)\\uf8ee\\n\\uf8f0yi−α(x0)−d∑\\nj=1βj(x0)xj\\ni\\uf8f9\\n\\uf8fb2\\n(6.11)\\nwith solution ˆf(x0) = ˆα(x0)+∑d\\nj=1ˆβj(x0)xj\\n0. In fact, an expansion such as\\n(6.10)willtellusthatthebiaswillonlyhavecomponentsof degreed+1and\\nhigher(Exercise6.2). Figure6.5illustrates localquadra ticregression.Local\\nlinear ﬁts tend to be biased in regions of curvature of the tru e function, a\\nphenomenon referred to as trimming the hills andﬁlling the valleys . Local\\nquadratic regression is generally able to correct this bias .\\nThere is of course a price to be paid for this bias reduction, a nd that is\\nincreased variance. The ﬁt in the right panel of Figure 6.5 is slightly more\\nwiggly, especially in the tails. Assuming the model yi=f(xi) +εi, with\\nεiindependent and identically distributed with mean zero and variance\\nσ2, Var(ˆf(x0)) =σ2||l(x0)||2, wherel(x0) is the vector of equivalent kernel\\nweights atx0. It can be shown (Exercise 6.3) that ||l(x0)||increases with d,\\nand so there is a bias–variance tradeoﬀ in selecting the poly nomial degree.\\nFigure 6.6 illustrates these variance curves for degree zer o, one and two', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 215}),\n",
       " Document(page_content='198 6. Kernel Smoothing Methods\\nVariance\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5Constant\\nLinear\\nQuadratic\\nFIGURE 6.6. The variances functions ||l(x)||2for local constant, linear and\\nquadratic regression, for a metric bandwidth ( λ= 0.2) tri-cube kernel.\\nlocal polynomials. To summarize some collected wisdom on th is issue:\\n•Local linear ﬁts can help bias dramatically at the boundarie s at a\\nmodest cost in variance. Local quadratic ﬁts do little at the bound-\\naries for bias, but increase the variance a lot.\\n•Local quadratic ﬁts tend to be most helpful in reducing bias d ue to\\ncurvature in the interior of the domain.\\n•Asymptotic analysis suggest that local polynomials of odd d egree\\ndominate those of even degree. This is largely due to the fact that\\nasymptotically the MSE is dominated by boundary eﬀects.\\nWhile it may be helpful to tinker, and move from local linear ﬁ ts at the\\nboundary to local quadratic ﬁts in the interior, we do not rec ommend such\\nstrategies. Usually the application will dictate the degre e of the ﬁt. For\\nexample, if we are interested in extrapolation, then the bou ndary is of\\nmore interest, and local linear ﬁts are probably more reliab le.\\n6.2 Selecting the Width of the Kernel\\nIn each of the kernels Kλ,λis a parameter that controls its width:\\n•For the Epanechnikov or tri-cube kernel with metric width, λis the\\nradius of the support region.\\n•For the Gaussian kernel, λis the standard deviation.\\n•λis the number kof nearest neighbors in k-nearest neighborhoods,\\noften expressed as a fraction or spank/Nof the total training sample.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 216}),\n",
       " Document(page_content='6.2 Selecting the Width of the Kernel 199\\n•••\\n••••••\\n•\\n••\\n••\\n•••••\\n••••• ••••••• •••• •• ••• ••••• • • ••••• •••• ••• • ••••• •••••••• • • • •• •• •••• • ••• • ••••••••••••••••\\n••\\n••\\n••••••\\n••\\n•• ••••••• •••• •• ••• •••••• • ••••• ••••••• • ••••• ••••••••• • • •• •• •••• • ••• • •••••• •••••• •• ••••• •• •• • •••• •• •• ••••••••••••••••••••• •• ••••• •••••\\n•\\n••••••• ••••••••• • • •• •• •••• • ••• • •••••• •••••• ••••••• •• •• • •••••• ••••••••••••••••••••••• • • •••••••••••\\n•••••••••••••••• •• •• •• •••• •••• • ••••••\\nFIGURE 6.7. Equivalent kernels for a local linear regression smoother (tri- cube\\nkernel; orange) and a smoothing spline (blue), with matching de grees of freedom.\\nThe vertical spikes indicates the target points.\\nThere is a natural bias–variance tradeoﬀ as we change the wid th of the\\naveraging window, which is most explicit for local averages :\\n•If the window is narrow, ˆf(x0) is an average of a small number of yi\\nclose tox0, and its variance will be relatively large—close to that of\\nan individual yi. The bias will tend to be small, again because each\\nof theE(yi) =f(xi) should be close to f(x0).\\n•If the window is wide, the variance of ˆf(x0) will be small relative to\\nthe variance of any yi, because of the eﬀects of averaging. The bias\\nwill be higher, because we are now using observations xifurther from\\nx0, and there is no guarantee that f(xi) will be close to f(x0).\\nSimilar arguments apply to local regression estimates, say local linear: as\\nthe width goes to zero, the estimates approach a piecewise-l inear function\\nthat interpolates the training data1; as the width gets inﬁnitely large, the\\nﬁt approaches the global linear least-squares ﬁt to the data .\\nThe discussion in Chapter 5 on selecting the regularization parameter for\\nsmoothing splines applies here, and will not be repeated. Lo cal regression\\nsmoothers are linear estimators; the smoother matrix in ˆf=Sλyis built up\\nfromtheequivalentkernels(6.8),andhas ijthentry{Sλ}ij=li(xj).Leave-\\none-out cross-validation is particularly simple (Exercis e 6.7), as is general-\\nized cross-validation, Cp(Exercise 6.10), and k-fold cross-validation. The\\neﬀective degrees of freedom is again deﬁned as trace( Sλ), and can be used\\nto calibrate the amount of smoothing. Figure 6.7 compares th e equivalent\\nkernels for a smoothing spline and local linear regression. The local regres-\\nsion smoother has a span of 40%, which results in df = trace( Sλ) = 5.86.\\nThe smoothing spline was calibrated to have the same df, and t heir equiv-\\nalent kernels are qualitatively quite similar.\\n1With uniformly spaced xi; with irregularly spaced xi, the behavior can deteriorate.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 217}),\n",
       " Document(page_content='200 6. Kernel Smoothing Methods\\n6.3 Local Regression in IRp\\nKernel smoothing and local regression generalize very natu rally to two or\\nmore dimensions. The Nadaraya–Watson kernel smoother ﬁts a constant\\nlocally with weights supplied by a p-dimensional kernel. Local linear re-\\ngression will ﬁt a hyperplane locally in X, by weighted least squares, with\\nweights supplied by a p-dimensional kernel. It is simple to implement and\\nis generally preferred to the local constant ﬁt for its super ior performance\\non the boundaries.\\nLetb(X) be a vector of polynomial terms in Xof maximum degree d.\\nFor example, with d= 1 andp= 2 we get b(X) = (1,X1,X2); withd= 2\\nwe getb(X) = (1,X1,X2,X2\\n1,X2\\n2,X1X2); and trivially with d= 0 we get\\nb(X) = 1. At each x0∈IRpsolve\\nmin\\nβ(x0)N∑\\ni=1Kλ(x0,xi)(yi−b(xi)Tβ(x0))2(6.12)\\nto producethe ﬁt ˆf(x0) =b(x0)Tˆβ(x0). Typically the kernel will be aradial\\nfunction, such as the radial Epanechnikov or tri-cube kerne l\\nKλ(x0,x) =D(||x−x0||\\nλ)\\n, (6.13)\\nwhere||·||is the Euclidean norm. Since the Euclidean norm depends on th e\\nunits in each coordinate, it makes most sense to standardize each predictor,\\nfor example, to unit standard deviation, prior to smoothing .\\nWhile boundary eﬀects are a problem in one-dimensional smoo thing,\\nthey are a much bigger problem in two or higher dimensions, si nce the\\nfraction of points on the boundary is larger. In fact, one of t he manifesta-\\ntions of the curse of dimensionality is that the fraction of p oints close to the\\nboundary increases to one as the dimension grows. Directly m odifying the\\nkernel to accommodate two-dimensional boundaries becomes very messy,\\nespecially for irregular boundaries. Local polynomial reg ression seamlessly\\nperforms boundary correction to the desired order in any dim ensions. Fig-\\nure 6.8 illustrates local linear regression on some measure ments from an\\nastronomical study with an unusual predictor design (star- shaped). Here\\nthe boundary is extremely irregular, and the ﬁtted surface m ust also inter-\\npolateoverregionsofincreasingdatasparsityasweapproa chtheboundary.\\nLocal regression becomes less useful in dimensions much hig her than two\\nor three. We have discussed in some detail the problems of dim ensional-\\nity, for example, in Chapter 2. It is impossible to simultane ously main-\\ntain localness (⇒low bias) and a sizable sample in the neighborhood ( ⇒\\nlow variance) as the dimension increases, without the total sample size in-\\ncreasing exponentially in p. Visualization of ˆf(X) also becomes diﬃcult in\\nhigher dimensions, and this is often one of the primary goals of smoothing.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 218}),\n",
       " Document(page_content='6.4 Structured Local Regression Models in IRp201\\nEast-WestSouth-NorthVelocity\\nEast-WestSouth-NorthVelocity\\nFIGURE 6.8. The left panel shows three-dimensional data, where the respo nse\\nis the velocity measurements on a galaxy, and the two predictor s record positions\\non the celestial sphere. The unusual “star”-shaped design in dicates the way the\\nmeasurements were made, and results in an extremely irregular b oundary. The\\nright panel shows the results of local linear regression smooth ing inIR2, using a\\nnearest-neighbor window with 15%of the data.\\nAlthough the scatter-cloud and wire-frame pictures in Figu re 6.8 look at-\\ntractive, it is quite diﬃcult to interpret the results excep t at a gross level.\\nFrom a data analysis perspective, conditional plots are far more useful.\\nFigure 6.9 shows an analysis of some environmental data with three pre-\\ndictors. The trellisdisplay here shows ozone as a function of radiation,\\nconditioned on the other two variables, temperature and win d speed. How-\\never, conditioning on the value of a variable really implies local to that\\nvalue (as in local regression). Above each of the panels in Fi gure 6.9 is an\\nindication of the range of values present in that panel for ea ch of the condi-\\ntioning values. In the panel itself the data subsets are disp layed (response\\nversus remaining variable), and a one-dimensional local li near regression is\\nﬁt to the data. Although this is not quite the same as looking a t slices of\\na ﬁtted three-dimensional surface, it is probably more usef ul in terms of\\nunderstanding the joint behavior of the data.\\n6.4 Structured Local Regression Models in IRp\\nWhen the dimension to sample-size ratio is unfavorable, loc al regression\\ndoes not help us much, unless we are willing to make some struc tural as-\\nsumptions about the model. Much of this book is about structu red regres-\\nsion and classiﬁcation models. Here we focus on some approac hes directly\\nrelated to kernel methods.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 219}),\n",
       " Document(page_content='202 6. Kernel Smoothing Methods\\n12345TempWind\\n0 50 150 250TempWind\\nTempWind\\n0 50 150 250TempWindTempWind\\nTempWind\\nTempWind\\n12345TempWind12345TempWind\\nTempWind\\nTempWind\\nTempWindTempWind\\nTempWind0 50 150 250\\nTempWind\\n12345TempWind0 50 150 250\\nSolar Radiation (langleys)Cube Root Ozone (cube root ppb)\\nFIGURE 6.9. Three-dimensional smoothing example. The response is (cube -root\\nof) ozone concentration, and the three predictors are tempe rature, wind speed and\\nradiation. The trellisdisplay shows ozone as a function of radiation, conditioned\\non intervals of temperature and wind speed (indicated by dark er green or orange\\nshaded bars). Each panel contains about 40%of the range of each of the condi-\\ntioned variables. The curve in each panel is a univariate local linear regression,\\nﬁt to the data in the panel.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 220}),\n",
       " Document(page_content='6.4 Structured Local Regression Models in IRp203\\n6.4.1 Structured Kernels\\nOne line of approach is to modify the kernel. The default sphe rical ker-\\nnel (6.13) gives equal weight to each coordinate, and so a nat ural default\\nstrategy is to standardize each variable to unit standard de viation. A more\\ngeneral approach is to use a positive semideﬁnite matrix Ato weigh the\\ndiﬀerent coordinates:\\nKλ,A(x0,x) =D((x−x0)TA(x−x0)\\nλ)\\n. (6.14)\\nEntire coordinates or directions can be downgraded or omitt ed by imposing\\nappropriate restrictions on A. For example, if Ais diagonal, then we can\\nincrease or decrease the inﬂuence of individual predictors Xjby increasing\\nor decreasing Ajj. Often the predictors are many and highly correlated,\\nsuchasthosearisingfromdigitizedanalogsignalsorimage s.Thecovariance\\nfunction of the predictors can be used to tailor a metric Athat focuses less,\\nsay, on high-frequency contrasts (Exercise 6.4). Proposal s have been made\\nfor learning the parameters for multidimensional kernels. For example, the\\nprojection-pursuitregressionmodeldiscussedinChapter 11isofthisﬂavor,\\nwhere low-rank versions of Aimply ridge functions for ˆf(X). More general\\nmodels for Aare cumbersome, and we favor instead the structured forms\\nfor the regression function discussed next.\\n6.4.2 Structured Regression Functions\\nWe are trying to ﬁt a regression function E(Y|X) =f(X1,X2,...,X p) in\\nIRp, in which every level of interaction is potentially present . It is natural\\nto consider analysis-of-variance (ANOVA) decompositions of the form\\nf(X1,X2,...,X p) =α+∑\\njgj(Xj)+∑\\nk<ℓgkℓ(Xk,Xℓ)+···(6.15)\\nandthenintroducestructurebyeliminatingsomeofthehigh er-orderterms.\\nAdditive models assume only main eﬀect terms: f(X) =α+∑p\\nj=1gj(Xj);\\nsecond-order models will have terms with interactions of or der at most\\ntwo, and so on. In Chapter 9, we describe iterative backﬁtting algorithms\\nfor ﬁtting such low-order interaction models. In the additi ve model, for\\nexample, if all but the kth term is assumed known, then we can estimate gk\\nbylocalregressionof Y−∑\\nj̸=kgj(Xj)onXk.Thisisdoneforeachfunction\\nin turn, repeatedly, until convergence. The important deta il is that at any\\nstage, one-dimensional local regression is all that is need ed. The same ideas\\ncan be used to ﬁt low-dimensional ANOVA decompositions.\\nAn important special case of these structured models are the class of\\nvarying coeﬃcient models . Suppose, for example, that we divide the ppre-\\ndictors inXinto a set (X1,X2,...,X q) withq <p, and the remainder of', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 221}),\n",
       " Document(page_content='204 6. Kernel Smoothing Methods\\n1012141618202224DepthFemale\\n20 30 40 50 60DepthFemale\\nDepthFemale\\n20 30 40 50 60DepthFemale\\nDepthFemale\\n20 30 40 50 60DepthFemaleDepthMale\\nDepthMale20 30 40 50 60\\nDepthMale\\nDepthMale20 30 40 50 60\\nDepthMale\\n1012141618202224DepthMale20 30 40 50 60\\nAgeDiameterAortic Diameter vs Age\\nFIGURE 6.10. In each panel the aorta diameter is modeled as a linear func-\\ntion ofage. The coeﬃcients of this model vary with genderanddepthdown\\ntheaorta(left is near the top, right is low down). There is a clear trend in the\\ncoeﬃcients of the linear model.\\nthe variables we collect in the vector Z. We then assume the conditionally\\nlinear model\\nf(X) =α(Z)+β1(Z)X1+···+βq(Z)Xq. (6.16)\\nFor givenZ, this is a linear model, but each of the coeﬃcients can vary\\nwithZ. It is natural to ﬁt such a model by locally weighted least squ ares:\\nmin\\nα(z0),β(z0)N∑\\ni=1Kλ(z0,zi)(yi−α(z0)−x1iβ1(z0)−···−xqiβq(z0))2.\\n(6.17)\\nFigure 6.10 illustrates the idea on measurements of the huma n aorta.\\nA longstanding claim has been that the aorta thickens with age. Here we\\nmodel the diameter of the aorta as a linear function of age, but allow the\\ncoeﬃcients to vary with genderanddepthdown the aorta. We used a local\\nregression model separately for males and females. While th e aorta clearly\\ndoes thicken with age at the higher regions of the aorta, the r elationship\\nfades with distance down the aorta. Figure 6.11 shows the int ercept and\\nslope as a function of depth.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 222}),\n",
       " Document(page_content='6.5 Local Likelihood and Other Models 205\\nMaleAge Intercept\\nDistance Down AortaAge Slope\\n0.0 0.2 0.4 0.6 0.8 1.0Female\\n14 16 18 20\\nDistance Down Aorta\\n0.0 0.4 0.8 1.2\\n0.0 0.2 0.4 0.6 0.8 1.0\\nFIGURE 6.11. The intercept and slope of ageas a function of distance down\\nthe aorta, separately for males and females. The yellow bands ind icate one stan-\\ndard error.\\n6.5 Local Likelihood and Other Models\\nThe concept of local regression and varying coeﬃcient model s is extremely\\nbroad: any parametric model can be made local if the ﬁtting me thod ac-\\ncommodates observation weights. Here are some examples:\\n•Associated with each observation yiis a parameter θi=θ(xi) =xT\\niβ\\nlinear in the covariate(s) xi, and inference for βis based on the log-\\nlikelihoodl(β) =∑N\\ni=1l(yi,xT\\niβ). We can model θ(X) more ﬂexibly\\nby using the likelihood local to x0for inference of θ(x0) =xT\\n0β(x0):\\nl(β(x0)) =N∑\\ni=1Kλ(x0,xi)l(yi,xT\\niβ(x0)).\\nMany likelihood models, in particular the family of general ized linear\\nmodels including logistic and log-linear models, involve t he covariates\\ninalinearfashion.Locallikelihoodallowsarelaxationfr omaglobally\\nlinear model to one that is locally linear.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 223}),\n",
       " Document(page_content='206 6. Kernel Smoothing Methods\\n•As above, except diﬀerent variables are associated with θfrom those\\nused for deﬁning the local likelihood:\\nl(θ(z0)) =N∑\\ni=1Kλ(z0,zi)l(yi,η(xi,θ(z0))).\\nFor example, η(x,θ) =xTθcould be a linear model in x. This will ﬁt\\na varying coeﬃcient model θ(z) by maximizing the local likelihood.\\n•Autoregressive time series models of order khave the form yt=\\nβ0+β1yt−1+β2yt−2+···+βkyt−k+εt. Denoting the lag setby\\nzt= (yt−1,yt−2,...,y t−k), the model looks like a standard linear\\nmodelyt=zT\\ntβ+εt, and is typically ﬁt by least squares. Fitting\\nby local least squares with a kernel K(z0,zt) allows the model to\\nvary according to the short-term history of the series. This is to be\\ndistinguished from the more traditional dynamic linear mod els that\\nvary by windowing time.\\nAs an illustration of local likelihood, we consider the loca l version of the\\nmulticlass linear logistic regression model (4.36) of Chap ter 4. The data\\nconsistoffeatures xiandanassociatedcategoricalresponse gi∈{1,2,...,J},\\nand the linear model has the form\\nPr(G=j|X=x) =eβj0+βT\\njx\\n1+∑J−1\\nk=1eβk0+βT\\nkx. (6.18)\\nThe local log-likelihood for this Jclass model can be written\\nN∑\\ni=1Kλ(x0,xi){\\nβgi0(x0)+βgi(x0)T(xi−x0)\\n−log[\\n1+J−1∑\\nk=1exp(\\nβk0(x0)+βk(x0)T(xi−x0))]}\\n.\\n(6.19)\\nNotice that\\n•we have used gias a subscript in the ﬁrst line to pick out the appro-\\npriate numerator;\\n•βJ0= 0 andβJ= 0 by the deﬁnition of the model;\\n•we have centered the local regressions at x0, so that the ﬁtted poste-\\nrior probabilities at x0are simply\\nˆPr(G=j|X=x0) =eˆβj0(x0)\\n1+∑J−1\\nk=1eˆβk0(x0). (6.20)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 224}),\n",
       " Document(page_content='6.5 Local Likelihood and Other Models 207\\nSystolic Blood PressurePrevalence CHD\\n100 140 180 2200.0 0.2 0.4 0.6 0.8 1.0\\nObesityPrevalence CHD\\n15 25 35 450.0 0.2 0.4 0.6 0.8 1.0\\nFIGURE 6.12. Each plot shows the binary response CHD (coronary heart dis-\\nease) as a function of a risk factor for the South African hear t disease data.\\nFor each plot we have computed the ﬁtted prevalence of CHD using a local linear\\nlogistic regression model. The unexpected increase in the pre valence of CHD at\\nthe lower ends of the ranges is because these are retrospectiv e data, and some of\\nthe subjects had already undergone treatment to reduce their blood pressure and\\nweight. The shaded region in the plot indicates an estimated p ointwise standard\\nerror band.\\nThis model can be used for ﬂexible multiclass classiﬁcation in moderately\\nlow dimensions, although successes have been reported with the high-\\ndimensional ZIP-code classiﬁcation problem. Generalized additive models\\n(Chapter 9) using kernel smoothing methods are closely rela ted, and avoid\\ndimensionality problems by assuming an additive structure for the regres-\\nsion function.\\nAs a simple illustration we ﬁt a two-class local linear logis tic model to\\nthe heart disease data of Chapter 4. Figure 6.12 shows the uni variate local\\nlogistic models ﬁt to two of the risk factors (separately). T his is a useful\\nscreeningdevicefordetectingnonlinearities,whentheda tathemselveshave\\nlittle visual information to oﬀer. In this case an unexpecte d anomaly is\\nuncovered in the data, which may have gone unnoticed with tra ditional\\nmethods.\\nSinceCHDis a binary indicator, we could estimate the conditional pre va-\\nlence Pr(G=j|x0) by simply smoothing this binary response directly with-\\nout resorting to a likelihood formulation. This amounts to ﬁ tting a locally\\nconstant logistic regression model (Exercise 6.5). In orde r to enjoy the bias-\\ncorrection of local-linear smoothing, it is more natural to operate on the\\nunrestricted logit scale.\\nTypically with logistic regression, we compute parameter e stimates as\\nwell as their standard errors. This can be done locally as wel l, and so', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 225}),\n",
       " Document(page_content='208 6. Kernel Smoothing Methods\\nSystolic Blood Pressure (for CHD group)Density Estimate\\n100 120 140 160 180 200 2200.0 0.005 0.010 0.015 0.020\\nFIGURE 6.13. A kernel density estimate for systolic blood pressure (for the\\nCHD group). The density estimate at each point is the average contribution from\\neach of the kernels at that point. We have scaled the kernels down by a factor of\\n10 to make the graph readable.\\nwe can produce, as shown in the plot, estimated pointwise sta ndard-error\\nbands about our ﬁtted prevalence.\\n6.6 Kernel Density Estimation and Classiﬁcation\\nKernel density estimation is an unsupervised learning proc edure, which\\nhistorically precedes kernel regression. It also leads nat urally to a simple\\nfamily of procedures for nonparametric classiﬁcation.\\n6.6.1 Kernel Density Estimation\\nSuppose we have a random sample x1,...,x Ndrawn from a probability\\ndensityfX(x), and we wish to estimate fXat a pointx0. For simplicity we\\nassume for now that X∈IR. Arguing as before, a natural local estimate\\nhas the form\\nˆfX(x0) =#xi∈N(x0)\\nNλ, (6.21)\\nwhereN(x0) is a small metric neighborhood around x0of widthλ. This\\nestimate is bumpy, and the smooth Parzenestimate is preferred\\nˆfX(x0) =1\\nNλN∑\\ni=1Kλ(x0,xi), (6.22)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 226}),\n",
       " Document(page_content='6.6 Kernel Density Estimation and Classiﬁcation 209\\nSystolic Blood PressureDensity Estimates\\n100 140 180 2200.0 0.010 0.020CHD\\nno CHD\\nSystolic Blood PressurePosterior Estimate\\n100 140 180 2200.0 0.2 0.4 0.6 0.8 1.0\\nFIGURE 6.14. The left panel shows the two separate density estimates for\\nsystolic blood pressure in the CHD versus no-CHD groups, using a Gaussian\\nkernel density estimate in each. The right panel shows the es timated posterior\\nprobabilities for CHD, using (6.25).\\nbecause it counts observations close to x0with weights that decrease with\\ndistance from x0.Inthis caseapopular choice for Kλis theGaussiankernel\\nKλ(x0,x) =φ(|x−x0|/λ). Figure 6.13 shows a Gaussian kernel density ﬁt\\ntothesamplevaluesfor systolic blood pressure fortheCHDgroup.Letting\\nφλdenote the Gaussian density with mean zero and standard-dev iationλ,\\nthen (6.22) has the form\\nˆfX(x) =1\\nNN∑\\ni=1φλ(x−xi)\\n= (ˆF ⋆φλ)(x), (6.23)\\nthe convolution of the sample empirical distribution ˆFwithφλ. The dis-\\ntribution ˆF(x) puts mass 1 /Nat each of the observed xi, and is jumpy; in\\nˆfX(x) we have smoothed ˆFby adding independent Gaussian noise to each\\nobservation xi.\\nThe Parzen density estimate is the equivalent of the local av erage, and\\nimprovements have been proposed along the lines of local reg ression [on the\\nlog scale for densities; see Loader (1999)]. We will not purs ue these here.\\nIn IRpthe natural generalization of the Gaussian density estimat e amounts\\nto using the Gaussian product kernel in (6.23),\\nˆfX(x0) =1\\nN(2λ2π)p\\n2N∑\\ni=1e−1\\n2(||xi−x0||/λ)2. (6.24)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 227}),\n",
       " Document(page_content='210 6. Kernel Smoothing Methods\\n0.00.51.0\\nFIGURE 6.15. The population class densities may have interesting structur e\\n(left) that disappears when the posterior probabilities are f ormed (right).\\n6.6.2 Kernel Density Classiﬁcation\\nOnecanusenonparametricdensityestimatesforclassiﬁcat ioninastraight-\\nforward fashion using Bayes’ theorem. Suppose for a Jclass problem we ﬁt\\nnonparametric density estimates ˆfj(X), j= 1,...,Jseparately in each of\\nthe classes, and we also have estimates of the class priors ˆ πj(usually the\\nsample proportions). Then\\nˆPr(G=j|X=x0) =ˆπjˆfj(x0)∑J\\nk=1ˆπkˆfk(x0). (6.25)\\nFigure 6.14 uses this method to estimate the prevalence of CH D for the\\nheart risk factor study, and should be compared with the left panel of Fig-\\nure 6.12. The main diﬀerence occurs in the region of high SBP i n the right\\npanel of Figure 6.14. In this region the data are sparse for bo th classes, and\\nsince the Gaussian kernel density estimates use metric kern els, the density\\nestimates are low and of poor quality (high variance) in thes e regions. The\\nlocal logistic regression method (6.20) uses the tri-cube k ernel withk-NN\\nbandwidth; this eﬀectively widens the kernel in this region , and makes use\\nof the local linear assumption to smooth out the estimate (on the logit\\nscale).\\nIf classiﬁcation is the ultimate goal, then learning the sep arate class den-\\nsities well may be unnecessary, and can in fact be misleading . Figure 6.15\\nshows an example where the densities are both multimodal, bu t the pos-\\nterior ratio is quite smooth. In learning the separate densi ties from data,\\none might decide to settle for a rougher, high-variance ﬁt to capture these\\nfeatures, which are irrelevant for the purposes of estimati ng the posterior\\nprobabilities. In fact, if classiﬁcation is the ultimate go al, then we need only\\nto estimate the posterior well near the decision boundary (f or two classes,\\nthis is the set{x|Pr(G= 1|X=x) =1\\n2}).\\n6.6.3 The Naive Bayes Classiﬁer\\nThis is a technique that has remained popular over the years, despite its\\nname (also known as “Idiot’s Bayes”!) It is especially appro priate when', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 228}),\n",
       " Document(page_content='6.6 Kernel Density Estimation and Classiﬁcation 211\\nthe dimension pof the feature space is high, making density estimation\\nunattractive. The naive Bayes model assumes that given a cla ssG=j, the\\nfeaturesXkare independent:\\nfj(X) =p∏\\nk=1fjk(Xk). (6.26)\\nWhile this assumption is generally not true, it does simplif y the estimation\\ndramatically:\\n•The individual class-conditional marginal densities fjkcan each be\\nestimated separately using one-dimensional kernel densit y estimates.\\nThis is in fact a generalization of the original naive Bayes p rocedures,\\nwhich used univariate Gaussians to represent these margina ls.\\n•If a component XjofXis discrete, then an appropriate histogram\\nestimate can be used. This provides a seamless way of mixing v ariable\\ntypes in a feature vector.\\nDespite these rather optimistic assumptions, naive Bayes c lassiﬁers often\\noutperform far more sophisticated alternatives. The reaso ns are related to\\nFigure 6.15: although the individual class density estimat es may be biased,\\nthis bias might not hurt the posterior probabilities as much , especially\\nnear the decision regions. In fact, the problem may be able to withstand\\nconsiderable bias for the savings in variance such a “naive” assumption\\nearns.\\nStarting from (6.26) we can derive the logit-transform (usi ng classJas\\nthe base):\\nlogPr(G=ℓ|X)\\nPr(G=J|X)= logπℓfℓ(X)\\nπJfJ(X)\\n= logπℓ∏p\\nk=1fℓk(Xk)\\nπJ∏p\\nk=1fJk(Xk)\\n= logπℓ\\nπJ+p∑\\nk=1logfℓk(Xk)\\nfJk(Xk)\\n=αℓ+p∑\\nk=1gℓk(Xk).(6.27)\\nThishastheformofa generalized additive model ,whichisdescribedinmore\\ndetail in Chapter 9. The models are ﬁt in quite diﬀerent ways t hough; their\\ndiﬀerences are explored in Exercise 6.9. The relationship b etween naive\\nBayes and generalized additive models is analogous to that b etween linear\\ndiscriminant analysis and logistic regression (Section 4. 4.5).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 229}),\n",
       " Document(page_content='212 6. Kernel Smoothing Methods\\n6.7 Radial Basis Functions and Kernels\\nIn Chapter 5, functions are represented as expansions in bas is functions:\\nf(x) =∑M\\nj=1βjhj(x). The art of ﬂexible modeling using basis expansions\\nconsists of picking an appropriate family of basis function s, and then con-\\ntrolling the complexity of the representation by selection , regularization, or\\nboth. Some of the families of basis functions have elements t hat are deﬁned\\nlocally; for example, B-splines are deﬁned locally in IR. If more ﬂexibility\\nis desired in a particular region, then that region needs to b e represented\\nby more basis functions (which in the case of B-splines translates to more\\nknots). Tensor products of IR-local basis functions delive r basis functions\\nlocal in IRp. Not all basis functions are local—for example, the truncate d\\npower bases for splines, or the sigmoidal basis functions σ(α0+αx) used\\nin neural-networks (see Chapter 11). The composed function f(x) can nev-\\nertheless show local behavior, because of the particular si gns and values\\nof the coeﬃcients causing cancellations of global eﬀects. F or example, the\\ntruncated power basis has an equivalent B-spline basis for the same space\\nof functions; the cancellation is exact in this case.\\nKernel methods achieve ﬂexibility by ﬁtting simple models i n a region\\nlocal to the target point x0. Localization is achieved via a weighting kernel\\nKλ, and individual observations receive weights Kλ(x0,xi).\\nRadial basis functions combine these ideas, by treating the kernel func-\\ntionsKλ(ξ,x) as basis functions. This leads to the model\\nf(x) =M∑\\nj=1Kλj(ξj,x)βj\\n=M∑\\nj=1D(||x−ξj||\\nλj)\\nβj, (6.28)\\nwhere each basis element is indexed by a location or prototype parameterξj\\nand a scale parameter λj. A popular choice for Dis the standard Gaussian\\ndensity function. There are several approaches to learning the parameters\\n{λj,ξj,βj}, j= 1,...,M. For simplicity we will focus on least squares\\nmethods for regression, and use the Gaussian kernel.\\n•Optimize the sum-of-squares with respect to all the paramet ers:\\nmin\\n{λj,ξj,βj}M\\n1N∑\\ni=1\\uf8eb\\n\\uf8edyi−β0−M∑\\nj=1βjexp{\\n−(xi−ξj)T(xi−ξj)\\nλ2\\nj}\\uf8f6\\n\\uf8f82\\n.\\n(6.29)\\nThis model is commonly referred to as an RBF network, an alter na-\\ntive to the sigmoidal neural network discussed in Chapter 11 ; theξj\\nandλjplaying the role of the weights. This criterion is nonconvex', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 230}),\n",
       " Document(page_content='6.7 Radial Basis Functions and Kernels 2130 2 4 6 8 0.0 0.4 0.8 1.2\\nFIGURE 6.16. Gaussian radial basis functions in IRwith ﬁxed width can leave\\nholes (top panel). Renormalized Gaussian radial basis function s avoid this prob-\\nlem, and produce basis functions similar in some respects to B-splines.\\nwith multiple local minima, and the algorithms for optimiza tion are\\nsimilar to those used for neural networks.\\n•Estimate the{λj,ξj}separately from the βj. Given the former, the\\nestimation of the latter is a simple least squares problem. O ften the\\nkernel parameters λjandξjare chosen in an unsupervised way using\\ntheXdistribution alone. One of the methods is to ﬁt a Gaussian\\nmixture density model to the training xi, which provides both the\\ncentersξjand the scales λj. Other even more adhoc approaches use\\nclustering methods to locate the prototypes ξj, and treat λj=λ\\nas a hyper-parameter. The obvious drawback of these approac hes is\\nthat the conditional distribution Pr( Y|X) and in particular E(Y|X)\\nis having no say in where the action is concentrated. On the po sitive\\nside, they are much simpler to implement.\\nWhile it would seem attractive to reduce the parameter set an d assume\\na constant value for λj=λ, this can have an undesirable side eﬀect of\\ncreating holes—regions of IRpwhere none of the kernels has appreciable\\nsupport, as illustrated in Figure 6.16 (upper panel). Renormalized radial\\nbasis functions,\\nhj(x) =D(||x−ξj||/λ)∑M\\nk=1D(||x−ξk||/λ), (6.30)\\navoid this problem (lower panel).\\nThe Nadaraya–Watson kernel regression estimator (6.2) in I Rpcan be\\nviewed as an expansion in renormalized radial basis functio ns,\\nˆf(x0) =∑N\\ni=1yiKλ(x0,xi)∑N\\ni=1Kλ(x0,xi)\\n=∑N\\ni=1yihi(x0) (6.31)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 231}),\n",
       " Document(page_content='214 6. Kernel Smoothing Methods\\nwith a basis function hilocated at every observation and coeﬃcients yi;\\nthat is,ξi=xi,ˆβi=yi, i= 1,...,N.\\nNote the similarity between the expansion (6.31) and the sol ution (5.50)\\non page 169 to the regularization problem induced by the kern elK. Radial\\nbasis functions form the bridge between the modern “kernel m ethods” and\\nlocal ﬁtting technology.\\n6.8 Mixture Models for Density Estimation and\\nClassiﬁcation\\nThemixturemodelisausefultoolfordensityestimation,an dcanbeviewed\\nas a kind of kernel method. The Gaussian mixture model has the form\\nf(x) =M∑\\nm=1αmφ(x;µm,Σm) (6.32)\\nwith mixing proportions αm,∑\\nmαm= 1, and each Gaussian density has\\na meanµmand covariance matrix Σm. In general, mixture models can use\\nany component densities in place of the Gaussian in (6.32): t he Gaussian\\nmixture model is by far the most popular.\\nThe parameters are usually ﬁt by maximum likelihood, using t he EM\\nalgorithm as described in Chapter 8. Some special cases aris e:\\n•If the covariance matrices are constrained to be scalar: Σm=σmI,\\nthen (6.32) has the form of a radial basis expansion.\\n•If in addition σm=σ >0 is ﬁxed, and M↑N, then the max-\\nimum likelihood estimate for (6.32) approaches the kernel d ensity\\nestimate (6.22) where ˆ αm= 1/Nand ˆµm=xm.\\nUsing Bayes’ theorem, separate mixture densities in each cl ass lead to ﬂex-\\nible models for Pr( G|X); this is taken up in some detail in Chapter 12.\\nFigure 6.17 shows an application of mixtures to the heart dis ease risk-\\nfactor study. In the top row are histograms of Agefor theno CHDandCHD\\ngroups separately, and then combined on the right. Using the combined\\ndata, we ﬁt a two-component mixture of the form (6.32) with th e (scalars)\\nΣ1andΣ2not constrained to be equal. Fitting was done via the EM\\nalgorithm (Chapter 8): note that the procedure does not use k nowledge of\\ntheCHDlabels. The resulting estimates were\\nˆµ1= 36.4,ˆΣ1= 157.7,ˆα1= 0.7,\\nˆµ2= 58.0,ˆΣ2= 15.6, ˆα2= 0.3.\\nThe component densities φ(ˆµ1,ˆΣ1) andφ(ˆµ2,ˆΣ2) are shown in the lower-\\nleft and middle panels. The lower-right panel shows these co mponent den-\\nsities (orange and blue) along with the estimated mixture de nsity (green).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 232}),\n",
       " Document(page_content='6.8 Mixture Models for Density Estimation and Classiﬁcation 215\\nNo CHD\\nAgeCount\\n20 30 40 50 600 5 10 15 20CHD\\nAgeCount\\n20 30 40 50 600 5 10 15Combined\\nAgeCount\\n20 30 40 50 600 5 10 15 20 25 30\\n20 30 40 50 600.00 0.02 0.04 0.06 0.08 0.10\\nAgeMixture Estimate\\n20 30 40 50 600.00 0.02 0.04 0.06 0.08 0.10\\nAgeMixture Estimate\\n20 30 40 50 600.00 0.02 0.04 0.06 0.08 0.10\\nAgeMixture Estimate\\nFIGURE 6.17. Application of mixtures to the heart disease risk-factor stu dy.\\n(Top row:) Histograms of Agefor theno CHDandCHDgroups separately, and\\ncombined. (Bottom row:) estimated component densities fro m a Gaussian mix-\\nture model, (bottom left, bottom middle); (bottom right:) Esti mated component\\ndensities (blue and orange) along with the estimated mixture d ensity (green). The\\norange density has a very large standard deviation, and appro ximates a uniform\\ndensity.\\nThe mixture model also provides an estimate of the probabili ty that\\nobservation ibelongs to component m,\\nˆrim=ˆαmφ(xi;ˆµm,ˆΣm)∑M\\nk=1ˆαkφ(xi;ˆµk,ˆΣk), (6.33)\\nwherexiisAgein our example. Suppose we threshold each value ˆ ri2and\\nhence deﬁne ˆδi=I(ˆri2>0.5). Then we can compare the classiﬁcation of\\neach observation by CHDand the mixture model:\\nMixture model\\nˆδ= 0ˆδ= 1\\nCHD No 232 70\\nYes 76 84\\nAlthough the mixture model did not use the CHDlabels, it has done a fair\\njob in discovering the two CHDsubpopulations. Linear logistic regression,\\nusing the CHDas a response, achieves the same error rate (32%) when ﬁt to\\nthese data using maximum-likelihood (Section 4.4).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 233}),\n",
       " Document(page_content='216 6. Kernel Smoothing Methods\\n6.9 Computational Considerations\\nKernel and local regression and density estimation are memory-based meth-\\nods: the model is the entire training data set, and the ﬁtting is done at\\nevaluation or prediction time. For many real-time applicat ions, this can\\nmake this class of methods infeasible.\\nThe computational cost to ﬁt at a single observation x0isO(N) ﬂops,\\nexcept in oversimpliﬁed cases (such as square kernels). By c omparison,\\nan expansion in Mbasis functions costs O(M) for one evaluation, and\\ntypicallyM∼O(logN). Basis function methods have an initial cost of at\\nleastO(NM2+M3).\\nThe smoothing parameter(s) λfor kernel methods are typically deter-\\nmined oﬀ-line, for example using cross-validation, at a cos t ofO(N2) ﬂops.\\nPopularimplementationsoflocalregression,suchasthe loessfunctionin\\nS-PLUS and Rand the locfitprocedure (Loader, 1999), use triangulation\\nschemes to reduce the computations. They compute the ﬁt exac tly atM\\ncarefully chosen locations ( O(NM)), and then use blending techniques to\\ninterpolate the ﬁt elsewhere ( O(M) per evaluation).\\nBibliographic Notes\\nThere is a vast literature on kernel methods which we will not attempt to\\nsummarize. Rather we will point to a few good references that themselves\\nhave extensive bibliographies. Loader (1999) gives excell ent coverage of lo-\\ncal regression and likelihood, and also describes state-of -the-art software\\nfor ﬁtting these models. Fan and Gijbels (1996) cover these m odels from\\na more theoretical aspect. Hastie and Tibshirani (1990) dis cuss local re-\\ngression in the context of additive modeling. Silverman (19 86) gives a good\\noverview of density estimation, as does Scott (1992).\\nExercises\\nEx. 6.1Show that the Nadaraya–Watson kernel smooth with ﬁxed metri c\\nbandwidth λand a Gaussian kernel is diﬀerentiable. What can be said for\\nthe Epanechnikov kernel? What can be said for the Epanechnik ov kernel\\nwith adaptive nearest-neighbor bandwidth λ(x0)?\\nEx. 6.2Show that∑N\\ni=1(xi−x0)li(x0) = 0for local linear regression.Deﬁne\\nbj(x0) =∑N\\ni=1(xi−x0)jli(x0). Show that b0(x0) = 1 for local polynomial\\nregression of any degree (including local constants). Show thatbj(x0) = 0\\nfor allj∈{1,2,...,k}for local polynomial regression of degree k. What\\nare the implications of this on the bias?', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 234}),\n",
       " Document(page_content='Exercises 217\\nEx. 6.3Show that||l(x)||(Section 6.1.2) increases with the degree of the\\nlocal polynomial.\\nEx. 6.4Suppose that the ppredictorsXarise from sampling relatively\\nsmooth analog curves at puniformly spaced abscissa values. Denote by\\nCov(X|Y) =Σthe conditional covariance matrix of the predictors, and\\nassume this does not change much with Y. Discuss the nature of Maha-\\nlanobischoiceA=Σ−1for the metric in (6.14). How does this compare\\nwithA=I? How might you construct a kernel Athat (a) downweights\\nhigh-frequency components in the distance metric; (b) igno res them\\ncompletely?\\nEx. 6.5Show that ﬁtting a locally constant multinomial logit model of\\nthe form (6.19) amounts to smoothing the binary response ind icators for\\neachclassseparatelyusingaNadaraya–Watsonkernelsmoot herwithkernel\\nweightsKλ(x0,xi).\\nEx. 6.6Suppose that all you have is software for ﬁtting local regres sion,\\nbut you can specify exactly which monomials are included in t he ﬁt. How\\ncould you use this software to ﬁt a varying-coeﬃcient model i n some of the\\nvariables?\\nEx. 6.7Derive an expression for the leave-one-out cross-validate d residual\\nsum-of-squares for local polynomial regression.\\nEx. 6.8Suppose that for continuous response Yand predictor X, we model\\nthe joint density of X,Yusing a multivariate Gaussian kernel estimator.\\nNote that the kernel in this case would be the product kernel φλ(X)φλ(Y).\\nShow that the conditional mean E(Y|X) derived from this estimate is a\\nNadaraya–Watson estimator. Extend this result to classiﬁc ation by pro-\\nviding a suitable kernel for the estimation of the joint dist ribution of a\\ncontinuous Xand discrete Y.\\nEx. 6.9Explore the diﬀerences between the naive Bayes model (6.27) and\\na generalized additive logistic regression model, in terms of (a) model as-\\nsumptions and (b) estimation. If all the variables Xkare discrete, what can\\nyou say about the corresponding GAM?\\nEx.6.10Supposewehave Nsamplesgeneratedfromthemodel yi=f(xi)+\\nεi, withεiindependent and identically distributed with mean zero and\\nvarianceσ2, thexiassumed ﬁxed (non random). We estimate fusing a\\nlinear smoother (local regression, smoothing spline, etc. ) with smoothing\\nparameterλ. Thus the vector of ﬁtted values is given by ˆf=Sλy. Consider\\nthein-sample prediction error\\nPE(λ) = E1\\nNN∑\\ni=1(y∗\\ni−ˆfλ(xi))2(6.34)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 235}),\n",
       " Document(page_content='218 6. Kernel Smoothing Methods\\nfor predicting new responses at the Ninput values. Show that the aver-\\nage squared residual on the training data, ASR( λ), is a biased estimate\\n(optimistic) for PE( λ), while\\nCλ= ASR(λ)+2σ2\\nNtrace(Sλ) (6.35)\\nis unbiased.\\nEx. 6.11 Show that for the Gaussian mixture model (6.32) the likeliho od\\nis maximized at + ∞, and describe how.\\nEx. 6.12 Write a computer program to perform a local linear discrimi-\\nnant analysis. At each query point x0, the training data receive weights\\nKλ(x0,xi) from a weighting kernel, and the ingredients for the linear deci-\\nsion boundaries (see Section 4.3) are computed by weighted a verages. Try\\nout your program on the zipcodedata, and show the training and test er-\\nrorsforaseriesofﬁvepre-chosenvaluesof λ.Thezipcodedataareavailable\\nfrom the book website www-stat.stanford.edu/ElemStatLearn .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 236}),\n",
       " Document(page_content='This is page 219\\nPrinter: Opaque this\\n7\\nModel Assessment and Selection\\n7.1 Introduction\\nThegeneralization performance of a learning method relates to its predic-\\ntion capability on independent test data. Assessment of thi s performance\\nis extremely important in practice, since it guides the choi ce of learning\\nmethod or model, and gives us a measure of the quality of the ul timately\\nchosen model.\\nIn this chapter we describe and illustrate the key methods fo r perfor-\\nmance assessment, and show how they are used to select models . We begin\\nthe chapter with a discussion of the interplay between bias, variance and\\nmodel complexity.\\n7.2 Bias, Variance and Model Complexity\\nFigure 7.1 illustrates the important issue in assessing the ability of a learn-\\ningmethodtogeneralize.Considerﬁrstthecaseofaquantit ativeorinterval\\nscale response. We have a target variable Y, a vector of inputs X, and a\\nprediction model ˆf(X) that has been estimated from a training set T.\\nThe loss function for measuring errors between Yandˆf(X) is denoted by\\nL(Y,ˆf(X)). Typical choices are\\nL(Y,ˆf(X)) ={\\n(Y−ˆf(X))2squared error\\n|Y−ˆf(X)|absolute error .(7.1)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 237}),\n",
       " Document(page_content='220 7. Model Assessment and Selection\\n0 5 10 15 20 25 30 350.0 0.2 0.4 0.6 0.8 1.0 1.2\\nModel Complexity (df)Prediction ErrorHigh Bias Low Bias\\nHigh Variance Low Variance\\nFIGURE 7.1. Behavior of test sample and training sample error as the model\\ncomplexity is varied. The light blue curves show the training er rorerr, while the\\nlight red curves show the conditional test error ErrTfor100training sets of size\\n50each, as the model complexity is increased. The solid curves sh ow the expected\\ntest error Errand the expected training error E[err].\\nTest error , also referred to as generalization error , is the prediction error\\nover an independent test sample\\nErrT= E[L(Y,ˆf(X))|T] (7.2)\\nwhere both XandYare drawn randomly from their joint distribution\\n(population). Here the training set Tis ﬁxed, and test error refers to the\\nerror for this speciﬁc training set. A related quantity is th e expected pre-\\ndiction error (or expected test error)\\nErr = E[L(Y,ˆf(X))] = E[Err T]. (7.3)\\nNote that this expectation averages over everything that is random, includ-\\ning the randomness in the training set that produced ˆf.\\nFigure 7.1 shows the prediction error (light red curves) Err Tfor 100\\nsimulated training sets each of size 50. The lasso (Section 3 .4.2) was used\\nto produce the sequence of ﬁts. The solid red curve is the aver age, and\\nhence an estimate of Err.\\nEstimation of Err Twill be our goal, although we will see that Err is\\nmore amenable to statistical analysis, and most methods eﬀe ctively esti-\\nmate the expected error. It does not seem possible to estimat e conditional', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 238}),\n",
       " Document(page_content='7.2 Bias, Variance and Model Complexity 221\\nerror eﬀectively, given only the information in the same tra ining set. Some\\ndiscussion of this point is given in Section 7.12.\\nTraining error is the average loss over the training sample\\nerr =1\\nNN∑\\ni=1L(yi,ˆf(xi)). (7.4)\\nWe would like to know the expected test error of our estimated model\\nˆf. As the model becomes more and more complex, it uses the train ing\\ndata more and is able to adapt to more complicated underlying structures.\\nHence there is a decrease in bias but an increase in variance. There is some\\nintermediate model complexity that gives minimum expected test error.\\nUnfortunately training error is not a good estimate of the te st error,\\nas seen in Figure 7.1. Training error consistently decrease s with model\\ncomplexity, typically dropping to zero if we increase the mo del complexity\\nenough. However, a model with zero training error is overﬁt t o the training\\ndata and will typically generalize poorly.\\nThe story is similar for a qualitative or categorical respon seGtaking\\none ofKvalues in a setG, labeled for convenience as 1 ,2,...,K. Typically\\nwe model the probabilities pk(X) = Pr(G=k|X) (or some monotone\\ntransformations fk(X)), and then ˆG(X) = argmax kˆpk(X). In some cases,\\nsuch as 1-nearest neighbor classiﬁcation (Chapters 2 and 13 ) we produce\\nˆG(X) directly. Typical loss functions are\\nL(G,ˆG(X)) =I(G̸=ˆG(X)) (0–1 loss) , (7.5)\\nL(G,ˆp(X)) =−2K∑\\nk=1I(G=k)log ˆpk(X)\\n=−2log ˆpG(X) (−2×log-likelihood) .(7.6)\\nThe quantity−2×the log-likelihood is sometimes referred to as the\\ndeviance.\\nAgain, test error here is Err T= E[L(G,ˆG(X))|T], the population mis-\\nclassiﬁcation error of the classiﬁer trained on T, and Err is the expected\\nmisclassiﬁcation error.\\nTraining error is the sample analogue, for example,\\nerr =−2\\nNN∑\\ni=1log ˆpgi(xi), (7.7)\\nthe sample log-likelihood for the model.\\nThe log-likelihood can be used as a loss-function for genera l response\\ndensities, such as the Poisson, gamma, exponential, log-no rmal and others.\\nIf Prθ(X)(Y) is the density of Y, indexed by a parameter θ(X) that depends\\non the predictor X, then\\nL(Y,θ(X)) =−2·logPrθ(X)(Y). (7.8)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 239}),\n",
       " Document(page_content='222 7. Model Assessment and Selection\\nThe “−2” in the deﬁnition makes the log-likelihood loss for the Gau ssian\\ndistribution match squared-error loss.\\nForeaseofexposition,fortheremainderofthischapterwew illuseYand\\nf(X) to represent all of the above situations, since we focus mai nly on the\\nquantitative response (squared-error loss) setting. For t he other situations,\\nthe appropriate translations are obvious.\\nIn this chapter we describe a number of methods for estimatin g the\\nexpected test error for a model. Typically our model will hav e a tuning\\nparameter or parameters αand so we can write our predictions as ˆfα(x).\\nThe tuning parameter varies the complexity of our model, and we wish to\\nﬁnd the value of αthat minimizes error, that is, produces the minimum of\\nthe average test error curve in Figure 7.1. Having said this, for brevity we\\nwill often suppress the dependence of ˆf(x) onα.\\nIt is important to note that there are in fact two separate goa ls that we\\nmight have in mind:\\nModel selection: estimating the performance of diﬀerent models in order\\nto choose the best one.\\nModel assessment: having chosen a ﬁnal model, estimating its predic-\\ntion error (generalization error) on new data.\\nIf we are in a data-rich situation, the best approach for both problems is\\nto randomly divide the dataset into three parts: a training s et, a validation\\nset, and a test set. The training set is used to ﬁt the models; t he validation\\nset is used to estimate prediction error for model selection ; the test set is\\nused for assessment of the generalization error of the ﬁnal c hosen model.\\nIdeally, the test set should be kept in a “vault,” and be broug ht out only\\nat the end of the data analysis. Suppose instead that we use th e test-set\\nrepeatedly, choosing the model with smallest test-set erro r. Then the test\\nset error of the ﬁnal chosen model will underestimate the tru e test error,\\nsometimes substantially.\\nIt is diﬃcult to give a general rule on how to choose the number of\\nobservations in each of the three parts, as this depends on th e signal-to-\\nnoise ratio in the data and the training sample size. A typica l split might\\nbe 50% for training, and 25% each for validation and testing:\\nTest Train Validation TestTrain Validation Test Validation Train Validation TestTrain\\nThe methods in this chapter are designed for situations wher e there is\\ninsuﬃcient data to split it into three parts. Again it is too d iﬃcult to give\\na general rule on how much training data is enough; among othe r things,\\nthis depends on the signal-to-noise ratio of the underlying function, and\\nthe complexity of the models being ﬁt to the data.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 240}),\n",
       " Document(page_content='7.3 The Bias–Variance Decomposition 223\\nThe methods of this chapter approximate the validation step either an-\\nalytically (AIC, BIC, MDL, SRM) or by eﬃcient sample re-use ( cross-\\nvalidation and the bootstrap). Besides their use in model se lection, we also\\nexamine to what extent each method provides a reliable estim ate of test\\nerror of the ﬁnal chosen model.\\nBefore jumping into these topics, we ﬁrst explore in more det ail the\\nnature of test error and the bias–variance tradeoﬀ.\\n7.3 The Bias–Variance Decomposition\\nAs in Chapter 2, if we assume that Y=f(X) +εwhere E(ε) = 0 and\\nVar(ε) =σ2\\nε, we can derive an expression for the expected prediction err or\\nof a regression ﬁt ˆf(X) at an input point X=x0, using squared-error loss:\\nErr(x0) =E[(Y−ˆf(x0))2|X=x0]\\n=σ2\\nε+[Eˆf(x0)−f(x0)]2+E[ˆf(x0)−Eˆf(x0)]2\\n=σ2\\nε+Bias2(ˆf(x0))+Var( ˆf(x0))\\n= Irreducible Error+Bias2+Variance. (7.9)\\nThe ﬁrst term is the variance of the target around its true mea nf(x0), and\\ncannot be avoided no matter how well we estimate f(x0), unlessσ2\\nε= 0.\\nThe second term is the squared bias, the amount by which the av erage of\\nour estimate diﬀers from the true mean; the last term is the va riance; the\\nexpected squared deviation of ˆf(x0) around its mean. Typically the more\\ncomplex we make the model ˆf, the lower the (squared) bias but the higher\\nthe variance.\\nFor thek-nearest-neighbor regression ﬁt, these expressions have t he sim-\\nple form\\nErr(x0) =E[(Y−ˆfk(x0))2|X=x0]\\n=σ2\\nε+[\\nf(x0)−1\\nkk∑\\nℓ=1f(x(ℓ))]2\\n+σ2\\nε\\nk.(7.10)\\nHere we assume for simplicity that training inputs xiare ﬁxed, and the ran-\\ndomness arises from the yi. The number of neighbors kis inversely related\\nto the model complexity. For small k, the estimate ˆfk(x) can potentially\\nadapt itself better to the underlying f(x). As we increase k, the bias—the\\nsquared diﬀerence between f(x0) and the average of f(x) at thek-nearest\\nneighbors—will typically increase, while the variance decr eases.\\nFor a linear model ﬁt ˆfp(x) =xTˆβ, where the parameter vector βwith\\npcomponents is ﬁt by least squares, we have\\nErr(x0) =E[(Y−ˆfp(x0))2|X=x0]', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 241}),\n",
       " Document(page_content='224 7. Model Assessment and Selection\\n=σ2\\nε+[f(x0)−Eˆfp(x0)]2+||h(x0)||2σ2\\nε.(7.11)\\nHereh(x0) =X(XTX)−1x0, theN-vector of linear weights that produce\\nthe ﬁtˆfp(x0) =x0T(XTX)−1XTy, and hence Var[ ˆfp(x0)] =||h(x0)||2σ2\\nε.\\nWhile this variance changes with x0, its average (with x0taken to be each\\nof the sample values xi) is (p/N)σ2\\nε, and hence\\n1\\nNN∑\\ni=1Err(xi) =σ2\\nε+1\\nNN∑\\ni=1[f(xi)−Eˆf(xi)]2+p\\nNσ2\\nε,(7.12)\\nthein-sample error. Here model complexity is directly related to the num-\\nber of parameters p.\\nThe test error Err( x0) for a ridge regression ﬁt ˆfα(x0) is identical in\\nform to (7.11), except the linear weights in the variance ter m are diﬀerent:\\nh(x0) =X(XTX+αI)−1x0. The bias term will also be diﬀerent.\\nFor a linear model family such as ridge regression, we can bre ak down\\nthe bias more ﬁnely. Let β∗denote the parameters of the best-ﬁtting linear\\napproximation to f:\\nβ∗= argmin\\nβE(\\nf(X)−XTβ)2. (7.13)\\nHere the expectation is taken with respect to the distributi on of the input\\nvariablesX. Then we can write the average squared bias as\\nEx0[\\nf(x0)−Eˆfα(x0)]2\\n= Ex0[\\nf(x0)−xT\\n0β∗]2+Ex0[\\nxT\\n0β∗−ExT\\n0ˆβα]2\\n= Ave[Model Bias]2+Ave[Estimation Bias]2\\n(7.14)\\nThe ﬁrst term on the right-hand side is the average squared model bias , the\\nerror between the best-ﬁtting linear approximation and the true function.\\nThe second term is the average squared estimation bias , the error between\\nthe average estimate E( xT\\n0ˆβ) and the best-ﬁtting linear approximation.\\nForlinearmodelsﬁtbyordinaryleastsquares,theestimati onbiasiszero.\\nFor restricted ﬁts, such as ridge regression, it is positive , and we trade it oﬀ\\nwith the beneﬁts of a reduced variance. The model bias can onl y be reduced\\nby enlarging the class of linear models to a richer collectio n of models, by\\nincluding interactions and transformations of the variabl es in the model.\\nFigure 7.2 shows the bias–variance tradeoﬀ schematically. In the case\\nof linear models, the model space is the set of all linear pred ictions from\\npinputs and the black dot labeled “closest ﬁt” is xTβ∗. The blue-shaded\\nregion indicates the error σεwith which we see the truth in the training\\nsample.\\nAlso shown is the variance of the least squares ﬁt, indicated by the large\\nyellow circle centered at the black dot labeled “closest ﬁt i n population,’', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 242}),\n",
       " Document(page_content='7.3 The Bias–Variance Decomposition 225\\nRealizationClosest fit in population\\nEstimation BiasSPACE\\nVarianceEstimationClosest fit\\nTruth\\nModel bias\\nRESTRICTEDShrunken fit\\nMODELSPACEMODEL\\nFIGURE 7.2. Schematic of the behavior of bias and variance. The model spa ce\\nis the set of all possible predictions from the model, with the “ closest ﬁt” labeled\\nwith a black dot. The model bias from the truth is shown, along wi th the variance,\\nindicated by the large yellow circle centered at the black dot labe led “closest ﬁt\\nin population.” A shrunken or regularized ﬁt is also shown, havi ng additional\\nestimation bias, but smaller prediction error due to its decr eased variance.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 243}),\n",
       " Document(page_content='226 7. Model Assessment and Selection\\nNow if we were to ﬁt a model with fewer predictors, or regulari ze the coef-\\nﬁcients by shrinking them toward zero (say), we would get the “shrunken\\nﬁt” shown in the ﬁgure. This ﬁt has an additional estimation b ias, due to\\nthe fact that it is not the closest ﬁt in the model space. On the other hand,\\nit has smaller variance. If the decrease in variance exceeds the increase in\\n(squared) bias, then this is worthwhile.\\n7.3.1 Example: Bias–Variance Tradeoﬀ\\nFigure 7.3 shows the bias–variance tradeoﬀ for two simulate d examples.\\nThere are 80 observations and 20 predictors, uniformly dist ributed in the\\nhypercube [0 ,1]20. The situations are as follows:\\nLeft panels: Yis 0 ifX1≤1/2 and 1 ifX1>1/2, and we apply k-nearest\\nneighbors.\\nRight panels: Yis 1 if∑10\\nj=1Xjis greater than 5 and 0 otherwise, and we\\nuse best subset linear regression of size p.\\nThe top row is regression with squared error loss; the bottom row is classi-\\nﬁcation with 0–1 loss. The ﬁgures show the prediction error ( red), squared\\nbias (green) and variance (blue), all computed for a large te st sample.\\nIn the regression problems, bias and variance add to produce the predic-\\ntion error curves, with minima at about k= 5 fork-nearest neighbors, and\\np≥10 for the linear model. For classiﬁcation loss (bottom ﬁgur es), some\\ninteresting phenomena can be seen. The bias and variance cur ves are the\\nsame as in the top ﬁgures, and prediction error now refers to m isclassiﬁ-\\ncation rate. We see that prediction error is no longer the sum of squared\\nbias and variance. For the k-nearest neighbor classiﬁer, prediction error\\ndecreases or stays the same as the number of neighbors is incr eased to 20,\\ndespite the fact that the squared bias is rising. For the line ar model classi-\\nﬁer the minimum occurs for p≥10 as in regression, but the improvement\\nover thep= 1 model is more dramatic. We see that bias and variance seem\\nto interact in determining prediction error.\\nWhy does this happen? There is a simple explanation for the ﬁr st phe-\\nnomenon. Suppose at a given input point, the true probabilit y of class 1 is\\n0.9 while the expected value of our estimate is 0 .6. Then the squared bias—\\n(0.6−0.9)2—is considerable, but the prediction error is zero since we ma ke\\nthe correct decision. In other words, estimation errors tha t leave us on the\\nright side of the decision boundary don’t hurt. Exercise 7.2 demonstrates\\nthis phenomenon analytically, and also shows the interacti on eﬀect between\\nbias and variance.\\nThe overall point is that the bias–variance tradeoﬀ behaves diﬀerently\\nfor 0–1 loss than it does for squared error loss. This in turn m eans that\\nthe best choices of tuning parameters may diﬀer substantial ly in the two', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 244}),\n",
       " Document(page_content='7.3 The Bias–Variance Decomposition 2270.0 0.1 0.2 0.3 0.4\\nNumber of Neighbors k50 40 30 20 10 0k−NN − Regression\\n5 10 15 200.0 0.1 0.2 0.3 0.4\\nSubset Size pLinear Model − Regression0.0 0.1 0.2 0.3 0.4\\nNumber of Neighbors k50 40 30 20 10 0k−NN − Classification\\n5 10 15 200.0 0.1 0.2 0.3 0.4\\nSubset Size pLinear Model − Classification\\nFIGURE 7.3. Expected prediction error (orange), squared bias (green) a nd vari-\\nance (blue) for a simulated example. The top row is regression wi th squared error\\nloss; the bottom row is classiﬁcation with 0–1loss. The models are k-nearest\\nneighbors (left) and best subset regression of size p(right). The variance and bias\\ncurves are the same in regression and classiﬁcation, but the p rediction error curve\\nis diﬀerent.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 245}),\n",
       " Document(page_content='228 7. Model Assessment and Selection\\nsettings. One should base the choice of tuning parameter on a n estimate of\\nprediction error, as described in the following sections.\\n7.4 Optimism of the Training Error Rate\\nDiscussions of error rate estimation can be confusing, beca use we have\\nto make clear which quantities are ﬁxed and which are random1. Before\\nwe continue, we need a few deﬁnitions, elaborating on the mat erial of Sec-\\ntion 7.2. Given a training set T={(x1,y1),(x2,y2),...(xN,yN)}the gen-\\neralization error of a model ˆfis\\nErrT= EX0,Y0[L(Y0,ˆf(X0))|T]; (7.15)\\nNotethatthetrainingset Tisﬁxedinexpression(7.15).Thepoint( X0,Y0)\\nis a new test data point, drawn from F, the joint distribution of the data.\\nAveraging over training sets Tyields the expected error\\nErr = E TEX0,Y0[L(Y0,ˆf(X0))|T], (7.16)\\nwhich is more amenable to statistical analysis. As mentione d earlier, it\\nturns out that most methods eﬀectively estimate the expecte d error rather\\nthan E T; see Section 7.12 for more on this point.\\nNow typically, the training error\\nerr =1\\nNN∑\\ni=1L(yi,ˆf(xi)) (7.17)\\nwill be less than the true error Err T, because the same data is being used\\nto ﬁt the method and assess its error (see Exercise 2.9). A ﬁtt ing method\\ntypically adapts to the training data, and hence the apparen t or training\\nerrorerr will be an overly optimistic estimate of the generalizat ion error\\nErrT.\\nPart of the discrepancy is due to where the evaluation points occur. The\\nquantity Err Tcan be thought of as extra-sample error, since the test input\\nvectors don’t need to coincide with the training input vecto rs. The nature\\nof the optimism in err is easiest to understand when we focus instead on\\nthein-sample error\\nErrin=1\\nNN∑\\ni=1EY0[L(Y0\\ni,ˆf(xi))|T] (7.18)\\nTheY0notation indicates that we observe Nnew response values at\\neach of the training points xi, i= 1,2,...,N. We deﬁne the optimism as\\n1Indeed, in the ﬁrst edition of our book, this section wasn’t suﬃciently clea r.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 246}),\n",
       " Document(page_content='7.4 Optimism of the Training Error Rate 229\\nthe diﬀerence between Err inand the training error err:\\nop≡Errin−err. (7.19)\\nThisistypicallypositivesince errisusuallybiaseddownwardasanestimate\\nof prediction error. Finally, the average optimism is the ex pectation of the\\noptimism over training sets\\nω≡Ey(op). (7.20)\\nHere the predictors in the training set are ﬁxed, and the expe ctation is\\nover the training set outcome values; hence we have used the n otation E y\\ninstead of E T. We can usually estimate only the expected error ωrather\\nthan op, in the same way that we can estimate the expected erro r Err\\nrather than the conditional error Err T.\\nFor squared error, 0–1, and other loss functions, one can sho w quite\\ngenerally that\\nω=2\\nNN∑\\ni=1Cov(ˆyi,yi), (7.21)\\nwhere Cov indicates covariance. Thus the amount by which err underesti-\\nmates the true error depends on how strongly yiaﬀects its own prediction.\\nThe harder we ﬁt the data, the greater Cov(ˆ yi,yi) will be, thereby increas-\\ningtheoptimism.Exercise7.4provesthisresultforsquare derrorlosswhere\\nˆyiis the ﬁtted value from the regression. For 0–1 loss, ˆ yi∈{0,1}is the\\nclassiﬁcation at xi, and for entropy loss, ˆ yi∈[0,1] is the ﬁtted probability\\nof class 1 at xi.\\nIn summary, we have the important relation\\nEy(Errin) = Ey(err)+2\\nNN∑\\ni=1Cov(ˆyi,yi). (7.22)\\nThis expression simpliﬁes if ˆ yiis obtained by a linear ﬁt with dinputs\\nor basis functions. For example,\\nN∑\\ni=1Cov(ˆyi,yi) =dσ2\\nε (7.23)\\nfor the additive error model Y=f(X)+ε, and so\\nEy(Errin) = Ey(err)+2·d\\nNσ2\\nε. (7.24)\\nExpression (7.23) is the basis for the deﬁnition of the eﬀective number of\\nparameters discussed in Section 7.6 The optimism increases linearly wi th', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 247}),\n",
       " Document(page_content='230 7. Model Assessment and Selection\\nthe number dof inputs or basis functions we use, but decreases as the\\ntraining sample size increases. Versions of (7.24) hold app roximately for\\nother error models, such as binary data and entropy loss.\\nAn obvious way to estimate prediction error is to estimate th e optimism\\nand then add it to the training error err. The methods described in the\\nnext section— Cp, AIC, BIC and others—work in this way, for a special\\nclass of estimates that are linear in their parameters.\\nIn contrast, cross-validation and bootstrap methods, desc ribed later in\\nthe chapter, are direct estimates of the extra-sample error Err. These gen-\\neral tools can be used with any loss function, and with nonlin ear, adaptive\\nﬁtting techniques.\\nIn-sample error is not usually of direct interest since futu re values of the\\nfeatures are not likely to coincide with their training set v alues. But for\\ncomparison between models, in-sample error is convenient a nd often leads\\nto eﬀective model selection. The reason is that the relative (rather than\\nabsolute) size of the error is what matters.\\n7.5 Estimates of In-Sample Prediction Error\\nThe general form of the in-sample estimates is\\nˆErrin=err+ ˆω, (7.25)\\nwhere ˆωis an estimate of the average optimism.\\nUsing expression (7.24), applicable when dparameters are ﬁt under\\nsquared error loss, leads to a version of the so-called Cpstatistic,\\nCp=err+2·d\\nNˆσε2. (7.26)\\nHere ˆσε2is an estimate of the noise variance, obtained from the mean-\\nsquarederrorofalow-biasmodel.Usingthiscriterionwead justthetraining\\nerror by a factor proportional to the number of basis functio ns used.\\nTheAkaike information criterion is a similar but more generally appli-\\ncable estimate of Err inwhen a log-likelihood loss function is used. It relies\\non a relationship similar to (7.24) that holds asymptotical ly asN→∞:\\n−2·E[logPr ˆθ(Y)]≈−2\\nN·E[loglik]+2·d\\nN. (7.27)\\nHere Pr θ(Y) is a family of densities for Y(containing the “true” density),\\nˆθis the maximum-likelihood estimate of θ, and “loglik” is the maximized\\nlog-likelihood:\\nloglik =N∑\\ni=1logPrˆθ(yi). (7.28)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 248}),\n",
       " Document(page_content='7.5 Estimates of In-Sample Prediction Error 231\\nFor example, for the logistic regression model, using the bi nomial log-\\nlikelihood, we have\\nAIC =−2\\nN·loglik+2·d\\nN. (7.29)\\nFor the Gaussian model (with variance σ2\\nε= ˆσε2assumed known), the AIC\\nstatistic is equivalent to Cp, and so we refer to them collectively as AIC.\\nTo use AIC for model selection, we simply choose the model giv ing small-\\nest AIC over the set of models considered. For nonlinear and o ther complex\\nmodels, we need to replace dby some measure of model complexity. We\\ndiscuss this in Section 7.6.\\nGiven a set of models fα(x) indexed by a tuning parameter α, denote\\nbyerr(α) andd(α) the training error and number of parameters for each\\nmodel. Then for this set of models we deﬁne\\nAIC(α) =err(α)+2·d(α)\\nNˆσε2. (7.30)\\nThe function AIC( α) provides an estimate of the test error curve, and we\\nﬁnd the tuning parameter ˆ αthat minimizes it. Our ﬁnal chosen model\\nisfˆα(x). Note that if the basis functions are chosen adaptively, (7 .23) no\\nlonger holds. For example, if we have a total of pinputs, and we choose\\nthe best-ﬁtting linear model with d < pinputs, the optimism will exceed\\n(2d/N)σ2\\nε. Put another way, by choosing the best-ﬁtting model with d\\ninputs, the eﬀective number of parameters ﬁt is more than d.\\nFigure 7.4 shows AIC in action for the phoneme recognition ex ample\\nof Section 5.2.3 on page 148. The input vector is the log-peri odogram of\\nthe spoken vowel, quantized to 256 uniformly spaced frequen cies. A lin-\\near logistic regression model is used to predict the phoneme class, with\\ncoeﬃcient function β(f) =∑M\\nm=1hm(f)θm, an expansion in Mspline ba-\\nsis functions. For any given M, a basis of natural cubic splines is used\\nfor thehm, with knots chosen uniformly over the range of frequencies ( so\\nd(α) =d(M) =M). Using AIC to select the number of basis functions will\\napproximately minimize Err( M) for both entropy and 0–1 loss.\\nThe simple formula\\n(2/N)N∑\\ni=1Cov(ˆyi,yi) = (2d/N)σ2\\nε\\nholds exactly for linear models with additive errors and squ ared error loss,\\nand approximately for linear models and log-likelihoods. I n particular, the\\nformula does not hold in general for 0–1 loss (Efron, 1986), a lthough many\\nauthors nevertheless use it in that context (right panel of F igure 7.4).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 249}),\n",
       " Document(page_content='232 7. Model Assessment and Selection\\nNumber of Basis FunctionsLog-likelihood\\n0.5 1.0 1.5 2.0 2.5Log-likelihood Loss\\n2 4 8 16 32 64 128O\\nO\\nO\\nOOO\\nO\\nOO\\nO\\nO\\nOOOOO\\nO\\nO\\nO\\nOOOOOTrain\\nTest\\nAIC\\nNumber of Basis FunctionsMisclassification Error\\n0.10 0.15 0.20 0.25 0.30 0.350-1 Loss\\n2 4 8 16 32 64 128O\\nO\\nO\\nOO\\nO\\nO\\nOO\\nO\\nO\\nOOOOOO\\nO\\nO\\nOOOOO\\nFIGURE 7.4. AIC used for model selection for the phoneme recogni-\\ntion example of Section 5.2.3. The logistic regression coeﬃci ent function\\nβ(f) =∑M\\nm=1hm(f)θmis modeled as an expansion in Mspline basis functions.\\nIn the left panel we see the AIC statistic used to estimate Errinusing log-likeli-\\nhood loss. Included is an estimate of Errbased on an independent test sample. It\\ndoes well except for the extremely over-parametrized case ( M= 256parameters\\nforN= 1000 observations). In the right panel the same is done for 0–1 loss .\\nAlthough the AIC formula does not strictly apply here, it does a re asonable job in\\nthis case.\\n7.6 The Eﬀective Number of Parameters\\nThe concept of “number of parameters” can be generalized, es pecially to\\nmodels where regularization is used in the ﬁtting. Suppose w e stack the\\noutcomesy1,y2,...,y Ninto a vector y, and similarly for the predictions\\nˆy. Then a linear ﬁtting method is one for which we can write\\nˆy=Sy, (7.31)\\nwhereSis anN×Nmatrix depending on the input vectors xibut not on\\ntheyi. Linear ﬁtting methods include linear regression on the ori ginal fea-\\ntures or on a derived basis set, and smoothing methods that us e quadratic\\nshrinkage, such as ridge regression and cubic smoothing spl ines. Then the\\neﬀective number of parameters is deﬁned as\\ndf(S) = trace( S), (7.32)\\nthe sum of the diagonal elements of S(also known as the eﬀective degrees-\\nof-freedom ). Note that if Sis an orthogonal-projection matrix onto a basis', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 250}),\n",
       " Document(page_content='7.7 The Bayesian Approach and BIC 233\\nsetspannedby Mfeatures,thentrace( S) =M.Itturnsoutthattrace( S) is\\nexactly the correct quantity to replace das the number of parameters in the\\nCpstatistic (7.26). If yarises from an additive-error model Y=f(X)+ε\\nwith Var(ε) =σ2\\nε, then one can show that∑N\\ni=1Cov(ˆyi,yi) = trace( S)σ2\\nε,\\nwhich motivates the more general deﬁnition\\ndf(ˆy) =∑N\\ni=1Cov(ˆyi,yi)\\nσ2ε(7.33)\\n(Exercises 7.4 and 7.5). Section 5.4.1 on page 153 gives some more intuition\\nfor the deﬁnition df = trace( S) in the context of smoothing splines.\\nFor models like neural networks, in which we minimize an erro r function\\nR(w) with weight decay penalty (regularization) α∑\\nmw2\\nm, the eﬀective\\nnumber of parameters has the form\\ndf(α) =M∑\\nm=1θm\\nθm+α, (7.34)\\nwhere theθmare the eigenvalues of the Hessian matrix ∂2R(w)/∂w∂wT.\\nExpression (7.34) follows from (7.32) if we make a quadratic approximation\\nto the error function at the solution (Bishop, 1995).\\n7.7 The Bayesian Approach and BIC\\nTheBayesianinformationcriterion(BIC),likeAIC,isappl icableinsettings\\nwhere the ﬁtting is carried out by maximization of a log-like lihood. The\\ngeneric form of BIC is\\nBIC =−2·loglik+(log N)·d. (7.35)\\nTheBICstatistic(times1/2)isalsoknownastheSchwarzcri terion(Schwarz,\\n1978).\\nUnder the Gaussian model, assuming the variance σ2\\nεis known,−2·loglik\\nequals(uptoaconstant)∑\\ni(yi−ˆf(xi))2/σ2\\nε,whichisN·err/σ2\\nεforsquared\\nerror loss. Hence we can write\\nBIC =N\\nσ2ε[\\nerr+(logN)·d\\nNσ2\\nε]\\n. (7.36)\\nTherefore BIC is proportional to AIC ( Cp), with the factor 2 replaced\\nby logN. Assuming N >e2≈7.4, BIC tends to penalize complex models\\nmore heavily, giving preference to simpler models in select ion. As with AIC,\\nσ2\\nεis typically estimated by the mean squared error of a low-bia s model.\\nFor classiﬁcation problems, use of the multinomial log-lik elihood leads to a\\nsimilar relationship with the AIC, using cross-entropy as t he error measure.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 251}),\n",
       " Document(page_content='234 7. Model Assessment and Selection\\nNote however that the misclassiﬁcation error measure does n ot arise in the\\nBIC context, since it does not correspond to the log-likelih ood of the data\\nunder any probability model.\\nDespite its similarity with AIC, BIC is motivated in quite a d iﬀerent\\nway. It arises in the Bayesian approach to model selection, w hich we now\\ndescribe.\\nSuppose we have a set of candidate models Mm,m= 1,...,M and\\ncorresponding model parameters θm, and we wish to choose a best model\\nfrom among them. Assuming we have a prior distribution Pr( θm|Mm) for\\nthe parameters of each model Mm, the posterior probability of a given\\nmodel is\\nPr(Mm|Z)∝Pr(Mm)·Pr(Z|Mm) (7.37)\\n∝Pr(Mm)·∫\\nPr(Z|θm,Mm)Pr(θm|Mm)dθm,\\nwhereZrepresents the training data {xi,yi}N\\n1. To compare two models\\nMmandMℓ, we form the posterior odds\\nPr(Mm|Z)\\nPr(Mℓ|Z)=Pr(Mm)\\nPr(Mℓ)·Pr(Z|Mm)\\nPr(Z|Mℓ). (7.38)\\nIf the odds are greater than one we choose model m, otherwise we choose\\nmodelℓ. The rightmost quantity\\nBF(Z) =Pr(Z|Mm)\\nPr(Z|Mℓ)(7.39)\\nis called the Bayes factor , the contribution of the data toward the posterior\\nodds.\\nTypically we assume that the prior over models is uniform, so that\\nPr(Mm) is constant. We need some way of approximating Pr( Z|Mm).\\nA so-called Laplace approximation to the integral followed by some other\\nsimpliﬁcations (Ripley, 1996, page 64) to (7.37) gives\\nlogPr(Z|Mm) = logPr( Z|ˆθm,Mm)−dm\\n2·logN+O(1).(7.40)\\nHereˆθmis a maximum likelihood estimate and dmis the number of free\\nparameters in model Mm. If we deﬁne our loss function to be\\n−2logPr(Z|ˆθm,Mm),\\nthis is equivalent to the BIC criterion of equation (7.35).\\nTherefore, choosing the model with minimum BIC is equivalen t to choos-\\ning the model with largest (approximate) posterior probabi lity. But this\\nframework gives us more. If we compute the BIC criterion for a set ofM,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 252}),\n",
       " Document(page_content='7.8 Minimum Description Length 235\\nmodels, giving BIC m,m= 1,2,...,M, then we can estimate the posterior\\nprobability of each model Mmas\\ne−1\\n2·BICm\\n∑M\\nℓ=1e−1\\n2·BICℓ. (7.41)\\nThus we can estimate not only the best model, but also assess t he relative\\nmerits of the models considered.\\nFor model selection purposes, there is no clear choice betwe en AIC and\\nBIC. BIC is asymptotically consistent as a selection criter ion. What this\\nmeans is that given a family of models, including the true mod el, the prob-\\nability that BIC will select the correct model approaches on e as the sample\\nsizeN→∞. This is not the case for AIC, which tends to choose models\\nwhich are too complex as N→∞. On the other hand, for ﬁnite samples,\\nBIC often chooses models that are too simple, because of its h eavy penalty\\non complexity.\\n7.8 Minimum Description Length\\nThe minimum description length (MDL) approach gives a selec tion cri-\\nterion formally identical to the BIC approach, but is motiva ted from an\\noptimal coding viewpoint. We ﬁrst review the theory of codin g for data\\ncompression, and then apply it to model selection.\\nWe think of our datum zas a message that we want to encode and\\nsend to someone else (the “receiver”). We think of our model a s a way of\\nencoding the datum, and will choose the most parsimonious mo del, that is\\nthe shortest code, for the transmission.\\nSuppose ﬁrst that the possible messages we might want to tran smit are\\nz1,z2,...,z m. Our code uses a ﬁnite alphabet of length A: for example, we\\nmight use a binary code {0,1}of lengthA= 2. Here is an example with\\nfour possible messages and a binary coding:\\nMessage z1z2z3z4\\nCode 010110111(7.42)\\nThis code is known as an instantaneous preﬁx code: no code is t he pre-\\nﬁx of any other, and the receiver (who knows all of the possibl e codes),\\nknows exactly when the message has been completely sent. We r estrict our\\ndiscussion to such instantaneous preﬁx codes.\\nOne could use the coding in (7.42) or we could permute the code s, for\\nexample use codes 110 ,10,111,0 forz1,z2,z3,z4. How do we decide which\\nto use? It depends on how often we will be sending each of the me ssages.\\nIf, for example, we will be sending z1most often, it makes sense to use the\\nshortest code 0 for z1. Using this kind of strategy—shorter codes for more\\nfrequent messages—the average message length will be shorte r.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 253}),\n",
       " Document(page_content='236 7. Model Assessment and Selection\\nIn general, if messages are sent with probabilities Pr( zi),i= 1,2,...,4,\\na famous theorem due to Shannon says we should use code length sli=\\n−log2Pr(zi) and the average message length satisﬁes\\nE(length)≥−∑\\nPr(zi)log2(Pr(zi)). (7.43)\\nThe right-hand side above is also called the entropy of the di stribution\\nPr(zi). The inequality is an equality when the probabilities sati sfypi=\\nA−li. In our example, if Pr( zi) = 1/2,1/4,1/8,1/8,respectively, then the\\ncoding shown in (7.42) is optimal and achieves the entropy lo wer bound.\\nIn general the lower bound cannot be achieved, but procedure s like the\\nHuﬀman coding scheme can get close to the bound. Note that wit h an\\ninﬁnite set of messages, the entropy is replaced by −∫\\nPr(z)log2Pr(z)dz.\\nFrom this result we glean the following:\\nTo transmit a random variable zhaving probability density func-\\ntionPr(z), we require about −log2Pr(z)bits of information.\\nWe henceforth change notation from log2Pr(z) to logPr(z) = logePr(z);\\nthis is for convenience, and just introduces an unimportant multiplicative\\nconstant.\\nNow we apply this result to the problem of model selection. We have\\na modelMwith parameters θ, and data Z= (X,y) consisting of both\\ninputs and outputs. Let the (conditional) probability of th e outputs under\\nthe model be Pr( y|θ,M,X), assume the receiver knows all of the inputs,\\nand we wish to transmit the outputs. Then the message length r equired to\\ntransmit the outputs is\\nlength =−logPr(y|θ,M,X)−logPr(θ|M), (7.44)\\nthe log-probability of the target values given the inputs. T he second term\\nis the average code length for transmitting the model parame tersθ, while\\nthe ﬁrst term is the average code length for transmitting the discrepancy\\nbetween the model and actual target values. For example supp ose we have\\na single target ywithy∼N(θ,σ2), parameter θ∼N(0,1) and no input\\n(for simplicity). Then the message length is\\nlength = constant+log σ+(y−θ)2\\n2σ2+θ2\\n2. (7.45)\\nNote that the smaller σis, the shorter on average is the message length,\\nsinceyis more concentrated around θ.\\nThe MDL principle says that we should choose the model that mi ni-\\nmizes (7.44). We recognize (7.44) as the (negative) log-pos terior distribu-\\ntion, and hence minimizing description length is equivalen t to maximizing\\nposterior probability. Hence the BIC criterion, derived as approximation to\\nlog-posterior probability, can also be viewed as a device fo r (approximate)\\nmodel choice by minimum description length.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 254}),\n",
       " Document(page_content='7.9 Vapnik–Chervonenkis Dimension 237\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 0.0 1.0\\nxsin(50·x)\\nFIGURE 7.5. The solid curve is the function sin(50x)forx∈[0,1]. The green\\n(solid) and blue (hollow) points illustrate how the associated i ndicator function\\nI(sin(αx)>0)can shatter (separate) an arbitrarily large number of points b y\\nchoosing an appropriately high frequency α.\\nNote that we have ignored the precision with which a random va riable\\nzis coded. With a ﬁnite code length we cannot code a continuous variable\\nexactly. However, if we code zwithin a tolerance δz, the message length\\nneededisthelogoftheprobabilityintheinterval[ z,z+δz]whichiswellap-\\nproximated by δzPr(z) ifδzis small. Since log δzPr(z) = logδz+logPr(z),\\nthis means we can just ignore the constant log δzand use logPr( z) as our\\nmeasure of message length, as we did above.\\nThe preceding view of MDL for model selection says that we sho uld\\nchoose the model with highest posterior probability. Howev er, many Bayes-\\nianswouldinsteaddoinferencebysamplingfromtheposteri ordistribution.\\n7.9 Vapnik–Chervonenkis Dimension\\nA diﬃculty in using estimates of in-sample error is the need t o specify the\\nnumber of parameters (or the complexity) dused in the ﬁt. Although the\\neﬀective number of parameters introduced in Section 7.6 is u seful for some\\nnonlinear models, it is not fully general. The Vapnik–Cherv onenkis (VC)\\ntheory provides such a general measure of complexity, and gi ves associated\\nbounds on the optimism. Here we give a brief review of this the ory.\\nSuppose we have a class of functions {f(x,α)}indexed by a parameter\\nvectorα, withx∈IRp. Assume for now that fis an indicator function,\\nthat is, takes the values 0 or 1. If α= (α0,α1) andfis the linear indi-\\ncator function I(α0+αT\\n1x >0), then it seems reasonable to say that the\\ncomplexity of the class fis the number of parameters p+ 1. But what\\naboutf(x,α) =I(sinα·x) whereαis any real number and x∈IR? The\\nfunction sin(50·x) is shown in Figure 7.5. This is a very wiggly function\\nthat gets even rougher as the frequency αincreases, but it has only one\\nparameter: despite this, it doesn’t seem reasonable to conc lude that it has\\nless complexity than the linear indicator function I(α0+α1x) inp= 1\\ndimension.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 255}),\n",
       " Document(page_content='238 7. Model Assessment and Selection\\nFIGURE 7.6. The ﬁrst three panels show that the class of lines in the plane\\ncan shatter three points. The last panel shows that this class c annot shatter four\\npoints, as no line will put the hollow points on one side and the s olid points on\\nthe other. Hence the VC dimension of the class of straight lines in the plane is\\nthree. Note that a class of nonlinear curves could shatter four p oints, and hence\\nhas VC dimension greater than three.\\nThe Vapnik–Chervonenkis dimension is a way of measuring the com-\\nplexity of a class of functions by assessing how wiggly its me mbers can\\nbe.\\nTheVC dimension of the class{f(x,α)}is deﬁned to be the\\nlargest number of points (in some conﬁguration) that can be\\nshattered by members of{f(x,α)}.\\nA set of points is said to be shattered by a class of functions i f, no matter\\nhow we assign a binary label to each point, a member of the clas s can\\nperfectly separate them.\\nFigure 7.6 shows that the VC dimension of linear indicator fu nctions\\nin the plane is 3 but not 4, since no four points can be shattere d by a\\nset of lines. In general, a linear indicator function in pdimensions has VC\\ndimensionp+1, which is also the number of free parameters. On the other\\nhand, it can be shown that the family sin( αx) has inﬁnite VC dimension,\\nas Figure 7.5 suggests. By appropriate choice of α, any set of points can be\\nshattered by this class (Exercise 7.8).\\nSo far we have discussed the VC dimension only of indicator fu nctions,\\nbut this can be extended to real-valued functions. The VC dim ension of a\\nclass of real-valued functions {g(x,α)}is deﬁned to be the VC dimension\\nof the indicator class {I(g(x,α)−β >0)}, whereβtakes values over the\\nrange ofg.\\nOne can use the VC dimension in constructing an estimate of (e xtra-\\nsample) prediction error; diﬀerent types of results are ava ilable. Using the\\nconcept of VC dimension, one can prove results about the opti mism of the\\ntrainingerrorwhenusingaclassoffunctions.Anexampleof sucharesultis\\nthe following. If we ﬁt Ntraining points using a class of functions {f(x,α)}\\nhaving VC dimension h, then with probability at least 1 −ηover training', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 256}),\n",
       " Document(page_content='7.9 Vapnik–Chervonenkis Dimension 239\\nsets:\\nErrT≤err+ǫ\\n2(\\n1+√\\n1+4·err\\nǫ)\\n(binary classiﬁcation)\\nErrT≤err\\n(1−c√ǫ)+(regression) (7.46)\\nwhereǫ=a1h[log(a2N/h)+1]−log(η/4)\\nN,\\nand 0<a1≤4,0<a2≤2\\nThese bounds hold simultaneously for all members f(x,α), and are taken\\nfrom Cherkassky and Mulier (2007, pages 116–118). They reco mmend the\\nvaluec= 1. For regression they suggest a1=a2= 1, and for classiﬁcation\\nthey make no recommendation, with a1= 4 anda2= 2 corresponding\\nto worst-case scenarios. They also give an alternative practical bound for\\nregression\\nErrT≤err(\\n1−√\\nρ−ρlogρ+logN\\n2N)−1\\n+(7.47)\\nwithρ=h\\nN, which is free of tuning constants. The bounds suggest that t he\\noptimism increases with hand decreases with Nin qualitative agreement\\nwith the AIC correction d/Ngiven in (7.24). However, the results in (7.46)\\nare stronger: rather than giving the expected optimism for e ach ﬁxed func-\\ntionf(x,α), they give probabilistic upper bounds for all functions f(x,α),\\nand hence allow for searching over the class.\\nVapnik’s structural risk minimization (SRM) approach ﬁts a nested se-\\nquence of models of increasing VC dimensions h1< h2<···, and then\\nchooses the model with the smallest value of the upper bound.\\nWe note that upper bounds like the ones in (7.46) are often ver y loose,\\nbut that doesn’t rule them out as good criteria for model sele ction, where\\nthe relative (not absolute) size of the test error is importa nt. The main\\ndrawback of this approach is the diﬃculty in calculating the VC dimension\\nof a class of functions. Often only a crude upper bound for VC d imension\\nis obtainable, and this may not be adequate. An example in whi ch the\\nstructural risk minimization program can be successfully c arried out is the\\nsupport vector classiﬁer, discussed in Section 12.2.\\n7.9.1 Example (Continued)\\nFigure 7.7 shows the results when AIC, BIC and SRM are used to s elect\\nthe model size for the examples of Figure 7.3. For the example s labeled KNN,\\nthe model index αrefers to neighborhood size, while for those labeled REGα\\nrefers to subset size. Using each selection method (e.g., AI C) we estimated\\nthe best model ˆ αand found its true prediction error Err T(ˆα) on a test\\nset. For the same training set we computed the prediction err or of the best', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 257}),\n",
       " Document(page_content='240 7. Model Assessment and Selection\\nreg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestAIC\\nreg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestBIC\\nreg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestSRM\\nFIGURE 7.7. Boxplots show the distribution of the relative error\\n100×[ErrT(ˆα)−minαErrT(α)]/[maxαErrT(α)−minαErrT(α)]over the four\\nscenarios of Figure 7.3. This is the error in using the chosen model relative to\\nthe best model. There are 100training sets each of size 80represented in each\\nboxplot, with the errors computed on test sets of size 10,000.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 258}),\n",
       " Document(page_content='7.10 Cross-Validation 241\\nand worst possible model choices: min αErrT(α) and max αErrT(α). The\\nboxplots show the distribution of the quantity\\n100×ErrT(ˆα)−minαErrT(α)\\nmaxαErrT(α)−minαErrT(α),\\nwhich represents the error in using the chosen model relativ e to the best\\nmodel. For linear regression the model complexity was measu red by the\\nnumber of features; as mentioned in Section 7.5, this undere stimates the\\ndf, since it does not chargefor the search for the best model of that size.\\nThis was also used for the VC dimension of the linear classiﬁe r. Fork-\\nnearest neighbors, we used the quantity N/k. Under an additive-error re-\\ngression model, this can be justiﬁed as the exact eﬀective de grees of free-\\ndom (Exercise 7.6); we do not know if it corresponds to the VC d imen-\\nsion. We used a1=a2= 1 for the constants in (7.46); the results for SRM\\nchangedwithdiﬀerentconstants,andthischoicegavethemo stfavorablere-\\nsults. We repeated the SRM selection using the alternative p ractical bound\\n(7.47), and got almost identical results. For misclassiﬁca tion error we used\\nˆσε2= [N/(N−d)]·err(α) for the least restrictive model ( k= 5 for KNN,\\nsincek= 1 results in zero training error). The AIC criterion seems t o work\\nwell in all four scenarios, despite the lack of theoretical s upport with 0–1\\nloss. BIC does nearly as well, while the performance of SRM is mixed.\\n7.10 Cross-Validation\\nProbably the simplest and most widely used method for estima ting predic-\\ntion error is cross-validation. This method directly estim ates the expected\\nextra-sample error Err = E[ L(Y,ˆf(X))], the average generalization error\\nwhen the method ˆf(X) is applied to an independent test sample from the\\njoint distribution of XandY. As mentioned earlier, we might hope that\\ncross-validation estimates the conditional error, with th e training setT\\nheld ﬁxed. But as we will see in Section 7.12, cross-validati on typically\\nestimates well only the expected prediction error.\\n7.10.1K-Fold Cross-Validation\\nIdeally, if we had enough data, we would set aside a validatio n set and use\\nit to assess the performance of our prediction model. Since d ata are often\\nscarce, this is usually not possible. To ﬁnesse the problem, K-fold cross-\\nvalidation uses part of the available data to ﬁt the model, an d a diﬀerent\\npart to test it. We split the data into Kroughly equal-sized parts; for\\nexample, when K= 5, the scenario looks like this:', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 259}),\n",
       " Document(page_content='242 7. Model Assessment and Selection\\nValidation Train1 2 3 4 5\\nTrain Train Train\\nFor thekth part (third above), we ﬁt the model to the other K−1 parts\\nof the data, and calculate the prediction error of the ﬁtted m odel when\\npredicting the kth part of the data. We do this for k= 1,2,...,Kand\\ncombine the Kestimates of prediction error.\\nHere are more details. Let κ:{1,...,N}↦→{1,...,K}be an indexing\\nfunction that indicates the partition to which observation iis allocated by\\nthe randomization. Denote by ˆf−k(x) the ﬁtted function, computed with\\nthekth part of the data removed. Then the cross-validation estim ate of\\nprediction error is\\nCV(ˆf) =1\\nNN∑\\ni=1L(yi,ˆf−κ(i)(xi)). (7.48)\\nTypical choices of Kare 5 or 10 (see below). The case K=Nis known\\nasleave-one-out cross-validation. In this case κ(i) =i, and for the ith\\nobservation the ﬁt is computed using all the data except the ith.\\nGiven a set of models f(x,α) indexed by a tuning parameter α, denote\\nbyˆf−k(x,α) theαth model ﬁt with the kth part of the data removed. Then\\nfor this set of models we deﬁne\\nCV(ˆf,α) =1\\nNN∑\\ni=1L(yi,ˆf−κ(i)(xi,α)). (7.49)\\nThe function CV( ˆf,α) provides an estimate of the test error curve, and we\\nﬁnd the tuning parameter ˆ αthat minimizes it. Our ﬁnal chosen model is\\nf(x,ˆα), which we then ﬁt to all the data.\\nIt is interesting to wonder about what quantity K-fold cross-validation\\nestimates. With K= 5 or 10, we might guess that it estimates the ex-\\npected error Err, since the training sets in each fold are qui te diﬀerent\\nfrom the original training set. On the other hand, if K=Nwe might\\nguess that cross-validation estimates the conditional err or Err T. It turns\\nout that cross-validation only estimates eﬀectively the av erage error Err,\\nas discussed in Section 7.12.\\nWhat value should we choose for K? WithK=N, the cross-validation\\nestimator is approximately unbiased for the true (expected ) prediction er-\\nror, but can have high variance because the N“training sets” are so similar\\nto one another. The computational burden is also considerab le, requiring\\nNapplications of the learning method. In certain special pro blems, this\\ncomputation can be done quickly—see Exercises 7.3 and 5.13.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 260}),\n",
       " Document(page_content='7.10 Cross-Validation 243\\nSize of Training Set1-Err\\n0 50 100 150 2000.0 0.2 0.4 0.6 0.8\\nFIGURE 7.8. Hypothetical learning curve for a classiﬁer on a given task: a\\nplot of1−Errversus the size of the training set N. With a dataset of 200\\nobservations, 5-fold cross-validation would use training sets of size 160, which\\nwould behave much like the full set. However, with a dataset of 50observations\\nﬁvefold cross-validation would use training sets of size 40, and this would result\\nin a considerable overestimate of prediction error.\\nOn the other hand, with K= 5 say, cross-validation has lower variance.\\nBut bias could be a problem, depending on how the performance of the\\nlearning method varies with the size of the training set. Fig ure 7.8 shows\\na hypothetical “learning curve” for a classiﬁer on a given ta sk, a plot of\\n1−Err versus the size of the training set N. The performance of the\\nclassiﬁer improves as the training set size increases to 100 observations;\\nincreasing the number further to 200 brings only a small bene ﬁt. If our\\ntraining set had 200 observations, ﬁvefold cross-validati on would estimate\\nthe performance of our classiﬁer over training sets of size 1 60, which from\\nFigure 7.8 is virtually the same as the performance for train ing set size\\n200. Thus cross-validation would not suﬀer from much bias. H owever if the\\ntraining set had 50 observations, ﬁvefold cross-validatio n would estimate\\nthe performance of our classiﬁer over training sets of size 4 0, and from the\\nﬁgure that would be an underestimate of 1 −Err. Hence as an estimate of\\nErr, cross-validation would be biased upward.\\nTo summarize, if the learning curve has a considerable slope at the given\\ntraining set size, ﬁve- or tenfold cross-validation will ov erestimate the true\\nprediction error. Whether this bias is a drawback in practic e depends on\\nthe objective. On the other hand, leave-one-out cross-vali dation has low\\nbias but can have high variance. Overall, ﬁve- or tenfold cro ss-validation\\nare recommended as a good compromise: see Breiman and Specto r (1992)\\nand Kohavi (1995).\\nFigure 7.9 shows the prediction error and tenfold cross-val idation curve\\nestimated from a single training set, from the scenario in th e bottom right\\npanel of Figure 7.3. This is a two-class classiﬁcation probl em, using a lin-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 261}),\n",
       " Document(page_content='244 7. Model Assessment and Selection\\nSubset Size pMisclassification Error\\n5 10 15 200.0 0.1 0.2 0.3 0.4 0.5 0.6•••\\n••\\n••\\n•\\n•\\n•••••• • •••••\\n•\\n••\\n•\\n•\\n••\\n••• • •• ••• • • •\\nFIGURE 7.9. Prediction error (orange) and tenfold cross-validation curv e\\n(blue) estimated from a single training set, from the scenario in the bottom right\\npanel of Figure 7.3.\\near model with best subsets regression of subset size p. Standard error bars\\nare shown, which are the standard errors of the individual mi sclassiﬁcation\\nerror rates for each of the ten parts. Both curves have minima atp= 10,\\nalthough the CV curve is rather ﬂat beyond 10. Often a “one-st andard\\nerror” rule is used with cross-validation, in which we choos e the most par-\\nsimonious model whose error is no more than one standard erro r above\\nthe error of the best model. Here it looks like a model with abo utp= 9\\npredictors would be chosen, while the true model uses p= 10.\\nGeneralized cross-validation providesaconvenientapproximationtoleave-\\none out cross-validation, for linear ﬁtting under squared- error loss. As de-\\nﬁned in Section 7.6, a linear ﬁtting method is one for which we can write\\nˆy=Sy. (7.50)\\nNow for many linear ﬁtting methods,\\n1\\nNN∑\\ni=1[yi−ˆf−i(xi)]2=1\\nNN∑\\ni=1[yi−ˆf(xi)\\n1−Sii]2\\n,(7.51)\\nwhereSiiis theith diagonal element of S(see Exercise 7.3). The GCV\\napproximation is\\nGCV(ˆf) =1\\nNN∑\\ni=1[\\nyi−ˆf(xi)\\n1−trace(S)/N]2\\n. (7.52)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 262}),\n",
       " Document(page_content='7.10 Cross-Validation 245\\nThe quantity trace( S) is the eﬀective number of parameters, as deﬁned in\\nSection 7.6.\\nGCV can have a computational advantage in some settings, whe re the\\ntrace ofScan be computed more easily than the individual elements Sii.\\nIn smoothing problems, GCV can also alleviate the tendency o f cross-\\nvalidation to undersmooth. The similarity between GCV and A IC can be\\nseen from the approximation 1 /(1−x)2≈1+2x(Exercise 7.7).\\n7.10.2 The Wrong and Right Way to Do Cross-validation\\nConsider a classiﬁcation problem with a large number of pred ictors, as may\\narise, for example, in genomic or proteomic applications. A typical strategy\\nfor analysis might be as follows:\\n1. Screen the predictors: ﬁnd a subset of “good” predictors t hat show\\nfairly strong (univariate) correlation with the class labe ls\\n2. Using just this subset of predictors, build a multivariat e classiﬁer.\\n3. Use cross-validation to estimate the unknown tuning para meters and\\nto estimate the prediction error of the ﬁnal model.\\nIs this a correct application of cross-validation? Conside r a scenario with\\nN= 50 samples in two equal-sized classes, and p= 5000 quantitative\\npredictors (standard Gaussian) that are independent of the class labels.\\nThe true (test) error rate of any classiﬁer is 50%. We carried out the above\\nrecipe, choosing in step (1) the 100 predictors having highe st correlation\\nwith the class labels, and then using a 1-nearest neighbor cl assiﬁer, based\\non just these 100 predictors, in step (2). Over 50 simulation s from this\\nsetting, the average CV error rate was 3%. This is far lower th an the true\\nerror rate of 50%.\\nWhat has happened? The problem is that the predictors have an unfair\\nadvantage, as they were chosen in step (1) on the basis of all of the samples .\\nLeaving samples out afterthe variables have been selected does not cor-\\nrectly mimic the application of the classiﬁer to a completel y independent\\ntest set, since these predictors “have already seen” the lef t out samples.\\nFigure 7.10 (top panel) illustrates the problem. We selecte d the 100 pre-\\ndictors having largest correlation with the class labels ov er all 50 samples.\\nThenwechosearandomsetof10samples,aswewoulddoinﬁve-f oldcross-\\nvalidation, and computed the correlations of the pre-selec ted 100 predictors\\nwith the class labels over just these 10 samples (top panel). We see that\\nthe correlations average about 0.28, rather than 0, as one mi ght expect.\\nHere is the correct way to carry out cross-validation in this example:\\n1. Divide the samples into Kcross-validation folds (groups) at random.\\n2. For each fold k= 1,2,...,K', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 263}),\n",
       " Document(page_content='246 7. Model Assessment and Selection\\nCorrelations of Selected Predictors with OutcomeFrequency\\n−1.0 −0.5 0.0 0.5 1.00 10 20 30Wrong way\\nCorrelations of Selected Predictors with OutcomeFrequency\\n−1.0 −0.5 0.0 0.5 1.00 10 20 30Right way\\nFIGURE 7.10. Cross-validation the wrong and right way: histograms shows t he\\ncorrelation of class labels, in 10randomly chosen samples, with the 100predic-\\ntors chosen using the incorrect (upper red) and correct (lowe r green) versions of\\ncross-validation.\\n(a) Find a subset of “good” predictors that show fairly stron g (uni-\\nvariate) correlation with the class labels, using all of the samples\\nexcept those in fold k.\\n(b) Using just this subset of predictors, build a multivaria te classi-\\nﬁer, using all of the samples except those in fold k.\\n(c) Use the classiﬁer to predict the class labels for the samp les in\\nfoldk.\\nTheerrorestimatesfromstep2(c)arethenaccumulated over allKfolds,to\\nproduce the cross-validation estimate of prediction error . The lower panel\\nof Figure 7.10 shows the correlations of class labels with th e 100 predictors\\nchosen in step 2(a) of the correct procedure, over the sample s in a typical\\nfoldk. We see that they average about zero, as they should.\\nIn general, with a multistep modeling procedure, cross-val idation must\\nbe applied to the entire sequence of modeling steps. In parti cular, samples\\nmust be “left out” before any selection or ﬁltering steps are applied. There\\nis one qualiﬁcation: initial unsupervised screening steps can be done be-\\nfore samples are left out. For example, we could select the 10 00 predictors', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 264}),\n",
       " Document(page_content='7.10 Cross-Validation 247\\nwith highest variance across all 50 samples, before startin g cross-validation.\\nSince this ﬁltering does not involve the class labels, it doe s not give the\\npredictors an unfair advantage.\\nWhile this point may seem obvious to the reader, we have seen t his\\nblunder committed many times in published papers in top rank journals.\\nWith the large numbers of predictors that are so common in gen omic and\\nother areas, the potential consequences of this error have a lso increased\\ndramatically; see Ambroise and McLachlan (2002) for a detai led discussion\\nof this issue.\\n7.10.3 Does Cross-Validation Really Work?\\nWeonceagainexaminethebehaviorofcross-validationinah igh-dimensional\\nclassiﬁcation problem. Consider a scenario with N= 20 samples in two\\nequal-sized classes, and p= 500 quantitative predictors that are indepen-\\ndent of the class labels. Once again, the true error rate of an y classiﬁer is\\n50%. Consider a simple univariate classiﬁer: a single split that minimizes\\nthe misclassiﬁcation error (a “stump”). Stumps are trees wi th a single split,\\nand are used in boosting methods (Chapter 10). A simple argum ent sug-\\ngests that cross-validation will not work properly in this s etting2:\\nFitting to the entire training set, we will ﬁnd a predictor th at\\nsplits the data very well. If we do 5-fold cross-validation, this\\nsame predictor should split any 4/5ths and 1/5th of the data\\nwell too, and hence its cross-validation error will be small (much\\nless than 50%.) Thus CV does not give an accurate estimate of\\nerror.\\nTo investigate whether this argument is correct, Figure 7.1 1 shows the\\nresult of a simulation from this setting. There are 500 predi ctors and 20\\nsamples, in each of two equal-sized classes, with all predic tors having a\\nstandard Gaussian distribution. The panel in the top left sh ows the number\\nof training errors for each of the 500 stumps ﬁt to the trainin g data. We\\nhave marked in color the six predictors yielding the fewest e rrors. In the top\\nright panel, the training errors are shown for stumps ﬁt to a r andom 4/5ths\\npartition of the data (16 samples), and tested on the remaini ng 1/5th (four\\nsamples). The colored points indicate the same predictors m arked in the\\ntop left panel. We see that the stump for the blue predictor (w hose stump\\nwas the best in the top left panel), makes two out of four test e rrors (50%),\\nand is no better than random.\\nWhat has happened? The preceding argument has ignored the fa ct that\\nin cross-validation, the model must be completely retraine d for each fold\\n2This argument was made to us by a scientist at a proteomics lab meeting, a nd led\\nto material in this section.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 265}),\n",
       " Document(page_content='248 7. Model Assessment and Selection\\n0 100 200 300 400 5002 3 4 5 6 7 8 9\\nPredictorError on Full Training Set\\n1 2 3 4 5 6 7 80 1 2 3 4\\nError on 4/5Error on 1/5\\n−1 0 1 2\\nPredictor 436 (blue)Class Label\\n0 1\\nfull\\n4/5\\n0.0 0.2 0.4 0.6 0.8 1.0\\nCV Errors\\nFIGURE 7.11. Simulation study to investigate the performance of cross vali -\\ndation in a high-dimensional problem where the predictors ar e independent of the\\nclass labels. The top-left panel shows the number of errors made b y individual\\nstump classiﬁers on the full training set ( 20observations). The top right panel\\nshows the errors made by individual stumps trained on a rando m split of the\\ndataset into 4/5ths (16observations) and tested on the remaining 1/5th (4ob-\\nservations). The best performers are depicted by colored dot s in each panel. The\\nbottom left panel shows the eﬀect of re-estimating the split po int in each fold: the\\ncolored points correspond to the four samples in the 1/5th validation set. The split\\npoint derived from the full dataset classiﬁes all four samples correctly, but when\\nthe split point is re-estimated on the 4/5ths data (as it should be), it commits\\ntwo errors on the four validation samples. In the bottom right w e see the overall\\nresult of ﬁve-fold cross-validation applied to 50simulated datasets. The average\\nerror rate is about 50%, as it should be.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 266}),\n",
       " Document(page_content='7.11 Bootstrap Methods 249\\nof the process. In the present example, this means that the be st predictor\\nand corresponding split point are found from 4 /5ths of the data. The eﬀect\\nof predictor choice is seen in the top right panel. Since the c lass labels are\\nindependent of the predictors, the performance of a stump on the 4/5ths\\ntraining data contains no information about its performanc e in the remain-\\ning 1/5th. The eﬀect of the choice of split point is shown in the bott om left\\npanel. Here we see the data for predictor 436, corresponding to the blue\\ndot in the top left plot. The colored points indicate the 1 /5th data, while\\nthe remaining points belong to the 4 /5ths. The optimal split points for this\\npredictor based on both the full training set and 4 /5ths data are indicated.\\nThe split based on the full data makes no errors on the 1 /5ths data. But\\ncross-validation must base its split on the 4 /5ths data, and this incurs two\\nerrors out of four samples.\\nThe results of applying ﬁve-fold cross-validation to each o f 50 simulated\\ndatasets is shown in the bottom right panel. As we would hope, the average\\ncross-validation error is around 50%, which is the true expe cted prediction\\nerror for this classiﬁer. Hence cross-validation has behav ed as it should.\\nOn the other hand, there is considerable variability in the e rror, underscor-\\ning the importance of reporting the estimated standard erro r of the CV\\nestimate. See Exercise 7.10 for another variation of this pr oblem.\\n7.11 Bootstrap Methods\\nThe bootstrap is a general tool for assessing statistical ac curacy. First we\\ndescribe the bootstrap in general, and then show how it can be used to\\nestimate extra-sample prediction error. As with cross-val idation, the boot-\\nstrap seeks to estimate the conditional error Err T, but typically estimates\\nwell only the expected prediction error Err.\\nSuppose we have a model ﬁt to a set of training data. We denote t he\\ntraining set by Z= (z1,z2,...,z N) wherezi= (xi,yi). The basic idea is\\nto randomly draw datasets with replacement from the trainin g data, each\\nsample the same size as the original training set. This is don eBtimes\\n(B= 100 say), producing Bbootstrap datasets, as shown in Figure 7.12.\\nThen we reﬁt the model to each of the bootstrap datasets, and e xamine\\nthe behavior of the ﬁts over the Breplications.\\nIn the ﬁgure, S(Z) is any quantity computed from the data Z, for ex-\\nample, the prediction at some input point. From the bootstra p sampling\\nwe can estimate any aspect of the distribution of S(Z), for example, its\\nvariance,\\nˆVar[S(Z)] =1\\nB−1B∑\\nb=1(S(Z∗b)−¯S∗)2, (7.53)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 267}),\n",
       " Document(page_content='250 7. Model Assessment and Selection\\n  Bootstrap\\nBootstrapreplications\\nsamples\\nsampleTrainingZ= (z1,z2,...,z N)Z∗1Z∗2Z∗BS(Z∗1)S(Z∗2) S(Z∗B)\\nFIGURE 7.12. Schematic of the bootstrap process. We wish to assess the sta -\\ntistical accuracy of a quantity S(Z)computed from our dataset. Btraining sets\\nZ∗b, b= 1,...,Beach of size Nare drawn with replacement from the original\\ndataset. The quantity of interest S(Z)is computed from each bootstrap training\\nset, and the values S(Z∗1),...,S(Z∗B)are used to assess the statistical accuracy\\nofS(Z).\\nwhere¯S∗=∑\\nbS(Z∗b)/B. Note thatˆVar[S(Z)] can be thought of as a\\nMonte-Carlo estimate of the variance of S(Z) under sampling from the\\nempirical distribution function ˆFfor the data ( z1,z2,...,z N).\\nHow can we apply the bootstrap to estimate prediction error? One ap-\\nproach would be to ﬁt the model in question on a set of bootstra p samples,\\nand then keep track of how well it predicts the original train ing set. If\\nˆf∗b(xi) is the predicted value at xi, from the model ﬁtted to the bth boot-\\nstrap dataset, our estimate is\\nˆErrboot=1\\nB1\\nNB∑\\nb=1N∑\\ni=1L(yi,ˆf∗b(xi)). (7.54)\\nHowever,itiseasytoseethat ˆErrbootdoesnotprovideagoodestimatein\\ngeneral. The reason is that the bootstrap datasets are actin g as the training\\nsamples, while the original training set is acting as the tes t sample, and\\nthese two samples have observations in common. This overlap can make\\noverﬁt predictions look unrealistically good, and is the re ason that cross-\\nvalidation explicitly uses non-overlapping data for the tr aining and test\\nsamples. Consider for example a 1-nearest neighbor classiﬁ er applied to a\\ntwo-class classiﬁcation problem with the same number of obs ervations in', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 268}),\n",
       " Document(page_content='7.11 Bootstrap Methods 251\\neach class, in which the predictors and class labels are in fa ct independent.\\nThen the true error rate is 0 .5. But the contributions to the bootstrap\\nestimateˆErrbootwill be zero unless the observation idoes not appear in the\\nbootstrap sample b. In this latter case it will have the correct expectation\\n0.5. Now\\nPr{observation i∈bootstrap sample b}= 1−(\\n1−1\\nN)N\\n≈1−e−1\\n= 0.632. (7.55)\\nHence the expectation of ˆErrbootis about 0.5×0.368 = 0.184, far below\\nthe correct error rate 0 .5.\\nBy mimicking cross-validation, a better bootstrap estimat e can be ob-\\ntained. For each observation, we only keep track of predicti ons from boot-\\nstrap samples not containing that observation. The leave-o ne-out bootstrap\\nestimate of prediction error is deﬁned by\\nˆErr(1)=1\\nNN∑\\ni=11\\n|C−i|∑\\nb∈C−iL(yi,ˆf∗b(xi)). (7.56)\\nHereC−iis the set of indices of the bootstrap samples bthat donotcontain\\nobservation i,and|C−i|isthenumberofsuchsamples.Incomputing ˆErr(1),\\nwe either have to choose Blarge enough to ensure that all of the |C−i|are\\ngreaterthanzero,orwecanjustleaveoutthetermsin(7.56) corresponding\\nto|C−i|’s that are zero.\\nThe leave-one out bootstrap solves the overﬁtting problem s uﬀered by\\nˆErrboot, but has the training-set-size bias mentioned in the discus sion of\\ncross-validation. The average number of distinct observat ions in each boot-\\nstrap sample is about 0 .632·N, so its bias will roughly behave like that of\\ntwofold cross-validation. Thus if the learning curve has co nsiderable slope\\nat sample size N/2, the leave-one out bootstrap will be biased upward as\\nan estimate of the true error.\\nThe “.632 estimator” is designed to alleviate this bias. It is deﬁn ed by\\nˆErr(.632)=.368·err+.632·ˆErr(1). (7.57)\\nThe derivation of the .632 estimator is complex; intuitively it pulls the\\nleave-one out bootstrap estimate down toward the training e rror rate, and\\nhencereducesitsupwardbias.Theuseoftheconstant.632re latesto(7.55).\\nThe.632 estimator works well in “light ﬁtting” situations, but c an break\\ndown in overﬁt ones. Here is an example due to Breiman et al. (1 984).\\nSuppose we have two equal-size classes, with the targets ind ependent of\\nthe class labels, and we apply a one-nearest neighbor rule. T henerr = 0,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 269}),\n",
       " Document(page_content='252 7. Model Assessment and Selection\\nˆErr(1)= 0.5 and soˆErr(.632)=.632×0.5 =.316. However, the true error\\nrate is 0.5.\\nOne can improve the .632 estimator by taking into account the amount\\nof overﬁtting. First we deﬁne γto be the no-information error rate : this\\nis the error rate of our prediction rule if the inputs and clas s labels were\\nindependent. An estimate of γis obtained by evaluating the prediction rule\\non all possible combinations of targets yiand predictors xi′\\nˆγ=1\\nN2N∑\\ni=1N∑\\ni′=1L(yi,ˆf(xi′)). (7.58)\\nFor example, consider the dichotomous classiﬁcation probl em: let ˆp1be\\nthe observed proportion of responses yiequaling 1, and let ˆ q1be the ob-\\nserved proportion of predictions ˆf(xi′) equaling 1. Then\\nˆγ= ˆp1(1−ˆq1)+(1−ˆp1)ˆq1. (7.59)\\nWith a rule like 1-nearest neighbors for which ˆ q1= ˆp1the value of ˆ γis\\n2ˆp1(1−ˆp1).Themulti-category generalization of(7.59) is ˆ γ=∑\\nℓˆpℓ(1−ˆqℓ).\\nUsing this, the relative overﬁtting rate is deﬁned to be\\nˆR=ˆErr(1)−err\\nˆγ−err, (7.60)\\na quantity that ranges from 0 if there is no overﬁtting ( ˆErr(1)=err) to 1\\nif the overﬁtting equals the no-information value ˆ γ−err. Finally, we deﬁne\\nthe “.632+” estimator by\\nˆErr(.632+)= (1−ˆw)·err+ ˆw·ˆErr(1)(7.61)\\nwith ˆw=.632\\n1−.368ˆR.\\nThe weight wranges from .632 ifˆR= 0 to 1 if ˆR= 1, soˆErr(.632+)\\nranges fromˆErr(.632)toˆErr(1). Again, the derivation of (7.61) is compli-\\ncated: roughly speaking, it produces a compromise between t he leave-one-\\nout bootstrap and the training error rate that depends on the amount of\\noverﬁtting. For the 1-nearest-neighbor problem with class labels indepen-\\ndent of the inputs, ˆ w=ˆR= 1, soˆErr(.632+)=ˆErr(1), which has the correct\\nexpectation of 0.5. In other problems with less overﬁtting, ˆErr(.632+)will\\nlie somewhere between err andˆErr(1).\\n7.11.1 Example (Continued)\\nFigure7.13showstheresultsoftenfoldcross-validationa ndthe.632+boot-\\nstrap estimate in the same four problems of Figures 7.7. As in that ﬁgure,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 270}),\n",
       " Document(page_content='7.11 Bootstrap Methods 253\\nreg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestCross−validation\\nreg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestBootstrap\\nFIGURE 7.13. Boxplots show the distribution of the relative error\\n100·[Errˆα−minαErr(α)]/[maxαErr(α)−minαErr(α)]over the four scenar-\\nios of Figure 7.3. This is the error in using the chosen model r elative to the best\\nmodel. There are 100training sets represented in each boxplot.\\nFigure 7.13 shows boxplots of 100 ·[Errˆα−minαErr(α)]/[maxαErr(α)−\\nminαErr(α)],theerrorinusingthechosenmodelrelativetothebestmo del.\\nThereare100diﬀerent trainingsets representedin eachbox plot. Both mea-\\nsures perform well overall, perhaps the same or slightly wor se than the AIC\\nin Figure 7.7.\\nOur conclusion is that for these particular problems and ﬁtt ing methods,\\nminimization of either AIC, cross-validation or bootstrap yields a model\\nfairly close to the best available. Note that for the purpose of model selec-\\ntion, any of the measures could be biased and it wouldn’t aﬀec t things, as\\nlong as the bias did not change the relative performance of th e methods.\\nFor example, the addition of a constant to any of the measures would not\\nchange the resulting chosen model. However, for many adapti ve, nonlinear\\ntechniques (like trees), estimation of the eﬀective number of parameters is\\nvery diﬃcult. This makes methods like AIC impractical and le aves us with\\ncross-validation or bootstrap as the methods of choice.\\nA diﬀerent question is: how well does each method estimate te st error?\\nOn the average the AIC criterion overestimated prediction e rror of its cho-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 271}),\n",
       " Document(page_content='254 7. Model Assessment and Selection\\nsenmodelby38%,37%,51%,and30%,respectively,overthefo urscenarios,\\nwith BIC performing similarly. In contrast, cross-validat ion overestimated\\nthe error by 1%, 4%, 0%, and 4%, with the bootstrap doing about the\\nsame. Hence the extra work involved in computing a cross-val idation or\\nbootstrap measure is worthwhile, if an accurate estimate of test error is\\nrequired. With other ﬁtting methods like trees, cross-vali dation and boot-\\nstrap can underestimate the true error by 10%, because the se arch for best\\ntree is strongly aﬀected by the validation set. In these situ ations only a\\nseparate test set will provide an unbiased estimate of test e rror.\\n7.12 Conditional or Expected Test Error?\\nFigures7.14and7.15examinethequestionofwhethercross- validationdoes\\na good job in estimating Err T, the error conditional on a given training set\\nT(expression (7.15) on page 228), as opposed to the expected t est error.\\nFor each of 100 training sets generated from the “reg/linear ” setting in\\nthe top-right panel of Figure 7.3, Figure 7.14 shows the cond itional error\\ncurvesErr Tasafunctionofsubsetsize(topleft).Thenexttwopanelssh ow\\n10-fold and N-fold cross-validation, the latter also known as leave-one -out\\n(LOO). The thick red curve in each plot is the expected error E rr, while\\nthe thick black curves are the expected cross-validation cu rves. The lower\\nright panel shows how well cross-validation approximates t he conditional\\nand expected error.\\nOne might have expected N-fold CV to approximate Err Twell, since it\\nalmostuses the full training sample to ﬁt a new test point. 10-fold C V, on\\nthe other hand, might be expected to estimate Err well, since it averages\\nover somewhat diﬀerent training sets. From the ﬁgure it appe ars 10-fold\\ndoes a better job than N-fold in estimating Err T, and estimates Err even\\nbetter. Indeed, the similarity of the two black curves with t he red curve\\nsuggests both CV curves are approximately unbiased for Err, with 10-fold\\nhaving less variance. Similar trends were reported by Efron (1983).\\nFigure7.15showsscatterplotsofboth10-foldand N-foldcross-validation\\nerror estimates versus the true conditional error for the 10 0 simulations.\\nAlthough the scatterplots do not indicate much correlation , the lower right\\npanel shows that for the most part the correlations are negat ive, a curi-\\nous phenomenon that has been observed before. This negative correlation\\nexplains why neither form of CV estimates Err Twell. The broken lines in\\neach plot are drawn at Err( p), the expected error for the best subset of\\nsizep. We see again that both forms of CV are approximately unbiase d for\\nexpected error, but the variation in test error for diﬀerent training sets is\\nquite substantial.\\nAmongthefourexperimental conditions in7.3,this“reg/li near”scenario\\nshowedthehighestcorrelationbetweenactualandpredicte dtesterror.This', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 272}),\n",
       " Document(page_content='7.12 Conditional or Expected Test Error? 255\\n5 10 15 200.1 0.2 0.3 0.4Prediction Error\\nSubset Size pError\\n5 10 15 200.1 0.2 0.3 0.410−Fold CV Error\\nSubset Size pError\\n5 10 15 200.1 0.2 0.3 0.4Leave−One−Out CV Error\\nSubset Size pError\\n5 10 15 200.015 0.025 0.035 0.045Approximation Error\\nSubset Size pMean Absolute DeviationET|CV10−Err|\\nET|CV10−ErrT|\\nET|CVN−ErrT|\\nFIGURE 7.14. Conditional prediction-error ErrT,10-fold cross-validation, and\\nleave-one-out cross-validation curves for a 100simulations from the top-right\\npanel in Figure 7.3. The thick red curve is the expected predi ction error Err,\\nwhile the thick black curves are the expected CV curves ETCV10andETCVN.\\nThe lower-right panel shows the mean absolute deviation of the CV curves from\\nthe conditional error, ET|CVK−ErrT|forK= 10(blue) andK=N(green),\\nas well as from the expected error ET|CV10−Err|(orange).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 273}),\n",
       " Document(page_content='256 7. Model Assessment and Selection\\n0.10 0.15 0.20 0.25 0.30 0.35 0.400.10 0.20 0.30 0.40Subset Size 1\\nPrediction ErrorCV Error\\n0.10 0.15 0.20 0.25 0.30 0.35 0.400.10 0.20 0.30 0.40Subset Size 5\\nPrediction ErrorCV Error\\n0.10 0.15 0.20 0.25 0.30 0.35 0.400.10 0.20 0.30 0.40Subset Size 10\\nPrediction ErrorCV Error\\n5 10 15 20−0.6 −0.4 −0.2 0.0 0.2\\nSubset SizeCorrelation\\nLeave−one−out\\n10−Fold\\nFIGURE 7.15. Plots of the CV estimates of error versus the true conditional\\nerror for each of the 100training sets, for the simulation setup in the top right\\npanel Figure 7.3. Both 10-fold and leave-one-out CV are depicted in diﬀerent\\ncolors. The ﬁrst three panels correspond to diﬀerent subset si zesp, and vertical\\nand horizontal lines are drawn at Err(p). Although there appears to be little cor-\\nrelation in these plots, we see in the lower right panel that for t he most part the\\ncorrelation is negative.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 274}),\n",
       " Document(page_content='Exercises 257\\nphenomenon also occurs for bootstrap estimates of error, an d we would\\nguess, for any other estimate of conditional prediction err or.\\nWe conclude that estimation of test error for a particular tr aining set is\\nnoteasyingeneral,givenjustthedatafromthatsametraini ngset.Instead,\\ncross-validation and related methods may provide reasonab le estimates of\\ntheexpected error Err.\\nBibliographic Notes\\nKey references for cross-validation are Stone (1974), Ston e (1977) and\\nAllen (1974). The AIC was proposed by Akaike (1973), while th e BIC\\nwas introduced by Schwarz (1978). Madigan and Raftery (1994 ) give an\\noverview of Bayesian model selection. The MDL criterion is d ue to Rissa-\\nnen(1983).CoverandThomas(1991)containsagooddescript ionofcoding\\ntheory and complexity. VC dimension is described in Vapnik ( 1996). Stone\\n(1977) showed that the AIC and leave-one out cross-validati on are asymp-\\ntotically equivalent. Generalized cross-validation is de scribed by Golub et\\nal. (1979) and Wahba (1980); a further discussion of the topi c may be found\\nin the monograph by Wahba (1990). See also Hastie and Tibshir ani (1990),\\nChapter 3. The bootstrap is due to Efron (1979); see Efron and Tibshi-\\nrani (1993) for an overview. Efron (1983) proposes a number o f bootstrap\\nestimates of prediction error, including the optimism and . 632 estimates.\\nEfron (1986) compares CV, GCV and bootstrap estimates of err or rates.\\nThe use of cross-validation and the bootstrap for model sele ction is stud-\\nied by Breiman and Spector (1992), Breiman (1992), Shao (199 6), Zhang\\n(1993) and Kohavi (1995). The .632+ estimator was proposed by Efron\\nand Tibshirani (1997).\\nCherkassky and Ma (2003) published a study on the performanc e of\\nSRM for model selection in regression, in response to our stu dy of section\\n7.9.1. They complained that we had been unfair to SRM because had not\\napplied it properly. Our response can be found in the same iss ue of the\\njournal (Hastie et al. (2003)).\\nExercises\\nEx. 7.1Derive the estimate of in-sample error (7.24).\\nEx. 7.2For 0–1 loss with Y∈{0,1}and Pr(Y= 1|x0) =f(x0), show that\\nErr(x0) = Pr(Y̸=ˆG(x0)|X=x0)\\n= Err B(x0)+|2f(x0)−1|Pr(ˆG(x0)̸=G(x0)|X=x0),\\n(7.62)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 275}),\n",
       " Document(page_content='258 7. Model Assessment and Selection\\nwhereˆG(x) =I(ˆf(x)>1\\n2),G(x) =I(f(x)>1\\n2) is the Bayes classiﬁer,\\nand Err B(x0) = Pr(Y̸=G(x0)|X=x0), the irreducible Bayes error atx0.\\nUsing the approximation ˆf(x0)∼N(Eˆf(x0),Var(ˆf(x0)), show that\\nPr(ˆG(x0)̸=G(x0)|X=x0)≈Φ(\\nsign(1\\n2−f(x0))(Eˆf(x0)−1\\n2)√\\nVar(ˆf(x0)))\\n.(7.63)\\nIn the above,\\nΦ(t) =1√\\n2π∫t\\n−∞exp(−t2/2)dt,\\nthe cumulative Gaussian distribution function. This is an i ncreasing func-\\ntion, with value 0 at t=−∞and value 1 at t= +∞.\\nWe can think of sign(1\\n2−f(x0))(Eˆf(x0)−1\\n2) as a kind of boundary-\\nbiasterm, as it depends on the true f(x0) only through which side of the\\nboundary (1\\n2) that it lies. Notice also that the bias and variance combine\\nin a multiplicative rather than additive fashion. If E ˆf(x0) is on the same\\nside of1\\n2asf(x0), then the bias is negative, and decreasing the variance\\nwill decrease the misclassiﬁcation error. On the other hand , if Eˆf(x0) is\\non the opposite side of1\\n2tof(x0), then the bias is positive and it pays to\\nincrease the variance! Such an increase will improve the cha nce that ˆf(x0)\\nfalls on the correct side of1\\n2(Friedman, 1997).\\nEx. 7.3Letˆf=Sybe a linear smoothing of y.\\n(a)IfSiiistheithdiagonalelementof S,showthatfor Sarisingfromleast\\nsquares projections and cubic smoothing splines, the cross -validated\\nresidual can be written as\\nyi−ˆf−i(xi) =yi−ˆf(xi)\\n1−Sii. (7.64)\\n(b) Use this result to show that |yi−ˆf−i(xi)|≥|yi−ˆf(xi)|.\\n(c) Find general conditions on any smoother Sto make result (7.64) hold.\\nEx. 7.4Consider the in-sample prediction error (7.18) and the trai ning\\nerrorerr in the case of squared-error loss:\\nErrin=1\\nNN∑\\ni=1EY0(Y0\\ni−ˆf(xi))2\\nerr =1\\nNN∑\\ni=1(yi−ˆf(xi))2.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 276}),\n",
       " Document(page_content='Exercises 259\\nAdd and subtract f(xi) and Eˆf(xi) in each expression and expand. Hence\\nestablish that the average optimism in the training error is\\n2\\nNN∑\\ni=1Cov(ˆyi,yi),\\nas given in (7.21).\\nEx. 7.5For a linear smoother ˆy=Sy, show that\\nN∑\\ni=1Cov(ˆyi,yi) = trace( S)σ2\\nε, (7.65)\\nwhich justiﬁes its use as the eﬀective number of parameters.\\nEx. 7.6Show that for an additive-error model, the eﬀective degrees -of-\\nfreedom for the k-nearest-neighbors regression ﬁt is N/k.\\nEx.7.7Usetheapproximation1 /(1−x)2≈1+2xtoexposetherelationship\\nbetween Cp/AIC (7.26) and GCV (7.52), the main diﬀerence being the\\nmodel used to estimate the noise variance σ2\\nε.\\nEx. 7.8Show that the set of functions {I(sin(αx)>0)}can shatter the\\nfollowing points on the line:\\nz1= 10−1,...,zℓ= 10−ℓ, (7.66)\\nfor anyℓ. Hence the VC dimension of the class {I(sin(αx)>0)}is inﬁnite.\\nEx. 7.9For the prostate data of Chapter 3, carry out a best-subset li near\\nregression analysis, as in Table 3.3 (third column from left ). Compute the\\nAIC, BIC, ﬁve- and tenfold cross-validation, and bootstrap .632 estimates\\nof prediction error. Discuss the results.\\nEx. 7.10 Referring to the example in Section 7.10.3, suppose instead that\\nall of theppredictors are binary, and hence there is no need to estimate\\nsplit points. The predictors are independent of the class la bels as before.\\nThen ifpis very large, we can probably ﬁnd a predictor that splits the\\nentire training data perfectly, and hence would split the va lidation data\\n(one-ﬁfth of data) perfectly as well. This predictor would t herefore have\\nzero cross-validation error. Does this mean that cross-val idation does not\\nprovide a good estimate of test error in this situation? [Thi s question was\\nsuggested by Li Ma.]', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 277}),\n",
       " Document(page_content='260 7. Model Assessment and Selection', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 278}),\n",
       " Document(page_content='This is page 261\\nPrinter: Opaque this\\n8\\nModel Inference and Averaging\\n8.1 Introduction\\nFor most of this book, the ﬁtting (learning) of models has bee n achieved by\\nminimizing a sum of squares for regression, or by minimizing cross-entropy\\nfor classiﬁcation. In fact, both of these minimizations are instances of the\\nmaximum likelihood approach to ﬁtting.\\nIn this chapter we provide a general exposition of the maximu m likeli-\\nhood approach, as well as the Bayesian method for inference. The boot-\\nstrap, introduced in Chapter 7, is discussed in this context , and its relation\\nto maximum likelihood and Bayes is described. Finally, we pr esent some\\nrelated techniques for model averaging and improvement, in cluding com-\\nmittee methods, bagging, stacking and bumping.\\n8.2 The Bootstrap and Maximum Likelihood\\nMethods\\n8.2.1 A Smoothing Example\\nThe bootstrap method provides a direct computational way of assessing\\nuncertainty, by sampling from the training data. Here we ill ustrate the\\nbootstrap in a simple one-dimensional smoothing problem, a nd show its\\nconnection to maximum likelihood.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 279}),\n",
       " Document(page_content='262 8. Model Inference and Averaging\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\\nxy\\n••••••\\n••••••••••\\n••••\\n•\\n•\\n•••\\n•••••••••\\n••••••••\\n••••\\n••\\n••\\n0.0 0.5 1.0 1.5 2.0 2.5 3.00.0 0.2 0.4 0.6 0.8 1.0\\nxB-spline Basis\\nFIGURE 8.1. (Left panel): Data for smoothing example. (Right panel:) Set of\\nsevenB-spline basis functions. The broken vertical lines indicate t he placement\\nof the three knots.\\nDenote the training data by Z={z1,z2,...,z N}, withzi= (xi,yi),\\ni= 1,2,...,N. Herexiis a one-dimensional input, and yithe outcome,\\neither continuous or categorical. As an example, consider t heN= 50 data\\npoints shown in the left panel of Figure 8.1.\\nSuppose we decide to ﬁt a cubic spline to the data, with three k nots\\nplaced at the quartiles of the Xvalues. This is a seven-dimensional lin-\\near space of functions, and can be represented, for example, by a linear\\nexpansion of B-spline basis functions (see Section 5.9.2):\\nµ(x) =7∑\\nj=1βjhj(x). (8.1)\\nHere thehj(x),j= 1,2,...,7 are the seven functions shown in the right\\npanel of Figure 8.1. We can think of µ(x) as representing the conditional\\nmean E(Y|X=x).\\nLetHbe theN×7 matrix with ijth element hj(xi). The usual estimate\\nofβ, obtained by minimizing the squared error over the training set, is\\ngiven by\\nˆβ= (HTH)−1HTy. (8.2)\\nThe corresponding ﬁt ˆ µ(x) =∑7\\nj=1ˆβjhj(x) is shown in the top left panel\\nof Figure 8.2.\\nThe estimated covariance matrix of ˆβis\\nˆVar(ˆβ) = (HTH)−1ˆσ2, (8.3)\\nwhere we have estimated the noise variance by ˆ σ2=∑N\\ni=1(yi−ˆµ(xi))2/N.\\nLettingh(x)T= (h1(x),h2(x),...,h 7(x)), the standard error of a predic-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 280}),\n",
       " Document(page_content='8.2 The Bootstrap and Maximum Likelihood Methods 263\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5••••••\\n••••••••••\\n••••\\n•\\n•\\n•••\\n•••••••••\\n••••••••\\n••••\\n••\\n••\\nxy\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\\nxy\\n••••••\\n••••••••••\\n••••\\n•\\n•\\n•••\\n•••••••••\\n••••••••\\n••••\\n••\\n••\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\\nxy\\n••••••\\n••••••••••\\n••••\\n•\\n•\\n•••\\n•••••••••\\n••••••••\\n••••\\n••\\n••\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\\nxy\\n••••••\\n••••••••••\\n••••\\n•\\n•\\n•••\\n•••••••••\\n••••••••\\n••••\\n••\\n••\\nFIGURE 8.2. (Top left:)B-spline smooth of data. (Top right:) B-spline smooth\\nplus and minus 1.96×standard error bands. (Bottom left:) Ten bootstrap repli-\\ncates of the B-spline smooth. (Bottom right:) B-spline smooth with 95% standard\\nerror bands computed from the bootstrap distribution.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 281}),\n",
       " Document(page_content='264 8. Model Inference and Averaging\\ntion ˆµ(x) =h(x)Tˆβis\\nˆse[ˆµ(x)] = [h(x)T(HTH)−1h(x)]1\\n2ˆσ. (8.4)\\nIn the top right panel of Figure 8.2 we have plotted ˆ µ(x)±1.96·ˆse[ˆµ(x)].\\nSince 1.96 is the 97.5% point of the standard normal distribu tion, these\\nrepresent approximate 100 −2×2.5% = 95% pointwise conﬁdence bands\\nforµ(x).\\nHere is how we could apply the bootstrap in this example. We dr awB\\ndatasets each of size N= 50 with replacement from our training data, the\\nsampling unit being the pair zi= (xi,yi). To each bootstrap dataset Z∗\\nwe ﬁt a cubic spline ˆ µ∗(x); the ﬁts from ten such samples are shown in the\\nbottom left panel of Figure 8.2. Using B= 200 bootstrap samples, we can\\nform a 95% pointwise conﬁdence band from the percentiles at e achx: we\\nﬁnd the 2.5%×200 = ﬁfth largest and smallest values at each x. These are\\nplotted in the bottom right panel of Figure 8.2. The bands loo k similar to\\nthose in the top right, being a little wider at the endpoints.\\nThere is actually a close connection between the least squar es estimates\\n(8.2)and(8.3),thebootstrap,andmaximumlikelihood.Sup posewefurther\\nassume that the model errors are Gaussian,\\nY=µ(X)+ε;ε∼N(0,σ2),\\nµ(x) =7∑\\nj=1βjhj(x). (8.5)\\nThe bootstrap method described above, in which we sample wit h re-\\nplacement from the training data, is called the nonparametric bootstrap .\\nThis really means that the method is “model-free,” since it u ses the raw\\ndata, not a speciﬁc parametric model, to generate new datase ts. Consider\\na variation of the bootstrap, called the parametric bootstrap , in which we\\nsimulate new responses by adding Gaussian noise to the predi cted values:\\ny∗\\ni= ˆµ(xi)+ε∗\\ni;ε∗\\ni∼N(0,ˆσ2);i= 1,2,...,N. (8.6)\\nThis process is repeated Btimes, where B= 200 say. The resulting boot-\\nstrap datasets have the form ( x1,y∗\\n1),...,(xN,y∗\\nN) and we recompute the\\nB-spline smooth on each. The conﬁdence bands from this method will ex-\\nactly equal the least squares bands in the top right panel, as the number of\\nbootstrap samples goes to inﬁnity. A function estimated fro m a bootstrap\\nsampley∗is given by ˆ µ∗(x) =h(x)T(HTH)−1HTy∗, and has distribution\\nˆµ∗(x)∼N(ˆµ(x),h(x)T(HTH)−1h(x)ˆσ2). (8.7)\\nNotice that the mean of this distribution is the least square s estimate, and\\nthe standard deviation is the same as the approximate formul a (8.4).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 282}),\n",
       " Document(page_content='8.2 The Bootstrap and Maximum Likelihood Methods 265\\n8.2.2 Maximum Likelihood Inference\\nIt turns out that the parametric bootstrap agrees with least squares in the\\nprevious example because the model (8.5) has additive Gauss ian errors. In\\ngeneral, the parametric bootstrap agrees not with least squ ares but with\\nmaximum likelihood, which we now review.\\nWe begin by specifying a probability density or probability mass function\\nfor our observations\\nzi∼gθ(z). (8.8)\\nIn this expression θrepresents one or more unknown parameters that gov-\\nern the distribution of Z. This is called a parametric model forZ. As an\\nexample, if Zhas a normal distribution with mean µand variance σ2, then\\nθ= (µ,σ2), (8.9)\\nand\\ngθ(z) =1√\\n2πσe−1\\n2(z−µ)2/σ2. (8.10)\\nMaximum likelihood is based on the likelihood function , given by\\nL(θ;Z) =N∏\\ni=1gθ(zi), (8.11)\\nthe probability of the observed data under the model gθ. The likelihood is\\ndeﬁned only up to a positive multiplier, which we have taken t o be one.\\nWe think of L(θ;Z) as a function of θ, with our data Zﬁxed.\\nDenote the logarithm of L(θ;Z) by\\nℓ(θ;Z) =N∑\\ni=1ℓ(θ;zi)\\n=N∑\\ni=1loggθ(zi), (8.12)\\nwhich we will sometimes abbreviate as ℓ(θ). This expression is called the\\nlog-likelihood, and each value ℓ(θ;zi) = loggθ(zi) is called a log-likelihood\\ncomponent. The method of maximum likelihood chooses the val ueθ=ˆθ\\nto maximize ℓ(θ;Z).\\nThe likelihood function can be used to assess the precision o fˆθ. We need\\na few more deﬁnitions. The score function is deﬁned by\\n˙ℓ(θ;Z) =N∑\\ni=1˙ℓ(θ;zi), (8.13)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 283}),\n",
       " Document(page_content='266 8. Model Inference and Averaging\\nwhere˙ℓ(θ;zi) =∂ℓ(θ;zi)/∂θ. Assuming that the likelihood takes its maxi-\\nmum in the interior of the parameter space, ˙ℓ(ˆθ;Z) = 0. The information\\nmatrixis\\nI(θ) =−N∑\\ni=1∂2ℓ(θ;zi)\\n∂θ∂θT. (8.14)\\nWhenI(θ) is evaluated at θ=ˆθ, it is often called the observed information .\\nTheFisher information (or expected information) is\\ni(θ) = Eθ[I(θ)]. (8.15)\\nFinally, let θ0denote the true value of θ.\\nA standard result says that the sampling distribution of the maximum\\nlikelihood estimator has a limiting normal distribution\\nˆθ→N(θ0,i(θ0)−1), (8.16)\\nasN→∞. Here we are independently sampling from gθ0(z). This suggests\\nthat the sampling distribution of ˆθmay be approximated by\\nN(ˆθ,i(ˆθ)−1) orN(ˆθ,I(ˆθ)−1), (8.17)\\nwhereˆθrepresents the maximum likelihood estimate from the observ ed\\ndata.\\nThe corresponding estimates for the standard errors of ˆθjare obtained\\nfrom\\n√\\ni(ˆθ)−1\\njjand√\\nI(ˆθ)−1\\njj. (8.18)\\nConﬁdence points for θjcan be constructed from either approximation\\nin (8.17). Such a conﬁdence point has the form\\nˆθj−z(1−α)·√\\ni(ˆθ)−1\\njjorˆθj−z(1−α)·√\\nI(ˆθ)−1\\njj,\\nrespectively, where z(1−α)is the 1−αpercentile of the standard normal\\ndistribution. More accurate conﬁdence intervals can be der ived from the\\nlikelihood function, by using the chi-squared approximati on\\n2[ℓ(ˆθ)−ℓ(θ0)]∼χ2\\np, (8.19)\\nwherepis the number of components in θ. The resulting 1 −2αconﬁ-\\ndence interval is the set of all θ0such that 2[ ℓ(ˆθ)−ℓ(θ0)]≤χ2\\np(1−2α),\\nwhereχ2\\np(1−2α)is the 1−2αpercentile of the chi-squared distribution with\\npdegrees of freedom.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 284}),\n",
       " Document(page_content='8.3 Bayesian Methods 267\\nLet’s return to our smoothing example to see what maximum lik elihood\\nyields. The parameters are θ= (β,σ2). The log-likelihood is\\nℓ(θ) =−N\\n2logσ22π−1\\n2σ2N∑\\ni=1(yi−h(xi)Tβ)2. (8.20)\\nThe maximum likelihood estimate is obtained by setting ∂ℓ/∂β= 0 and\\n∂ℓ/∂σ2= 0, giving\\nˆβ= (HTH)−1HTy,\\nˆσ2=1\\nN∑\\n(yi−ˆµ(xi))2,(8.21)\\nwhich are the same as the usual estimates given in (8.2) and be low (8.3).\\nThe information matrix for θ= (β,σ2) is block-diagonal, and the block\\ncorresponding to βis\\nI(β) = (HTH)/σ2, (8.22)\\nso that the estimated variance ( HTH)−1ˆσ2agrees with the least squares\\nestimate (8.3).\\n8.2.3 Bootstrap versus Maximum Likelihood\\nIn essence the bootstrap is a computer implementation of non parametric or\\nparametric maximum likelihood. The advantage of the bootst rap over the\\nmaximum likelihood formula is that it allows us to compute ma ximum like-\\nlihood estimates of standard errors and other quantities in settings where\\nno formulas are available.\\nIn our example, suppose that we adaptively choose by cross-v alidation\\nthe number and position of the knots that deﬁne the B-splines, rather\\nthan ﬁx them in advance. Denote by λthe collection of knots and their\\npositions. Then the standard errors and conﬁdence bands sho uld account\\nfor the adaptive choice of λ, but there is no way to do this analytically.\\nWith the bootstrap, we compute the B-spline smooth with an adaptive\\nchoice of knots for each bootstrap sample. The percentiles o f the resulting\\ncurves capture the variability from both the noise in the tar gets as well as\\nthat from ˆλ. In this particular example the conﬁdence bands (not shown)\\ndon’t look much diﬀerent than the ﬁxed λbands. But in other problems,\\nwhere more adaptation is used, this can be an important eﬀect to capture.\\n8.3 Bayesian Methods\\nIn the Bayesian approach to inference, we specify a sampling model Pr( Z|θ)\\n(density or probability mass function) for our data given th e parameters,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 285}),\n",
       " Document(page_content='268 8. Model Inference and Averaging\\nand a prior distribution for the parameters Pr( θ) reﬂecting our knowledge\\naboutθbefore we see the data. We then compute the posterior distrib ution\\nPr(θ|Z) =Pr(Z|θ)·Pr(θ)∫\\nPr(Z|θ)·Pr(θ)dθ, (8.23)\\nwhich represents our updated knowledge about θafter we see the data. To\\nunderstand this posterior distribution, one might draw sam ples from it or\\nsummarize by computing its mean or mode. The Bayesian approa ch diﬀers\\nfrom the standard (“frequentist”) method for inference in i ts use of a prior\\ndistribution to express the uncertainty present before see ing the data, and\\nto allow the uncertainty remaining after seeing the data to b e expressed in\\nthe form of a posterior distribution.\\nTheposteriordistributionalsoprovidesthebasisforpred ictingthevalues\\nof a future observation znew, via the predictive distribution :\\nPr(znew|Z) =∫\\nPr(znew|θ)·Pr(θ|Z)dθ. (8.24)\\nIn contrast, the maximum likelihood approach would use Pr( znew|ˆθ),\\nthe data density evaluated at the maximum likelihood estima te, to predict\\nfuture data. Unlike the predictive distribution (8.24), th is does not account\\nfor the uncertainty in estimating θ.\\nLet’s walk through the Bayesian approach in our smoothing ex ample.\\nWe start with the parametric model given by equation (8.5), a nd assume\\nfor the moment that σ2is known. We assume that the observed feature\\nvaluesx1,x2,...,x Nare ﬁxed, so that the randomness in the data comes\\nsolely from yvarying around its mean µ(x).\\nThe second ingredient we need is a prior distribution. Distr ibutions on\\nfunctions are fairly complex entities: one approach is to us e a Gaussian\\nprocess prior in which we specify the prior covariance betwe en any two\\nfunction values µ(x) andµ(x′) (Wahba, 1990; Neal, 1996).\\nHere we take a simpler route: by considering a ﬁnite B-spline basis for\\nµ(x),wecaninsteadprovideapriorforthecoeﬃcients β,andthisimplicitly\\ndeﬁnes a prior for µ(x). We choose a Gaussian prior centered at zero\\nβ∼N(0,τΣ) (8.25)\\nwith the choices of the prior correlation matrix Σand variance τto be\\ndiscussed below. The implicit process prior for µ(x) is hence Gaussian,\\nwith covariance kernel\\nK(x,x′) = cov[ µ(x),µ(x′)]\\n=τ·h(x)TΣh(x′). (8.26)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 286}),\n",
       " Document(page_content='8.3 Bayesian Methods 269\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-3 -2 -1 0 1 2 3µ(x)\\nx\\nFIGURE 8.3. Smoothing example: Ten draws from the Gaussian prior distri-\\nbution for the function µ(x).\\nThe posterior distribution for βis also Gaussian, with mean and covariance\\nE(β|Z) =(\\nHTH+σ2\\nτΣ−1)−1\\nHTy,\\ncov(β|Z) =(\\nHTH+σ2\\nτΣ−1)−1\\nσ2,(8.27)\\nwith the corresponding posterior values for µ(x),\\nE(µ(x)|Z) =h(x)T(\\nHTH+σ2\\nτΣ−1)−1\\nHTy,\\ncov[µ(x),µ(x′)|Z] =h(x)T(\\nHTH+σ2\\nτΣ−1)−1\\nh(x′)σ2.(8.28)\\nHow do we choose the prior correlation matrix Σ? In some settings the\\nprior can be chosen from subject matter knowledge about the p arameters.\\nHere we are willing to say the function µ(x) should be smooth, and have\\nguaranteed this by expressing µin a smooth low-dimensional basis of B-\\nsplines. Hence we can take the prior correlation matrix to be the identity\\nΣ=I. When the number of basis functions is large, this might not b e suf-\\nﬁcient, and additional smoothness can be enforced by imposi ng restrictions\\nonΣ; this is exactly the case with smoothing splines (Section 5. 8.1).\\nFigure 8.3 shows ten draws from the corresponding prior for µ(x). To\\ngenerateposteriorvaluesofthefunction µ(x),wegeneratevalues β′fromits\\nposterior (8.27), giving corresponding posterior value µ′(x) =∑7\\n1β′\\njhj(x).\\nTen such posterior curves are shown in Figure 8.4. Two diﬀere nt values\\nwere used for the prior variance τ, 1 and 1000. Notice how similar the\\nright panel looks to the bootstrap distribution in the botto m left panel', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 287}),\n",
       " Document(page_content='270 8. Model Inference and Averaging\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5••••••\\n••••••••••\\n••••\\n•\\n•\\n•••\\n•••••••••\\n••••••••\\n••••\\n••\\n••\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5••••••\\n••••••••••\\n••••\\n•\\n•\\n•••\\n•••••••••\\n••••••••\\n••••\\n••\\n••µ(x)µ(x)\\nx xτ= 1 τ= 1000\\nFIGURE 8.4. Smoothing example: Ten draws from the posterior distributio n\\nfor the function µ(x), for two diﬀerent values of the prior variance τ. The purple\\ncurves are the posterior means.\\nof Figure 8.2 on page 263. This similarity is no accident. As τ→∞, the\\nposterior distribution (8.27) and the bootstrap distribut ion (8.7) coincide.\\nOn the other hand, for τ= 1, the posterior curves µ(x) in the left panel\\nof Figure 8.4 are smoother than the bootstrap curves, becaus e we have\\nimposed more prior weight on smoothness.\\nThe distribution (8.25) with τ→∞is called a noninformative prior for\\nθ.InGaussianmodels,maximumlikelihoodandparametricboo tstrapanal-\\nyses tend to agree with Bayesian analyses that use a noninfor mative prior\\nfor the free parameters. These tend to agree, because with a c onstant prior,\\nthe posterior distribution is proportional to the likeliho od. This correspon-\\ndence also extends to the nonparametric case, where the nonp arametric\\nbootstrap approximates a noninformative Bayes analysis; S ection 8.4 has\\nthe details.\\nWe have, however, done some things that are not proper from a B ayesian\\npoint of view. We have used a noninformative (constant) prio r forσ2and\\nreplaced it with the maximum likelihood estimate ˆ σ2in the posterior. A\\nmore standard Bayesian analysis would also put a prior on σ(typically\\ng(σ)∝1/σ), calculate a joint posterior for µ(x) andσ, and then integrate\\noutσ, rather than just extract the maximum of the posterior distr ibution\\n(“MAP” estimate).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 288}),\n",
       " Document(page_content='8.4 Relationship Between the Bootstrap and Bayesian Infere nce 271\\n8.4 Relationship Between the Bootstrap and\\nBayesian Inference\\nConsider ﬁrst a very simple example, in which we observe a sin gle obser-\\nvationzfrom a normal distribution\\nz∼N(θ,1). (8.29)\\nTo carry out a Bayesian analysis for θ, we need to specify a prior. The\\nmost convenient and common choice would be θ∼N(0,τ) giving posterior\\ndistribution\\nθ|z∼N(z\\n1+1/τ,1\\n1+1/τ)\\n. (8.30)\\nNow the larger we take τ, the more concentrated the posterior becomes\\naround the maximum likelihood estimate ˆθ=z. In the limit as τ→∞we\\nobtain a noninformative (constant) prior, and the posterio r distribution is\\nθ|z∼N(z,1). (8.31)\\nThis is the same as a parametric bootstrap distribution in wh ich we gen-\\nerate bootstrap values z∗from the maximum likelihood estimate of the\\nsampling density N(z,1).\\nThere are three ingredients that make this correspondence w ork:\\n1. The choice of noninformative prior for θ.\\n2. The dependence of the log-likelihood ℓ(θ;Z) on the data Zonly\\nthrough the maximum likelihood estimate ˆθ. Hence we can write the\\nlog-likelihood as ℓ(θ;ˆθ).\\n3. The symmetry of the log-likelihood in θandˆθ, that is,ℓ(θ;ˆθ) =\\nℓ(ˆθ;θ)+constant.\\nProperties (2) and (3) essentially only hold for the Gaussia n distribu-\\ntion. However, they also hold approximately for the multino mial distribu-\\ntion, leading to a correspondence between the nonparametri c bootstrap\\nand Bayes inference, which we outline next.\\nAssumethatwehaveadiscretesamplespacewith Lcategories.Let wjbe\\nthe probability that a sample point falls in category j, and ˆwjthe observed\\nproportion in category j. Letw= (w1,w2,...,w L),ˆw= (ˆw1,ˆw2,...,ˆwL).\\nDenote our estimator by S(ˆw); take as a prior distribution for wa sym-\\nmetric Dirichlet distribution with parameter a:\\nw∼DiL(a1), (8.32)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 289}),\n",
       " Document(page_content='272 8. Model Inference and Averaging\\nthat is, the prior probability mass function is proportiona l to∏L\\nℓ=1wa−1\\nℓ.\\nThen the posterior density of wis\\nw∼DiL(a1+Nˆw), (8.33)\\nwhereNis the sample size. Letting a→0 to obtain a noninformative prior\\ngives\\nw∼DiL(Nˆw). (8.34)\\nNow the bootstrap distribution, obtained by sampling with r eplacement\\nfrom the data, can be expressed as sampling the category prop ortions from\\na multinomial distribution. Speciﬁcally,\\nNˆw∗∼Mult(N,ˆw), (8.35)\\nwhere Mult( N,ˆw) denotes a multinomial distribution, having probability\\nmassfunction(N\\nNˆw∗\\n1,...,Nˆw∗\\nL)∏ˆwNˆw∗\\nℓ\\nℓ.Thisdistributionissimilartothepos-\\nterior distribution above, having the same support, same me an, and nearly\\nthe same covariance matrix. Hence the bootstrap distributi on ofS(ˆw∗) will\\nclosely approximate the posterior distribution of S(w).\\nIn this sense, the bootstrap distribution represents an (ap proximate)\\nnonparametric, noninformative posterior distribution fo r our parameter.\\nBut this bootstrap distribution is obtained painlessly—wit hout having to\\nformally specify a prior and without having to sample from th e posterior\\ndistribution. Hence we might think of the bootstrap distrib ution as a “poor\\nman’s” Bayes posterior. By perturbing the data, the bootstr ap approxi-\\nmates the Bayesian eﬀect of perturbing the parameters, and i s typically\\nmuch simpler to carry out.\\n8.5 The EM Algorithm\\nThe EM algorithm is a popular tool for simplifying diﬃcult ma ximum\\nlikelihood problems. We ﬁrst describe it in the context of a s imple mixture\\nmodel.\\n8.5.1 Two-Component Mixture Model\\nIn this section we describe a simple mixture model for densit y estimation,\\nand the associated EM algorithm for carrying out maximum lik elihood\\nestimation. This has a natural connection to Gibbs sampling methods for\\nBayesian inference. Mixture models are discussed and demon strated in sev-\\neral other parts of the book, in particular Sections 6.8, 12. 7 and 13.2.3.\\nThe left panel of Figure 8.5 shows a histogram of the 20 ﬁctiti ous data\\npoints in Table 8.1.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 290}),\n",
       " Document(page_content='8.5 The EM Algorithm 273\\n0 2 4 60.0 0.2 0.4 0.6 0.8 1.0\\ny ydensity\\n0 2 4 60.0 0.2 0.4 0.6 0.8 1.0• •• •••••••\\n•\\n•\\n••••• •• •\\nFIGURE 8.5. Mixture example. (Left panel:) Histogram of data. (Right pane l:)\\nMaximum likelihood ﬁt of Gaussian densities (solid red) and resp onsibility (dotted\\ngreen) of the left component density for observation y, as a function of y.\\nTABLE 8.1. Twenty ﬁctitious data points used in the two-component mixt ure\\nexample in Figure 8.5.\\n-0.39 0.12 0.94 1.67 1.76 2.44 3.72 4.28 4.92 5.53\\n0.06 0.48 1.01 1.68 1.80 3.25 4.12 4.60 5.28 6.22\\nWe would like to model the density of the data points, and due t o the\\napparent bi-modality, a Gaussian distribution would not be appropriate.\\nThere seems to be two separate underlying regimes, so instea d we model\\nYas a mixture of two normal distributions:\\nY1∼N(µ1,σ2\\n1),\\nY2∼N(µ2,σ2\\n2), (8.36)\\nY= (1−∆)·Y1+∆·Y2,\\nwhere ∆∈{0,1}with Pr(∆ = 1) = π. Thisgenerative representation is\\nexplicit: generate a ∆ ∈{0,1}with probability π, and then depending on\\nthe outcome, deliver either Y1orY2. Letφθ(x) denote the normal density\\nwith parameters θ= (µ,σ2). Then the density of Yis\\ngY(y) = (1−π)φθ1(y)+πφθ2(y). (8.37)\\nNow suppose we wish to ﬁt this model to the data in Figure 8.5 by maxi-\\nmum likelihood. The parameters are\\nθ= (π,θ1,θ2) = (π,µ1,σ2\\n1,µ2,σ2\\n2). (8.38)\\nThe log-likelihood based on the Ntraining cases is\\nℓ(θ;Z) =N∑\\ni=1log[(1−π)φθ1(yi)+πφθ2(yi)]. (8.39)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 291}),\n",
       " Document(page_content='274 8. Model Inference and Averaging\\nDirect maximization of ℓ(θ;Z) is quite diﬃcult numerically, because of\\nthe sum of terms inside the logarithm. There is, however, a si mpler ap-\\nproach. We consider unobserved latent variables ∆ itaking values 0 or 1 as\\nin (8.36): if ∆ i= 1 thenYicomes from model 2, otherwise it comes from\\nmodel 1. Suppose we knew the values of the ∆ i’s. Then the log-likelihood\\nwould be\\nℓ0(θ;Z,∆) =N∑\\ni=1[(1−∆i)logφθ1(yi)+∆ilogφθ2(yi)]\\n+N∑\\ni=1[(1−∆i)log(1−π)+∆ilogπ],(8.40)\\nand the maximum likelihood estimates of µ1andσ2\\n1would be the sample\\nmean and variance for those data with ∆ i= 0, and similarly those for µ2\\nandσ2\\n2would be the sample mean and variance of the data with ∆ i= 1.\\nThe estimate of πwould be the proportion of ∆ i= 1.\\nSince the values of the ∆ i’s are actually unknown, we proceed in an\\niterative fashion, substituting for each ∆ iin (8.40) its expected value\\nγi(θ) = E(∆ i|θ,Z) = Pr(∆ i= 1|θ,Z), (8.41)\\nalso called the responsibility of model 2 for observation i. We use a proce-\\ndure called the EM algorithm, given in Algorithm 8.1 for the s pecial case of\\nGaussian mixtures. In the expectation step, we do a soft assignment of each\\nobservation to each model: the current estimates of the para meters are used\\nto assign responsibilities according to the relative densi ty of the training\\npoints under each model. In the maximization step, these responsibilities\\nare used in weighted maximum-likelihood ﬁts to update the es timates of\\nthe parameters.\\nA good way to construct initial guesses for ˆ µ1and ˆµ2is simply to choose\\ntwo of the yiat random. Both ˆ σ2\\n1and ˆσ2\\n2can be set equal to the overall\\nsample variance∑N\\ni=1(yi−¯y)2/N. The mixing proportion ˆ πcan be started\\nat the value 0 .5.\\nNote that the actual maximizer of the likelihood occurs when we put a\\nspike of inﬁnite height at any one data point, that is, ˆ µ1=yifor some\\niand ˆσ2\\n1= 0. This gives inﬁnite likelihood, but is not a useful soluti on.\\nHence we are actually looking for a good local maximum of the l ikelihood,\\none for which ˆ σ2\\n1,ˆσ2\\n2>0. To further complicate matters, there can be\\nmore than one local maximum having ˆ σ2\\n1,ˆσ2\\n2>0. In our example, we\\nran the EM algorithm with a number of diﬀerent initial guesse s for the\\nparameters, all having ˆ σ2\\nk>0.5, and chose the run that gave us the highest\\nmaximized likelihood. Figure 8.6 shows the progress of the E M algorithm in\\nmaximizingthelog-likelihood.Table8.2shows ˆ π=∑\\niˆγi/N,themaximum\\nlikelihood estimate of the proportion of observations in cl ass 2, at selected\\niterations of the EM procedure.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 292}),\n",
       " Document(page_content='8.5 The EM Algorithm 275\\nAlgorithm 8.1 EM Algorithm for Two-component Gaussian Mixture.\\n1. Take initial guesses for the parameters ˆ µ1,ˆσ2\\n1,ˆµ2,ˆσ2\\n2,ˆπ(see text).\\n2.Expectation Step : compute the responsibilities\\nˆγi=ˆπφˆθ2(yi)\\n(1−ˆπ)φˆθ1(yi)+ ˆπφˆθ2(yi), i= 1,2,...,N. (8.42)\\n3.Maximization Step : compute the weighted means and variances:\\nˆµ1=∑N\\ni=1(1−ˆγi)yi∑N\\ni=1(1−ˆγi),ˆσ2\\n1=∑N\\ni=1(1−ˆγi)(yi−ˆµ1)2\\n∑N\\ni=1(1−ˆγi),\\nˆµ2=∑N\\ni=1ˆγiyi∑N\\ni=1ˆγi,ˆσ2\\n2=∑N\\ni=1ˆγi(yi−ˆµ2)2\\n∑N\\ni=1ˆγi,\\nand the mixing probability ˆ π=∑N\\ni=1ˆγi/N.\\n4. Iterate steps 2 and 3 until convergence.\\nTABLE 8.2. Selected iterations of the EM algorithm for mixture example.\\nIteration ˆ π\\n1 0.485\\n5 0.493\\n10 0.523\\n15 0.544\\n20 0.546\\nThe ﬁnal maximum likelihood estimates are\\nˆµ1= 4.62, ˆσ2\\n1= 0.87,\\nˆµ2= 1.06, ˆσ2\\n2= 0.77,\\nˆπ= 0.546.\\nTherightpanelofFigure8.5showstheestimatedGaussianmi xturedensity\\nfromthisprocedure(solidredcurve),alongwiththerespon sibilities(dotted\\ngreen curve). Note that mixtures are also useful for supervi sed learning; in\\nSection 6.7 we show how the Gaussian mixture model leads to a v ersion of\\nradial basis functions.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 293}),\n",
       " Document(page_content='276 8. Model Inference and Averaging\\nIterationObserved Data Log-likelihood\\n5 10 15 20-44 -43 -42 -41 -40 -39\\nooooooooooooooo o o o o o\\nFIGURE 8.6. EM algorithm: observed data log-likelihood as a function of the\\niteration number.\\n8.5.2 The EM Algorithm in General\\nThe above procedure is an example of the EM (or Baum–Welch) al gorithm\\nfor maximizing likelihoods in certain classes of problems. These problems\\nare ones for which maximization of the likelihood is diﬃcult , but made\\neasier by enlarging the sample with latent (unobserved) dat a. This is called\\ndata augmentation . Here the latent data are the model memberships ∆ i.\\nIn other problems, the latent data are actual data that shoul d have been\\nobserved but are missing.\\nAlgorithm 8.2 gives the general formulation of the EM algori thm. Our\\nobserved data is Z, having log-likelihood ℓ(θ;Z) depending on parameters\\nθ. The latent or missing data is Zm, so that the complete data is T=\\n(Z,Zm) with log-likelihood ℓ0(θ;T),ℓ0based on the complete density. In\\nthe mixture problem ( Z,Zm) = (y,∆), andℓ0(θ;T) is given in (8.40).\\nIn our mixture example, E( ℓ0(θ′;T)|Z,ˆθ(j)) is simply (8.40) with the ∆ i\\nreplaced by the responsibilities ˆ γi(ˆθ), and the maximizers in step 3 are just\\nweighted means and variances.\\nWe now give an explanation of why the EM algorithm works in gen eral.\\nSince\\nPr(Zm|Z,θ′) =Pr(Zm,Z|θ′)\\nPr(Z|θ′), (8.44)\\nwe can write\\nPr(Z|θ′) =Pr(T|θ′)\\nPr(Zm|Z,θ′). (8.45)\\nIntermsoflog-likelihoods,wehave ℓ(θ′;Z) =ℓ0(θ′;T)−ℓ1(θ′;Zm|Z),where\\nℓ1is based on the conditional density Pr( Zm|Z,θ′). Taking conditional\\nexpectations with respect to the distribution of T|Zgoverned by parameter\\nθgives\\nℓ(θ′;Z) = E[ℓ0(θ′;T)|Z,θ]−E[ℓ1(θ′;Zm|Z)|Z,θ]', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 294}),\n",
       " Document(page_content='8.5 The EM Algorithm 277\\nAlgorithm 8.2 The EM Algorithm.\\n1. Start with initial guesses for the parameters ˆθ(0).\\n2.Expectation Step : at thejth step, compute\\nQ(θ′,ˆθ(j)) = E(ℓ0(θ′;T)|Z,ˆθ(j)) (8.43)\\nas a function of the dummy argument θ′.\\n3.Maximization Step : determine the new estimate ˆθ(j+1)as the maxi-\\nmizer ofQ(θ′,ˆθ(j)) overθ′.\\n4. Iterate steps 2 and 3 until convergence.\\n≡Q(θ′,θ)−R(θ′,θ). (8.46)\\nIn theMstep, the EM algorithm maximizes Q(θ′,θ) overθ′, rather than\\nthe actual objective function ℓ(θ′;Z). Why does it succeed in maximizing\\nℓ(θ′;Z)?Notethat R(θ∗,θ)istheexpectationofalog-likelihoodofadensity\\n(indexed by θ∗), with respect to the same density indexed by θ, and hence\\n(by Jensen’s inequality) is maximized as a function of θ∗, whenθ∗=θ(see\\nExercise 8.1). So if θ′maximizesQ(θ′,θ), we see that\\nℓ(θ′;Z)−ℓ(θ;Z) = [Q(θ′,θ)−Q(θ,θ)]−[R(θ′,θ)−R(θ,θ)]\\n≥0. (8.47)\\nHence the EM iteration never decreases the log-likelihood.\\nThis argument also makes it clear that a full maximization in theM\\nstep is not necessary: we need only to ﬁnd a value ˆθ(j+1)so thatQ(θ′,ˆθ(j))\\nincreases as a function of the ﬁrst argument, that is, Q(ˆθ(j+1),ˆθ(j))>\\nQ(ˆθ(j),ˆθ(j)).Suchproceduresarecalled GEM (generalized EM) algorithms.\\nThe EM algorithm can also be viewed as a minorization procedu re: see\\nExercise 8.7.\\n8.5.3 EM as a Maximization–Maximization Procedure\\nHere is a diﬀerent view of the EM procedure, as a joint maximiz ation\\nalgorithm. Consider the function\\nF(θ′,˜P) = E˜P[ℓ0(θ′;T)]−E˜P[log˜P(Zm)]. (8.48)\\nHere˜P(Zm) is any distribution over the latent data Zm. In the mixture\\nexample, ˜P(Zm) comprises the set of probabilities γi= Pr(∆ i= 1|θ,Z).\\nNote thatFevaluated at ˜P(Zm) = Pr(Zm|Z,θ′), is the log-likelihood of', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 295}),\n",
       " Document(page_content='278 8. Model Inference and Averaging\\n1 2 3 4 50 1 2 3 40.10.3\\n0.50.7\\n0.9Model Parameters\\nLatent Data ParametersEMEM\\nFIGURE 8.7. Maximization–maximization view of the EM algorithm. Shown\\nare the contours of the (augmented) observed data log-likeliho odF(θ′,˜P). The\\nEstep is equivalent to maximizing the log-likelihood over the par ameters of the\\nlatent data distribution. The Mstep maximizes it over the parameters of the\\nlog-likelihood. The red curve corresponds to the observed data log-likelihood, a\\nproﬁleobtained by maximizing F(θ′,˜P)for each value of θ′.\\nthe observed data, from (8.46)1. The function Fexpands the domain of\\nthe log-likelihood, to facilitate its maximization.\\nThe EM algorithm can be viewed as a joint maximization method forF\\noverθ′and˜P(Zm), by ﬁxing one argument and maximizing over the other.\\nThe maximizer over ˜P(Zm) for ﬁxedθ′can be shown to be\\n˜P(Zm) = Pr(Zm|Z,θ′) (8.49)\\n(Exercise8.2).Thisisthedistributioncomputedbythe Estep,forexample,\\n(8.42) in the mixture example. In the Mstep, we maximize F(θ′,˜P) overθ′\\nwith˜Pﬁxed:thisisthesameasmaximizingtheﬁrsttermE ˜P[ℓ0(θ′;T)|Z,θ]\\nsince the second term does not involve θ′.\\nFinally, since F(θ′,˜P) and the observed data log-likelihood agree when\\n˜P(Zm) = Pr(Zm|Z,θ′), maximization of the former accomplishes maxi-\\nmization of the latter. Figure 8.7 shows a schematic view of t his process.\\nThis view of the EM algorithm leads to alternative maximizat ion proce-\\n1(8.46) holds for all θ, including θ=θ′.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 296}),\n",
       " Document(page_content='8.6 MCMC for Sampling from the Posterior 279\\nAlgorithm 8.3 Gibbs Sampler.\\n1. Take some initial values U(0)\\nk,k= 1,2,...,K.\\n2. Repeat for t= 1,2,...,.:\\nFork= 1,2,...,KgenerateU(t)\\nkfrom\\nPr(U(t)\\nk|U(t)\\n1,...,U(t)\\nk−1,U(t−1)\\nk+1,...,U(t−1)\\nK).\\n3. Continue step 2 until the joint distribution of ( U(t)\\n1,U(t)\\n2,...,U(t)\\nK)\\ndoes not change.\\ndures. For example, one does not need to maximize with respec t to all of\\nthe latent data parameters at once, but could instead maximi ze over one\\nof them at a time, alternating with the Mstep.\\n8.6 MCMC for Sampling from the Posterior\\nHaving deﬁned a Bayesian model, one would like to draw sample s from\\nthe resulting posterior distribution, in order to make infe rences about the\\nparameters. Except for simple models, this is often a diﬃcul t computa-\\ntional problem. In this section we discuss the Markov chain Monte Carlo\\n(MCMC) approach to posterior sampling. We will see that Gibb s sampling,\\nan MCMC procedure, is closely related to the EM algorithm: th e main dif-\\nference is that it samples from the conditional distributio ns rather than\\nmaximizing over them.\\nConsider ﬁrst the following abstract problem. We have rando m variables\\nU1,U2,...,U Kand we wish to draw a sample from their joint distribution.\\nSupposethisisdiﬃcult todo,butitiseasytosimulate fromt heconditional\\ndistributions Pr( Uj|U1,U2,...,U j−1,Uj+1,...,U K), j= 1,2,...,K. The\\nGibbs sampling procedure alternatively simulates from eac h of these distri-\\nbutions and when the process stabilizes, provides a sample f rom the desired\\njoint distribution. The procedure is deﬁned in Algorithm 8. 3.\\nUnder regularity conditions it can be shown that this proced ure even-\\ntually stabilizes, and the resulting random variables are i ndeed a sample\\nfrom the joint distribution of U1,U2,...,U K. This occurs despite the fact\\nthat the samples ( U(t)\\n1,U(t)\\n2,...,U(t)\\nK) are clearly not independent for dif-\\nferentt. More formally, Gibbs sampling produces a Markov chain whos e\\nstationary distribution is the true joint distribution, an d hence the term\\n“Markov chain Monte Carlo.” It is not surprising that the tru e joint dis-\\ntribution is stationary under this process, as the successi ve steps leave the\\nmarginal distributions of the Uk’s unchanged.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 297}),\n",
       " Document(page_content='280 8. Model Inference and Averaging\\nNote that we don’t need to know the explicit form of the condit ional\\ndensities,butjustneedtobeabletosamplefromthem.After theprocedure\\nreaches stationarity, the marginal density of any subset of the variables\\ncan be approximated by a density estimate applied to the samp le values.\\nHowever if the explicit form of the conditional density Pr( Uk,|Uℓ,ℓ̸=k)\\nis available, a better estimate of say the marginal density o fUkcan be\\nobtained from (Exercise 8.3):\\nˆPrUk(u) =1\\n(M−m+1)M∑\\nt=mPr(u|U(t)\\nℓ,ℓ̸=k). (8.50)\\nHere we have averaged over the last M−m+1 members of the sequence,\\nto allow for an initial “burn-in” period before stationarit y is reached.\\nNowgettingbacktoBayesianinference,ourgoalistodrawas amplefrom\\nthe joint posterior of the parameters given the data Z. Gibbs sampling will\\nbe helpful if it is easy to sample from the conditional distri bution of each\\nparameter given the other parameters and Z. An example—the Gaussian\\nmixture problem—is detailed next.\\nThere is a close connection between Gibbs sampling from a pos terior and\\nthe EM algorithm in exponential family models. The key is to c onsider the\\nlatent data Zmfrom the EM procedure to be another parameter for the\\nGibbs sampler. To make this explicit for the Gaussian mixtur e problem,\\nwe take our parameters to be ( θ,Zm). For simplicity we ﬁx the variances\\nσ2\\n1,σ2\\n2and mixing proportion πat their maximum likelihood values so that\\nthe only unknown parameters in θare the means µ1andµ2. The Gibbs\\nsampler for the mixture problem is given in Algorithm 8.4. We see that\\nsteps 2(a) and 2(b) are the same as the EandMsteps of the EM pro-\\ncedure, except that we sample rather than maximize. In step 2 (a), rather\\nthan compute the maximum likelihood responsibilities γi= E(∆ i|θ,Z),\\nthe Gibbs sampling procedure simulates the latent data ∆ ifrom the distri-\\nbutions Pr(∆ i|θ,Z). In step 2(b), rather than compute the maximizers of\\nthe posterior Pr( µ1,µ2,∆|Z) we simulate from the conditional distribution\\nPr(µ1,µ2|∆,Z).\\nFigure 8.8 shows 200 iterations of Gibbs sampling, with the m ean param-\\netersµ1(lower) and µ2(upper) shown in the left panel, and the proportion\\nof class 2 observations∑\\ni∆i/Non the right. Horizontal broken lines have\\nbeen drawn at the maximum likelihood estimate values ˆ µ1,ˆµ2and∑\\niˆγi/N\\nin each case. The values seem to stabilize quite quickly, and are distributed\\nevenly around the maximum likelihood values.\\nThe above mixture model was simpliﬁed, in order to make the cl ear\\nconnection between Gibbs sampling and the EM algorithm. Mor e realisti-\\ncally, one would put a prior distribution on the variances σ2\\n1,σ2\\n2and mixing\\nproportion π, and include separate Gibbs sampling steps in which we sam-\\nple from their posterior distributions, conditional on the other parameters.\\nOne can also incorporate proper (informative) priors for th e mean param-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 298}),\n",
       " Document(page_content='8.6 MCMC for Sampling from the Posterior 281\\nAlgorithm 8.4 Gibbs sampling for mixtures.\\n1. Take some initial values θ(0)= (µ(0)\\n1,µ(0)\\n2).\\n2. Repeat for t= 1,2,...,.\\n(a) Fori= 1,2,...,Ngenerate ∆(t)\\ni∈{0,1}with Pr(∆(t)\\ni= 1) =\\nˆγi(θ(t)), from equation (8.42).\\n(b) Set\\nˆµ1=∑N\\ni=1(1−∆(t)\\ni)·yi∑N\\ni=1(1−∆(t)\\ni),\\nˆµ2=∑N\\ni=1∆(t)\\ni·yi∑N\\ni=1∆(t)\\ni,\\nand generate µ(t)\\n1∼N(ˆµ1,ˆσ2\\n1) andµ(t)\\n2∼N(ˆµ2,ˆσ2\\n2).\\n3. Continue step 2 until the joint distribution of ( ∆(t),µ(t)\\n1,µ(t)\\n2) doesn’t\\nchange\\nGibbs IterationMean Parameters\\n0 50 100 150 2000 2 4 6 8\\nGibbs IterationMixing Proportion\\n0 50 100 150 2000.3 0.4 0.5 0.6 0.7\\nFIGURE 8.8. Mixture example. (Left panel:) 200values of the two mean param-\\neters from Gibbs sampling; horizontal lines are drawn at the max imum likelihood\\nestimates ˆµ1,ˆµ2. (Right panel:) Proportion of values with ∆i= 1, for each of the\\n200Gibbs sampling iterations; a horizontal line is drawn at∑\\niˆγi/N.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 299}),\n",
       " Document(page_content='282 8. Model Inference and Averaging\\neters. These priors must not be improper as this will lead to a degenerate\\nposterior, with all the mixing weight on one component.\\nGibbs sampling is just one of a number of recently developed p rocedures\\nfor sampling from posterior distributions. It uses conditi onal sampling of\\neach parameter given the rest, and is useful when the structu re of the prob-\\nlem makes this sampling easy to carry out. Other methods do no t require\\nsuch structure, for example the Metropolis–Hastings algorithm. These and\\nother computational Bayesian methods have been applied to s ophisticated\\nlearning algorithms such as Gaussian process models and neu ral networks.\\nDetails may be found in the references given in the Bibliogra phic Notes at\\nthe end of this chapter.\\n8.7 Bagging\\nEarlier we introduced the bootstrap as a way of assessing the accuracy of a\\nparameter estimate or a prediction. Here we show how to use th e bootstrap\\nto improve the estimate or prediction itself. In Section 8.4 we investigated\\nthe relationship between the bootstrap and Bayes approache s, and found\\nthat the bootstrap mean is approximately a posterior averag e. Bagging\\nfurther exploits this connection.\\nConsider ﬁrst the regression problem. Suppose we ﬁt a model t o our\\ntraining data Z={(x1,y1),(x2,y2),...,(xN,yN)}, obtaining the predic-\\ntionˆf(x) at inputx. Bootstrap aggregation or baggingaverages this predic-\\ntion over a collection of bootstrap samples, thereby reduci ng its variance.\\nFor each bootstrap sample Z∗b,b= 1,2,...,B, we ﬁt our model, giving\\nprediction ˆf∗b(x). The bagging estimate is deﬁned by\\nˆfbag(x) =1\\nBB∑\\nb=1ˆf∗b(x). (8.51)\\nDenote by ˆPthe empirical distribution putting equal probability 1 /Non\\neach of the data points ( xi,yi). In fact the “true” bagging estimate is\\ndeﬁned by E ˆPˆf∗(x), where Z∗={(x∗\\n1,y∗\\n1),(x∗\\n2,y∗\\n2),...,(x∗\\nN,y∗\\nN)}and each\\n(x∗\\ni,y∗\\ni)∼ˆP. Expression (8.51) is a Monte Carlo estimate of the true\\nbagging estimate, approaching it as B→∞.\\nThe bagged estimate (8.51) will diﬀer from the original esti mateˆf(x)\\nonly when the latter is a nonlinear or adaptive function of th e data. For\\nexample, to bag the B-spline smooth of Section 8.2.1, we average the curves\\nin the bottom left panel of Figure 8.2 at each value of x. TheB-spline\\nsmoother is linear in the data if we ﬁx the inputs; hence if we s ample using\\nthe parametric bootstrap in equation (8.6), then ˆfbag(x)→ˆf(x) asB→∞\\n(Exercise 8.4). Hence bagging just reproduces the original smooth in the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 300}),\n",
       " Document(page_content='8.7 Bagging 283\\ntop left panel of Figure 8.2. The same is approximately true i f we were to\\nbag using the nonparametric bootstrap.\\nA more interesting example is a regression tree, where ˆf(x) denotes the\\ntree’s prediction at input vector x(regression trees are described in Chap-\\nter 9). Each bootstrap tree will typically involve diﬀerent features than the\\noriginal, and might have a diﬀerent number of terminal nodes . The bagged\\nestimate is the average prediction at xfrom these Btrees.\\nNow suppose our tree produces a classiﬁer ˆG(x) for aK-class response.\\nHere it is useful to consider an underlying indicator-vecto r function ˆf(x),\\nwith value a single one and K−1 zeroes, such that ˆG(x) = argmax kˆf(x).\\nThen the bagged estimate ˆfbag(x) (8.51) is a K-vector [p1(x),p2(x),...,\\npK(x)], withpk(x) equal to the proportion of trees predicting class katx.\\nThe bagged classiﬁer selects the class with the most “votes” from theB\\ntrees,ˆGbag(x) = argmax kˆfbag(x).\\nOften we require the class-probability estimates at x, rather than the\\nclassiﬁcations themselves. It is tempting to treat the voti ng proportions\\npk(x) as estimates of these probabilities. A simple two-class ex ample shows\\nthat they fail in this regard. Suppose the true probability o f class 1 at xis\\n0.75, and each of the bagged classiﬁers accurately predict a 1. Thenp1(x) =\\n1, which is incorrect. For many classiﬁers ˆG(x), however, there is already\\nan underlying function ˆf(x) that estimates the class probabilities at x(for\\ntrees, the class proportions in the terminal node). An alter native bagging\\nstrategy is to average these instead, rather than the vote in dicator vectors.\\nNot only does this produce improved estimates of the class pr obabilities,\\nbutitalsotendstoproducebaggedclassiﬁerswithlowervar iance,especially\\nfor smallB(see Figure 8.10 in the next example).\\n8.7.1 Example: Trees with Simulated Data\\nWe generated a sample of size N= 30, with two classes and p= 5 features,\\neach having a standard Gaussian distribution with pairwise correlation\\n0.95.Theresponse YwasgeneratedaccordingtoPr( Y= 1|x1≤0.5) = 0.2,\\nPr(Y= 1|x1>0.5) = 0.8. The Bayes error is 0 .2. A test sample of size 2000\\nwas also generated from the same population. We ﬁt classiﬁca tion trees to\\nthe training sample and to each of 200 bootstrap samples (cla ssiﬁcation\\ntrees are described in Chapter 9). No pruning was used. Figur e 8.9 shows\\nthe original tree and eleven bootstrap trees. Notice how the trees are all\\ndiﬀerent, with diﬀerent splitting features and cutpoints. The test error for\\nthe original tree and the bagged tree is shown in Figure 8.10. In this ex-\\nample the trees have high variance due to the correlation in t he predictors.\\nBagging succeeds in smoothing out this variance and hence re ducing the\\ntest error.\\nBagging can dramatically reduce the variance of unstable pr ocedures\\nlike trees, leading to improved prediction. A simple argume nt shows why', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 301}),\n",
       " Document(page_content='284 8. Model Inference and Averaging\\n|x.1 < 0.395\\n010\\n101\\n10Original Tree\\n|x.1 < 0.555\\n0\\n1001b = 1\\n|x.2 < 0.205\\n0101\\n01b = 2\\n|x.2 < 0.285\\n1 1010b = 3\\n|x.3 < 0.985\\n0\\n1\\n011 1b = 4\\n|x.4 < −1.36\\n0\\n1\\n1010\\n10b = 5\\n|x.1 < 0.395\\n1 10 01b = 6\\n|x.1 < 0.395\\n01011b = 7\\n|x.3 < 0.985\\n010 010b = 8\\n|x.1 < 0.395\\n0\\n1\\n0110b = 9\\n|x.1 < 0.555\\n101\\n01b = 10\\n|x.1 < 0.555\\n0 101b = 11\\nFIGURE 8.9. Bagging trees on simulated dataset. The top left panel shows th e\\noriginal tree. Eleven trees grown on bootstrap samples are sho wn. For each tree,\\nthe top split is annotated.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 302}),\n",
       " Document(page_content='8.7 Bagging 285\\n0 50 100 150 2000.20 0.25 0.30 0.35 0.40 0.45 0.50\\nNumber of Bootstrap SamplesTest ErrorBagged TreesOriginal Tree\\nBayesConsensus\\nProbability\\nFIGURE 8.10. Error curves for the bagging example of Figure 8.9. Shown is\\nthe test error of the original tree and bagged trees as a funct ion of the number of\\nbootstrap samples. The orange points correspond to the conse nsus vote, while the\\ngreen points average the probabilities.\\nbagging helps under squared-error loss, in short because av eraging reduces\\nvariance and leaves bias unchanged.\\nAssume our training observations ( xi,yi), i= 1,...,Nare indepen-\\ndently drawn from a distribution P, and consider the ideal aggregate es-\\ntimatorfag(x) = EPˆf∗(x). Herexis ﬁxed and the bootstrap dataset Z∗\\nconsists of observations x∗\\ni,y∗\\ni,i= 1,2,...,Nsampled fromP. Note that\\nfag(x) is a bagging estimate, drawing bootstrap samples from the a ctual\\npopulationPrather than the data. It is not an estimate that we can use\\nin practice, but is convenient for analysis. We can write\\nEP[Y−ˆf∗(x)]2= EP[Y−fag(x)+fag(x)−ˆf∗(x)]2\\n= EP[Y−fag(x)]2+EP[ˆf∗(x)−fag(x)]2\\n≥EP[Y−fag(x)]2. (8.52)\\nThe extra error on the right-hand side comes from the varianc e ofˆf∗(x)\\naround its mean fag(x). Therefore true population aggregation never in-\\ncreases mean squared error. This suggests that bagging—draw ing samples\\nfrom the training data— will often decrease mean-squared er ror.\\nThe above argument does not hold for classiﬁcation under 0-1 loss, be-\\ncause of the nonadditivity of bias and variance. In that sett ing, bagging a', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 303}),\n",
       " Document(page_content='286 8. Model Inference and Averaging\\ngood classiﬁer can make it better, but bagging a bad classiﬁe r can make it\\nworse. Here is a simple example, using a randomized rule. Sup poseY= 1\\nfor allx, and the classiﬁer ˆG(x) predictsY= 1 (for all x) with proba-\\nbility 0.4 and predicts Y= 0 (for all x) with probability 0.6. Then the\\nmisclassiﬁcation error of ˆG(x) is 0.6 but that of the bagged classiﬁer is 1.0.\\nFor classiﬁcation we can understand the bagging eﬀect in ter ms of a\\nconsensus of independent weak learners (Dietterich, 2000a). Let the Bayes\\noptimal decision at xbeG(x) = 1 in a two-class example. Suppose each\\nof the weak learners G∗\\nbhave an error-rate eb=e <0.5, and letS1(x) =∑B\\nb=1I(G∗\\nb(x) = 1) be the consensus vote for class 1. Since the weak learn-\\ners are assumed to be independent, S1(x)∼Bin(B,1−e), and Pr(S1>\\nB/2)→1 asBgets large. This concept has been popularized outside of\\nstatistics as the “Wisdom of Crowds” (Surowiecki, 2004) — th e collective\\nknowledge of a diverse and independent body of people typica lly exceeds\\nthe knowledge of any single individual, and can be harnessed by voting.\\nOf course, the main caveat here is “independent,” and bagged trees are\\nnot. Figure 8.11 illustrates the power of a consensus vote in a simulated\\nexample, where only 30% of the voters have some knowledge.\\nInChapter15weseehowrandomforestsimproveonbaggingbyr educing\\nthe correlation between the sampled trees.\\nNote that when we bag a model, any simple structure in the mode l is\\nlost. As an example, a bagged tree is no longer a tree. For inte rpretation\\nof the model this is clearly a drawback. More stable procedur es like near-\\nest neighbors are typically not aﬀected much by bagging. Unf ortunately,\\nthe unstable models most helped by bagging are unstable beca use of the\\nemphasis on interpretability, and this is lost in the baggin g process.\\nFigure 8.12 shows an example where bagging doesn’t help. The 100 data\\npoints shown have two features and two classes, separated by the gray\\nlinear boundary x1+x2= 1. We choose as our classiﬁer ˆG(x) a single\\naxis-oriented split, choosing the split along either x1orx2that produces\\nthe largest decrease in training misclassiﬁcation error.\\nThe decision boundary obtained from bagging the 0-1 decisio n rule over\\nB= 50 bootstrap samples is shown by the blue curve in the left pa nel.\\nIt does a poor job of capturing the true boundary. The single s plit rule,\\nderived from the training data, splits near 0 (the middle of t he range of x1\\norx2), and hence has little contribution away from the center. Av eraging\\nthe probabilities rather than the classiﬁcations does not h elp here. Bagging\\nestimates the expected class probabilities from the single split rule, that is,\\naveraged over many replications. Note that the expected cla ss probabilities\\ncomputed by bagging cannot be realized on any single replica tion, in the\\nsame way that a woman cannot have 2.4 children. In this sense, bagging\\nincreases somewhat the space of models of the individual bas e classiﬁer.\\nHowever, it doesn’t help in this and many other examples wher e a greater\\nenlargement of the model class is needed. “Boosting” is a way of doing this', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 304}),\n",
       " Document(page_content='8.7 Bagging 2870 2 4 6 8 10\\nP −  Probability of Informed Person Being CorrectExpected Correct out of 10Wisdom of Crowds\\nConsensus\\nIndividual\\n0.25 0.50 0.75 1.00\\nFIGURE 8.11. Simulated academy awards voting. 50members vote in 10 cat-\\negories, each with 4nominations. For any category, only 15voters have some\\nknowledge, represented by their probability of selecting the “ correct” candidate in\\nthat category (so P= 0.25means they have no knowledge). For each category, the\\n15experts are chosen at random from the 50. Results show the expected correct\\n(based on 50simulations) for the consensus, as well as for the individuals . The\\nerror bars indicate one standard deviation. We see, for exam ple, that if the 15\\ninformed for a category have a 50% chance of selecting the correct candidate, the\\nconsensus doubles the expected performance of an individual.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 305}),\n",
       " Document(page_content='288 8. Model Inference and Averaging\\n•••\\n••••\\n•••\\n••\\n••\\n•\\n••••\\n•••\\n•••\\n•••\\n•\\n••\\n•••\\n•\\n•• •• •••\\n•••\\n••\\n•\\n•\\n••\\n•••\\n••\\n•\\n•\\n••••\\n•••\\n••\\n•\\n••\\n••\\n•••\\n••\\n••• •••\\n••\\n•••\\n•••\\n•\\n••••\\n•\\n• ••\\n••\\n••\\n••• •••\\n••••\\n••\\n••\\n•••\\n••\\n••\\n•\\n••\\n•\\n•\\n••\\n•\\n••\\n••\\n••\\n•\\n••\\n••\\n•••\\n••\\n•••\\n••\\n•••••\\n•\\n••\\n•••••\\n••\\n•••\\n•••••\\n••••\\n••\\n•••\\n•••\\n••\\n•\\n•\\n••\\n•••\\n•\\n•Bagged Decision Rule\\n•••\\n••••\\n•••\\n••\\n••\\n•\\n••••\\n•••\\n•••\\n•••\\n•\\n••\\n•••\\n•\\n•• •• •••\\n•••\\n••\\n•\\n•\\n••\\n•••\\n••\\n•\\n•\\n••••\\n•••\\n••\\n•\\n••\\n••\\n•••\\n••\\n••• •••\\n••\\n•••\\n•••\\n•\\n••••\\n•\\n• ••\\n••\\n••\\n••• •••\\n••••\\n••\\n••\\n•••\\n••\\n••\\n•\\n••\\n•\\n•\\n••\\n•\\n••\\n••\\n••\\n•\\n••\\n••\\n•••\\n••\\n•••\\n••\\n•••••\\n•\\n••\\n•••••\\n••\\n•••\\n•••••\\n••••\\n••\\n•••\\n•••\\n••\\n•\\n•\\n••\\n•••\\n•\\n•Boosted Decision Rule\\nFIGURE 8.12. Data with two features and two classes, separated by a linear\\nboundary. (Left panel:) Decision boundary estimated from ba gging the decision\\nrule from a single split, axis-oriented classiﬁer. (Right panel: ) Decision boundary\\nfrom boosting the decision rule of the same classiﬁer. The test error rates are\\n0.166, and0.065, respectively. Boosting is described in Chapter 10.\\nand is described in Chapter 10. The decision boundary in the r ight panel is\\nthe result of the boosting procedure, and it roughly capture s the diagonal\\nboundary.\\n8.8 Model Averaging and Stacking\\nIn Section 8.4 we viewed bootstrap values of an estimator as a pproximate\\nposterior values of a corresponding parameter, from a kind o f nonparamet-\\nric Bayesian analysis. Viewed in this way, the bagged estima te (8.51) is\\nan approximate posterior Bayesian mean. In contrast, the tr aining sample\\nestimate ˆf(x) corresponds to the mode of the posterior. Since the posteri or\\nmean (not mode) minimizes squared-error loss, it is not surp rising that\\nbagging can often reduce mean squared-error.\\nHere we discuss Bayesian model averaging more generally. We have a\\nset of candidate models Mm, m= 1,...,Mfor our training set Z. These\\nmodels may be of the same type with diﬀerent parameter values (e.g.,\\nsubsets in linear regression), or diﬀerent models for the sa me task (e.g.,\\nneural networks and regression trees).\\nSupposeζis some quantity of interest, for example, a prediction f(x) at\\nsome ﬁxed feature value x. The posterior distribution of ζis\\nPr(ζ|Z) =M∑\\nm=1Pr(ζ|Mm,Z)Pr(Mm|Z), (8.53)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 306}),\n",
       " Document(page_content='8.8 Model Averaging and Stacking 289\\nwith posterior mean\\nE(ζ|Z) =M∑\\nm=1E(ζ|Mm,Z)Pr(Mm|Z). (8.54)\\nThisBayesianpredictionisaweightedaverageoftheindivi dualpredictions,\\nwith weights proportional to the posterior probability of e ach model.\\nThis formulation leads to a number of diﬀerent model-averag ing strate-\\ngies.Committee methods take a simple unweighted average of the predic-\\ntions from each model, essentially giving equal probabilit y to each model.\\nMore ambitiously, the development in Section 7.7 shows the B IC criterion\\ncan be used to estimate posterior model probabilities. This is applicable\\nin cases where the diﬀerent models arise from the same parame tric model,\\nwith diﬀerent parameter values. The BIC gives weight to each model de-\\npending on how well it ﬁts and how many parameters it uses. One can also\\ncarry out the Bayesian recipe in full. If each model Mmhas parameters\\nθm, we write\\nPr(Mm|Z)∝Pr(Mm)·Pr(Z|Mm)\\n∝Pr(Mm)·∫\\nPr(Z|θm,Mm)Pr(θm|Mm)dθm.\\n(8.55)\\nIn principle one can specify priors Pr( θm|Mm) and numerically com-\\npute the posterior probabilities from (8.55), to be used as m odel-averaging\\nweights. However, we have seen no real evidence that this is w orth all of\\nthe eﬀort, relative to the much simpler BIC approximation.\\nHow can we approach model averaging from a frequentist viewp oint?\\nGiven predictions ˆf1(x),ˆf2(x),...,ˆfM(x), under squared-error loss, we can\\nseek the weights w= (w1,w2,...,w M) such that\\nˆw= argmin\\nwEP[\\nY−M∑\\nm=1wmˆfm(x)]2\\n. (8.56)\\nHeretheinputvalue xisﬁxedandthe Nobservationsinthedataset Z(and\\nthe targetY) are distributed according to P. The solution is the population\\nlinear regression of YonˆF(x)T≡[ˆf1(x),ˆf2(x),...,ˆfM(x)]:\\nˆw= EP[ˆF(x)ˆF(x)T]−1EP[ˆF(x)Y]. (8.57)\\nNow the full regression has smaller error than any single mod el\\nEP[\\nY−M∑\\nm=1ˆwmˆfm(x)]2\\n≤EP[\\nY−ˆfm(x)]2\\n∀m (8.58)\\nso combining models never makes things worse, at the populat ion level.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 307}),\n",
       " Document(page_content='290 8. Model Inference and Averaging\\nOf course the population linear regression (8.57) is not ava ilable, and it\\nis natural to replace it with the linear regression over the t raining set. But\\nthere are simple examples where this does not work well. For e xample, if\\nˆfm(x), m= 1,2,...,Mrepresent the prediction from the best subset of\\ninputs of size mamongMtotal inputs, then linear regression would put all\\nof the weight on the largest model, that is, ˆ wM= 1,ˆwm= 0, m<M . The\\nproblem is that we have not put each of the models on the same fo oting\\nby taking into account their complexity (the number of input smin this\\nexample).\\nStacked generalization , orstacking, is a way of doing this. Let ˆf−i\\nm(x)\\nbe the prediction at x, using model m, applied to the dataset with the\\nith training observation removed. The stacking estimate of t he weights is\\nobtained from the least squares linear regression of yionˆf−i\\nm(xi), m=\\n1,2,...,M. In detail the stacking weights are given by\\nˆwst= argmin\\nwN∑\\ni=1[\\nyi−M∑\\nm=1wmˆf−i\\nm(xi)]2\\n. (8.59)\\nThe ﬁnal prediction is∑\\nmˆwst\\nmˆfm(x). By using the cross-validated pre-\\ndictions ˆf−i\\nm(x), stacking avoids giving unfairly high weight to models wit h\\nhigher complexity. Better results can be obtained by restri cting the weights\\nto be nonnegative, and to sum to 1. This seems like a reasonabl e restriction\\nif we interpret the weights as posterior model probabilitie s as in equation\\n(8.54), and it leads to a tractable quadratic programming pr oblem.\\nThere is a close connection between stacking and model selec tion via\\nleave-one-outcross-validation(Section7.10).Ifwerest ricttheminimization\\nin (8.59) to weight vectors wthat have one unit weight and the rest zero,\\nthis leads to a model choice ˆ mwith smallest leave-one-out cross-validation\\nerror. Rather than choose a single model, stacking combines them with\\nestimated optimal weights. This will often lead to better pr ediction, but\\nless interpretability than the choice of only one of the Mmodels.\\nThe stacking idea is actually more general than described ab ove. One\\ncan use any learning method, not just linear regression, to c ombine the\\nmodels as in (8.59); the weights could also depend on the inpu t location\\nx. In this way, learning methods are “stacked” on top of one ano ther, to\\nimprove prediction performance.\\n8.9 Stochastic Search: Bumping\\nThe ﬁnal method described in this chapter does not involve av eraging or\\ncombining models, but rather is a technique for ﬁnding a bett er single\\nmodel.Bumping uses bootstrap sampling to move randomly through model\\nspace. For problems where ﬁtting method ﬁnds many local mini ma, bump-\\ning can help the method to avoid getting stuck in poor solutio ns.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 308}),\n",
       " Document(page_content='8.9 Stochastic Search: Bumping 291\\nRegular 4-Node Tree\\n••\\n•••\\n••\\n•\\n•••\\n• ••\\n••\\n••\\n••\\n•\\n••\\n•••\\n•\\n••\\n•\\n•\\n•••\\n••\\n•\\n••\\n••\\n••\\n••\\n••••••\\n••••\\n•\\n•\\n• ••\\n•••\\n•\\n••\\n••\\n•••\\n••\\n• •••\\n••\\n•\\n••\\n•••\\n••\\n•••\\n•\\n••••\\n•\\n•••\\n•\\n•\\n•••\\n••\\n•\\n•••••\\n•••\\n••\\n•\\n•\\n••\\n•\\n••\\n••••• •\\n••\\n•\\n•••\\n••\\n••\\n••\\n•••••\\n•\\n••••\\n•\\n••\\n•\\n••\\n•\\n••\\n••\\n••\\n••\\n••••\\n•\\n•• •••\\n••\\n•\\n•••\\n••\\n••\\n•\\n•••\\n•••\\n••\\n•••\\n••\\n••\\n•••\\n••\\n•\\n••\\n••\\n•••\\n••\\n••\\n••\\n••\\n•\\n•\\n••\\n••\\n•••\\n••\\n•••\\n•• •\\n•••\\n••\\n••\\n•\\n•••\\n••\\n•\\n•••\\n••\\n••\\n•••\\n••••\\n••\\n•\\n••\\n•\\n•••\\n•\\n•••\\n•••\\n•••\\n••••\\n••\\n•\\n••\\n•\\n••••••\\n•\\n••\\n••\\n••\\n•\\n•\\n••\\n••••\\n•••\\n••\\n•••\\n•••••\\n•••\\n••\\n••\\n••\\n•\\n••\\n•\\n••\\n•\\n••\\n••\\n•••\\n•••\\n••••\\n•••••\\n•\\n• •• ••\\n•\\n•••\\n•\\n••\\n•••\\n•\\n•\\n•••\\n••\\n••\\n•\\n•\\n••\\n••\\n••Bumped 4-Node Tree\\n••\\n•••\\n••\\n•\\n•••\\n• ••\\n••\\n••\\n••\\n•\\n••\\n•••\\n•\\n••\\n•\\n•\\n•••\\n••\\n•\\n••\\n••\\n••\\n••\\n••••••\\n••••\\n•\\n•\\n• ••\\n•••\\n•\\n••\\n••\\n•••\\n••\\n• •••\\n••\\n•\\n••\\n•••\\n••\\n•••\\n•\\n••••\\n•\\n•••\\n•\\n•\\n•••\\n••\\n•\\n•••••\\n•••\\n••\\n•\\n•\\n••\\n•\\n••\\n••••• •\\n••\\n•\\n•••\\n••\\n••\\n••\\n•••••\\n•\\n••••\\n•\\n••\\n•\\n••\\n•\\n••\\n••\\n••\\n••\\n••••\\n•\\n•• •••\\n••\\n•\\n•••\\n••\\n••\\n•\\n•••\\n•••\\n••\\n•••\\n••\\n••\\n•••\\n••\\n•\\n••\\n••\\n•••\\n••\\n••\\n••\\n••\\n•\\n•\\n••\\n••\\n•••\\n••\\n•••\\n•• •\\n•••\\n••\\n••\\n•\\n•••\\n••\\n•\\n•••\\n••\\n••\\n•••\\n••••\\n••\\n•\\n••\\n•\\n•••\\n•\\n•••\\n•••\\n•••\\n••••\\n••\\n•\\n••\\n•\\n••••••\\n•\\n••\\n••\\n••\\n•\\n•\\n••\\n••••\\n•••\\n••\\n•••\\n•••••\\n•••\\n••\\n••\\n••\\n•\\n••\\n•\\n••\\n•\\n••\\n••\\n•••\\n•••\\n••••\\n•••••\\n•\\n• •• ••\\n•\\n•••\\n•\\n••\\n•••\\n•\\n•\\n•••\\n••\\n••\\n•\\n•\\n••\\n••\\n••\\nFIGURE 8.13. Data with two features and two classes (blue and orange), dis-\\nplaying a pure interaction. The left panel shows the partition found by three splits\\nof a standard, greedy, tree-growing algorithm. The vertical grey line near the left\\nedge is the ﬁrst split, and the broken lines are the two subseque nt splits. The al-\\ngorithm has no idea where to make a good initial split, and make s a poor choice.\\nThe right panel shows the near-optimal splits found by bumpin g the tree-growing\\nalgorithm 20times.\\nAs in bagging, we draw bootstrap samples and ﬁt a model to each . But\\nrather than average the predictions, we choose the model est imated from a\\nbootstrap sample that best ﬁts the training data. In detail, we draw boot-\\nstrap samples Z∗1,...,Z∗Band ﬁt our model to each, giving predictions\\nˆf∗b(x), b= 1,2,...,Bat input point x. We then choose the model that\\nproduces the smallest prediction error, averaged over the original training\\nset. For squared error, for example, we choose the model obtaine d from\\nbootstrap sample ˆb, where\\nˆb= argmin\\nbN∑\\ni=1[yi−ˆf∗b(xi)]2. (8.60)\\nThe corresponding model predictions are ˆf∗ˆb(x). By convention we also\\ninclude the original training sample in the set of bootstrap samples, so that\\nthe method is free to pick the original model if it has the lowe st training\\nerror.\\nBy perturbing the data, bumping tries to move the ﬁtting proc edure\\naround to good areas of model space. For example, if a few data points are\\ncausing the procedure to ﬁnd a poor solution, any bootstrap s ample that\\nomits those data points should procedure a better solution.\\nFor another example, consider the classiﬁcation data in Fig ure 8.13, the\\nnotorious exclusive or (XOR) problem. There are two classes (blue and\\norange) and two input features, with the features exhibitin g a pure inter-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 309}),\n",
       " Document(page_content='292 8. Model Inference and Averaging\\naction. By splitting the data at x1= 0 and then splitting each resulting\\nstrata atx2= 0, (or vice versa) a tree-based classiﬁer could achieve per -\\nfect discrimination. However, the greedy, short-sighted C ART algorithm\\n(Section 9.2) tries to ﬁnd the best split on either feature, a nd then splits\\nthe resulting strata. Because of the balanced nature of the d ata, all initial\\nsplits onx1orx2appear to be useless, and the procedure essentially gener-\\nates a random split at the top level. The actual split found fo r these data is\\nshownintheleftpanelofFigure8.13.Bybootstrapsampling fromthedata,\\nbumping breaks the balance in the classes, and with a reasona ble number\\nof bootstrap samples (here 20), it will by chance produce at l east one tree\\nwith initial split near either x1= 0 orx2= 0. Using just 20 bootstrap\\nsamples, bumping found the near optimal splits shown in the r ight panel\\nof Figure 8.13. This shortcoming of the greedy tree-growing algorithm is\\nexacerbated if we add a number of noise features that are inde pendent of\\nthe class label. Then the tree-growing algorithm cannot dis tinguishx1or\\nx2from the others, and gets seriously lost.\\nSince bumping compares diﬀerent models on the training data , one must\\nensure that the models have roughly the same complexity. In t he case of\\ntrees, this would mean growing trees with the same number of t erminal\\nnodes on each bootstrap sample. Bumping can also help in prob lems where\\nit is diﬃcult to optimize the ﬁtting criterion, perhaps beca use of a lack of\\nsmoothness. The trick is to optimize a diﬀerent, more conven ient criterion\\nover the bootstrap samples, and then choose the model produc ing the best\\nresults for the desired criterion on the training sample.\\nBibliographic Notes\\nThere are many books on classical statistical inference: Co x and Hink-\\nley (1974) and Silvey (1975) give nontechnical accounts. Th e bootstrap\\nis due to Efron (1979) and is described more fully in Efron and Tibshi-\\nrani (1993) and Hall (1992). A good modern book on Bayesian in ference\\nis Gelman et al. (1995). A lucid account of the application of Bayesian\\nmethods to neural networks is given in Neal (1996). The stati stical appli-\\ncation of Gibbs sampling is due to Geman and Geman (1984), and Gelfand\\nand Smith (1990), with related work by Tanner and Wong (1987) . Markov\\nchain Monte Carlo methods, including Gibbs sampling and the Metropolis–\\nHastings algorithm, are discussed in Spiegelhalter et al. ( 1996). The EM\\nalgorithm is due to Dempster et al. (1977); as the discussant s in that pa-\\nper make clear, there was much related, earlier work. The vie w of EM as\\na joint maximization scheme for a penalized complete-data l og-likelihood\\nwas elucidated by Neal and Hinton (1998); they credit Csisza r and Tusn´ ady\\n(1984) and Hathaway (1986) as having noticed this connectio n earlier. Bag-\\nging was proposed by Breiman (1996a). Stacking is due to Wolp ert (1992);', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 310}),\n",
       " Document(page_content='Exercises 293\\nBreiman (1996b) contains an accessible discussion for stat isticians. Leblanc\\nand Tibshirani (1996) describe variations on stacking base d on the boot-\\nstrap. Model averaging in the Bayesian framework has been re cently advo-\\ncated by Madigan and Raftery (1994). Bumping was proposed by Tibshi-\\nrani and Knight (1999).\\nExercises\\nEx. 8.1Letr(y) andq(y) be probability density functions. Jensen’s in-\\nequality states that for a random variable Xand a convex function φ(x),\\nE[φ(X)]≥φ[E(X)]. Use Jensen’s inequality to show that\\nEqlog[r(Y)/q(Y)] (8.61)\\nis maximized as a function of r(y) whenr(y) =q(y). Hence show that\\nR(θ,θ)≥R(θ′,θ) as stated below equation (8.46).\\nEx. 8.2Consider the maximization of the log-likelihood (8.48), ov er dis-\\ntributions ˜P(Zm) such that ˜P(Zm)≥0 and∑\\nZm˜P(Zm) = 1. Use La-\\ngrange multipliers to show that the solution is the conditio nal distribution\\n˜P(Zm) = Pr(Zm|Z,θ′), as in (8.49).\\nEx. 8.3Justify the estimate (8.50), using the relationship\\nPr(A) =∫\\nPr(A|B)d(Pr(B)).\\nEx. 8.4Consider the bagging method of Section 8.7. Let our estimate ˆf(x)\\nbe theB-spline smoother ˆ µ(x) of Section 8.2.1. Consider the parametric\\nbootstrap of equation (8.6), applied to this estimator. Sho w that if we bag\\nˆf(x), using the parametric bootstrap to generate the bootstrap samples,\\nthe bagging estimate ˆfbag(x) converges to the original estimate ˆf(x) as\\nB→∞.\\nEx. 8.5Suggest generalizations of each of the loss functions in Fig ure 10.4\\nto more than two classes, and design an appropriate plot to co mpare them.\\nEx. 8.6Consider the bone mineral density data of Figure 5.6.\\n(a) Fit a cubic smooth spline to the relative change in spinal BMD, as a\\nfunction of age. Use cross-validation to estimate the optim al amount\\nof smoothing. Construct pointwise 90% conﬁdence bands for t he un-\\nderlying function.\\n(b) Compute the posterior mean and covariance for the true fu nction via\\n(8.28), and compare the posterior bands to those obtained in (a).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 311}),\n",
       " Document(page_content='294 8. Model Inference and Averaging\\n(c)Compute100bootstrapreplicates oftheﬁttedcurves,as inthebottom\\nleft panel of Figure 8.2. Compare the results to those obtain ed in (a)\\nand (b).\\nEx. 8.7EM as a minorization algorithm (Hunter and Lange, 2004; Wu and\\nLange, 2007). A function g(x,y) to said to minorize a functionf(x) if\\ng(x,y)≤f(x), g(x,x) =f(x) (8.62)\\nfor allx,yin the domain. This is useful for maximizing f(x) since it is easy\\nto show that f(x) is non-decreasing under the update\\nxs+1= argmaxxg(x,xs) (8.63)\\nThere are analogous deﬁnitions for majorization , for minimizing a function\\nf(x).Theresultingalgorithmsareknownas MMalgorithms,for“Minorize-\\nMaximize” or “Majorize-Minimize.”\\nShow that the EM algorithm (Section 8.5.2) is an example of an MM al-\\ngorithm, using Q(θ′,θ)+logPr( Z|θ)−Q(θ,θ) to minorize the observed data\\nlog-likelihood ℓ(θ′;Z). (Note that only the ﬁrst term involves the relevant\\nparameterθ′).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 312}),\n",
       " Document(page_content='This is page 295\\nPrinter: Opaque this\\n9\\nAdditive Models, Trees, and Related\\nMethods\\nIn this chapter we begin our discussion of some speciﬁc metho ds for super-\\nvised learning. These techniques each assume a (diﬀerent) s tructured form\\nfor the unknown regression function, and by doing so they ﬁne sse the curse\\nof dimensionality. Of course, they pay the possible price of misspecifying\\nthe model, and so in each case there is a tradeoﬀ that has to be m ade. They\\ntake oﬀ where Chapters 3–6 left oﬀ. We describe ﬁve related te chniques:\\ngeneralized additive models, trees, multivariate adaptiv e regression splines,\\nthe patient rule induction method, and hierarchical mixtur es of experts.\\n9.1 Generalized Additive Models\\nRegression models play an important role in many data analys es, providing\\nprediction and classiﬁcation rules, and data analytic tool s for understand-\\ning the importance of diﬀerent inputs.\\nAlthough attractively simple, the traditional linear mode l often fails in\\nthese situations: in real life, eﬀects are often not linear. In earlier chapters\\nwe described techniques that used predeﬁned basis function s to achieve\\nnonlinearities. This section describes more automatic ﬂex ible statistical\\nmethods that may be used to identify and characterize nonlin ear regression\\neﬀects. These methods are called “generalized additive mod els.”\\nIn the regression setting, a generalized additive model has the form\\nE(Y|X1,X2,...,X p) =α+f1(X1)+f2(X2)+···+fp(Xp).(9.1)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 313}),\n",
       " Document(page_content='296 9. Additive Models, Trees, and Related Methods\\nAs usualX1,X2,...,X prepresentpredictors and Yis the outcome; the fj’s\\nare unspeciﬁed smooth (“nonparametric”) functions. If we w ere to model\\neach function using an expansion of basis functions (as in Ch apter 5), the\\nresulting model could then be ﬁt by simple least squares. Our approach\\nhere is diﬀerent: we ﬁt each function using a scatterplot smo other (e.g., a\\ncubic smoothing spline or kernel smoother), and provide an a lgorithm for\\nsimultaneously estimating all pfunctions (Section 9.1.1).\\nFor two-class classiﬁcation, recall the logistic regressi on model for binary\\ndata discussed in Section 4.4. We relate the mean of the binar y response\\nµ(X) = Pr(Y= 1|X) to the predictors via a linear regression model and\\nthelogitlink function:\\nlog(µ(X)\\n1−µ(X))\\n=α+β1X1+···+βpXp. (9.2)\\nTheadditivelogisticregressionmodelreplaceseachlineartermbyamor e\\ngeneral functional form\\nlog(µ(X)\\n1−µ(X))\\n=α+f1(X1)+···+fp(Xp), (9.3)\\nwhere again each fjis an unspeciﬁed smooth function. While the non-\\nparametric form for the functions fjmakes the model more ﬂexible, the\\nadditivity is retained and allows us to interpret the model i n much the\\nsame way as before. The additive logistic regression model i s an example\\nof a generalized additive model. In general, the conditiona l meanµ(X) of\\na response Yis related to an additive function of the predictors via a link\\nfunctiong:\\ng[µ(X)] =α+f1(X1)+···+fp(Xp). (9.4)\\nExamples of classical link functions are the following:\\n•g(µ) =µis the identity link, used for linear and additive models for\\nGaussian response data.\\n•g(µ) = logit(µ)asabove,or g(µ) = probit(µ),theprobitlinkfunction,\\nfor modeling binomial probabilities. The probit function i s the inverse\\nGaussian cumulative distribution function: probit( µ) = Φ−1(µ).\\n•g(µ) = log(µ) for log-linear or log-additive models for Poisson count\\ndata.\\nAll three of these arise from exponential family sampling mo dels, which\\nin addition include the gamma and negative-binomial distri butions. These\\nfamilies generate the well-known class of generalized line ar models, which\\nare all extended in the same way to generalized additive mode ls.\\nThe functions fjare estimated in a ﬂexible manner, using an algorithm\\nwhose basic building block is a scatterplot smoother. The es timated func-\\ntionˆfjcan then reveal possible nonlinearities in the eﬀect of Xj. Not all', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 314}),\n",
       " Document(page_content='9.1 Generalized Additive Models 297\\nof the functions fjneed to be nonlinear. We can easily mix in linear and\\nother parametric forms with the nonlinear terms, a necessit y when some of\\nthe inputs are qualitative variables (factors). The nonlin ear terms are not\\nrestricted to main eﬀects either; we can have nonlinear comp onents in two\\nor more variables, or separate curves in Xjfor each level of the factor Xk.\\nThus each of the following would qualify:\\n•g(µ) =XTβ+αk+f(Z)—asemiparametric model, where Xis a\\nvector of predictors to be modeled linearly, αkthe eﬀect for the kth\\nlevel of a qualitative input V, and the eﬀect of predictor Zis modeled\\nnonparametrically.\\n•g(µ) =f(X) +gk(Z)—againkindexes the levels of a qualitative\\ninputV, and thus creates an interaction term g(V,Z) =gk(Z) for\\nthe eﬀect of VandZ.\\n•g(µ) =f(X)+g(Z,W) wheregis a nonparametric function in two\\nfeatures.\\nAdditive models can replace linear models in a wide variety o f settings,\\nfor example an additive decomposition of time series,\\nYt=St+Tt+εt, (9.5)\\nwhereStis a seasonal component, Ttis a trend and εis an error term.\\n9.1.1 Fitting Additive Models\\nIn this section we describe a modular algorithm for ﬁtting ad ditive models\\nand their generalizations. The building block is the scatte rplot smoother\\nfor ﬁtting nonlinear eﬀects in a ﬂexible way. For concretene ss we use as our\\nscatterplot smoother the cubic smoothing spline described in Chapter 5.\\nThe additive model has the form\\nY=α+p∑\\nj=1fj(Xj)+ε, (9.6)\\nwhere the error term εhas mean zero. Given observations xi,yi, a criterion\\nlike the penalized sum of squares (5.9) of Section 5.4 can be s peciﬁed for\\nthis problem,\\nPRSS(α,f1,f2,...,f p) =N∑\\ni=1(\\nyi−α−p∑\\nj=1fj(xij))2\\n+p∑\\nj=1λj∫\\nf′′\\nj(tj)2dtj,\\n(9.7)\\nwheretheλj≥0aretuningparameters.Itcanbeshownthattheminimizer\\nof (9.7) is an additive cubic spline model; each of the functi onsfjis a', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 315}),\n",
       " Document(page_content='298 9. Additive Models, Trees, and Related Methods\\nAlgorithm 9.1 The Backﬁtting Algorithm for Additive Models.\\n1. Initialize: ˆ α=1\\nN∑N\\n1yi,ˆfj≡0,∀i,j.\\n2. Cycle:j= 1,2,...,p,..., 1,2,...,p,...,\\nˆfj← Sj[\\n{yi−ˆα−∑\\nk̸=jˆfk(xik)}N\\n1]\\n,\\nˆfj←ˆfj−1\\nNN∑\\ni=1ˆfj(xij).\\nuntil the functions ˆfjchange less than a prespeciﬁed threshold.\\ncubic spline in the component Xj, with knots at each of the unique values\\nofxij, i= 1,...,N. However, without further restrictions on the model,\\nthe solution is not unique. The constant αis not identiﬁable, since we\\ncan add or subtract any constants to each of the functions fj, and adjust\\nαaccordingly. The standard convention is to assume that∑N\\n1fj(xij) =\\n0∀j—the functions average zero over the data. It is easily seen th at ˆα=\\nave(yi) in this case. If in addition to this restriction, the matrix of input\\nvalues (having ijth entryxij) has full column rank, then (9.7) is a strictly\\nconvex criterion and the minimizer is unique. If the matrix i s singular, then\\nthelinear part of the components fjcannot be uniquely determined (while\\nthe nonlinear parts can!)(Buja et al., 1989).\\nFurthermore, a simple iterative procedure exists for ﬁndin g the solution.\\nWe set ˆα= ave(yi), and it never changes. We apply a cubic smoothing\\nsplineSjto the targets{yi−ˆα−∑\\nk̸=jˆfk(xik)}N\\n1, as a function of xij,\\nto obtain a new estimate ˆfj. This is done for each predictor in turn, using\\nthe current estimates of the other functions ˆfkwhen computing yi−ˆα−∑\\nk̸=jˆfk(xik).Theprocessiscontinueduntiltheestimates ˆfjstabilize.This\\nprocedure, given in detail in Algorithm 9.1, is known as “bac kﬁtting” and\\nthe resulting ﬁt is analogous to a multiple regression for li near models.\\nIn principle, the second step in (2) of Algorithm 9.1 is not ne eded, since\\nthe smoothing spline ﬁt to a mean-zero response has mean zero (Exer-\\ncise 9.1). In practice, machine rounding can cause slippage , and the ad-\\njustment is advised.\\nThis same algorithm can accommodate other ﬁtting methods in exactly\\nthe same way, by specifying appropriate smoothing operator sSj:\\n•other univariate regression smoothers such as local polyno mial re-\\ngression and kernel methods;', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 316}),\n",
       " Document(page_content='9.1 Generalized Additive Models 299\\n•linear regression operators yielding polynomial ﬁts, piec ewise con-\\nstant ﬁts, parametric spline ﬁts, series and Fourier ﬁts;\\n•more complicated operators such as surface smoothers for se cond or\\nhigher-order interactions or periodic smoothers for seaso nal eﬀects.\\nIf we consider the operation of smoother Sjonly at the training points, it\\ncan be represented by an N×Noperator matrix Sj(see Section 5.4.1).\\nThenthedegreesoffreedomforthe jthtermare(approximately)computed\\nas dfj= trace[Sj]−1, by analogy with degrees of freedom for smoothers\\ndiscussed in Chapters 5 and 6.\\nFor a large class of linear smoothers Sj, backﬁtting is equivalent to a\\nGauss–Seidel algorithm for solving a certain linear system of equations.\\nDetails are given in Exercise 9.2.\\nFor the logistic regression model and other generalized add itive models,\\nthe appropriate criterion is a penalized log-likelihood. T o maximize it, the\\nbackﬁtting procedure is used in conjunction with a likeliho od maximizer.\\nThe usual Newton–Raphson routine for maximizing log-likel ihoods in gen-\\neralized linear models can be recast as an IRLS (iteratively reweighted\\nleast squares) algorithm. This involves repeatedly ﬁtting a weighted linear\\nregression of a working response variable on the covariates ; each regression\\nyields a new value of the parameter estimates, which in turn g ive new work-\\ning responses and weights, and the process is iterated (see S ection 4.4.1).\\nIn the generalized additive model, the weighted linear regr ession is simply\\nreplaced by a weighted backﬁtting algorithm. We describe th e algorithm in\\nmore detail for logistic regression below, and more general ly in Chapter 6\\nof Hastie and Tibshirani (1990).\\n9.1.2 Example: Additive Logistic Regression\\nProbably the most widely used model in medical research is th e logistic\\nmodel for binary data. In this model the outcome Ycan be coded as 0\\nor 1, with 1 indicating an event (like death or relapse of a dis ease) and\\n0 indicating no event. We wish to model Pr( Y= 1|X), the probability of\\nan event given values of the prognostic factors XT= (X1,...,X p). The\\ngoal is usually to understand the roles of the prognostic fac tors, rather\\nthan to classify new individuals. Logistic models are also u sed in applica-\\ntions where one is interested in estimating the class probab ilities, for use\\nin risk screening. Apart from medical applications, credit risk screening is\\na popular application.\\nThe generalized additive logistic model has the form\\nlogPr(Y= 1|X)\\nPr(Y= 0|X)=α+f1(X1)+···+fp(Xp). (9.8)\\nThe functions f1,f2,...,f pare estimated by a backﬁtting algorithm\\nwithin a Newton–Raphson procedure, shown in Algorithm 9.2.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 317}),\n",
       " Document(page_content='300 9. Additive Models, Trees, and Related Methods\\nAlgorithm 9.2 Local Scoring Algorithm for the Additive Logistic Regres-\\nsion Model.\\n1. Compute starting values: ˆ α= log[¯y/(1−¯y)], where ¯y= ave(yi), the\\nsample proportion of ones, and set ˆfj≡0∀j.\\n2. Deﬁne ˆηi= ˆα+∑\\njˆfj(xij) and ˆpi= 1/[1+exp(−ˆηi)].\\nIterate:\\n(a) Construct the working target variable\\nzi= ˆηi+(yi−ˆpi)\\nˆpi(1−ˆpi).\\n(b) Construct weights wi= ˆpi(1−ˆpi)\\n(c) Fit an additive model to the targets ziwith weights wi, us-\\ning a weighted backﬁtting algorithm. This gives new estimat es\\nˆα,ˆfj,∀j\\n3. Continue step 2. until the change in the functions falls be low a pre-\\nspeciﬁed threshold.\\nTheadditivemodelﬁttinginstep(2)ofAlgorithm9.2requir esaweighted\\nscatterplot smoother. Most smoothing procedures can accep t observation\\nweights (Exercise 5.12); see Chapter 3 of Hastie and Tibshir ani (1990) for\\nfurther details.\\nThe additive logistic regression model can be generalized f urther to han-\\ndle more than two classes, using the multilogit formulation as outlined in\\nSection 4.4. While the formulation is a straightforward ext ension of (9.8),\\nthe algorithms for ﬁtting such models are more complex. See Y ee and Wild\\n(1996) for details, and the VGAMsoftware currently available from:\\nhttp://www.stat.auckland.ac.nz/ ∼yee.\\nExample: Predicting Email Spam\\nWe apply a generalized additive model to the spamdata introduced in\\nChapter 1. The data consists of information from 4601 email m essages, in\\na study to screen email for “spam” (i.e., junk email). The dat a is publicly\\navailable at ftp.ics.uci.edu , and was donated by George Forman from\\nHewlett-Packard laboratories, Palo Alto, California.\\nThe response variable is binary, with values emailorspam, and there are\\n57 predictors as described below:\\n•48 quantitative predictors—the percentage of words in the em ail that\\nmatch a given word. Examples include business ,address,internet ,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 318}),\n",
       " Document(page_content='9.1 Generalized Additive Models 301\\nTABLE 9.1. Test data confusion matrix for the additive logistic regress ion model\\nﬁt to the spam training data. The overall test error rate is 5.5%.\\nPredicted Class\\nTrue Class email(0)spam(1)\\nemail(0) 58.3% 2.5%\\nspam(1) 3.0% 36.3%\\nfree, andgeorge. The idea was that these could be customized for\\nindividual users.\\n•6 quantitative predictors—the percentage of characters in t he email\\nthat match a given character. The characters are ch;,ch(,ch[,ch!,\\nch$, andch#.\\n•The average length of uninterrupted sequences of capital le tters:\\nCAPAVE.\\n•The length of the longest uninterrupted sequence of capital letters:\\nCAPMAX.\\n•The sum of the length of uninterrupted sequences of capital l etters:\\nCAPTOT.\\nWecoded spamas1and emailaszero.Atestsetofsize1536wasrandomly\\nchosen, leaving 3065 observations in the training set. A gen eralized additive\\nmodelwasﬁt,usingacubicsmoothingsplinewithanominalfo urdegreesof\\nfreedom for each predictor. What this means is that for each p redictorXj,\\nthesmoothing-splineparameter λjwaschosensothattrace[ Sj(λj)]−1 = 4,\\nwhereSj(λ) is the smoothing spline operator matrix constructed using the\\nobserved values xij, i= 1,...,N. This is a convenient way of specifying\\nthe amount of smoothing in such a complex model.\\nMost of the spampredictors have a very long-tailed distribution. Before\\nﬁtting the GAM model, we log-transformed each variable (act ually log(x+\\n0.1)), but the plots in Figure 9.1 are shown as a function of the o riginal\\nvariables.\\nThe test error rates are shown in Table 9.1; the overall error rate is 5.3%.\\nBy comparison, a linear logistic regression has a test error rate of 7.6%.\\nTable 9.2 shows the predictors that are highly signiﬁcant in the additive\\nmodel.\\nFor ease of interpretation, in Table 9.2 the contribution fo r each variable\\nis decomposed into a linear component and the remaining nonl inear com-\\nponent. The top block of predictors are positively correlat ed with spam,\\nwhile the bottom block is negatively correlated. The linear component is a\\nweighted least squares linear ﬁt of the ﬁtted curve on the pre dictor, while\\nthe nonlinear part is the residual. The linear component of a n estimated', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 319}),\n",
       " Document(page_content='302 9. Additive Models, Trees, and Related Methods\\nTABLE 9.2. Signiﬁcant predictors from the additive model ﬁt to the spam train-\\ning data. The coeﬃcients represent the linear part of ˆfj, along with their standard\\nerrors and Z-score. The nonlinear P-value is for a test of nonlin earity of ˆfj.\\nName Num. df Coeﬃcient Std. Error ZScore Nonlinear\\nP-value\\nPositive eﬀects\\nour 53.9 0.566 0.114 4.970 0.052\\nover 63.9 0.244 0.195 1.249 0.004\\nremove 74.0 0.949 0.183 5.201 0.093\\ninternet 84.0 0.524 0.176 2.974 0.028\\nfree 163.9 0.507 0.127 4.010 0.065\\nbusiness 173.8 0.779 0.186 4.179 0.194\\nhpl 263.8 0.045 0.250 0.181 0.002\\nch! 524.0 0.674 0.128 5.283 0.164\\nch$ 533.9 1.419 0.280 5.062 0.354\\nCAPMAX 563.8 0.247 0.228 1.080 0.000\\nCAPTOT 574.0 0.755 0.165 4.566 0.063\\nNegative eﬀects\\nhp 253.9−1.404 0.224 −6.262 0.140\\ngeorge 273.7−5.003 0.744 −6.722 0.045\\n1999 373.8−0.672 0.191 −3.512 0.011\\nre 453.9−0.620 0.133 −4.649 0.597\\nedu 464.0−1.183 0.209 −5.647 0.000\\nfunction is summarized by the coeﬃcient, standard error and Z-score; the\\nlatter is the coeﬃcient divided by its standard error, and is considered\\nsigniﬁcant if it exceeds the appropriate quantile of a stand ard normal dis-\\ntribution. The column labeled nonlinearP-valueis a test of nonlinearity\\nof the estimated function. Note, however, that the eﬀect of e ach predictor\\nis fully adjusted for the entire eﬀects of the other predicto rs, not just for\\ntheir linear parts. The predictors shown in the table were ju dged signiﬁ-\\ncant by at least one of the tests (linear or nonlinear) at the p= 0.01 level\\n(two-sided).\\nFigure 9.1 shows the estimated functions for the signiﬁcant predictors\\nappearing in Table 9.2. Many of the nonlinear eﬀects appear t o account for\\na strong discontinuity at zero. For example, the probabilit y ofspamdrops\\nsigniﬁcantly as the frequency of georgeincreases from zero, but then does\\nnot change much after that. This suggests that one might repl ace each of\\nthefrequencypredictorsbyanindicatorvariableforazero count,andresort\\nto a linear logistic model. This gave a test error rate of 7 .4%; including the\\nlinear eﬀects of the frequencies as well dropped the test err or to 6.6%. It\\nappears that the nonlinearities in the additive model have a n additional\\npredictive power.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 320}),\n",
       " Document(page_content='9.1 Generalized Additive Models 303\\n0 2 4 6 8-5 0 5\\n0 1 2 3-5 0 5\\n0 2 4 6-5 0 5 10\\n0 2 4 6 8 10-5 0 5 10\\n0 2 4 6 8 10-5 0 5 10\\n0 2 4 6-5 0 5 10\\n0 5 10 15 20-10 -5 0\\n0 5 10-10 -5 0\\n0 10 20 30-10 -5 0 5\\n0 2 4 6-5 0 5\\n0 5 10 15 20-10 -5 0 5\\n0 5 10 15-10 -5 0\\n0 10 20 30-5 0 5 10\\n0 1 2 3 4 5 6-5 0 5 10\\n0 2000 6000 10000-5 0 5\\n0 5000 10000 15000-5 0 5our over remove internet\\nfree business hp hpl\\ngeorge 1999 re edu\\nch! ch$ CAPMAX CAPTOTˆf(our)\\nˆf(over)\\nˆf(remove)\\nˆf(internet )ˆf(free)\\nˆf(business )\\nˆf(hp)\\nˆf(hpl)ˆf(george)\\nˆf(1999)\\nˆf(re)\\nˆf(edu)ˆf(ch!)\\nˆf(ch$)\\nˆf(CAPMAX)\\nˆf(CAPTOT)\\nFIGURE 9.1. Spam analysis: estimated functions for signiﬁcant predicto rs. The\\nrug plot along the bottom of each frame indicates the observed values of the cor-\\nresponding predictor. For many of the predictors the nonline arity picks up the\\ndiscontinuity at zero.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 321}),\n",
       " Document(page_content='304 9. Additive Models, Trees, and Related Methods\\nIt is more serious to classify a genuine emailmessage as spam, since then\\na good email would be ﬁltered out and would not reach the user. We can\\nalter the balance between the class error rates by changing t he losses (see\\nSection 2.4). If we assign a loss L01for predicting a true class 0 as class 1,\\nandL10for predicting a true class 1 as class 0, then the estimated Ba yes\\nrule predicts class 1 if its probability is greater than L01/(L01+L10). For\\nexample, if we take L01= 10,L10= 1 then the (true) class 0 and class 1\\nerror rates change to 0.8% and 8.7%.\\nMore ambitiously, we can encourage the model to ﬁt better dat a in the\\nclass 0 by using weights L01for the class 0 observations and L10for the\\nclass 1 observations. As above, we then use the estimated Bay es rule to\\npredict. This gave error rates of 1.2% and 8.0% in (true) clas s 0 and class 1,\\nrespectively. We discuss below the issue of unequal losses f urther, in the\\ncontext of tree-based models.\\nAfter ﬁtting an additive model, one should check whether the inclusion\\nof some interactions can signiﬁcantly improve the ﬁt. This c an be done\\n“manually,” by inserting products of some or all of the signi ﬁcant inputs,\\nor automatically via the MARS procedure (Section 9.4).\\nThis example uses the additive model in an automatic fashion . As a data\\nanalysis tool, additive models are often used in a more inter active fashion,\\nadding and dropping terms to determine their eﬀect. By calib rating the\\namount of smoothing in terms of df j, one can move seamlessly between\\nlinear models (df j= 1) and partially linear models, where some terms are\\nmodeled more ﬂexibly. See Hastie and Tibshirani (1990) for m ore details.\\n9.1.3 Summary\\nAdditive models provide a useful extension of linear models , making them\\nmoreﬂexiblewhilestillretainingmuchoftheirinterpreta bility.Thefamiliar\\ntools for modeling and inference in linear models are also av ailable for\\nadditive models, seen for example in Table 9.2. The backﬁtti ng procedure\\nfor ﬁtting these models is simple and modular, allowing one t o choose a\\nﬁtting method appropriate for each input variable. As a resu lt they have\\nbecome widely used in the statistical community.\\nHowever additive models can have limitations for large data -mining ap-\\nplications. The backﬁtting algorithm ﬁts all predictors, w hich is not feasi-\\nble or desirable when a large number are available. The BRUTO procedure\\n(Hastie and Tibshirani, 1990, Chapter 9) combines backﬁtti ng with selec-\\ntion of inputs, but is not designed for large data-mining pro blems. There\\nhas also been recent work using lasso-type penalties to esti mate sparse ad-\\nditive models, for example the COSSO procedure of Lin and Zha ng (2006)\\nand the SpAM proposal of Ravikumar et al. (2008). For large pr oblems a\\nforward stagewise approach suchas boosting(Chapter 10) is moreeﬀective,\\nand also allows for interactions to be included in the model.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 322}),\n",
       " Document(page_content='9.2 Tree-Based Methods 305\\n9.2 Tree-Based Methods\\n9.2.1 Background\\nTree-based methods partition the feature space into a set of rectangles, and\\nthen ﬁt a simple model (like a constant) in each one. They are c onceptually\\nsimple yet powerful. We ﬁrst describe a popular method for tr ee-based\\nregression and classiﬁcation called CART, and later contra st it with C4.5,\\na major competitor.\\nLet’s consider a regression problem with continuous respon seYand in-\\nputsX1andX2, each taking values in the unit interval. The top left panel\\nof Figure 9.2 shows a partition of the feature space by lines t hat are parallel\\nto the coordinate axes. In each partition element we can mode lYwith a\\ndiﬀerent constant. However, there is a problem: although ea ch partitioning\\nline has a simple description like X1=c, some of the resulting regions are\\ncomplicated to describe.\\nTo simplify matters, we restrict attention to recursive bin ary partitions\\nlike that in the top right panel of Figure 9.2. We ﬁrst split th e space into\\ntwo regions, and model the response by the mean of Yin each region.\\nWe choose the variable and split-point to achieve the best ﬁt . Then one\\nor both of these regions are split into two more regions, and t his process\\nis continued, until some stopping rule is applied. For examp le, in the top\\nright panel of Figure 9.2, we ﬁrst split at X1=t1. Then the region X1≤t1\\nis split atX2=t2and the region X1>t1is split atX1=t3. Finally, the\\nregionX1>t3is split atX2=t4. The result of this process is a partition\\ninto the ﬁve regions R1,R2,...,R 5shown in the ﬁgure. The corresponding\\nregression model predicts Ywith a constant cmin regionRm, that is,\\nˆf(X) =5∑\\nm=1cmI{(X1,X2)∈Rm}. (9.9)\\nThis same model can be represented by the binary tree in the bo ttom left\\npanel of Figure 9.2. The full dataset sits at the top of the tre e. Observations\\nsatisfying the condition at each junction are assigned to th e left branch,\\nand the others to the right branch. The terminal nodes or leav es of the\\ntree correspond to the regions R1,R2,...,R 5. The bottom right panel of\\nFigure 9.2 is a perspective plot of the regression surface fr om this model.\\nFor illustration, we chose the node means c1=−5,c2=−7,c3= 0,c4=\\n2,c5= 4 to make this plot.\\nA key advantage of the recursive binary tree is its interpret ability. The\\nfeature space partition is fully described by a single tree. With more than\\ntwo inputs, partitions like that in the top right panel of Fig ure 9.2 are\\ndiﬃcult to draw, but the binary tree representation works in the same\\nway. This representation is also popular among medical scie ntists, perhaps\\nbecause it mimics the way that a doctor thinks. The tree strat iﬁes the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 323}),\n",
       " Document(page_content='306 9. Additive Models, Trees, and Related Methods\\n|t1t2\\nt3t4\\nR1R1\\nR2R2\\nR3R3\\nR4R4\\nR5R5\\nX1X1 X1\\nX2X2X2\\nX1≤t1\\nX2≤t2 X1≤t3\\nX2≤t4\\nFIGURE 9.2. Partitions and CART. Top right panel shows a partition of a\\ntwo-dimensional feature space by recursive binary splittin g, as used in CART,\\napplied to some fake data. Top left panel shows a general partit ion that cannot\\nbe obtained from recursive binary splitting. Bottom left pane l shows the tree cor-\\nresponding to the partition in the top right panel, and a persp ective plot of the\\nprediction surface appears in the bottom right panel.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 324}),\n",
       " Document(page_content='9.2 Tree-Based Methods 307\\npopulation into strata of high and low outcome, on the basis o f patient\\ncharacteristics.\\n9.2.2 Regression Trees\\nWe now turn to the question of how to grow a regression tree. Ou r data\\nconsists of pinputs and a response, for each of Nobservations: that is,\\n(xi,yi) fori= 1,2,...,N, withxi= (xi1,xi2,...,x ip). The algorithm\\nneeds to automatically decide on the splitting variables an d split points,\\nand also what topology (shape) the tree should have. Suppose ﬁrst that we\\nhave a partition into MregionsR1,R2,...,R M, and we model the response\\nas a constant cmin each region:\\nf(x) =M∑\\nm=1cmI(x∈Rm). (9.10)\\nIf we adopt as our criterion minimization of the sum of square s∑(yi−\\nf(xi))2, it is easy to see that the best ˆ cmis just the average of yiin region\\nRm:\\nˆcm= ave(yi|xi∈Rm). (9.11)\\nNow ﬁnding the best binary partition in terms of minimum sum o f squares\\nis generally computationally infeasible. Hence we proceed with a greedy\\nalgorithm. Starting with all of the data, consider a splitti ng variable jand\\nsplit points, and deﬁne the pair of half-planes\\nR1(j,s) ={X|Xj≤s}andR2(j,s) ={X|Xj>s}. (9.12)\\nThen we seek the splitting variable jand split point sthat solve\\nmin\\nj, s[\\nmin\\nc1∑\\nxi∈R1(j,s)(yi−c1)2+min\\nc2∑\\nxi∈R2(j,s)(yi−c2)2]\\n.(9.13)\\nFor any choice jands, the inner minimization is solved by\\nˆc1= ave(yi|xi∈R1(j,s)) and ˆc2= ave(yi|xi∈R2(j,s)).(9.14)\\nFor each splitting variable, the determination of the split pointscan\\nbe done very quickly and hence by scanning through all of the i nputs,\\ndetermination of the best pair ( j,s) is feasible.\\nHaving found the best split, we partition the data into the tw o resulting\\nregions and repeat the splitting process on each of the two re gions. Then\\nthis process is repeated on all of the resulting regions.\\nHowlargeshouldwegrowthetree?Clearlyaverylargetreemi ghtoverﬁt\\nthe data, while a small tree might not capture the important s tructure.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 325}),\n",
       " Document(page_content='308 9. Additive Models, Trees, and Related Methods\\nTree size is a tuning parameter governing the model’s comple xity, and the\\noptimal tree size should be adaptively chosen from the data. One approach\\nwouldbetosplittreenodesonlyifthedecreaseinsum-of-sq uaresduetothe\\nsplit exceeds some threshold. This strategy is too short-si ghted, however,\\nsince a seemingly worthless split might lead to a very good sp lit below it.\\nThe preferred strategy is to grow a large tree T0, stopping the splitting\\nprocess only when some minimum node size (say 5) is reached. T hen this\\nlarge tree is pruned using cost-complexity pruning , which we now describe.\\nWe deﬁne a subtree T⊂T0to be any tree that can be obtained by\\npruningT0, that is, collapsing any number of its internal (non-termin al)\\nnodes. We index terminal nodes by m, with node mrepresenting region\\nRm. Let|T|denote the number of terminal nodes in T. Letting\\nNm= #{xi∈Rm},\\nˆcm=1\\nNm∑\\nxi∈Rmyi,\\nQm(T) =1\\nNm∑\\nxi∈Rm(yi−ˆcm)2,(9.15)\\nwe deﬁne the cost complexity criterion\\nCα(T) =|T|∑\\nm=1NmQm(T)+α|T|. (9.16)\\nThe idea is to ﬁnd, for each α, the subtree Tα⊆T0to minimize Cα(T).\\nThe tuning parameter α≥0 governs the tradeoﬀ between tree size and its\\ngoodness of ﬁt to the data. Large values of αresult in smaller trees Tα, and\\nconversely for smaller values of α. As the notation suggests, with α= 0 the\\nsolution is the full tree T0. We discuss how to adaptively choose αbelow.\\nFor eachαone can show that there is a unique smallest subtree Tαthat\\nminimizesCα(T). To ﬁndTαwe useweakest link pruning : we successively\\ncollapse the internal node that produces the smallest per-n ode increase in∑\\nmNmQm(T), and continue until we produce the single-node (root) tree .\\nThis gives a (ﬁnite) sequence of subtrees, and one can show th is sequence\\nmust contain Tα. See Breiman et al. (1984) or Ripley (1996) for details.\\nEstimation of αis achieved by ﬁve- or tenfold cross-validation: we choose\\nthe value ˆαto minimize the cross-validated sum of squares. Our ﬁnal tre e\\nisTˆα.\\n9.2.3 Classiﬁcation Trees\\nIf the target is a classiﬁcation outcome taking values 1 ,2,...,K, the only\\nchanges needed in the tree algorithm pertain to the criteria for splitting\\nnodes and pruning the tree. For regression we used the square d-error node', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 326}),\n",
       " Document(page_content='9.2 Tree-Based Methods 309\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5\\npEntropy\\nGini index\\nMisclassification error\\nFIGURE 9.3. Node impurity measures for two-class classiﬁcation, as a func tion\\nof the proportion pin class 2. Cross-entropy has been scaled to pass through\\n(0.5,0.5).\\nimpurity measure Qm(T) deﬁned in (9.15), but this is not suitable for\\nclassiﬁcation. In a node m, representing a region RmwithNmobservations,\\nlet\\nˆpmk=1\\nNm∑\\nxi∈RmI(yi=k),\\nthe proportion of class kobservations in node m. We classify the obser-\\nvations in node mto classk(m) = argmax kˆpmk, the majority class in\\nnodem. Diﬀerent measures Qm(T) of node impurity include the following:\\nMisclassiﬁcation error:1\\nNm∑\\ni∈RmI(yi̸=k(m)) = 1−ˆpmk(m).\\nGini index:∑\\nk̸=k′ˆpmkˆpmk′=∑K\\nk=1ˆpmk(1−ˆpmk).\\nCross-entropy or deviance: −∑K\\nk=1ˆpmklog ˆpmk.\\n(9.17)\\nFor two classes, if pis the proportion in the second class, these three mea-\\nsures are 1−max(p,1−p), 2p(1−p) and−plogp−(1−p)log(1−p),\\nrespectively. They are shown in Figure 9.3. All three are sim ilar, but cross-\\nentropy and the Gini index are diﬀerentiable, and hence more amenable to\\nnumerical optimization. Comparing (9.13) and (9.15), we se e that we need\\nto weight the node impurity measures by the number NmLandNmRof\\nobservations in the two child nodes created by splitting nod em.\\nInaddition,cross-entropyandtheGiniindexaremoresensi tivetochanges\\nin the node probabilities than the misclassiﬁcation rate. F or example, in\\na two-class problem with 400 observations in each class (den ote this by\\n(400,400)), suppose one split created nodes (300 ,100) and (100 ,300), while', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 327}),\n",
       " Document(page_content='310 9. Additive Models, Trees, and Related Methods\\nthe other created nodes (200 ,400) and (200 ,0). Both splits produce a mis-\\nclassiﬁcation rate of 0.25, but the second split produces a p ure node and is\\nprobablypreferable.BoththeGiniindexandcross-entropy arelowerforthe\\nsecond split. For this reason, either the Gini index or cross -entropy should\\nbe used when growing the tree. To guide cost-complexity prun ing, any of\\nthe three measures can be used, but typically it is the miscla ssiﬁcation rate.\\nThe Gini index can be interpreted in two interesting ways. Ra ther than\\nclassify observations to the majority class in the node, we c ould classify\\nthem to class kwith probability ˆ pmk. Then the expected training error\\nrate of this rule in the node is∑\\nk̸=k′ˆpmkˆpmk′—the Gini index. Similarly,\\nif we code each observation as 1 for class kand zero otherwise, the variance\\nover the node of this 0-1 response is ˆ pmk(1−ˆpmk). Summing over classes\\nkagain gives the Gini index.\\n9.2.4 Other Issues\\nCategorical Predictors\\nWhen splitting a predictor having qpossible unordered values, there are\\n2q−1−1 possible partitions of the qvalues into two groups, and the com-\\nputations become prohibitive for large q. However, with a 0 −1 outcome,\\nthis computation simpliﬁes. We order the predictor classes according to the\\nproportion falling in outcome class 1. Then we split this pre dictor as if it\\nwere an ordered predictor. One can show this gives the optima l split, in\\ntermsofcross-entropyorGiniindex,amongallpossible2q−1−1splits.This\\nresult also holds for a quantitative outcome and square erro r loss—the cat-\\negories are ordered by increasing mean of the outcome. Altho ugh intuitive,\\nthe proofs of these assertions are not trivial. The proof for binary outcomes\\nis given in Breiman et al. (1984) and Ripley (1996); the proof for quantita-\\ntive outcomes can be found in Fisher (1958). For multicatego ry outcomes,\\nno such simpliﬁcations are possible, although various appr oximations have\\nbeen proposed (Loh and Vanichsetakul, 1988).\\nThe partitioning algorithm tends to favor categorical pred ictors with\\nmany levels q; the number of partitions grows exponentially in q, and the\\nmore choices we have, the more likely we can ﬁnd a good one for t he data\\nat hand. This can lead to severe overﬁtting if qis large, and such variables\\nshould be avoided.\\nThe Loss Matrix\\nIn classiﬁcation problems, the consequences of misclassif ying observations\\nare more serious in some classes than others. For example, it is probably\\nworse to predict that a person will not have a heart attack whe n he/she\\nactually will, than vice versa. To account for this, we deﬁne aK×Kloss\\nmatrixL, withLkk′being the loss incurred for classifying a class kobser-\\nvation as class k′. Typically no loss is incurred for correct classiﬁcations,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 328}),\n",
       " Document(page_content='9.2 Tree-Based Methods 311\\nthat is,Lkk= 0∀k. To incorporate the losses into the modeling process,\\nwe could modify the Gini index to∑\\nk̸=k′Lkk′ˆpmkˆpmk′; this would be the\\nexpected loss incurred by the randomized rule. This works fo r the multi-\\nclass case, but in the two-class case has no eﬀect, since the c oeﬃcient of\\nˆpmkˆpmk′isLkk′+Lk′k. For two classes a better approach is to weight the\\nobservations in class kbyLkk′. This can be used in the multiclass case only\\nif, as a function of k,Lkk′doesn’t depend on k′. Observation weighting can\\nbe used with the deviance as well. The eﬀect of observation we ighting is to\\nalter the prior probability on the classes. In a terminal nod e, the empirical\\nBayes rule implies that we classify to class k(m) = argmin k∑\\nℓLℓkˆpmℓ.\\nMissing Predictor Values\\nSuppose our data has some missing predictor values in some or all of the\\nvariables. We might discard any observation with some missi ng values, but\\nthis could lead to serious depletion of the training set. Alt ernatively we\\nmight try to ﬁll in (impute) the missing values, with say the m ean of that\\npredictor over the nonmissing observations. For tree-base d models, there\\nare two better approaches. The ﬁrst is applicable to categor ical predictors:\\nwe simply make a new category for “missing.” From this we migh t dis-\\ncover that observations with missing values for some measur ement behave\\ndiﬀerently than those with nonmissing values. The second mo re general\\napproach is the construction of surrogate variables. When c onsidering a\\npredictor for a split, we use only the observations for which that predictor\\nis not missing. Having chosen the best (primary) predictor a nd split point,\\nwe form a list of surrogate predictors and split points. The ﬁ rst surrogate\\nis the predictor and corresponding split point that best mim ics the split of\\nthe training data achieved by the primary split. The second s urrogate is\\nthe predictor and corresponding split point that does secon d best, and so\\non. When sending observations down the tree either in the tra ining phase\\nor during prediction, we use the surrogate splits in order, i f the primary\\nsplitting predictor is missing. Surrogate splits exploit c orrelations between\\npredictorstotryandalleviatetheeﬀectofmissingdata.Th ehigherthecor-\\nrelation between the missing predictor and the other predic tors, the smaller\\nthe loss of information due to the missing value. The general problem of\\nmissing data is discussed in Section 9.6.\\nWhy Binary Splits?\\nRather than splitting each node into just two groups at each s tage (as\\nabove),wemightconsidermultiwaysplitsintomorethantwo groups.While\\nthis can sometimes be useful, it is not a good general strateg y. The problem\\nis that multiway splits fragment the data too quickly, leavi ng insuﬃcient\\ndata at the next level down. Hence we would want to use such spl its only\\nwhen needed. Since multiway splits can be achieved by a serie s of binary\\nsplits, the latter are preferred.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 329}),\n",
       " Document(page_content='312 9. Additive Models, Trees, and Related Methods\\nOther Tree-Building Procedures\\nThe discussion above focuses on the CART (classiﬁcation and regression\\ntree) implementation of trees. The other popular methodolo gy is ID3 and\\nits later versions, C4.5 and C5.0 (Quinlan, 1993). Early ver sions of the\\nprogram were limited to categorical predictors, and used a t op-down rule\\nwith no pruning. With more recent developments, C5.0 has bec ome quite\\nsimilar to CART. The most signiﬁcant feature unique to C5.0 i s a scheme\\nforderivingrulesets.Afteratreeisgrown,thesplittingr ulesthatdeﬁnethe\\nterminal nodes can sometimes be simpliﬁed: that is, one or mo re condition\\ncan be dropped without changing the subset of observations t hat fall in\\nthe node. We end up with a simpliﬁed set of rules deﬁning each t erminal\\nnode; these no longer follow a tree structure, but their simp licity might\\nmake them more attractive to the user.\\nLinear Combination Splits\\nRather than restricting splits to be of the form Xj≤s, one can allow splits\\nalong linear combinations of the form∑ajXj≤s. The weights ajand\\nsplit pointsare optimized to minimize the relevant criterion (such as th e\\nGini index). While this can improve the predictive power of t he tree, it can\\nhurt interpretability. Computationally, the discretenes s of the split point\\nsearch precludes the use of a smooth optimization for the wei ghts. A better\\nway to incorporate linear combination splits is in the hiera rchical mixtures\\nof experts (HME) model, the topic of Section 9.5.\\nInstability of Trees\\nOne major problem with trees is their high variance. Often a s mall change\\nin the data can result in a very diﬀerent series of splits, mak ing interpre-\\ntation somewhat precarious. The major reason for this insta bility is the\\nhierarchical nature of the process: the eﬀect of an error in t he top split\\nis propagated down to all of the splits below it. One can allev iate this to\\nsome degree by trying to use a more stable split criterion, bu t the inherent\\ninstability is not removed. It is the price to be paid for esti mating a simple,\\ntree-based structure from the data. Bagging (Section 8.7) averages many\\ntrees to reduce this variance.\\nLack of Smoothness\\nAnother limitation of trees is the lack of smoothness of the p rediction sur-\\nface, as can be seen in the bottom right panel of Figure 9.2. In classiﬁcation\\nwith 0/1 loss, this doesn’t hurt much, since bias in estimati on of the class\\nprobabilities has a limited eﬀect. However, this can degrad e performance\\nin the regression setting, where we would normally expect th e underlying\\nfunction to be smooth. The MARS procedure, described in Sect ion 9.4,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 330}),\n",
       " Document(page_content='9.2 Tree-Based Methods 313\\nTABLE 9.3. Spam data: confusion rates for the 17-node tree (chosen by cross–\\nvalidation) on the test data. Overall error rate is 9.3%.\\nPredicted\\nTrueemail spam\\nemail 57.3% 4.0%\\nspam 5.3% 33.4%\\ncan be viewed as a modiﬁcation of CART designed to alleviate t his lack of\\nsmoothness.\\nDiﬃculty in Capturing Additive Structure\\nAnother problem with trees is their diﬃculty in modeling add itive struc-\\nture.Inregression,suppose,forexample,that Y=c1I(X1<t1)+c2I(X2<\\nt2)+εwhereεis zero-mean noise. Then a binary tree might make its ﬁrst\\nsplit onX1neart1. At the next level down it would have to split both nodes\\nonX2att2in order to capture the additive structure. This might happe n\\nwithsuﬃcientdata,butthemodelisgivennospecialencoura gementtoﬁnd\\nsuch structure. If there were ten rather than two additive eﬀ ects, it would\\ntake many fortuitous splits to recreate the structure, and t he data analyst\\nwould be hard pressed to recognize it in the estimated tree. T he “blame”\\nhere can again be attributed to the binary tree structure, wh ich has both\\nadvantages and drawbacks. Again the MARS method (Section 9. 4) gives\\nup this tree structure in order to capture additive structur e.\\n9.2.5 Spam Example (Continued)\\nWe applied the classiﬁcation tree methodology to the spamexample intro-\\nduced earlier. We used the deviance measure to grow the tree a nd mis-\\nclassiﬁcation rate to prune it. Figure 9.4 shows the 10-fold cross-validation\\nerror rate as a function of the size of the pruned tree, along w ith±2 stan-\\ndard errors of the mean, from the ten replications. The test e rror curve is\\nshown in orange. Note that the cross-validation error rates are indexed by\\na sequence of values of αandnottree size; for trees grown in diﬀerent folds,\\na value ofαmight imply diﬀerent sizes. The sizes shown at the base of the\\nplot refer to|Tα|, the sizes of the pruned originaltree.\\nTheerrorﬂattensoutataround17terminalnodes,givingthe prunedtree\\nin Figure 9.5. Of the 13 distinct features chosen by the tree, 11 overlap with\\nthe 16 signiﬁcant features in the additive model (Table 9.2) . The overall\\nerror rate shown in Table 9.3 is about 50% higher than for the a dditive\\nmodel in Table 9.1.\\nConsider the rightmost branches of the tree. We branch to the right\\nwith aspamwarning if more than 5.5% of the characters are the $ sign.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 331}),\n",
       " Document(page_content='314 9. Additive Models, Trees, and Related Methods\\n0 10 20 30 400.0 0.1 0.2 0.3 0.4\\nTree SizeMisclassification Rate176 21 7 5 3 2 0α\\nFIGURE 9.4. Results for spamexample. The blue curve is the 10-fold cross-val-\\nidation estimate of misclassiﬁcation rate as a function of tr ee size, with standard\\nerror bars. The minimum occurs at a tree size with about 17terminal nodes (using\\nthe “one-standard-error” rule). The orange curve is the test error, which tracks\\nthe CV error quite closely. The cross-validation is indexed by v alues ofα, shown\\nabove. The tree sizes shown below refer to |Tα|, the size of the original tree indexed\\nbyα.\\nHowever, if in addition the phrase hpoccurs frequently, then this is likely\\nto be company business and we classify as email. All of the 22 cases in\\nthe test set satisfying these criteria were correctly class iﬁed. If the second\\ncondition is not met, and in addition the average length of re peated capital\\nlettersCAPAVEis larger than 2.9, then we classify as spam. Of the 227 test\\ncases, only seven were misclassiﬁed.\\nIn medical classiﬁcation problems, the terms sensitivity andspeciﬁcity\\nare used to characterize a rule. They are deﬁned as follows:\\nSensitivity: probability of predicting disease given true state is disea se.\\nSpeciﬁcity: probability of predicting non-disease given true state is n on-\\ndisease.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 332}),\n",
       " Document(page_content='9.2 Tree-Based Methods 315\\n600/1536\\n280/1177\\n180/1065\\n 80/861\\n 80/652\\n 77/423\\n 20/238\\n 19/236   1/2 57/185\\n 48/113\\n 37/101   1/12  9/72  3/229  0/209100/204\\n 36/123\\n 16/94\\n 14/89   3/5  9/29 16/81  9/112\\n  6/109   0/3 48/359\\n 26/337\\n 19/110\\n 18/109   0/1  7/227  0/22\\nspam\\nspamspamspamspam\\nspamspam\\nspam\\nspam\\nspam\\nspamspamemail\\nemailemail\\nemailemailemailemail\\nemail\\nemail\\nemail\\nemailemailemail\\nemailemailemailemailemailemailemailemail\\nch$<0.0555\\nremove<0.06\\nch!<0.191\\ngeorge<0.005\\nhp<0.03\\nCAPMAX<10.5\\nreceive<0.125 edu<0.045\\nour<1.2CAPAVE<2.7505\\nfree<0.065\\nbusiness<0.145george<0.15hp<0.405\\nCAPAVE<2.907\\n1999<0.58ch$>0.0555\\nremove>0.06\\nch!>0.191\\ngeorge>0.005\\nhp>0.03\\nCAPMAX>10.5\\nreceive>0.125 edu>0.045\\nour>1.2CAPAVE>2.7505\\nfree>0.065\\nbusiness>0.145george>0.15hp>0.405\\nCAPAVE>2.907\\n1999>0.58\\nFIGURE 9.5. The pruned tree for the spamexample. The split variables are\\nshown in blue on the branches, and the classiﬁcation is shown in every node.The\\nnumbers under the terminal nodes indicate misclassiﬁcation rates on the test data.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 333}),\n",
       " Document(page_content='316 9. Additive Models, Trees, and Related Methods\\nSpecificitySensitivity\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0• •\\n•• •\\n•\\n•\\n•\\n••\\n•\\n•••• ••••••••••••••• ••••• ••••••••• •••••••••••••\\n••\\n•\\n•\\n••\\n••\\n•••\\n•\\n•\\n•\\n•\\n•Tree (0.95)\\nGAM (0.98)\\nWeighted Tree (0.90)\\nFIGURE 9.6. ROC curves for the classiﬁcation rules ﬁt to the spamdata. Curves\\nthat are closer to the northeast corner represent better class iﬁers. In this case the\\nGAM classiﬁer dominates the trees. The weighted tree achieves better sensitivity\\nfor higher speciﬁcity than the unweighted tree. The numbers in the legend repre-\\nsent the area under the curve.\\nIf we think of spamandemailas the presence and absence of disease, re-\\nspectively, then from Table 9.3 we have\\nSensitivity = 100×33.4\\n33.4+5.3= 86.3%,\\nSpeciﬁcity = 100×57.3\\n57.3+4.0= 93.4%.\\nIn this analysis we have used equal losses. As before let Lkk′be the\\nloss associated with predicting a class kobject as class k′. By varying the\\nrelative sizes of the losses L01andL10, we increase the sensitivity and\\ndecrease the speciﬁcity of the rule, or vice versa. In this ex ample, we want\\nto avoid marking good emailasspam, and thus we want the speciﬁcity to\\nbe very high. We can achieve this by setting L01>1 say, with L10= 1.\\nThe Bayes’ rule in each terminal node classiﬁes to class 1 ( spam) if the\\nproportion of spamis≥L01/(L10+L01), and class zero otherwise. The', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 334}),\n",
       " Document(page_content='9.3 PRIM: Bump Hunting 317\\nreceiveroperatingcharacteristiccurve(ROC)isacommonl yusedsummary\\nfor assessing the tradeoﬀ between sensitivity and speciﬁci ty. It is a plot of\\nthesensitivityversusspeciﬁcityaswevarytheparameters ofaclassiﬁcation\\nrule. Varying the loss L01between 0.1 and 10, and applying Bayes’ rule to\\nthe 17-node tree selected in Figure 9.4, produced the ROC cur ve shown\\nin Figure 9.6. The standard error of each curve near 0.9 is app roximately√\\n0.9(1−0.9)/1536 = 0.008, and hence the standard error of the diﬀerence\\nis about 0.01. We see that in order to achieve a speciﬁcity of close to 100 %,\\nthe sensitivity has to drop to about 50%. The area under the cu rve is a\\ncommonly used quantitative summary; extending the curve li nearly in each\\ndirection so that it is deﬁned over [0 ,100], the area is approximately 0 .95.\\nFor comparison, we have included the ROC curve for the GAM mod el ﬁt\\nto these data in Section 9.2; it gives a better classiﬁcation rule for any loss,\\nwith an area of 0 .98.\\nRather than just modifying the Bayes rule in the nodes, it is b etter to\\ntake full account of the unequal losses in growing the tree, a s was done\\nin Section 9.2. With just two classes 0 and 1, losses may be inc orporated\\ninto the tree-growing process by using weight Lk,1−kfor an observation in\\nclassk. Here we chose L01= 5,L10= 1 and ﬁt the same size tree as before\\n(|Tα|= 17). This tree has higher sensitivity at high values of the s peciﬁcity\\nthan the original tree, but does more poorly at the other extr eme. Its top\\nfew splits are the same as the original tree, and then it depar ts from it.\\nFor this application the tree grown using L01= 5 is clearly better than the\\noriginal tree.\\nThe area under the ROC curve, used above, is sometimes called thec-\\nstatistic. Interestingly, it can be shown that the area under the ROC cu rve\\nisequivalenttotheMann-WhitneyUstatistic(orWilcoxonr ank-sumtest),\\nfor the median diﬀerence between the prediction scores in th e two groups\\n(HanleyandMcNeil,1982).Forevaluatingthecontribution ofanadditional\\npredictor when added to a standard model, the c-statistic may not be an\\ninformative measure. The new predictor can be very signiﬁca nt in terms\\nof the change in model deviance, but show only a small increas e in thec-\\nstatistic. For example, removal of the highly signiﬁcant te rmgeorgefrom\\nthe model of Table 9.2 results in a decrease in the c-statistic of less than\\n0.01. Instead, it is useful to examine how the additional predi ctor changes\\nthe classiﬁcation on an individual sample basis. A good disc ussion of this\\npoint appears in Cook (2007).\\n9.3 PRIM: Bump Hunting\\nTree-based methods (for regression) partition the feature space into box-\\nshaped regions, to try to make the response averages in each b ox as diﬀer-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 335}),\n",
       " Document(page_content='318 9. Additive Models, Trees, and Related Methods\\nent as possible. The splitting rules deﬁning the boxes are re lated to each\\nthrough a binary tree, facilitating their interpretation.\\nThepatientruleinductionmethod(PRIM)alsoﬁndsboxesint hefeature\\nspace, but seeks boxes in which the response average is high. Hence it looks\\nfor maxima in the target function, an exercise known as bump hunting . (If\\nminimaratherthanmaximaaredesired,onesimplyworkswith thenegative\\nresponse values.)\\nPRIM also diﬀers from tree-based partitioning methods in th at the box\\ndeﬁnitions are not described by a binary tree. This makes int erpretation of\\nthe collection of rules more diﬃcult; however, by removing t he binary tree\\nconstraint, the individual rules are often simpler.\\nThe main box construction method in PRIM works from the top do wn,\\nstarting with a box containing all of the data. The box is comp ressed along\\none face by a small amount, and the observations then falling outside the\\nbox arepeeledoﬀ. The face chosen for compression is the one resulting in\\nthe largest box mean, after the compression is performed. Th en the process\\nisrepeated,stoppingwhenthecurrentboxcontainssomemin imumnumber\\nof data points.\\nThis process is illustrated in Figure 9.7. There are 200 data points uni-\\nformly distributed over the unit square. The color-coded pl ot indicates the\\nresponseYtaking the value 1 (red) when 0 .5<X1<0.8 and 0.4<X2<\\n0.6. and zero (blue) otherwise. The panels shows the successiv e boxes found\\nby the top-down peeling procedure, peeling oﬀ a proportion α= 0.1 of the\\nremaining data points at each stage.\\nFigure 9.8 shows the mean of the response values in the box, as the box\\nis compressed.\\nAfter the top-down sequence is computed, PRIM reverses the p rocess,\\nexpanding along any edge, if such an expansion increases the box mean.\\nThis is called pasting. Since the top-down procedure is greedy at each step,\\nsuch an expansion is often possible.\\nThe result of these steps is a sequence of boxes, with diﬀeren t numbers\\nof observation in each box. Cross-validation, combined wit h the judgment\\nof the data analyst, is used to choose the optimal box size.\\nDenote byB1the indices of the observations in the box found in step 1.\\nThePRIMprocedurethenremovestheobservationsin B1fromthetraining\\nset, and the two-step process—top down peeling, followed by b ottom-up\\npasting—is repeated on the remaining dataset. This entire pr ocess is re-\\npeated several times, producing a sequence of boxes B1,B2,...,B k. Each\\nbox is deﬁned by a set of rules involving a subset of predictor s like\\n(a1≤X1≤b1) and (b1≤X3≤b2).\\nA summary of the PRIM procedure is given Algorithm 9.3.\\nPRIM can handle a categorical predictor by considering all p artitions of\\nthe predictor, as in CART. Missing values are also handled in a manner\\nsimilar to CART. PRIM is designed for regression (quantitat ive response', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 336}),\n",
       " Document(page_content='9.3 PRIM: Bump Hunting 319\\n1\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no2\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no3\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no4\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no\\n5\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no6\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no7\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no8\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no\\n12\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no17\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no22\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no27\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no\\nFIGURE 9.7. Illustration of PRIM algorithm. There are two classes, indicat ed\\nby the blue (class 0) and red (class 1) points. The procedure starts with a rectangle\\n(broken black lines) surrounding all of the data, and then peels away points along\\none edge by a prespeciﬁed amount in order to maximize the mean of the points\\nremaining in the box. Starting at the top left panel, the sequen ce of peelings is\\nshown, until a pure red region is isolated in the bottom right p anel. The iteration\\nnumber is indicated at the top of each panel.\\nNumber of Observations in BoxBox Mean\\n50 100 1500.2 0.4 0.6 0.8 1.0\\n•••••••••••••••••••••••••••\\nFIGURE 9.8. Box mean as a function of number of observations in the box.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 337}),\n",
       " Document(page_content='320 9. Additive Models, Trees, and Related Methods\\nAlgorithm 9.3 Patient Rule Induction Method.\\n1. Start with all of the training data, and a maximal box conta ining all\\nof the data.\\n2. Consider shrinking the box by compressing one face, so as t o peel oﬀ\\nthe proportion αof observations having either the highest values of\\na predictor Xj, or the lowest. Choose the peeling that produces the\\nhighest response mean in the remaining box. (Typically α= 0.05 or\\n0.10.)\\n3. Repeat step 2 until some minimal number of observations (s ay 10)\\nremain in the box.\\n4. Expand the box along any face, as long as the resulting box m ean\\nincreases.\\n5. Steps 1–4 give a sequence of boxes, with diﬀerent numbers o f obser-\\nvations in each box. Use cross-validation to choose a member of the\\nsequence. Call the box B1.\\n6. Remove the data in box B1from the dataset and repeat steps 2–5 to\\nobtain a second box, and continue to get as many boxes as desir ed.\\nvariable); a two-class outcome can be handled simply by codi ng it as 0 and\\n1. There is no simple way to deal with k >2 classes simultaneously: one\\napproach is to run PRIM separately for each class versus a bas eline class.\\nAn advantage of PRIM over CART is its patience. Because of its bi-\\nnary splits, CART fragments the data quite quickly. Assumin g splits of\\nequal size, with Nobservations it can only make log2(N)−1 splits before\\nrunning out of data. If PRIM peels oﬀ a proportion αof training points\\nat each stage, it can perform approximately −log(N)/log(1−α) peeling\\nsteps before running out of data. For example, if N= 128 and α= 0.10,\\nthen log2(N)−1 = 6 while−log(N)/log(1−α)≈46. Taking into account\\nthat there must be an integer number of observations at each s tage, PRIM\\nin fact can peel only 29 times. In any case, the ability of PRIM to be more\\npatient should help the top-down greedy algorithm ﬁnd a bett er solution.\\n9.3.1 Spam Example (Continued)\\nWe applied PRIM to the spamdata, with the response coded as 1 for spam\\nand 0 for email.\\nThe ﬁrst two boxes found by PRIM are summarized below:', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 338}),\n",
       " Document(page_content='9.4 MARS: Multivariate Adaptive Regression Splines 321\\nRule 1 Global Mean Box Mean Box Support\\nTraining 0.3931 0.9607 0.1413\\nTest 0.3958 1.0000 0.1536\\nRule 1\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3ch!>0.029\\nCAPAVE>2.331\\nyour>0.705\\n1999<0.040\\nCAPTOT>79.50\\nedu<0.070\\nre<0.535\\nch;<0.030\\nRule 2 Remain Mean Box Mean Box Support\\nTraining 0.2998 0.9560 0.1043\\nTest 0.2862 0.9264 0.1061\\nRule 2{\\nremove>0.010\\ngeorge<0.110\\nThe box support is the proportion of observations falling in the box.\\nThe ﬁrst box is purely spam, and contains about 15% of the test data.\\nThe second box contains 10.6% of the test observations, 92.6 % of which\\narespam. Together the two boxes contain 26% of the data and are about\\n97%spam. The next few boxes (not shown) are quite small, containing o nly\\nabout 3% of the data.\\nThe predictors are listed in order of importance. Interesti ngly the top\\nsplitting variables in the CART tree (Figure 9.5) do not appe ar in PRIM’s\\nﬁrst box.\\n9.4 MARS: Multivariate Adaptive Regression\\nSplines\\nMARS is an adaptive procedure for regression, and is well sui ted for high-\\ndimensional problems (i.e., a large number of inputs). It ca n be viewed as a\\ngeneralization of stepwise linear regression or a modiﬁcat ion of the CART\\nmethod to improve the latter’s performance in the regressio n setting. We\\nintroduceMARSfromtheﬁrstpointofview,andlatermakethe connection\\nto CART.\\nMARS uses expansions in piecewise linear basis functions of the form\\n(x−t)+and (t−x)+. The “+” means positive part, so\\n(x−t)+={\\nx−t,ifx>t,\\n0,otherwise,and (t−x)+={\\nt−x,ifx<t,\\n0,otherwise.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 339}),\n",
       " Document(page_content='322 9. Additive Models, Trees, and Related Methods\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5(x−t)+ (t−x)+\\nxtBasis Function\\nFIGURE 9.9. The basis functions (x−t)+(solid orange) and (t−x)+(broken\\nblue) used by MARS.\\nAs an example, the functions ( x−0.5)+and (0.5−x)+are shown in Fig-\\nure 9.9.\\nEach function is piecewise linear, with a knotat the value t. In the\\nterminology of Chapter 5, these are linear splines. We call t he two functions\\nareﬂected pair in the discussion below. The idea is to form reﬂected pairs\\nfor each input Xjwith knots at each observed value xijof that input.\\nTherefore, the collection of basis functions is\\nC={(Xj−t)+,(t−Xj)+}t∈ {x1j,x2j,...,x Nj}\\nj= 1,2,...,p.(9.18)\\nIf all of the input values are distinct, there are 2 Npbasis functions alto-\\ngether. Note that although each basis function depends only on a single\\nXj, for example, h(X) = (Xj−t)+, it is considered as a function over the\\nentire input space IRp.\\nThe model-building strategy is like a forward stepwise line ar regression,\\nbut instead of using the original inputs, we are allowed to us e functions\\nfrom the setCand their products. Thus the model has the form\\nf(X) =β0+M∑\\nm=1βmhm(X), (9.19)\\nwhere each hm(X) is a function in C, or a product of two or more such\\nfunctions.\\nGiven a choice for the hm, the coeﬃcients βmare estimated by minimiz-\\ning the residual sum-of-squares, that is, by standard linea r regression. The\\nreal art, however, is in the construction of the functions hm(x). We start\\nwith only the constant function h0(X) = 1 in our model, and all functions\\nin the setCare candidate functions. This is depicted in Figure 9.10.\\nAt each stage we consider as a new basis function pair all prod ucts of a\\nfunctionhmin the model set Mwith one of the reﬂected pairs in C. We\\nadd to the model Mthe term of the form\\nˆβM+1hℓ(X)·(Xj−t)++ˆβM+2hℓ(X)·(t−Xj)+, hℓ∈M,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 340}),\n",
       " Document(page_content='9.4 MARS: Multivariate Adaptive Regression Splines 323\\nX1\\nX1X1\\nX1\\nX2X2\\nX2X2\\nX2\\nXpXpXpConstant\\nFIGURE 9.10. Schematic of the MARS forward model-building procedure. On\\nthe left are the basis functions currently in the model: initial ly, this is the constant\\nfunctionh(X) = 1. On the right are all candidate basis functions to be conside red\\nin building the model. These are pairs of piecewise linear basis functions as in\\nFigure 9.9, with knots tat all unique observed values xijof each predictor Xj.\\nAt each stage we consider all products of a candidate pair wit h a basis function\\nin the model. The product that decreases the residual error th e most is added into\\nthe current model. Above we illustrate the ﬁrst three steps of t he procedure, with\\nthe selected functions shown in red.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 341}),\n",
       " Document(page_content='324 9. Additive Models, Trees, and Related Methods\\nX1X2h(X1,X2)\\nFIGURE 9.11. The function h(X1,X2) = (X1−x51)+·(x72−X2)+, resulting\\nfrom multiplication of two piecewise linear MARS basis functio ns.\\nthat produces the largest decrease in training error. Here ˆβM+1andˆβM+2\\nare coeﬃcients estimated by least squares, along with all th e otherM+1\\ncoeﬃcients in the model. Then the winning products are added to the\\nmodel and the process is continued until the model set Mcontains some\\npreset maximum number of terms.\\nForexample,attheﬁrststageweconsideraddingtothemodel afunction\\nof the form β1(Xj−t)++β2(t−Xj)+;t∈{xij}, since multiplication by\\nthe constant function just produces the function itself. Su ppose the best\\nchoice is ˆβ1(X2−x72)++ˆβ2(x72−X2)+. Then this pair of basis functions\\nis added to the set M, and at the next stage we consider including a pair\\nof products the form\\nhm(X)·(Xj−t)+andhm(X)·(t−Xj)+, t∈{xij},\\nwhere forhmwe have the choices\\nh0(X) = 1,\\nh1(X) = (X2−x72)+,or\\nh2(X) = (x72−X2)+.\\nThe third choice produces functions such as ( X1−x51)+·(x72−X2)+,\\ndepicted in Figure 9.11.\\nAt the end of this process we have a large model of the form (9.1 9). This\\nmodel typically overﬁts the data, and so a backward deletion procedure\\nis applied. The term whose removal causes the smallest incre ase in resid-\\nual squared error is deleted from the model at each stage, pro ducing an\\nestimated best model ˆfλof each size (number of terms) λ. One could use\\ncross-validation to estimate the optimal value of λ, but for computational', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 342}),\n",
       " Document(page_content='9.4 MARS: Multivariate Adaptive Regression Splines 325\\nsavingstheMARSprocedureinsteadusesgeneralizedcross- validation.This\\ncriterion is deﬁned as\\nGCV(λ) =∑N\\ni=1(yi−ˆfλ(xi))2\\n(1−M(λ)/N)2. (9.20)\\nThe valueM(λ) is the eﬀective number of parameters in the model: this\\naccounts both for the number of terms in the models, plus the n umber\\nof parameters used in selecting the optimal positions of the knots. Some\\nmathematical and simulation results suggest that one shoul d pay a price\\nof three parameters for selecting a knot in a piecewise linea r regression.\\nThus if there are rlinearly independent basis functions in the model, and\\nKknotswereselectedintheforwardprocess,theformulais M(λ) =r+cK,\\nwherec= 3. (When the model is restricted to be additive—details belo w—\\na penalty of c= 2 is used). Using this, we choose the model along the\\nbackward sequence that minimizes GCV( λ).\\nWhythesepiecewiselinearbasisfunctions,andwhythispar ticularmodel\\nstrategy? A key property of the functions of Figure 9.9 is the ir ability to\\noperate locally; they are zero over part of their range. When they are mul-\\ntiplied together, as in Figure 9.11, the result is nonzero on ly over the small\\npart of the feature space where both component functions are nonzero. As\\na result, the regression surface is built up parsimoniously , using nonzero\\ncomponents locally—only where they are needed. This is impor tant, since\\none should “spend” parameters carefully in high dimensions , as they can\\nrunoutquickly.Theuseofotherbasisfunctionssuchaspoly nomials,would\\nproduce a nonzero product everywhere, and would not work as w ell.\\nThe second important advantage of the piecewise linear basi s function\\nconcerns computation. Consider the product of a function in Mwith each\\nof theNreﬂected pairs for an input Xj. This appears to require the ﬁtting\\nofNsingle-input linear regression models, each of which uses O(N) oper-\\nations, making a total of O(N2) operations. However, we can exploit the\\nsimple form of the piecewise linear function. We ﬁrst ﬁt the r eﬂected pair\\nwith rightmost knot. As the knot is moved successively one po sition at a\\ntime to the left, the basis functions diﬀer by zero over the le ft part of the\\ndomain, and by a constant over the right part. Hence after eac h such move\\nwe can update the ﬁt in O(1) operations. This allows us to try every knot\\nin onlyO(N) operations.\\nThe forward modeling strategy in MARS is hierarchical, in th e sense that\\nmultiway products are built up from products involving term s already in\\nthemodel.Forexample,afour-wayproductcanonlybeaddedt othemodel\\nif one of its three-way components is already in the model. Th e philosophy\\nhereisthatahigh-orderinteractionwilllikelyonlyexist ifsomeofitslower-\\norder “footprints” exist as well. This need not be true, but i s a reasonable\\nworking assumption and avoids the search over an exponentia lly growing\\nspace of alternatives.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 343}),\n",
       " Document(page_content='326 9. Additive Models, Trees, and Related Methods\\nRank of ModelTest Misclassification Error\\n0 20 40 60 80 1000.1 0.2 0.3 0.4\\n• • • • • • • • • • •••••• •• •• • • •••• • ••• • • •• • ••••• ••••••• ••••••••••••••• ••• ••••• ••••••••••••••••••••••••••••••\\n0.055GCV choice\\nFIGURE 9.12. Spam data: test error misclassiﬁcation rate for the MARS pro-\\ncedure, as a function of the rank (number of independent basi s functions) in the\\nmodel.\\nThere is one restriction put on the formation of model terms: each input\\ncan appear at most once in a product. This prevents the format ion of\\nhigher-order powers of an input, which increase or decrease too sharply\\nnear the boundaries of the feature space. Such powers can be a pproximated\\nin a more stable way with piecewise linear functions.\\nA useful option in the MARS procedure is to set an upper limit o n\\nthe order of interaction. For example, one can set a limit of t wo, allowing\\npairwise products of piecewise linear functions, but not th ree- or higher-\\nway products. This can aid in the interpretation of the ﬁnal m odel. An\\nupper limit of one results in an additive model.\\n9.4.1 Spam Example (Continued)\\nWe applied MARS to the “spam” data analyzed earlier in this ch apter. To\\nenhanceinterpretability,werestrictedMARStosecond-de greeinteractions.\\nAlthough the target is a two-class variable, we used the squa red-error loss\\nfunction nonetheless (see Section 9.4.3). Figure 9.12 show s the test error\\nmisclassiﬁcation rate as a function of the rank (number of in dependent ba-\\nsis functions) in the model. The error rate levels oﬀ at about 5.5%, which is\\nslightly higher than that of the generalized additive model (5.3%) discussed\\nearlier. GCV chose a model size of 60, which is roughly the sma llest model\\ngiving optimal performance. The leading interactions foun d by MARS in-\\nvolved inputs ( ch$, remove ), (ch$, free ) and (hp, CAPTOT ). However, these\\ninteractions give no improvement in performance over the ge neralized ad-\\nditive model.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 344}),\n",
       " Document(page_content='9.4 MARS: Multivariate Adaptive Regression Splines 327\\n9.4.2 Example (Simulated Data)\\nHere we examine the performance of MARS in three contrasting scenarios.\\nThere areN= 100 observations, and the predictors X1,X2,...,X pand\\nerrorsεhave independent standard normal distributions.\\nScenario 1: The data generation model is\\nY= (X1−1)++(X1−1)+·(X2−.8)++0.12·ε.(9.21)\\nThe noise standard deviation 0.12 was chosen so that the sign al-to-\\nnoise ratio was about 5. We call this the tensor-product scen ario; the\\nproduct term gives a surface that looks like that of Figure 9. 11.\\nScenario2: Thisisthesameasscenario1,butwith p= 20totalpredictors;\\nthat is, there are 18 inputs that are independent of the respo nse.\\nScenario 3: This has the structure of a neural network:\\nℓ1=X1+X2+X3+X4+X5,\\nℓ2=X6−X7+X8−X9+X10,\\nσ(t) = 1/(1+e−t),\\nY=σ(ℓ1)+σ(ℓ2)+0.12·ε.(9.22)\\nScenarios 1 and 2 are ideally suited for MARS, while scenario 3 contains\\nhigh-order interactions and may be diﬃcult for MARS to appro ximate. We\\nran ﬁve simulations from each model, and recorded the result s.\\nIn scenario 1, MARS typically uncovered the correct model al most per-\\nfectly. In scenario 2, it found the correct structure but als o found a few\\nextraneous terms involving other predictors.\\nLetµ(x) be the true mean of Y, and let\\nMSE0= ave x∈Test(¯y−µ(x))2,\\nMSE = ave x∈Test(ˆf(x)−µ(x))2.(9.23)\\nThese represent the mean-square error of the constant model and the ﬁtted\\nMARSmodel,estimatedbyaveragingatthe1000testvaluesof x.Table9.4\\nshows the proportional decrease in model error or R2for each scenario:\\nR2=MSE0−MSE\\nMSE0. (9.24)\\nThe values shown are means and standard error over the ﬁve sim ulations.\\nThe performance of MARS is degraded only slightly by the incl usion of the\\nuseless inputs in scenario 2; it performs substantially wor se in scenario 3.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 345}),\n",
       " Document(page_content='328 9. Additive Models, Trees, and Related Methods\\nTABLE 9.4. Proportional decrease in model error ( R2) when MARS is applied\\nto three diﬀerent scenarios.\\nScenario Mean (S.E.)\\n1: Tensor product p= 2 0.97 (0.01)\\n2: Tensor product p= 20 0.96 (0.01)\\n3: Neural network 0.79 (0.01)\\n9.4.3 Other Issues\\nMARS for Classiﬁcation\\nThe MARS method and algorithm can be extended to handle class iﬁcation\\nproblems. Several strategies have been suggested.\\nFor two classes, one can code the output as 0/1 and treat the pr oblem as\\na regression; we did this for the spamexample. For more than two classes,\\none can use the indicator response approach described in Sec tion 4.2. One\\ncodes theKresponse classes via 0/1 indicator variables, and then per-\\nforms a multi-response MARS regression. For the latter we us e a common\\nset of basis functions for all response variables. Classiﬁc ation is made to\\nthe class with the largest predicted response value. There a re, however, po-\\ntential masking problems with this approach, as described i n Section 4.2.\\nA generally superior approach is the “optimal scoring” meth od discussed\\nin Section 12.5.\\nStoneetal.(1997)developedahybridofMARScalledPolyMAR Sspecif-\\nicallydesignedtohandleclassiﬁcationproblems.Itusest hemultiplelogistic\\nframework described in Section 4.4. It grows the model in a fo rward stage-\\nwise fashion like MARS, but at each stage uses a quadratic app roximation\\nto the multinomial log-likelihood to search for the next bas is-function pair.\\nOnce found, the enlarged model is ﬁt by maximum likelihood, a nd the\\nprocess is repeated.\\nRelationship of MARS to CART\\nAlthough they might seem quite diﬀerent, the MARS and CART st rategies\\nactuallyhavestrongsimilarities.SupposewetaketheMARS procedureand\\nmake the following changes:\\n•Replacethepiecewiselinearbasisfunctionsbystepfuncti onsI(x−t>\\n0) andI(x−t≤0).\\n•When a model term is involved in a multiplication by a candida te\\nterm, it gets replaced by the interaction, and hence is not av ailable\\nfor further interactions.\\nWiththesechanges,theMARSforwardprocedureisthesameas theCART\\ntree-growing algorithm. Multiplying a step function by a pa ir of reﬂected', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 346}),\n",
       " Document(page_content='9.5 Hierarchical Mixtures of Experts 329\\nstep functions is equivalent to splitting a node at the step. The second\\nrestriction implies that a node may not be split more than onc e, and leads\\nto the attractive binary-tree representation of the CART mo del. On the\\nother hand, it is this restriction that makes it diﬃcult for C ART to model\\nadditive structures. MARS forgoes the tree structure and ga ins the ability\\nto capture additive eﬀects.\\nMixed Inputs\\nMars can handle “mixed” predictors—quantitative and qualit ative—in a\\nnatural way, much like CART does. MARS considers all possibl e binary\\npartitions of the categories for a qualitative predictor in to two groups.\\nEach such partition generates a pair of piecewise constant b asis functions—\\nindicator functions for the two sets of categories. This bas is pair is now\\ntreated as any other, and is used in forming tensor products w ith other\\nbasis functions already in the model.\\n9.5 Hierarchical Mixtures of Experts\\nThe hierarchical mixtures of experts (HME) procedure can be viewed as a\\nvariant of tree-based methods. The main diﬀerence is that th e tree splits\\nare not hard decisions but rather soft probabilistic ones. A t each node an\\nobservation goes left or right with probabilities dependin g on its input val-\\nues. This has some computational advantages since the resul ting parameter\\noptimization problem is smooth, unlike the discrete split p oint search in the\\ntree-based approach. The soft splits might also help in pred iction accuracy\\nand provide a useful alternative description of the data.\\nThere are other diﬀerences between HMEs and the CART impleme nta-\\ntion of trees. In an HME, a linear (or logistic regression) mo del is ﬁt in\\neach terminal node, instead of a constant as in CART. The spli ts can be\\nmultiway, not just binary, and the splits are probabilistic functions of a\\nlinear combination of inputs, rather than a single input as i n the standard\\nuse of CART. However, the relative merits of these choices ar e not clear,\\nand most were discussed at the end of Section 9.2.\\nAsimpletwo-levelHMEmodelinshowninFigure9.13.Itcanbe thought\\nof as a tree with soft splits at each non-terminal node. Howev er, the inven-\\ntors of this methodology use a diﬀerent terminology. The ter minal nodes\\nare called experts, and the non-terminal nodes are called gating networks .\\nThe idea is that each expert provides an opinion (prediction ) about the\\nresponse, and these are combined together by the gating netw orks. As we\\nwill see, the model is formally a mixture model, and the two-l evel model\\nin the ﬁgure can be extend to multiple levels, hence the name hierarchical\\nmixtures of experts .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 347}),\n",
       " Document(page_content='330 9. Additive Models, Trees, and Related Methods\\ng1 g2\\ng1|1 g2|1g1|2 g2|2Gating GatingGatingGating\\nGating GatingGating GatingGating\\nNetwork Network NetworkNetwork\\nNetwork\\nNetworkNetwork\\nNetwork Network NetworkNetwork\\nNetworkNetwork NetworkNetwork Network\\nNetwork Network NetworkNetwork\\nExpert Expert Expert Expert Expert Expert Expert Expert Expert Expert Expert\\nPr(y|x,θ11) Pr( y|x,θ21) Pr( y|x,θ12) Pr(y|x,θ22)\\nFIGURE 9.13. A two-level hierarchical mixture of experts (HME) model.\\nConsider the regression or classiﬁcation problem, as descr ibed earlier in\\nthe chapter. The data is ( xi,yi),i= 1,2,...,N, withyieither a continuous\\nor binary-valued response, and xia vector-valued input. For ease of nota-\\ntion we assume that the ﬁrst element of xiis one, to account for intercepts.\\nHere is how an HME is deﬁned. The top gating network has the out put\\ngj(x,γj) =eγT\\njx\\n∑K\\nk=1eγT\\nkx, j= 1,2,...,K, (9.25)\\nwhere each γjis a vector of unknown parameters. This represents a soft\\nK-way split ( K= 2 in Figure 9.13.) Each gj(x,γj) is the probability of\\nassigning an observation with feature vector xto thejth branch. Notice\\nthat withK= 2 groups, if we take the coeﬃcient of one of the elements of\\nxto be +∞, then we get a logistic curve with inﬁnite slope. In this case ,\\nthe gating probabilities are either 0 or 1, corresponding to a hard split on\\nthat input.\\nAt the second level, the gating networks have a similar form:\\ngℓ|j(x,γjℓ) =eγT\\njℓx\\n∑K\\nk=1eγT\\njkx, ℓ= 1,2,...,K. (9.26)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 348}),\n",
       " Document(page_content='9.5 Hierarchical Mixtures of Experts 331\\nThis is the probability of assignment to the ℓth branch, given assignment\\nto thejth branch at the level above.\\nAteachexpert(terminalnode),wehaveamodelfortherespon sevariable\\nof the form\\nY∼Pr(y|x,θjℓ). (9.27)\\nThis diﬀers according to the problem.\\nRegression: The Gaussian linear regression model is used, with θjℓ=\\n(βjℓ,σ2\\njℓ):\\nY=βT\\njℓx+εandε∼N(0,σ2\\njℓ). (9.28)\\nClassiﬁcation: The linear logistic regression model is used:\\nPr(Y= 1|x,θjℓ) =1\\n1+e−θT\\njℓx. (9.29)\\nDenoting the collection of all parameters by Ψ = {γj,γjℓ,θjℓ}, the total\\nprobability that Y=yis\\nPr(y|x,Ψ) =K∑\\nj=1gj(x,γj)K∑\\nℓ=1gℓ|j(x,γjℓ)Pr(y|x,θjℓ).(9.30)\\nThis is a mixture model, with the mixture probabilities dete rmined by the\\ngating network models.\\nTo estimate the parameters, we maximize the log-likelihood of the data,∑\\nilogPr(yi|xi,Ψ), over the parameters in Ψ. The most convenient method\\nfor doing this is the EM algorithm, which we describe for mixt ures in\\nSection 8.5. We deﬁne latent variables ∆ j, all of which are zero except for\\na single one. We interpret these as the branching decisions m ade by the top\\nlevel gating network. Similarly we deﬁne latent variables ∆ ℓ|jto describe\\nthe gating decisions at the second level.\\nIn the E-step, the EM algorithm computes the expectations of the ∆j\\nand ∆ ℓ|jgiven the current values of the parameters. These expectati ons\\nare then used as observation weights in the M-step of the proc edure, to\\nestimate the parameters in the expert networks. The paramet ers in the\\ninternal nodes are estimated by a version of multiple logist ic regression.\\nThe expectations of the ∆ jor ∆ℓ|jare probability proﬁles, and these are\\nused as the response vectors for these logistic regressions .\\nThe hierarchical mixtures of experts approach is a promisin g competitor\\nto CART trees. By using soft splits rather than hard decision rules it can\\ncapturesituationswherethetransitionfromlowtohighres ponseisgradual.\\nThe log-likelihood is a smooth function of the unknown weigh ts and hence\\nis amenable to numerical optimization. The model is similar to CART with\\nlinear combination splits, but the latter is more diﬃcult to optimize. On', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 349}),\n",
       " Document(page_content='332 9. Additive Models, Trees, and Related Methods\\nthe other hand, to our knowledge there are no methods for ﬁndi ng a good\\ntree topology for the HME model, as there are in CART. Typical ly one uses\\na ﬁxed tree of some depth, possibly the output of the CART proc edure.\\nThe emphasis in the research on HMEs has been on prediction ra ther than\\ninterpretation of the ﬁnal model. A close cousin of the HME is thelatent\\nclass model (Lin et al., 2000), which typically has only one layer; here\\nthe nodes or latent classes are interpreted as groups of subj ects that show\\nsimilar response behavior.\\n9.6 Missing Data\\nIt is quite common to have observations with missing values f or one or more\\ninput features. The usual approach is to impute (ﬁll-in) the missing values\\nin some way.\\nHowever,theﬁrstissueindealingwiththeproblemisdeterm iningwheth-\\ner the missing data mechanism has distorted the observed dat a. Roughly\\nspeaking, data are missing at random if the mechanism result ing in its\\nomission is independent of its (unobserved) value. A more pr ecise deﬁnition\\nis given in Little and Rubin (2002). Suppose yis the response vector and X\\nis theN×pmatrix of inputs (some of which are missing). Denote by Xobs\\nthe observed entries in Xand letZ= (y,X),Zobs= (y,Xobs). Finally, if R\\nis an indicator matrix with ijth entry 1 if xijis missing and zero otherwise,\\nthen the data is said to be missing at random (MAR) if the distribution of\\nRdepends on the data Zonly through Zobs:\\nPr(R|Z,θ) = Pr(R|Zobs,θ). (9.31)\\nHereθare any parameters in the distribution of R. Data are said to be\\nmissing completely at random (MCAR) if the distribution of Rdoesn’t\\ndepend on the observed or missing data:\\nPr(R|Z,θ) = Pr(R|θ). (9.32)\\nMCAR is a stronger assumption than MAR: most imputation meth ods rely\\non MCAR for their validity.\\nForexample,ifapatient’smeasurementwasnottakenbecaus ethedoctor\\nfelt he was too sick, that observation would not be MAR or MCAR . In this\\ncasethemissingdatamechanismcausesourobservedtrainin gdatatogivea\\ndistorted picture of the true population, and data imputati on is dangerous\\nin this instance. Often the determination of whether featur es are MCAR\\nmust be made from information about the data collection proc ess. For\\ncategorical features, one way to diagnose this problem is to code “missing”\\nas an additional class. Then we ﬁt our model to the training da ta and see\\nif class “missing” is predictive of the response.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 350}),\n",
       " Document(page_content='9.6 Missing Data 333\\nAssuming the features are missing completely at random, the re are a\\nnumber of ways of proceeding:\\n1. Discard observations with any missing values.\\n2. Rely on the learning algorithm to deal with missing values in its\\ntraining phase.\\n3. Impute all missing values before training.\\nApproach (1) can be used if the relative amount of missing dat a is small,\\nbut otherwise should be avoided. Regarding (2), CART is one l earning\\nalgorithmthatdealseﬀectivelywithmissingvalues,throu ghsurrogate splits\\n(Section 9.2.4). MARS and PRIM use similar approaches. In ge neralized\\nadditive modeling, all observations missing for a given inp ut feature are\\nomitted when the partial residuals are smoothed against tha t feature in\\nthe backﬁtting algorithm, and their ﬁtted values are set to z ero. Since the\\nﬁtted curves have mean zero (when the model includes an inter cept), this\\namounts to assigning the average ﬁtted value to the missing o bservations.\\nFor most learning methods, the imputation approach (3) is ne cessary.\\nThe simplest tactic is to impute the missing value with the me an or median\\nof the nonmissing values for that feature. (Note that the abo ve procedure\\nfor generalized additive models is analogous to this.)\\nIf the features have at least some moderate degree of depende nce, one\\ncan do better by estimating a predictive model for each featu re given the\\nother features and then imputing each missing value by its pr ediction from\\nthe model. In choosing the learning method for imputation of the features,\\none must remember that this choice is distinct from the metho d used for\\npredicting yfromX. Thus a ﬂexible, adaptive method will often be pre-\\nferred, even for the eventual purpose of carrying out a linea r regression of y\\nonX. In addition, if there are many missing feature values in the training\\nset, the learning method must itself be able to deal with miss ing feature\\nvalues. CART therefore is an ideal choice for this imputatio n “engine.”\\nAfter imputation, missing values are typically treated as i f they were ac-\\ntually observed. This ignores the uncertainty due to the imp utation, which\\nwill itself introduce additional uncertainty into estimat es and predictions\\nfrom the response model. One can measure this additional unc ertainty by\\ndoing multiple imputations and hence creating many diﬀeren t training sets.\\nThe predictive model for ycan be ﬁt to each training set, and the variation\\nacross training sets can be assessed. If CART was used for the imputation\\nengine,themultipleimputations couldbedonebysamplingf romthevalues\\nin the corresponding terminal nodes.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 351}),\n",
       " Document(page_content='334 9. Additive Models, Trees, and Related Methods\\n9.7 Computational Considerations\\nWithNobservations and ppredictors, additive model ﬁtting requires some\\nnumbermpof applications of a one-dimensional smoother or regressio n\\nmethod. The required number of cycles mof the backﬁtting algorithm is\\nusually less than 20 and often less than 10, and depends on the amount\\nof correlation in the inputs. With cubic smoothing splines, for example,\\nNlogNoperations are needed for an initial sort and Noperations for the\\nspline ﬁt. Hence the total operations for an additive model ﬁ t ispNlogN+\\nmpN.\\nTrees require pNlogNoperations for an initial sort for each predictor,\\nand typically another pNlogNoperations for the split computations. If the\\nsplits occurred near the edges of the predictor ranges, this number could\\nincrease to N2p.\\nMARS requires Nm2+pmNoperations to add a basis function to a\\nmodel with mterms already present, from a pool of ppredictors. Hence to\\nbuild anM-term model requires NM3+pM2Ncomputations, which can\\nbe quite prohibitive if Mis a reasonable fraction of N.\\nEach of the components of an HME are typically inexpensive to ﬁt at\\neach M-step: Np2for the regressions, and Np2K2for aK-class logistic\\nregression. The EM algorithm, however, can take a long time t o converge,\\nand so sizable HME models are considered costly to ﬁt.\\nBibliographic Notes\\nThe most comprehensive source for generalized additive mod els is the text\\nof that name by Hastie and Tibshirani (1990). Diﬀerent appli cations of\\nthis work in medical problems are discussed in Hastie et al. ( 1989) and\\nHastie and Herman (1990), and the software implementation i n Splus is\\ndescribed in Chambers and Hastie (1991). Green and Silverma n (1994)\\ndiscuss penalization and spline models in a variety of setti ngs. Efron and\\nTibshirani (1991) give an exposition of modern development s in statistics\\n(including generalized additive models), for a nonmathema tical audience.\\nClassiﬁcation and regression trees date back at least as far as Morgan and\\nSonquist (1963). We have followed the modern approaches of B reiman et\\nal. (1984) and Quinlan (1993). The PRIM method is due to Fried man\\nand Fisher (1999), while MARS is introduced in Friedman (199 1), with an\\nadditiveprecursorinFriedmanandSilverman(1989).Hiera rchicalmixtures\\nof experts were proposed in Jordan and Jacobs (1994); see als o Jacobs et\\nal. (1991).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 352}),\n",
       " Document(page_content='Exercises 335\\nExercises\\nEx. 9.1Show that a smoothing spline ﬁt of yitoxipreserves the linear\\npartof the ﬁt. In other words, if yi= ˆyi+ri, where ˆyirepresents the\\nlinear regression ﬁts, and Sis the smoothing matrix, then Sy=ˆy+Sr.\\nShow that the same is true for local linear regression (Secti on 6.1.1). Hence\\nargue that the adjustment step in the second line of (2) in Alg orithm 9.1\\nis unnecessary.\\nEx. 9.2LetAbe a known k×kmatrix,bbe a known k-vector, and z\\nbe an unknown k-vector. A Gauss–Seidel algorithm for solving the linear\\nsystem of equations Az=bworks by successively solving for element zjin\\nthejth equation, ﬁxing all other zj’s at their current guesses. This process\\nis repeated for j= 1,2,...,k,1,2,...,k,..., until convergence (Golub and\\nVan Loan, 1983).\\n(a) Consider an additive model with Nobservations and pterms, with\\nthejth term to be ﬁt by a linear smoother Sj. Consider the following\\nsystem of equations:\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8edI S1S1···S1\\nS2I S2···S2\\n...............\\nSpSpSp···I\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8edf1\\nf2\\n...\\nfp\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8=\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8edS1y\\nS2y\\n...\\nSpy\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8. (9.33)\\nHere each fjis anN-vector of evaluations of the jth function at\\nthe data points, and yis anN-vector of the response values. Show\\nthat backﬁtting is a blockwise Gauss–Seidel algorithm for s olving this\\nsystem of equations.\\n(b) LetS1andS2be symmetric smoothing operators (matrices) with\\neigenvalues in [0 ,1). Consider a backﬁtting algorithm with response\\nvectoryand smoothers S1,S2. Show that with any starting values,\\nthe algorithm converges and give a formula for the ﬁnal itera tes.\\nEx.9.3Backﬁtting equations. Considerabackﬁttingprocedurewithorthog-\\nonal projections, and let Dbe the overall regression matrix whose columns\\nspanV=Lcol(S1)⊕Lcol(S2)⊕···⊕L col(Sp), whereLcol(S) denotes the\\ncolumn space of a matrix S. Show that the estimating equations\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8edI S1S1···S1\\nS2I S2···S2\\n...............\\nSpSpSp···I\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8edf1\\nf2\\n...\\nfp\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8=\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8edS1y\\nS2y\\n...\\nSpy\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\nare equivalent to the least squares normal equations DTDβ=DTywhere\\nβis the vector of coeﬃcients.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 353}),\n",
       " Document(page_content='336 9. Additive Models, Trees, and Related Methods\\nEx. 9.4Suppose the same smoother Sis used to estimate both terms in a\\ntwo-term additive model (i.e., both variables are identica l). Assume that S\\nis symmetric with eigenvalues in [0 ,1). Show that the backﬁtting residual\\nconverges to ( I+S)−1(I−S)y, and that the residual sum of squares con-\\nverges upward. Can the residual sum of squares converge upwa rd in less\\nstructured situations? How does this ﬁt compare to the ﬁt wit h a single\\nterm ﬁt by S? [Hint: Use the eigen-decomposition of Sto help with this\\ncomparison.]\\nEx. 9.5Degrees of freedom of a tree . Given data yiwith mean f(xi) and\\nvarianceσ2, and a ﬁtting operation y→ˆy, let’s deﬁne the degrees of\\nfreedom of a ﬁt by∑\\nicov(yi,ˆyi)/σ2.\\nConsider a ﬁt ˆyestimated by a regression tree, ﬁt to a set of predictors\\nX1,X2,...,X p.\\n(a) In terms of the number of terminal nodes m, give a rough formula for\\nthe degrees of freedom of the ﬁt.\\n(b) Generate 100 observations with predictors X1,X2,...,X 10as inde-\\npendent standard Gaussian variates and ﬁx these values.\\n(c) Generate response values also as standard Gaussian ( σ2= 1), indepen-\\ndent of the predictors. Fit regression trees to the data of ﬁx ed size 1,5\\nand 10 terminal nodes and hence estimate the degrees of freed om of\\neach ﬁt. [Do ten simulations of the response and average the r esults,\\nto get a good estimate of degrees of freedom.]\\n(d) Compare your estimates of degrees of freedom in (a) and (c ) and\\ndiscuss.\\n(e) If the regression tree ﬁt were a linear operation, we coul d writeˆy=Sy\\nfor some matrix S. Then the degrees of freedom would be tr( S).\\nSuggest a way to compute an approximate Smatrix for a regression\\ntree, compute it and compare the resulting degrees of freedo m to\\nthose in (a) and (c).\\nEx. 9.6Consider the ozone data of Figure 6.9.\\n(a) Fit an additive model to the cube root of ozone concentrat ion. as a\\nfunction of temperature, wind speed, and radiation. Compar e your\\nresults to those obtained via the trellis display in Figure 6 .9.\\n(b)Fittrees,MARS,andPRIMtothesamedata,andcomparethe results\\nto those found in (a) and in Figure 6.9.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 354}),\n",
       " Document(page_content='This is page 337\\nPrinter: Opaque this\\n10\\nBoosting and Additive Trees\\n10.1 Boosting Methods\\nBoosting is one of the most powerful learning ideas introduc ed in the last\\ntwenty years. It was originally designed for classiﬁcation problems, but as\\nwill be seen in this chapter, it can proﬁtably be extended to r egression\\nas well. The motivation for boosting was a procedure that com bines the\\noutputs of many “weak” classiﬁers to produce a powerful “com mittee.”\\nFrom this perspective boosting bears a resemblance to baggi ng and other\\ncommittee-based approaches (Section 8.8). However we shal l see that the\\nconnection is at best superﬁcial and that boosting is fundam entally diﬀer-\\nent.\\nWe begin by describing the most popular boosting algorithm d ue to\\nFreund and Schapire (1997) called “AdaBoost.M1.” Consider a two-class\\nproblem, with the output variable coded as Y∈{−1,1}. Given a vector of\\npredictor variables X, a classiﬁer G(X) produces a prediction taking one\\nof the two values {−1,1}. The error rate on the training sample is\\nerr =1\\nNN∑\\ni=1I(yi̸=G(xi)),\\nand the expected error rate on future predictions is E XYI(Y̸=G(X)).\\nA weak classiﬁer is one whose error rate is only slightly bett er than\\nrandom guessing. The purpose of boosting is to sequentially apply the\\nweak classiﬁcation algorithm to repeatedly modiﬁed versio ns of the data,\\nthereby producing a sequence of weak classiﬁers Gm(x),m= 1,2,...,M.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 355}),\n",
       " Document(page_content='338 10. Boosting and Additive Trees\\nTraining  SampleWeighted  SampleWeighted  SampleWeighted  Sample\\nTraining  SampleWeighted  SampleWeighted  SampleWeighted  SampleWeighted  Sample\\nTraining  SampleWeighted  Sample\\nTraining  SampleWeighted  SampleWeighted  SampleWeighted  Sample\\nWeighted  SampleWeighted  SampleWeighted  Sample\\nTraining  SampleWeighted  SampleG(x) = sign[∑M\\nm=1αmGm(x)]\\nGM(x)\\nG3(x)\\nG2(x)\\nG1(x)Final Classifier\\nFIGURE 10.1. Schematic of AdaBoost. Classiﬁers are trained on weighted ve r-\\nsions of the dataset, and then combined to produce a ﬁnal pred iction.\\nThe predictions from all of them are then combined through a w eighted\\nmajority vote to produce the ﬁnal prediction:\\nG(x) = sign(M∑\\nm=1αmGm(x))\\n. (10.1)\\nHereα1,α2,...,α Mare computed by the boosting algorithm, and weight\\nthe contribution of each respective Gm(x). Their eﬀect is to give higher\\ninﬂuence to the more accurate classiﬁers in the sequence. Fi gure 10.1 shows\\na schematic of the AdaBoost procedure.\\nThe data modiﬁcations at each boosting step consist of apply ing weights\\nw1,w2,...,w Nto each of the training observations ( xi,yi), i= 1,2,...,N.\\nInitially all of the weights are set to wi= 1/N, so that the ﬁrst step simply\\ntrains the classiﬁer on the data in the usual manner. For each successive\\niterationm= 2,3,...,Mthe observation weights are individually modi-\\nﬁed and the classiﬁcation algorithm is reapplied to the weig hted observa-\\ntions. At step m, those observations that were misclassiﬁed by the classiﬁe r\\nGm−1(x)inducedatthepreviousstephavetheirweightsincreased, whereas\\nthe weights are decreased for those that were classiﬁed corr ectly. Thus as\\niterations proceed, observations that are diﬃcult to class ify correctly re-\\nceive ever-increasing inﬂuence. Each successive classiﬁe r is thereby forced', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 356}),\n",
       " Document(page_content='10.1 Boosting Methods 339\\nAlgorithm 10.1 AdaBoost.M1.\\n1. Initialize the observation weights wi= 1/N, i= 1,2,...,N.\\n2. Form= 1 toM:\\n(a) Fit a classiﬁer Gm(x) to the training data using weights wi.\\n(b) Compute\\nerrm=∑N\\ni=1wiI(yi̸=Gm(xi))\\n∑N\\ni=1wi.\\n(c) Compute αm= log((1−errm)/errm).\\n(d) Setwi←wi·exp[αm·I(yi̸=Gm(xi))], i= 1,2,...,N.\\n3. OutputG(x) = sign[∑M\\nm=1αmGm(x)]\\n.\\nto concentrate on those training observations that are miss ed by previous\\nones in the sequence.\\nAlgorithm 10.1 shows the details of the AdaBoost.M1 algorit hm. The\\ncurrent classiﬁer Gm(x) is induced on the weighted observations at line 2a.\\nThe resulting weighted error rate is computed at line 2b. Lin e 2c calculates\\nthe weight αmgiven toGm(x) in producing the ﬁnal classiﬁer G(x) (line\\n3). The individual weights of each of the observations are up dated for the\\nnext iteration at line 2d. Observations misclassiﬁed by Gm(x) have their\\nweights scaled by a factor exp( αm), increasing their relative inﬂuence for\\ninducing the next classiﬁer Gm+1(x) in the sequence.\\nThe AdaBoost.M1 algorithm is known as “Discrete AdaBoost” i n Fried-\\nman et al. (2000), because the base classiﬁer Gm(x) returns a discrete class\\nlabel. If the base classiﬁer instead returns a real-valued p rediction (e.g.,\\na probability mapped to the interval [ −1,1]), AdaBoost can be modiﬁed\\nappropriately (see “Real AdaBoost” in Friedman et al. (2000 )).\\nThe power of AdaBoost to dramatically increase the performa nce of even\\na very weak classiﬁer is illustrated in Figure 10.2. The feat uresX1,...,X 10\\nare standard independent Gaussian, and the deterministic t argetYis de-\\nﬁned by\\nY={\\n1 if∑10\\nj=1X2\\nj>χ2\\n10(0.5),\\n−1 otherwise .(10.2)\\nHereχ2\\n10(0.5) = 9.34 is the median of a chi-squared random variable with\\n10 degrees of freedom (sum of squares of 10 standard Gaussian s). There are\\n2000trainingcases,withapproximately1000casesineachc lass,and10,000\\ntestobservations. Heretheweak classiﬁeris justa“stump” : atwoterminal-\\nnode classiﬁcation tree. Applying this classiﬁer alone to t he training data\\nset yields a very poor test set error rate of 45.8%, compared t o 50% for', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 357}),\n",
       " Document(page_content='340 10. Boosting and Additive Trees\\n0 100 200 300 4000.0 0.1 0.2 0.3 0.4 0.5\\nBoosting IterationsTest ErrorSingle Stump\\n244 Node Tree\\nFIGURE 10.2. Simulated data (10.2): test error rate for boosting with stum ps,\\nas a function of the number of iterations. Also shown are the te st error rate for\\na single stump, and a 244-node classiﬁcation tree.\\nrandom guessing. However, as boosting iterations proceed t he error rate\\nsteadily decreases, reaching 5.8% after 400 iterations. Th us, boosting this\\nsimple very weak classiﬁer reduces its prediction error rat e by almost a\\nfactor of four. It also outperforms a single large classiﬁca tion tree (error\\nrate 24.7%). Since its introduction, much has been written to explai n the\\nsuccess of AdaBoost in producing accurate classiﬁers. Most of this work\\nhas centered on using classiﬁcation trees as the “base learn er”G(x), where\\nimprovements are often most dramatic. In fact, Breiman (NIP S Workshop,\\n1996) referred to AdaBoost with trees as the “best oﬀ-the-sh elf classiﬁer in\\nthe world” (see also Breiman (1998)). This is especially the case for data-\\nmining applications, as discussed more fully in Section 10. 7 later in this\\nchapter.\\n10.1.1 Outline of This Chapter\\nHere is an outline of the developments in this chapter:\\n•We show that AdaBoost ﬁts an additive model in a base learner,\\noptimizing a novel exponential loss function. This loss fun ction is', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 358}),\n",
       " Document(page_content='10.2 Boosting Fits an Additive Model 341\\nvery similar to the (negative) binomial log-likelihood (Se ctions 10.2–\\n10.4).\\n•The population minimizer of the exponential loss function i s shown\\nto be the log-odds of the class probabilities (Section 10.5) .\\n•We describe loss functions for regression and classiﬁcatio n that are\\nmore robust than squared error or exponential loss (Section 10.6).\\n•It is argued that decision trees are an ideal base learner for data\\nmining applications of boosting (Sections 10.7 and 10.9).\\n•We develop a class of gradient boosted models (GBMs), for boo sting\\ntrees with any loss function (Section 10.10).\\n•The importance of “slow learning” is emphasized, and implem ented\\nby shrinkage of each new term that enters the model (Section 1 0.12),\\nas well as randomization (Section 10.12.2).\\n•Toolsforinterpretationoftheﬁttedmodelaredescribed(S ection10.13).\\n10.2 Boosting Fits an Additive Model\\nThe success of boosting is really not very mysterious. The ke y lies in ex-\\npression (10.1). Boosting is a way of ﬁtting an additive expa nsion in a set\\nof elementary “basis” functions. Here the basis functions a re the individual\\nclassiﬁersGm(x)∈{−1,1}. More generally, basis function expansions take\\nthe form\\nf(x) =M∑\\nm=1βmb(x;γm), (10.3)\\nwhereβm,m= 1,2,...,Mare the expansion coeﬃcients, and b(x;γ)∈IR\\nare usually simple functions of the multivariate argument x, characterized\\nby a set of parameters γ. We discuss basis expansions in some detail in\\nChapter 5.\\nAdditive expansions like this are at the heart of many of the l earning\\ntechniques covered in this book:\\n•In single-hidden-layer neural networks (Chapter 11), b(x;γ) =σ(γ0+\\nγT\\n1x), whereσ(t) = 1/(1+e−t) is the sigmoid function, and γparam-\\neterizes a linear combination of the input variables.\\n•Insignalprocessing,wavelets(Section5.9.1)areapopula rchoicewith\\nγparameterizing the location and scale shifts of a “mother” w avelet.\\n•Multivariate adaptive regression splines (Section 9.4) us es truncated-\\npower spline basis functions where γparameterizes the variables and\\nvalues for the knots.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 359}),\n",
       " Document(page_content='342 10. Boosting and Additive Trees\\nAlgorithm 10.2 Forward Stagewise Additive Modeling.\\n1. Initialize f0(x) = 0.\\n2. Form= 1 toM:\\n(a) Compute\\n(βm,γm) = argmin\\nβ,γN∑\\ni=1L(yi,fm−1(xi)+βb(xi;γ)).\\n(b) Setfm(x) =fm−1(x)+βmb(x;γm).\\n•For trees,γparameterizes the split variables and split points at the\\ninternal nodes, and the predictions at the terminal nodes.\\nTypically these models are ﬁt by minimizing a loss function a veraged\\nover the training data, such as the squared-error or a likeli hood-based loss\\nfunction,\\nmin\\n{βm,γm}M\\n1N∑\\ni=1L(\\nyi,M∑\\nm=1βmb(xi;γm))\\n. (10.4)\\nFor many loss functions L(y,f(x)) and/or basis functions b(x;γ), this re-\\nquires computationally intensive numerical optimization techniques. How-\\never, a simple alternative often can be found when it is feasi ble to rapidly\\nsolve the subproblem of ﬁtting just a single basis function,\\nmin\\nβ,γN∑\\ni=1L(yi,βb(xi;γ)). (10.5)\\n10.3 Forward Stagewise Additive Modeling\\nForward stagewise modeling approximates the solution to (1 0.4) by sequen-\\ntially adding new basis functions to the expansion without a djusting the\\nparameters and coeﬃcients of those that have already been ad ded. This is\\noutlined in Algorithm 10.2. At each iteration m, one solves for the optimal\\nbasis function b(x;γm) and corresponding coeﬃcient βmto add to the cur-\\nrent expansion fm−1(x). This produces fm(x), and the process is repeated.\\nPreviously added terms are not modiﬁed.\\nFor squared-error loss\\nL(y,f(x)) = (y−f(x))2, (10.6)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 360}),\n",
       " Document(page_content='10.4 Exponential Loss and AdaBoost 343\\none has\\nL(yi,fm−1(xi)+βb(xi;γ)) = (yi−fm−1(xi)−βb(xi;γ))2\\n= (rim−βb(xi;γ))2, (10.7)\\nwhererim=yi−fm−1(xi) is simply the residual of the current model\\non theith observation. Thus, for squared-error loss, the term βmb(x;γm)\\nthat best ﬁts the current residuals is added to the expansion at each step.\\nThis idea is the basis for “least squares” regression boosti ng discussed in\\nSection 10.10.2. However, as we show near the end of the next s ection,\\nsquared-error loss is generally not a good choice for classi ﬁcation; hence\\nthe need to consider other loss criteria.\\n10.4 Exponential Loss and AdaBoost\\nWe now show that AdaBoost.M1 (Algorithm 10.1) is equivalent to forward\\nstagewise additive modeling (Algorithm 10.2) using the los s function\\nL(y,f(x)) = exp(−yf(x)). (10.8)\\nThe appropriateness of this criterion is addressed in the ne xt section.\\nFor AdaBoost the basis functions are the individual classiﬁ ersGm(x)∈\\n{−1,1}. Using the exponential loss function, one must solve\\n(βm,Gm) = argmin\\nβ,GN∑\\ni=1exp[−yi(fm−1(xi)+βG(xi))]\\nfor the classiﬁer Gmand corresponding coeﬃcient βmto be added at each\\nstep. This can be expressed as\\n(βm,Gm) = argmin\\nβ,GN∑\\ni=1w(m)\\niexp(−βyiG(xi)) (10.9)\\nwithw(m)\\ni= exp(−yifm−1(xi)). Since each w(m)\\nidepends neither on β\\nnorG(x), it can be regarded as a weight that is applied to each observ a-\\ntion. This weight depends on fm−1(xi), and so the individual weight values\\nchange with each iteration m.\\nThe solution to (10.9) can be obtained in two steps. First, fo r any value\\nofβ >0, the solution to (10.9) for Gm(x) is\\nGm= argmin\\nGN∑\\ni=1w(m)\\niI(yi̸=G(xi)), (10.10)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 361}),\n",
       " Document(page_content='344 10. Boosting and Additive Trees\\nwhich is the classiﬁer that minimizes the weighted error rat e in predicting\\ny. This can be easily seen by expressing the criterion in (10.9 ) as\\ne−β·∑\\nyi=G(xi)w(m)\\ni+eβ·∑\\nyi̸=G(xi)w(m)\\ni,\\nwhich in turn can be written as\\n(\\neβ−e−β)\\n·N∑\\ni=1w(m)\\niI(yi̸=G(xi))+e−β·N∑\\ni=1w(m)\\ni.(10.11)\\nPlugging this Gminto (10.9) and solving for βone obtains\\nβm=1\\n2log1−errm\\nerrm, (10.12)\\nwhere err mis the minimized weighted error rate\\nerrm=∑N\\ni=1w(m)\\niI(yi̸=Gm(xi))\\n∑N\\ni=1w(m)\\ni. (10.13)\\nThe approximation is then updated\\nfm(x) =fm−1(x)+βmGm(x),\\nwhich causes the weights for the next iteration to be\\nw(m+1)\\ni=w(m)\\ni·e−βmyiGm(xi). (10.14)\\nUsing the fact that −yiGm(xi) = 2·I(yi̸=Gm(xi))−1, (10.14) becomes\\nw(m+1)\\ni=w(m)\\ni·eαmI(yi̸=Gm(xi))·e−βm, (10.15)\\nwhereαm= 2βmis the quantity deﬁned at line 2(c) of AdaBoost.M1\\n(Algorithm 10.1). The factor e−βmin (10.15) multiplies all weights by the\\nsame value, so it has no eﬀect. Thus (10.15) is equivalent to l ine 2(d) of\\nAlgorithm 10.1.\\nOne can view line 2(a) of the Adaboost.M1 algorithm as a metho d for\\napproximatelysolvingtheminimizationin(10.11)andhenc e(10.10).Hence\\nwe conclude that AdaBoost.M1 minimizes the exponential los s criterion\\n(10.8) via a forward-stagewise additive modeling approach .\\nFigure 10.3 shows the training-set misclassiﬁcation error rate and aver-\\nage exponential loss for the simulated data problem (10.2) o f Figure 10.2.\\nThe training-set misclassiﬁcation error decreases to zero at around 250 it-\\nerations (and remains there), but the exponential loss keep s decreasing.\\nNotice also in Figure 10.2 that the test-set misclassiﬁcati on error continues\\nto improve after iteration 250. Clearly Adaboost is not opti mizing training-\\nset misclassiﬁcation error; the exponential loss is more se nsitive to changes\\nin the estimated class probabilities.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 362}),\n",
       " Document(page_content='10.5 Why Exponential Loss? 345\\n0 100 200 300 4000.0 0.2 0.4 0.6 0.8 1.0\\nBoosting IterationsTraining Error\\nMisclassification RateExponential Loss\\nFIGURE 10.3. Simulated data, boosting with stumps: misclassiﬁcation erro r\\nrate on the training set, and average exponential loss: (1/N)∑N\\ni=1exp(−yif(xi)).\\nAfter about 250iterations, the misclassiﬁcation error is zero, while the exp onential\\nloss continues to decrease.\\n10.5 Why Exponential Loss?\\nThe AdaBoost.M1 algorithm was originally motivated from a v ery diﬀer-\\nent perspective than presented in the previous section. Its equivalence to\\nforward stagewise additive modeling based on exponential l oss was only\\ndiscovered ﬁve years after its inception. By studying the pr operties of the\\nexponential loss criterion, one can gain insight into the pr ocedure and dis-\\ncover ways it might be improved.\\nThe principal attraction of exponential loss in the context of additive\\nmodeling is computational; it leads to the simple modular re weighting Ad-\\naBoost algorithm. However, it is of interest to inquire abou t its statistical\\nproperties. What does it estimate and how well is it being est imated? The\\nﬁrst question is answered by seeking its population minimiz er.\\nIt is easy to show (Friedman et al., 2000) that\\nf∗(x) = argmin\\nf(x)EY|x(e−Yf(x)) =1\\n2logPr(Y= 1|x)\\nPr(Y=−1|x),(10.16)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 363}),\n",
       " Document(page_content='346 10. Boosting and Additive Trees\\nor equivalently\\nPr(Y= 1|x) =1\\n1+e−2f∗(x).\\nThus, the additive expansion produced by AdaBoost is estima ting one-\\nhalf the log-odds of P(Y= 1|x). This justiﬁes using its sign as the classiﬁ-\\ncation rule in (10.1).\\nAnother loss criterion with the same population minimizer i s the bi-\\nnomial negative log-likelihood or deviance (also known as cross-entropy),\\ninterpreting fas the logit transform. Let\\np(x) = Pr(Y= 1|x) =ef(x)\\ne−f(x)+ef(x)=1\\n1+e−2f(x)(10.17)\\nand deﬁne Y′= (Y+1)/2∈{0,1}. Then the binomial log-likelihood loss\\nfunction is\\nl(Y,p(x)) =Y′logp(x)+(1−Y′)log(1−p(x)),\\nor equivalently the deviance is\\n−l(Y,f(x)) = log(\\n1+e−2Yf(x))\\n. (10.18)\\nSince the population maximizer of log-likelihood is at the t rue probabilities\\np(x) = Pr(Y= 1|x), we see from (10.17) that the population minimizers of\\nthe deviance E Y|x[−l(Y,f(x))] and E Y|x[e−Yf(x)]are thesame. Thus, using\\neither criterion leads to the same solution at the populatio n level. Note that\\ne−Yfitself is not a proper log-likelihood, since it is not the log arithm of\\nany probability mass function for a binary random variable Y∈{−1,1}.\\n10.6 Loss Functions and Robustness\\nIn this section we examine the diﬀerent loss functions for cl assiﬁcation and\\nregression more closely, and characterize them in terms of t heir robustness\\nto extreme data.\\nRobust Loss Functions for Classiﬁcation\\nAlthough both the exponential (10.8) and binomial deviance (10.18) yield\\nthe same solution when applied to the population joint distr ibution, the\\nsame is not true for ﬁnite data sets. Both criteria are monoto ne decreasing\\nfunctions of the “margin” yf(x). In classiﬁcation (with a −1/1 response)\\nthemarginplaysaroleanalogoustotheresiduals y−f(x)inregression.The\\nclassiﬁcation rule G(x) = sign[f(x)] implies that observations with positive\\nmarginyif(xi)>0 are classiﬁed correctly whereas those with negative\\nmarginyif(xi)<0 are misclassiﬁed. The decision boundary is deﬁned by', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 364}),\n",
       " Document(page_content='10.6 Loss Functions and Robustness 347\\n−2 −1 0 1 20.0 0.5 1.0 1.5 2.0 2.5 3.0Misclassification\\nExponential\\nBinomial Deviance\\nSquared Error\\nSupport VectorLoss\\nyf\\nFIGURE 10.4. Loss functions for two-class classiﬁcation. The response is\\ny=±1; the prediction is f, with class prediction sign(f). The losses are\\nmisclassiﬁcation: I(sign(f)̸=y); exponential: exp(−yf); binomial deviance:\\nlog(1+exp( −2yf)); squared error: (y−f)2; and support vector: (1−yf)+(see\\nSection 12.3). Each function has been scaled so that it passes through the point\\n(0,1).\\nf(x) = 0. The goal of the classiﬁcation algorithm is to produce po sitive\\nmargins as frequently as possible. Any loss criterion used f or classiﬁcation\\nshould penalize negative margins more heavily than positiv e ones since\\npositive margin observations are already correctly classi ﬁed.\\nFigure 10.4 shows both the exponential (10.8) and binomial d eviance\\ncriteria as a function of the margin yf(x). Also shown is misclassiﬁcation\\nlossL(y,f(x)) =I(yf(x)<0), which gives unit penalty for negative mar-\\ngin values, and no penalty at all for positive ones. Both the e xponential\\nand deviance loss can be viewed as monotone continuous appro ximations\\nto misclassiﬁcation loss. They continuously penalize incr easingly negative\\nmargin values more heavily than they reward increasingly po sitive ones.\\nThe diﬀerence between them is in degree. The penalty associa ted with bi-\\nnomial deviance increases linearly for large increasingly negative margin,\\nwhereas the exponential criterion increases the inﬂuence o f such observa-\\ntions exponentially.\\nAt any point in the training process the exponential criteri on concen-\\ntrates much more inﬂuence on observations with large negati ve margins.\\nBinomial deviance concentrates relatively less inﬂuence o n such observa-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 365}),\n",
       " Document(page_content='348 10. Boosting and Additive Trees\\ntions, more evenly spreading the inﬂuence among all of the da ta. It is\\ntherefore far more robust in noisy settings where the Bayes e rror rate is\\nnot close to zero, and especially in situations where there i s misspeciﬁcation\\nof the class labels in the training data. The performance of A daBoost has\\nbeen empirically observed to dramatically degrade in such s ituations.\\nAlso shown in the ﬁgure is squared-error loss. The minimizer of the cor-\\nresponding risk on the population is\\nf∗(x) = argmin\\nf(x)EY|x(Y−f(x))2= E(Y|x) = 2·Pr(Y= 1|x)−1.(10.19)\\nAs before the classiﬁcation rule is G(x) = sign[f(x)]. Squared-error loss\\nis not a good surrogate for misclassiﬁcation error. As seen i n Figure 10.4, it\\nis not a monotone decreasing function of increasing margin yf(x). For mar-\\ngin valuesyif(xi)>1 it increases quadratically, thereby placing increasing\\ninﬂuence (error) on observations that are correctly classi ﬁed with increas-\\ning certainty, thereby reducing the relative inﬂuence of th ose incorrectly\\nclassiﬁedyif(xi)<0. Thus, if class assignment is the goal, a monotone de-\\ncreasing criterion serves as a better surrogate loss functi on. Figure 12.4 on\\npage 426 in Chapter 12 includes a modiﬁcation of quadratic lo ss, the “Hu-\\nberized” square hinge loss (Rosset et al., 2004b), which enj oys the favorable\\nproperties of the binomial deviance, quadratic loss and the SVM hinge loss.\\nIt has the same population minimizer as the quadratic (10.19 ), is zero for\\nyf(x)>1, and becomes linear for yf(x)<−1. Since quadratic functions\\nare easier to compute with than exponentials, our experienc e suggests this\\nto be a useful alternative to the binomial deviance.\\nWithK-class classiﬁcation, the response Ytakes values in the unordered\\nsetG={G1,...,Gk}(see Sections 2.4 and 4.4). We now seek a classiﬁer\\nG(x) taking values in G. It is suﬃcient to know the class conditional proba-\\nbilitiespk(x) = Pr(Y=Gk|x),k= 1,2,...,K, for then the Bayes classiﬁer\\nis\\nG(x) =Gkwherek= argmax\\nℓpℓ(x). (10.20)\\nIn principal, though, we need not learn the pk(x), but simply which one is\\nlargest. However, in data mining applications the interest is often more in\\nthe class probabilities pℓ(x), ℓ= 1,...,Kthemselves, rather than in per-\\nforming a class assignment. As in Section 4.4, the logistic m odel generalizes\\nnaturally to Kclasses,\\npk(x) =efk(x)\\n∑K\\nl=1efl(x), (10.21)\\nwhich ensures that 0 ≤pk(x)≤1 and that they sum to one. Note that\\nhere we have Kdiﬀerent functions, one per class. There is a redundancy\\nin the functions fk(x), since adding an arbitrary h(x) to each leaves the\\nmodel unchanged. Traditionally one of them is set to zero: fo r example,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 366}),\n",
       " Document(page_content='10.6 Loss Functions and Robustness 349\\nfK(x) = 0, as in (4.17). Here we prefer to retain the symmetry, and i mpose\\nthe constraint∑K\\nk=1fk(x) = 0. The binomial deviance extends naturally\\nto theK-classmultinomial deviance loss function:\\nL(y,p(x)) =−K∑\\nk=1I(y=Gk)logpk(x)\\n=−K∑\\nk=1I(y=Gk)fk(x)+log(K∑\\nℓ=1efℓ(x))\\n.(10.22)\\nAsinthetwo-classcase,thecriterion(10.22)penalizesin correctpredictions\\nonly linearly in their degree of incorrectness.\\nZhu et al. (2005) generalize the exponential loss for K-class problems.\\nSee Exercise 10.5 for details.\\nRobust Loss Functions for Regression\\nIn the regression setting, analogous to the relationship be tween exponential\\nloss and binomial log-likelihood is the relationship betwe en squared-error\\nlossL(y,f(x)) = (y−f(x))2and absolute loss L(y,f(x)) =|y−f(x)|. The\\npopulation solutions are f(x) = E(Y|x) for squared-error loss, and f(x) =\\nmedian(Y|x) for absolute loss; for symmetric error distributions thes e are\\nthe same. However, on ﬁnite samples squared-error loss plac es much more\\nemphasis on observations with large absolute residuals |yi−f(xi)|during\\nthe ﬁtting process. It is thus far less robust, and its perfor mance severely\\ndegrades for long-tailed error distributions and especial ly for grossly mis-\\nmeasuredy-values (“outliers”). Other more robust criteria, such as a bso-\\nlute loss, perform much better in these situations. In the st atistical ro-\\nbustness literature, a variety of regression loss criteria have been proposed\\nthat provide strong resistance (if not absolute immunity) t o gross outliers\\nwhile being nearly as eﬃcient as least squares for Gaussian e rrors. They\\nare often better than either for error distributions with mo derately heavy\\ntails. One such criterion is the Huber loss criterion used fo r M-regression\\n(Huber, 1964)\\nL(y,f(x)) ={\\n[y−f(x)]2for|y−f(x)|≤δ,\\n2δ|y−f(x)|−δ2otherwise.(10.23)\\nFigure 10.5 compares these three loss functions.\\nThese considerations suggest that when robustness is a conc ern, as is\\nespecially the case in data mining applications (see Sectio n 10.7), squared-\\nerror loss for regression and exponential loss for classiﬁc ation are not the\\nbest criteria from a statistical perspective. However, the y both lead to the\\nelegant modular boosting algorithms in the context of forwa rd stagewise\\nadditive modeling. For squared-error loss one simply ﬁts th e base learner\\nto the residuals from the current model yi−fm−1(xi) at each step. For', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 367}),\n",
       " Document(page_content='350 10. Boosting and Additive Trees\\n−3 −2 −1 0 1 2 30 2 4 6 8Squared Error\\nAbsolute Error\\nHuberLoss\\ny−f\\nFIGURE 10.5. A comparison of three loss functions for regression, plotted a s a\\nfunction of the margin y−f. The Huber loss function combines the good properties\\nof squared-error loss near zero and absolute error loss when |y−f|is large.\\nexponential loss one performs a weighted ﬁt of the base learn er to the\\noutput values yi, with weights wi= exp(−yifm−1(xi)). Using other more\\nrobust criteria directly in their place does not give rise to such simple\\nfeasible boosting algorithms. However, in Section 10.10.2 we show how one\\ncan derive simple elegant boosting algorithms based on any d iﬀerentiable\\nlosscriterion,therebyproducinghighlyrobustboostingp roceduresfordata\\nmining.\\n10.7 “Oﬀ-the-Shelf” Procedures for Data Mining\\nPredictive learning is an important aspect of data mining. A s can be seen\\nfrom this book, a wide variety of methods have been developed for predic-\\ntive learning from data. For each particular method there ar e situations\\nfor which it is particularly well suited, and others where it performs badly\\ncompared to the best that can be done with that data. We have at tempted\\nto characterize appropriate situations in our discussions of each of the re-\\nspective methods. However, it is seldom known in advance whi ch procedure\\nwillperformbestorevenwellforanygivenproblem.Table10 .1summarizes\\nsome of the characteristics of a number of learning methods.\\nIndustrial and commercial data mining applications tend to be especially\\nchallenging in terms of the requirements placed on learning procedures.\\nData sets are often very large in terms of number of observati ons and\\nnumber of variables measured on each of them. Thus, computat ional con-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 368}),\n",
       " Document(page_content='10.7 “Oﬀ-the-Shelf” Procedures for Data Mining 351\\nTABLE 10.1. Some characteristics of diﬀerent learning methods. Key: ▲= good,\\n◆=fair, and ▼=poor.\\nCharacteristic Neural SVM Trees MARS k-NN,\\nNets Kernels\\nNatural handling of data\\nof “mixed” type▼ ▼ ▲ ▲ ▼\\nHandlingofmissingvalues ▼ ▼ ▲ ▲ ▲\\nRobustness to outliers in\\ninput space▼ ▼ ▲ ▼ ▲\\nInsensitive to monotone\\ntransformations of inputs▼ ▼ ▲ ▼ ▼\\nComputational scalability\\n(largeN)▼ ▼ ▲ ▲ ▼\\nAbility to deal with irrel-\\nevant inputs▼ ▼ ▲ ▲ ▼\\nAbility to extract linear\\ncombinations of features▲ ▲ ▼ ▼ ◆\\nInterpretability ▼ ▼ ◆ ▲ ▼\\nPredictive power ▲ ▲ ▼ ◆ ▲\\nsiderations play an important role. Also, the data are usual lymessy: the\\ninputs tend to be mixtures of quantitative, binary, and cate gorical vari-\\nables, the latter often with many levels. There are generall y many missing\\nvalues, complete observations being rare. Distributions o f numeric predic-\\ntor and response variables are often long-tailed and highly skewed. This\\nis the case for the spam data (Section 9.1.2); when ﬁtting a ge neralized\\nadditive model, we ﬁrst log-transformed each of the predict ors in order to\\nget a reasonable ﬁt. In addition they usually contain a subst antial fraction\\nof gross mis-measurements (outliers). The predictor varia bles are generally\\nmeasured on very diﬀerent scales.\\nIn data mining applications, usually only a small fraction o f the large\\nnumber of predictor variables that have been included in the analysis are\\nactually relevant to prediction. Also, unlike many applica tions such as pat-\\ntern recognition, there is seldom reliable domain knowledg e to help create\\nespecially relevant features and/or ﬁlter out the irreleva nt ones, the inclu-\\nsion of which dramatically degrades the performance of many methods.\\nInaddition,dataminingapplicationsgenerallyrequirein terpretablemod-\\nels. It is not enough to simply produce predictions. It is als o desirable to\\nhave information providing qualitative understanding of t he relationship', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 369}),\n",
       " Document(page_content='352 10. Boosting and Additive Trees\\nbetween joint values of the input variables and the resultin g predicted re-\\nsponse value. Thus, black box methods such as neural networks, which can\\nbe quite useful in purely predictive settings such as patter n recognition,\\nare far less useful for data mining.\\nThese requirements of speed, interpretability and the mess y nature of\\nthe data sharply limit the usefulness of most learning proce dures as oﬀ-\\nthe-shelf methods for data mining. An “oﬀ-the-shelf” metho d is one that\\ncan be directly applied to the data without requiring a great deal of time-\\nconsuming data preprocessing or careful tuning of the learn ing procedure.\\nOf all the well-known learning methods, decision trees come closest to\\nmeeting the requirements for serving as an oﬀ-the-shelf pro cedure for data\\nmining.Theyarerelativelyfasttoconstructandtheyprodu ceinterpretable\\nmodels (if the trees are small). As discussed in Section 9.2, they naturally\\nincorporate mixtures of numeric and categorical predictor variables and\\nmissing values. They are invariant under (strictly monoton e) transforma-\\ntions of the individual predictors. As a result, scaling and /or more general\\ntransformations are not an issue, and they are immune to the e ﬀects of pre-\\ndictor outliers. They perform internal feature selection a s an integral part\\nof the procedure. They are thereby resistant, if not complet ely immune,\\nto the inclusion of many irrelevant predictor variables. Th ese properties of\\ndecision trees are largely the reason that they have emerged as the most\\npopular learning method for data mining.\\nTrees have one aspect that prevents them from being the ideal tool for\\npredictive learning, namely inaccuracy. They seldom provi de predictive ac-\\ncuracy comparable to the best that can be achieved with the da ta at hand.\\nAs seen in Section 10.1, boosting decision trees improves th eir accuracy,\\noften dramatically. At the same time it maintains most of the ir desirable\\nproperties for data mining. Some advantages of trees that ar e sacriﬁced by\\nboosting are speed, interpretability, and, for AdaBoost, r obustness against\\noverlapping class distributions and especially mislabeli ng of the training\\ndata. A gradient boosted model (GBM) is a generalization of t ree boosting\\nthat attempts to mitigate these problems, so as to produce an accurate and\\neﬀective oﬀ-the-shelf procedure for data mining.\\n10.8 Example: Spam Data\\nBefore we go into the details of gradient boosting, we demons trate its abili-\\nties on a two-class classiﬁcation problem. The spam data are introduced in\\nChapter 1, and used as an example for many of the procedures in Chapter 9\\n(Sections 9.1.2, 9.2.5, 9.3.1 and 9.4.1).\\nApplying gradient boosting to these data resulted in a test e rror rate of\\n4.5%, using the same test set as was used in Section 9.1.2. By c omparison,\\nan additive logistic regression achieved 5.5%, a CART tree f ully grown and', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 370}),\n",
       " Document(page_content='10.9 Boosting Trees 353\\npruned by cross-validation 8.7%, and MARS 5.5%. The standar d error of\\nthese estimates is around 0.6%, although gradient boosting is signiﬁcantly\\nbetter than all of them using the McNemar test (Exercise 10.6 ).\\nIn Section 10.13 below we develop a relative importance meas ure for\\neach predictor, as well as a partial dependence plot describ ing a predictor’s\\ncontribution to the ﬁtted model. We now illustrate these for the spam data.\\nFigure 10.6 displays the relative importance spectrum for a ll 57 predictor\\nvariables. Clearly some predictors are more important than others in sep-\\naratingspamfromemail. The frequencies of the character strings !,$,hp,\\nandremoveare estimated to be the four most relevant predictor variabl es.\\nAt the other end of the spectrum, the character strings 857,415,table, and\\n3dhave virtually no relevance.\\nThe quantity being modeled here is the log-odds of spamversusemail\\nf(x) = logPr(spam|x)\\nPr(email|x)(10.24)\\n(see Section 10.13 below). Figure 10.7 shows the partial dep endence of the\\nlog-odds on selected important predictors, two positively associated with\\nspam(!andremove), and two negatively associated ( eduandhp). These\\nparticular dependencies are seen to be essentially monoton ic. There is a\\ngeneral agreement with the corresponding functions found b y the additive\\nlogistic regression model; see Figure 9.1 on page 303.\\nRunning a gradient boosted model on these data with J= 2 terminal-\\nnode trees produces a purely additive (main eﬀects) model fo r the log-\\nodds, with a corresponding error rate of 4.7%, as compared to 4.5% for the\\nfull gradient boosted model (with J= 5 terminal-node trees). Although\\nnot signiﬁcant, this slightly higher error rate suggests th at there may be\\ninteractions among some of the important predictor variabl es. This can\\nbe diagnosed through two-variable partial dependence plot s. Figure 10.8\\nshows one of the several such plots displaying strong intera ction eﬀects.\\nOne sees that for very low frequencies of hp, the log-odds of spamare\\ngreatly increased. For high frequencies of hp, the log-odds of spamtend to\\nbe much lower and roughly constant as a function of !. As the frequency\\nofhpdecreases, the functional relationship with !strengthens.\\n10.9 Boosting Trees\\nRegression and classiﬁcation trees are discussed in detail in Section 9.2.\\nThey partition the space of all joint predictor variable val ues into disjoint\\nregionsRj,j= 1,2,...,J, as represented by the terminal nodes of the tree.\\nA constant γjis assigned to each such region and the predictive rule is\\nx∈Rj⇒f(x) =γj.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 371}),\n",
       " Document(page_content='354 10. Boosting and Additive Trees\\n!$hpremovefreeCAPAVEyourCAPMAXgeorgeCAPTOTeduyouourmoneywill1999businessre(receiveinternet000emailmeeting;650overmailpmpeopletechnologyhplallorderaddressmakefontprojectdataoriginalreportconferencelab[creditparts#85tablecsdirect415857telnetlabsaddresses3d\\n0 20 40 60 80 100\\nRelative Importance\\nFIGURE 10.6. Predictor variable importance spectrum for the spamdata. The\\nvariable names are written on the vertical axis.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 372}),\n",
       " Document(page_content='10.9 Boosting Trees 355\\n!Partial Dependence\\n0.0 0.2 0.4 0.6 0.8 1.0-0.2 0.0 0.2 0.4 0.6 0.8 1.0\\nremovePartial Dependence\\n0.0 0.2 0.4 0.6-0.2 0.0 0.2 0.4 0.6 0.8 1.0\\neduPartial Dependence\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.6 -0.2 0.0 0.2\\nhpPartial Dependence\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1.0 -0.6 -0.2 0.0 0.2\\nFIGURE 10.7. Partial dependence of log-odds of spamon four important pre-\\ndictors. The red ticks at the base of the plots are deciles of the input variable.\\n0.51.01.52.02.53.00.20.40.60.81.0-1.0-0.5 0.0 0.5 1.0\\nhp!\\nFIGURE 10.8. Partial dependence of the log-odds of spamvs.emailas a func-\\ntion of joint frequencies of hpand the character !.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 373}),\n",
       " Document(page_content='356 10. Boosting and Additive Trees\\nThus a tree can be formally expressed as\\nT(x;Θ) =J∑\\nj=1γjI(x∈Rj), (10.25)\\nwith parameters Θ = {Rj,γj}J\\n1.Jis usually treated as a meta-parameter.\\nThe parameters are found by minimizing the empirical risk\\nˆΘ = argmin\\nΘJ∑\\nj=1∑\\nxi∈RjL(yi,γj). (10.26)\\nThis is a formidable combinatorial optimization problem, a nd we usually\\nsettle for approximate suboptimal solutions. It is useful t o divide the opti-\\nmization problem into two parts:\\nFindingγjgivenRj:Given theRj, estimating the γjis typically trivial,\\nand often ˆγj= ¯yj, the mean of the yifalling in region Rj. For mis-\\nclassiﬁcation loss, ˆ γjis the modal class of the observations falling in\\nregionRj.\\nFindingRj:This is the diﬃcult part, for which approximate solutions ar e\\nfound. Note also that ﬁnding the Rjentails estimating the γjas well.\\nA typical strategy is to use a greedy, top-down recursive par titioning\\nalgorithm to ﬁnd the Rj. In addition, it is sometimes necessary to\\napproximate (10.26) by a smoother and more convenient crite rion for\\noptimizing the Rj:\\n˜Θ = argmin\\nΘN∑\\ni=1˜L(yi,T(xi,Θ)). (10.27)\\nThen given the ˆRj=˜Rj, theγjcan be estimated more precisely\\nusing the original criterion.\\nIn Section 9.2 we described such a strategy for classiﬁcatio n trees. The Gini\\nindex replaced misclassiﬁcation loss in the growing of the t ree (identifying\\ntheRj).\\nThe boosted tree model is a sum of such trees,\\nfM(x) =M∑\\nm=1T(x;Θm), (10.28)\\ninduced in a forward stagewise manner (Algorithm 10.2). At e ach step in\\nthe forward stagewise procedure one must solve\\nˆΘm= argmin\\nΘmN∑\\ni=1L(yi,fm−1(xi)+T(xi;Θm)) (10.29)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 374}),\n",
       " Document(page_content='10.9 Boosting Trees 357\\nfor the region set and constants Θ m={Rjm,γjm}Jm\\n1of the next tree, given\\nthe current model fm−1(x).\\nGiven the regions Rjm, ﬁnding the optimal constants γjmin each region\\nis typically straightforward:\\nˆγjm= argmin\\nγjm∑\\nxi∈RjmL(yi,fm−1(xi)+γjm).(10.30)\\nFinding the regions is diﬃcult, and even more diﬃcult than fo r a single\\ntree. For a few special cases, the problem simpliﬁes.\\nFor squared-error loss, the solution to (10.29) is no harder than for a\\nsingle tree. It is simply the regression tree that best predi cts the current\\nresidualsyi−fm−1(xi), and ˆγjmis the mean of these residuals in each\\ncorresponding region.\\nFor two-class classiﬁcation and exponential loss, this sta gewise approach\\ngives rise to the AdaBoost method for boosting classiﬁcatio n trees (Algo-\\nrithm 10.1). In particular, if the trees T(x;Θm) are restricted to be scaled\\nclassiﬁcation trees, then we showed in Section 10.4 that the solution to\\n(10.29) is the tree that minimizes the weighted error rate∑N\\ni=1w(m)\\niI(yi̸=\\nT(xi;Θm)) with weights w(m)\\ni=e−yifm−1(xi). By a scaled classiﬁcation\\ntree, we mean βmT(x;Θm), with the restriction that γjm∈{−1,1}).\\nWithout this restriction, (10.29) still simpliﬁes for expo nential loss to a\\nweighted exponential criterion for the new tree:\\nˆΘm= argmin\\nΘmN∑\\ni=1w(m)\\niexp[−yiT(xi;Θm)]. (10.31)\\nItisstraightforwardtoimplementagreedyrecursive-part itioningalgorithm\\nusing this weighted exponential loss as a splitting criteri on. Given the Rjm,\\none can show (Exercise 10.7) that the solution to (10.30) is t he weighted\\nlog-odds in each corresponding region\\nˆγjm=1\\n2log∑\\nxi∈Rjmw(m)\\niI(yi= 1)\\n∑\\nxi∈Rjmw(m)\\niI(yi=−1). (10.32)\\nThis requires a specialized tree-growing algorithm; in pra ctice, we prefer\\nthe approximation presented below that uses a weighted leas t squares re-\\ngression tree.\\nUsing loss criteria such as the absolute error or the Huber lo ss (10.23) in\\nplace of squared-error loss for regression, and the devianc e (10.22) in place\\nof exponential loss for classiﬁcation, will serve to robust ify boosting trees.\\nUnfortunately, unlike their nonrobust counterparts, thes e robust criteria\\ndo not give rise to simple fast boosting algorithms.\\nFor more general loss criteria the solution to (10.30), give n theRjm,\\nis typically straightforward since it is a simple “location ” estimate. For', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 375}),\n",
       " Document(page_content='358 10. Boosting and Additive Trees\\nabsolute loss it is just the median of the residuals in each re spective region.\\nFor the other criteria fast iterative algorithms exist for s olving (10.30),\\nand usually their faster “single-step” approximations are adequate. The\\nproblem is tree induction. Simple fast algorithms do not exi st for solving\\n(10.29) for these more general loss criteria, and approxima tions like (10.27)\\nbecome essential.\\n10.10 Numerical Optimization via Gradient\\nBoosting\\nFast approximate algorithms for solving (10.29) with any di ﬀerentiable loss\\ncriterion can be derived by analogy to numerical optimizati on. The loss in\\nusingf(x) to predict yon the training data is\\nL(f) =N∑\\ni=1L(yi,f(xi)). (10.33)\\nThe goal is to minimize L(f) with respect to f, where here f(x) is con-\\nstrained to be a sum of trees (10.28). Ignoring this constrai nt, minimizing\\n(10.33) can be viewed as a numerical optimization\\nˆf= argmin\\nfL(f), (10.34)\\nwhere the “parameters” f∈IRNare the values of the approximating func-\\ntionf(xi) at each of the Ndata points xi:\\nf={f(x1),f(x2),...,f(xN)}T.\\nNumerical optimization procedures solve (10.34) as a sum of component\\nvectors\\nfM=M∑\\nm=0hm,hm∈IRN,\\nwheref0=h0is an initial guess, and each successive fmis induced based\\non the current parameter vector fm−1, which is the sum of the previously\\ninduced updates. Numerical optimization methods diﬀer in t heir prescrip-\\ntions for computing each increment vector hm(“step”).\\n10.10.1 Steepest Descent\\nSteepest descent chooses hm=−ρmgmwhereρmis a scalar and gm∈IRN\\nis the gradient of L(f) evaluated at f=fm−1. The components of the\\ngradient gmare\\ngim=[∂L(yi,f(xi))\\n∂f(xi)]\\nf(xi)=fm−1(xi)(10.35)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 376}),\n",
       " Document(page_content='10.10 Numerical Optimization via Gradient Boosting 359\\nThestep length ρmis the solution to\\nρm= argmin\\nρL(fm−1−ρgm). (10.36)\\nThe current solution is then updated\\nfm=fm−1−ρmgm\\nand the process repeated at the next iteration. Steepest des cent can be\\nviewed as a very greedy strategy, since −gmis the local direction in IRN\\nfor whichL(f) is most rapidly decreasing at f=fm−1.\\n10.10.2 Gradient Boosting\\nForward stagewise boosting (Algorithm 10.2) is also a very g reedy strategy.\\nAt each step the solution tree is the one that maximally reduc es (10.29),\\ngiven the current model fm−1and its ﬁts fm−1(xi). Thus, the tree predic-\\ntionsT(xi;Θm) are analogous to the components of the negative gradient\\n(10.35). The principal diﬀerence between them is that the tr ee components\\ntm={T(x1;Θm),...,T(xN;Θm)}Tare not independent. They are con-\\nstrained to be the predictions of a Jm-terminal node decision tree, whereas\\nthe negative gradient is the unconstrained maximal descent direction.\\nThe solution to (10.30) in the stagewise approach is analogo us to the line\\nsearch (10.36) in steepest descent. The diﬀerence is that (1 0.30) performs\\na separate line search for those components of tmthat correspond to each\\nseparate terminal region {T(xi;Θm)}xi∈Rjm.\\nIf minimizing loss on the training data (10.33) were the only goal, steep-\\nest descent would be the preferred strategy. The gradient (1 0.35) is trivial\\nto calculate for any diﬀerentiable loss function L(y,f(x)), whereas solving\\n(10.29) is diﬃcult for the robust criteria discussed in Sect ion 10.6. Unfor-\\ntunately the gradient (10.35) is deﬁned only at the training data points xi,\\nwhereas the ultimate goal is to generalize fM(x) to new data not repre-\\nsented in the training set.\\nA possible resolution to this dilemma is to induce a tree T(x;Θm) at the\\nmth iteration whose predictions tmare as close as possible to the negative\\ngradient. Using squared error to measure closeness, this le ads us to\\n˜Θm= argmin\\nΘN∑\\ni=1(−gim−T(xi;Θ))2. (10.37)\\nThat is, one ﬁts the tree Tto the negative gradient values (10.35) by least\\nsquares. As noted in Section 10.9 fast algorithms exist for l east squares\\ndecision tree induction. Although the solution regions ˜Rjmto (10.37) will\\nnot be identical to the regions Rjmthat solve (10.29), it is generally sim-\\nilar enough to serve the same purpose. In any case, the forwar d stagewise', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 377}),\n",
       " Document(page_content='360 10. Boosting and Additive Trees\\nTABLE 10.2. Gradients for commonly used loss functions.\\nSetting Loss Function −∂L(yi,f(xi))/∂f(xi)\\nRegression1\\n2[yi−f(xi)]2yi−f(xi)\\nRegression|yi−f(xi)|sign[yi−f(xi)]\\nRegression Huber yi−f(xi) for|yi−f(xi)|≤δm\\nδmsign[yi−f(xi)] for|yi−f(xi)|>δm\\nwhereδm=αth-quantile{|yi−f(xi)|}\\nClassiﬁcation Deviance kth component: I(yi=Gk)−pk(xi)\\nboosting procedure, and top-down decision tree induction, are themselves\\napproximation procedures. After constructing the tree (10 .37), the corre-\\nsponding constants in each region are given by (10.30).\\nTable 10.2 summarizes the gradients for commonly used loss f unctions.\\nFor squared error loss, the negative gradient is just the ord inary residual\\n−gim=yi−fm−1(xi), so that (10.37) on its own is equivalent to standard\\nleast-squares boosting. With absolute error loss, the nega tive gradient is\\nthesignof the residual, so at each iteration (10.37) ﬁts the tree to t he\\nsign of the current residuals by least squares. For Huber M-r egression, the\\nnegative gradient is a compromise between these two (see the table).\\nFor classiﬁcation the loss function is the multinomial devi ance (10.22),\\nandKleast squares trees are constructed at each iteration. Each treeTkm\\nis ﬁt to its respective negative gradient vector gkm,\\n−gikm=[∂L(yi,f1(xi),...,f K(xi))\\n∂fk(xi)]\\nf(xi)=fm−1(xi)\\n=I(yi=Gk)−pk(xi), (10.38)\\nwithpk(x) given by (10.21). Although Kseparate trees are built at each\\niteration, they are related through (10.21). For binary cla ssiﬁcation ( K=\\n2), only one tree is needed (exercise 10.10).\\n10.10.3 Implementations of Gradient Boosting\\nAlgorithm 10.3 presents the generic gradient tree-boostin g algorithm for\\nregression. Speciﬁc algorithms are obtained by inserting d iﬀerent loss cri-\\nteriaL(y,f(x)). The ﬁrst line of the algorithm initializes to the optimal\\nconstant model, which is just a single terminal node tree. Th e components\\nof the negative gradient computed at line 2(a) are referred t o as general-\\nized orpseudoresiduals,r. Gradients for commonly used loss functions are\\nsummarized in Table 10.2.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 378}),\n",
       " Document(page_content='10.11 Right-Sized Trees for Boosting 361\\nAlgorithm 10.3 Gradient Tree Boosting Algorithm.\\n1. Initialize f0(x) = argmin γ∑N\\ni=1L(yi,γ).\\n2. Form= 1 toM:\\n(a) Fori= 1,2,...,Ncompute\\nrim=−[∂L(yi,f(xi))\\n∂f(xi)]\\nf=fm−1.\\n(b) Fit a regression tree to the targets rimgiving terminal regions\\nRjm, j= 1,2,...,J m.\\n(c) Forj= 1,2,...,J mcompute\\nγjm= argmin\\nγ∑\\nxi∈RjmL(yi,fm−1(xi)+γ).\\n(d) Update fm(x) =fm−1(x)+∑Jm\\nj=1γjmI(x∈Rjm).\\n3. Output ˆf(x) =fM(x).\\nThe algorithm for classiﬁcation is similar. Lines 2(a)–(d) are repeated\\nKtimes at each iteration m, once for each class using (10.38). The result\\nat line 3 is Kdiﬀerent (coupled) tree expansions fkM(x),k= 1,2,...,K.\\nThese produce probabilities via (10.21) or do classiﬁcatio n as in (10.20).\\nDetails are given in Exercise 10.9. Two basic tuning paramet ers are the\\nnumber of iterations Mand the sizes of each of the constituent trees\\nJm, m= 1,2,...,M.\\nThe original implementation of this algorithm was called MA RT for\\n“multiple additive regression trees,” and was referred to i n the ﬁrst edi-\\ntion of this book. Many of the ﬁgures in this chapter were prod uced by\\nMART. Gradient boosting as described here is implemented in the Rgbm\\npackage (Ridgeway, 1999, “Gradient Boosted Models”), and i s freely avail-\\nable. The gbmpackage is used in Section 10.14.2, and extensively in Chap-\\nters 16 and 15. Another R implementation of boosting is mboost(Hothorn\\nand B¨ uhlmann, 2006). A commercial implementation of gradi ent boost-\\ning/MART called TreeNet\\uf8e8is available from Salford Systems, Inc.\\n10.11 Right-Sized Trees for Boosting\\nHistorically, boosting was considered to be a technique for combining mod-\\nels, here trees. As such, the tree building algorithm was reg arded as a', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 379}),\n",
       " Document(page_content='362 10. Boosting and Additive Trees\\nprimitive that produced models to be combined by the boostin g proce-\\ndure. In this scenario, the optimal size of each tree is estim ated separately\\nin the usual manner when it is built (Section 9.2). A very larg e (oversized)\\ntree is ﬁrst induced, and then a bottom-up procedure is emplo yed to prune\\nit to the estimated optimal number of terminal nodes. This ap proach as-\\nsumes implicitly that each tree is the last one in the expansi on (10.28).\\nExcept perhaps for the very last tree, this is clearly a very p oor assump-\\ntion. The result is that trees tend to be much too large, espec ially during\\nthe early iterations. This substantially degrades perform ance and increases\\ncomputation.\\nThe simplest strategy for avoiding this problem is to restri ct all trees\\nto be the same size, Jm=J∀m. At each iteration a J-terminal node\\nregression tree is induced. Thus Jbecomes a meta-parameter of the entire\\nboosting procedure, to be adjusted to maximize estimated pe rformance for\\nthe data at hand.\\nOne can get an idea of useful values for Jby considering the properties\\nof the “target” function\\nη= argmin\\nfEXYL(Y,f(X)). (10.39)\\nHere the expected value is over the population joint distrib ution of (X,Y).\\nThe target function η(x) is the one with minimum prediction risk on future\\ndata. This is the function we are trying to approximate.\\nOne relevant property of η(X) is the degree to which the coordinate vari-\\nablesXT= (X1,X2,...,X p) interact with one another. This is captured\\nby its ANOVA (analysis of variance) expansion\\nη(X) =∑\\njηj(Xj)+∑\\njkηjk(Xj,Xk)+∑\\njklηjkl(Xj,Xk,Xl)+···.(10.40)\\nThe ﬁrst sum in (10.40) is over functions of only a single pred ictor variable\\nXj.Theparticularfunctions ηj(Xj)arethosethatjointlybestapproximate\\nη(X) under the loss criterion being used. Each such ηj(Xj) is called the\\n“main eﬀect” of Xj. The second sum is over those two-variable functions\\nthat when added to the main eﬀects best ﬁt η(X). These are called the\\nsecond-order interactions of each respective variable pai r (Xj,Xk). The\\nthirdsumrepresentsthird-orderinteractions,andsoon.F ormanyproblems\\nencountered in practice, low-order interaction eﬀects ten d to dominate.\\nWhen this is the case, models that produce strong higher-ord er interaction\\neﬀects, such as large decision trees, suﬀer in accuracy.\\nThe interaction level of tree-based approximations is limi ted by the tree\\nsizeJ. Namely, no interaction eﬀects of level greater than J−1 are pos-\\nsible. Since boosted models are additive in the trees (10.28 ), this limit\\nextends to them as well. Setting J= 2 (single split “decision stump”)\\nproduces boosted models with only main eﬀects; no interacti ons are per-\\nmitted. With J= 3, two-variable interaction eﬀects are also allowed, and', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 380}),\n",
       " Document(page_content='10.11 Right-Sized Trees for Boosting 363\\nNumber of TermsTest Error\\n0 100 200 300 4000.0 0.1 0.2 0.3 0.4Stumps\\n10 Node\\n100 Node\\nAdaboost\\nFIGURE 10.9. Boosting with diﬀerent sized trees, applied to the example (10 .2)\\nused in Figure 10.2. Since the generative model is additive, stumps perform the\\nbest. The boosting algorithm used the binomial deviance loss i n Algorithm 10.3;\\nshown for comparison is the AdaBoost Algorithm 10.1.\\nso on. This suggests that the value chosen for Jshould reﬂect the level\\nof dominant interactions of η(x). This is of course generally unknown, but\\nin most situations it will tend to be low. Figure 10.9 illustr ates the eﬀect\\nof interaction order (choice of J) on the simulation example (10.2). The\\ngenerative function is additive (sum of quadratic monomial s), so boosting\\nmodels with J >2 incurs unnecessary variance and hence the higher test\\nerror. Figure 10.10 compares the coordinate functions foun d by boosted\\nstumps with the true functions.\\nAlthough in many applications J= 2 will be insuﬃcient, it is unlikely\\nthatJ >10 will be required. Experience so far indicates that 4 ≤J≤8\\nworks well in the context of boosting, with results being fai rly insensitive\\nto particular choices in this range. One can ﬁne-tune the val ue forJby\\ntrying several diﬀerent values and choosing the one that pro duces the low-\\nest risk on a validation sample. However, this seldom provid es signiﬁcant\\nimprovement over using J≃6.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 381}),\n",
       " Document(page_content='364 10. Boosting and Additive Trees\\nCoordinate Functions for Additive Logistic Trees\\nf1(x1) f2(x2)f3(x3)f4(x4)f5(x5)\\nf6(x6) f7(x7)f8(x8)f9(x9)f10(x10)\\nFIGURE 10.10. Coordinate functions estimated by boosting stumps for the s im-\\nulated example used in Figure 10.9. The true quadratic functio ns are shown for\\ncomparison.\\n10.12 Regularization\\nBesides the size of the constituent trees, J, the other meta-parameter of\\ngradient boosting is the number of boosting iterations M. Each iteration\\nusuallyreducesthetrainingrisk L(fM),sothatfor Mlargeenoughthisrisk\\ncan be made arbitrarily small. However, ﬁtting the training data too well\\ncan lead to overﬁtting, which degrades the risk on future pre dictions. Thus,\\nthere is an optimal number M∗minimizing future risk that is application\\ndependent. A convenient way to estimate M∗is to monitor prediction risk\\nas a function of Mon a validation sample. The value of Mthat minimizes\\nthis risk is taken to be an estimate of M∗. This is analogous to the early\\nstopping strategy often used with neural networks (Section 11.4).\\n10.12.1 Shrinkage\\nControlling the value of Mis not the only possible regularization strategy.\\nAs with ridge regression and neural networks, shrinkage tec hniques can be\\nemployedaswell(seeSections3.4.1and11.5).Thesimplest implementation\\nof shrinkage in the context of boosting is to scale the contri bution of each\\ntree by a factor 0 <ν <1 when it is added to the current approximation.\\nThat is, line 2(d) of Algorithm 10.3 is replaced by\\nfm(x) =fm−1(x)+ν·J∑\\nj=1γjmI(x∈Rjm). (10.41)\\nThe parameter νcan be regarded as controlling the learning rate of the\\nboosting procedure. Smaller values of ν(more shrinkage) result in larger\\ntraining risk for the same number of iterations M. Thus, both νandM\\ncontrol prediction risk on the training data. However, thes e parameters do', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 382}),\n",
       " Document(page_content='10.12 Regularization 365\\nnot operate independently. Smaller values of νlead to larger values of M\\nfor the same training risk, so that there is a tradeoﬀ between them.\\nEmpirically it has been found (Friedman, 2001) that smaller values ofν\\nfavor better test error, and require correspondingly large r values ofM. In\\nfact, the best strategy appears to be to set νto be very small ( ν <0.1)\\nand then choose Mby early stopping. This yields dramatic improvements\\n(over no shrinkage ν= 1) for regression and for probability estimation. The\\ncorresponding improvements in misclassiﬁcation risk via ( 10.20) are less,\\nbut still substantial. The price paid for these improvement s is computa-\\ntional: smaller values of νgive rise to larger values of M, and computation\\nis proportional to the latter. However, as seen below, many i terations are\\ngenerally computationally feasible even on very large data sets. This is\\npartly due to the fact that small trees are induced at each ste p with no\\npruning.\\nFigure 10.11 shows test error curves for the simulated examp le (10.2) of\\nFigure 10.2. A gradient boosted model (MART) was trained usi ng binomial\\ndeviance, using either stumps or six terminal-node trees, a nd with or with-\\nout shrinkage. The beneﬁts of shrinkage are evident, especi ally when the\\nbinomial deviance is tracked. With shrinkage, each test err or curve reaches\\na lower value, and stays there for many iterations.\\nSection 16.2.1 draws a connection between forward stagewis e shrinkage\\nin boosting and the use of an L1penalty for regularizing model parame-\\nters (the “lasso”). We argue that L1penalties may be superior to the L2\\npenalties used by methods such as the support vector machine .\\n10.12.2 Subsampling\\nWe saw in Section 8.7 that bootstrap averaging (bagging) imp roves the\\nperformance of a noisy classiﬁer through averaging. Chapte r 15 discusses\\nin some detail the variance-reduction mechanism of this sam pling followed\\nby averaging. We can exploit the same device in gradient boos ting, both\\nto improve performance and computational eﬃciency.\\nWithstochastic gradient boosting (Friedman, 1999), at each iteration we\\nsample a fraction ηof the training observations (without replacement),\\nand grow the next tree using that subsample. The rest of the al gorithm is\\nidentical. A typical value for ηcan be1\\n2, although for large N,ηcan be\\nsubstantially smaller than1\\n2.\\nNot only does the sampling reduce the computing time by the sa me\\nfractionη, but in many cases it actually produces a more accurate model .\\nFigure 10.12 illustrates the eﬀect of subsampling using the simulated\\nexample (10.2), both as a classiﬁcation and as a regression e xample. We\\nsee in both cases that sampling along with shrinkage slightl y outperformed\\nthe rest. It appears here that subsampling without shrinkag e does poorly.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 383}),\n",
       " Document(page_content='366 10. Boosting and Additive Trees\\nBoosting IterationsTest Set Deviance\\n0 500 1000 1500 20000.0 0.5 1.0 1.5 2.0No shrinkage\\nShrinkage=0.2Stumps\\nDeviance\\nBoosting IterationsTest Set Misclassification Error\\n0 500 1000 1500 20000.0 0.1 0.2 0.3 0.4 0.5No shrinkage\\nShrinkage=0.2Stumps\\nMisclassification Error\\nBoosting IterationsTest Set Deviance\\n0 500 1000 1500 20000.0 0.5 1.0 1.5 2.0No shrinkage\\nShrinkage=0.66-Node Trees\\nDeviance\\nBoosting IterationsTest Set Misclassification Error\\n0 500 1000 1500 20000.0 0.1 0.2 0.3 0.4 0.5No shrinkage\\nShrinkage=0.66-Node Trees\\nMisclassification Error\\nFIGURE 10.11. Test error curves for simulated example (10.2) of Figure 10.9,\\nusing gradient boosting (MART). The models were trained usin g binomial de-\\nviance, either stumps or six terminal-node trees, and with or without shrinkage.\\nThe left panels report test deviance, while the right panels show misclassiﬁcation\\nerror. The beneﬁcial eﬀect of shrinkage can be seen in all cas es, especially for\\ndeviance in the left panels.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 384}),\n",
       " Document(page_content='10.13 Interpretation 367\\n0 200 400 600 800 10000.4 0.6 0.8 1.0 1.2 1.4\\nBoosting IterationsTest Set DevianceDeviance4−Node Trees\\n0 200 400 600 800 10000.30 0.35 0.40 0.45 0.50\\nBoosting IterationsTest Set Absolute ErrorNo shrinkage\\nShrink=0.1\\nSample=0.5\\nShrink=0.1 Sample=0.5Absolute Error\\nFIGURE 10.12. Test-error curves for the simulated example (10.2), showing\\nthe eﬀect of stochasticity. For the curves labeled “Sample = 0.5”, a diﬀerent 50%\\nsubsample of the training data was used each time a tree was gro wn. In the left\\npanel the models were ﬁt by gbmusing a binomial deviance loss function; in the\\nright-hand panel using square-error loss.\\nThe downside is that we now have four parameters to set: J,M,νand\\nη. Typically some early explorations determine suitable val ues forJ,νand\\nη, leavingMas the primary parameter.\\n10.13 Interpretation\\nSingle decision trees are highly interpretable. The entire model can be com-\\npletely represented by a simple two-dimensional graphic (b inary tree) that\\nis easily visualized. Linear combinations of trees (10.28) lose this important\\nfeature, and must therefore be interpreted in a diﬀerent way .\\n10.13.1 Relative Importance of Predictor Variables\\nIndataminingapplicationstheinputpredictorvariablesa reseldomequally\\nrelevant. Often only a few of them have substantial inﬂuence on the re-\\nsponse; the vast majority are irrelevant and could just as we ll have not\\nbeen included. It is often useful to learn the relative impor tance or contri-\\nbution of each input variable in predicting the response.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 385}),\n",
       " Document(page_content='368 10. Boosting and Additive Trees\\nFor a single decision tree T, Breiman et al. (1984) proposed\\nI2\\nℓ(T) =J−1∑\\nt=1ˆı2\\ntI(v(t) =ℓ) (10.42)\\nas a measure of relevance for each predictor variable Xℓ. The sum is over\\ntheJ−1 internal nodes of the tree. At each such node t, one of the input\\nvariablesXv(t)isusedtopartitiontheregionassociatedwiththatnodeint o\\ntwosubregions;withineachaseparateconstantisﬁttother esponsevalues.\\nThe particular variable chosen is the one that gives maximal estimated\\nimprovement ˆ ı2\\ntin squared error risk over that for a constant ﬁt over the\\nentire region. The squared relative importance of variable Xℓis the sum of\\nsuch squared improvements over all internal nodes for which it was chosen\\nas the splitting variable.\\nThis importance measure is easily generalized to additive t ree expansions\\n(10.28); it is simply averaged over the trees\\nI2\\nℓ=1\\nMM∑\\nm=1I2\\nℓ(Tm). (10.43)\\nDue to the stabilizing eﬀect of averaging, this measure turn s out to be more\\nreliable than is its counterpart (10.42) for a single tree. A lso, because of\\nshrinkage (Section 10.12.1) the masking of important varia bles by others\\nwith which they are highly correlated is much less of a proble m. Note\\nthat (10.42) and (10.43) refer to squaredrelevance; the actual relevances\\nare their respective square roots. Since these measures are relative, it is\\ncustomary to assign the largest a value of 100 and then scale t he others\\naccordingly. Figure 10.6 shows the relevant importance of t he 57 inputs in\\npredicting spamversusemail.\\nForK-class classiﬁcation, Kseparate models fk(x),k= 1,2,...,Kare\\ninduced, each consisting of a sum of trees\\nfk(x) =M∑\\nm=1Tkm(x). (10.44)\\nIn this case (10.43) generalizes to\\nI2\\nℓk=1\\nMM∑\\nm=1I2\\nℓ(Tkm). (10.45)\\nHereIℓkis the relevance of Xℓin separating the class kobservations from\\nthe other classes. The overall relevance of Xℓis obtained by averaging over\\nall of the classes\\nI2\\nℓ=1\\nKK∑\\nk=1I2\\nℓk. (10.46)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 386}),\n",
       " Document(page_content='10.13 Interpretation 369\\nFigures 10.23 and 10.24 illustrate the use of these averaged and separate\\nrelative importances.\\n10.13.2 Partial Dependence Plots\\nAfter the most relevant variables have been identiﬁed, the n ext step is to\\nattempt to understand the nature of the dependence of the app roximation\\nf(X) on their joint values. Graphical renderings of the f(X) as a function\\nof its arguments provides a comprehensive summary of its dep endence on\\nthe joint values of the input variables.\\nUnfortunately, such visualization is limited to low-dimen sional views.\\nWe can easily display functions of one or two arguments, eith er continuous\\nor discrete (or mixed), in a variety of diﬀerent ways; this bo ok is ﬁlled\\nwith such displays. Functions of slightly higher dimension s can be plotted\\nby conditioning on particular sets of values of all but one or two of the\\narguments, producing a trellisof plots (Becker et al., 1996).1\\nFor more than two or three variables, viewing functions of th e corre-\\nsponding higher-dimensional arguments is more diﬃcult. A u seful alterna-\\ntive can sometimes be to view a collection of plots, each one o f which shows\\nthe partial dependence of the approximation f(X) on a selected small sub-\\nset of the input variables. Although such a collection can se ldom provide a\\ncomprehensive depiction of the approximation, it can often produce helpful\\nclues, especially when f(x) is dominated by low-order interactions (10.40).\\nConsiderthesubvector XSofℓ<poftheinputpredictorvariables XT=\\n(X1,X2,...,X p), indexed byS⊂{1,2,...,p}. LetCbe the complement\\nset, withS∪C={1,2,...,p}. A general function f(X) will in principle\\ndepend on all of the input variables: f(X) =f(XS,XC). One way to deﬁne\\nthe average or partialdependence of f(X) onXSis\\nfS(XS) = EXCf(XS,XC). (10.47)\\nThis is a marginal average of f, and can serve as a useful description of the\\neﬀect of the chosen subset on f(X) when, for example, the variables in XS\\ndo not have strong interactions with those in XC.\\nPartial dependence functions can be used to interpret the re sults of any\\n“black box” learning method. They can be estimated by\\n¯fS(XS) =1\\nNN∑\\ni=1f(XS,xiC), (10.48)\\nwhere{x1C,x2C,...,x NC}are the values of XCoccurring in the training\\ndata. This requires a pass over the data for each set of joint v alues ofXSfor\\nwhich¯fS(XS) is to be evaluated. This can be computationally intensive,\\n1lattice in R.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 387}),\n",
       " Document(page_content='370 10. Boosting and Additive Trees\\nevenformoderatelysizeddatasets.Fortunatelywithdecis iontrees, ¯fS(XS)\\n(10.48) can be rapidly computed from the tree itself without reference to\\nthe data (Exercise 10.11).\\nIt is important to note that partial dependence functions de ﬁned in\\n(10.47) represent the eﬀect of XSonf(X) after accounting for the (av-\\nerage) eﬀects of the other variables XConf(X). They are notthe eﬀect\\nofXSonf(X)ignoring the eﬀects of XC. The latter is given by the con-\\nditional expectation\\n˜fS(XS) = E(f(XS,XC)|XS), (10.49)\\nand is the best least squares approximation to f(X) by a function of XS\\nalone. The quantities ˜fS(XS) and¯fS(XS) will be the same only in the\\nunlikely event that XSandXCare independent. For example, if the eﬀect\\nof the chosen variable subset happens to be purely additive,\\nf(X) =h1(XS)+h2(XC). (10.50)\\nThen (10.47) produces the h1(XS) up to an additive constant. If the eﬀect\\nis purely multiplicative,\\nf(X) =h1(XS)·h2(XC), (10.51)\\nthen (10.47) produces h1(XS) up to a multiplicative constant factor. On\\nthe other hand, (10.49) will not produce h1(XS) in either case. In fact,\\n(10.49) can produce strong eﬀects on variable subsets for wh ichf(X) has\\nno dependence at all.\\nViewing plots of the partial dependence of the boosted-tree approxima-\\ntion (10.28) on selected variables subsets can help to provi de a qualitative\\ndescription of its properties. Illustrations are shown in S ections 10.8 and\\n10.14. Owing to the limitations of computer graphics, and hu man percep-\\ntion, the size of the subsets XSmust be small ( l≈1,2,3). There are of\\ncourse a large number of such subsets, but only those chosen f rom among\\nthe usually much smaller set of highly relevant predictors a re likely to be\\ninformative. Also, those subsets whose eﬀect on f(X) is approximately\\nadditive (10.50) or multiplicative (10.51) will be most rev ealing.\\nForK-class classiﬁcation, there are Kseparate models (10.44), one for\\neachclass.Eachoneisrelatedtotherespectiveprobabilit ies(10.21)through\\nfk(X) = logpk(X)−1\\nKK∑\\nl=1logpl(X). (10.52)\\nThus eachfk(X) is a monotone increasing function of its respective prob-\\nability on a logarithmic scale. Partial dependence plots of each respective\\nfk(X) (10.44) on its most relevant predictors (10.45) can help re veal how\\nthelog-oddsofrealizingthatclassdependontherespectiv einputvariables.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 388}),\n",
       " Document(page_content='10.14 Illustrations 371\\n10.14 Illustrations\\nInthissectionweillustrategradientboostingonanumbero flargerdatasets,\\nusing diﬀerent loss functions as appropriate.\\n10.14.1 California Housing\\nThis data set (Pace and Barry, 1997) is available from the Car negie-Mellon\\nStatLibrepository2. It consists of aggregated data from each of 20,460\\nneighborhoods (1990 census block groups) in California. Th e response vari-\\nableYis the median house value in each neighborhood measured in un its of\\n$100,000. The predictor variables are demographics such as median income\\nMedInc, housing density as reﬂected by the number of houses House, and the\\naverage occupancy in each house AveOccup . Also included as predictors are\\nthe location of each neighborhood ( longitude andlatitude ), and several\\nquantities reﬂecting the properties of the houses in the nei ghborhood: av-\\nerage number of rooms AveRooms and bedrooms AveBedrms . There are thus\\na total of eight predictors, all numeric.\\nWe ﬁt a gradient boosting model using the MART procedure, wit hJ= 6\\nterminal nodes, a learning rate (10.41) of ν= 0.1, and the Huber loss\\ncriterion for predicting the numeric response. We randomly divided the\\ndataset into a training set (80%) and a test set (20%).\\nFigure 10.13 shows the average absolute error\\nAAE = E|y−ˆfM(x)| (10.53)\\nas a function for number of iterations Mon both the training data and test\\ndata. The test error is seen to decrease monotonically with i ncreasingM,\\nmore rapidly during the early stages and then leveling oﬀ to b eing nearly\\nconstant as iterations increase. Thus, the choice of a parti cular value of M\\nis not critical, as long as it is not too small. This tends to be the case in\\nmany applications. The shrinkage strategy (10.41) tends to eliminate the\\nproblem of overﬁtting, especially for larger data sets.\\nThe value of AAE after 800 iterations is 0.31. This can be comp ared to\\nthatoftheoptimal constantpredictormedian {yi}whichis0.89.Intermsof\\nmore familiar quantities, the squared multiple correlatio n coeﬃcient of this\\nmodel isR2= 0.84. Pace and Barry (1997) use a sophisticated spatial auto-\\nregression procedure, where prediction for each neighborh ood is based on\\nmedian house values in nearby neighborhoods, using the othe r predictors as\\ncovariates. Experimenting with transformations they achi evedR2= 0.85,\\npredicting log Y. Using log Yas the response the corresponding value for\\ngradient boosting was R2= 0.86.\\n2http://lib.stat.cmu.edu.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 389}),\n",
       " Document(page_content='372 10. Boosting and Additive Trees\\n0 200 400 600 8000.0 0.2 0.4 0.6 0.8\\nIterations MAbsolute ErrorTraining and Test Absolute Error\\nTrain Error\\nTest Error\\nFIGURE 10.13. Average-absolute error as a function of number of iterations\\nfor the California housing data.\\nFigure 10.14 displays the relative variable importances fo r each of the\\neight predictor variables. Not surprisingly, median incom e in the neigh-\\nborhood is the most relevant predictor. Longitude, latitud e, and average\\noccupancy all have roughly half the relevance of income, whe reas the others\\nare somewhat less inﬂuential.\\nFigure 10.15 shows single-variable partial dependence plo ts on the most\\nrelevantnonlocation predictors.Notethattheplotsareno tstrictlysmooth.\\nThis is a consequence of using tree-based models. Decision t rees produce\\ndiscontinuous piecewise constant models (10.25). This car ries over to sums\\nof trees (10.28), with of course many more pieces. Unlike mos t of the meth-\\nods discussed in this book, there is no smoothness constrain t imposed on\\nthe result. Arbitrarily sharp discontinuities can be model ed. The fact that\\nthese curves generally exhibit a smooth trend is because tha t is what is\\nestimated to best predict the response for this problem. Thi s is often the\\ncase.\\nThe hash marks at the base of each plot delineate the deciles o f the\\ndata distribution of the corresponding variables. Note tha t here the data\\ndensity is lower near the edges, especially for larger value s. This causes the\\ncurves to be somewhat less well determined in those regions. The vertical\\nscales of the plots are the same, and give a visual comparison of the relative\\nimportance of the diﬀerent variables.\\nThe partial dependence of median house value on median incom e is\\nmonotonicincreasing,beingnearlylinearoverthemainbod yofdata.House\\nvalue is generally monotonic decreasing with increasing av erage occupancy,\\nexcept perhaps for average occupancy rates less than one. Me dian house', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 390}),\n",
       " Document(page_content='10.14 Illustrations 373\\nMedIncLongitudeAveOccupLatitudeHouseAgeAveRoomsAveBedrmsPopulation\\n0 20 40 60 80 100\\nRelative importance\\nFIGURE 10.14. Relative importance of the predictors for the California hous ing\\ndata.\\nvalue has a nonmonotonic partial dependence on average numb er of rooms.\\nIt has a minimum at approximately three rooms and is increasi ng both for\\nsmaller and larger values.\\nMedian house value is seen to have a very weak partial depende nce on\\nhouse age that is inconsistent with its importance ranking ( Figure 10.14).\\nThis suggests that this weak main eﬀect may be masking strong er interac-\\ntioneﬀectswithothervariables.Figure10.16showsthetwo -variablepartial\\ndependence of housing value on joint values of median age and average oc-\\ncupancy. An interaction between these two variables is appa rent. For values\\nof average occupancy greater than two, house value is nearly independent\\nofmedianage,whereasforvalueslessthantwothereisastro ngdependence\\non age.\\nFigure 10.17 shows the two-variable partial dependence of t he ﬁtted\\nmodel on joint values of longitude and latitude, displayed a s a shaded\\ncontour plot. There is clearly a very strong dependence of me dian house\\nvalue on the neighborhood location in California. Note that Figure 10.17 is\\nnota plot of house value versus location ignoring the eﬀects of the other\\npredictors (10.49). Like all partial dependence plots, it r epresents the eﬀect\\nof location after accounting for the eﬀects of the other neig hborhood and\\nhouseattributes(10.47).Itcanbeviewedasrepresentinga nextrapremium\\none pays for location. This premium is seen to be relatively l arge near the\\nPaciﬁc coast especially in the Bay Area and Los Angeles–San D iego re-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 391}),\n",
       " Document(page_content='374 10. Boosting and Additive Trees\\nMedIncPartial Dependence\\n2 4 6 8 10-0.5 0.0 0.5 1.0 1.5 2.0\\nAveOccupPartial Dependence\\n2 3 4 5-1.0 -0.5 0.0 0.5 1.0 1.5\\nHouseAgePartial Dependence\\n10 20 30 40 50-1.0 -0.5 0.0 0.5 1.0\\nAveRoomsPartial Dependence\\n4 6 8 10-1.0 -0.5 0.0 0.5 1.0 1.5\\nFIGURE 10.15. Partial dependence of housing value on the nonlocation vari-\\nables for the California housing data. The red ticks at the base of the plot are\\ndeciles of the input variables.\\n2\\n3\\n4\\n510203040500.00.51.0\\nAveOccupHouseAge\\nFIGURE 10.16. Partial dependence of house value on median age and aver-\\nage occupancy. There appears to be a strong interaction eﬀec t between these two\\nvariables.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 392}),\n",
       " Document(page_content='10.14 Illustrations 375\\n−124 −122 −120 −118 −116 −11434 36 38 40 42\\nLongitudeLatitude\\n−1.0−0.5 0.0 0.5 1.0\\nFIGURE 10.17. Partial dependence of median house value on location in Cal-\\nifornia. One unit is $100,000, at1990prices, and the values plotted are relative\\nto the overall median of $180,000.\\ngions. In the northern, central valley, and southeastern de sert regions of\\nCalifornia, location costs considerably less.\\n10.14.2 New Zealand Fish\\nPlant and animal ecologists use regression models to predic t species pres-\\nence, abundance and richness as a function of environmental variables.\\nAlthough for many years simple linear and parametric models were popu-\\nlar, recent literature shows increasing interest in more so phisticated mod-\\nels such as generalized additive models (Section 9.1, GAM), multivariate\\nadaptive regression splines (Section 9.4, MARS) and booste d regression\\ntrees (Leathwick et al., 2005; Leathwick et al., 2006). Here we model the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 393}),\n",
       " Document(page_content='376 10. Boosting and Additive Trees\\npresence and abundance of the Black Oreo Dory , a marine ﬁsh found in the\\noceanic waters around New Zealand.3\\nFigure 10.18 shows the locations of 17,000 trawls (deep-wat er net ﬁshing,\\nwith a maximum depth of 2km), and the red points indicate thos e 2353\\ntrawls for which the Black Oreo was present, one of over a hund red species\\nregularly recorded. The catch size in kg for each species was recorded for\\neach trawl. Along with the species catch, a number of environ mental mea-\\nsurements are available for each trawl. These include the av erage depth of\\nthe trawl ( AvgDepth ), and the temperature and salinity of the water. Since\\nthe latter two are strongly correlated with depth, Leathwic k et al. (2006)\\nderived instead TempResid andSalResid , the residuals obtained when these\\ntwo measures are adjusted for depth (via separate non-param etric regres-\\nsions).SSTGradis a measure of the gradient of the sea surface temperature,\\nandChlais a broad indicator of ecosytem productivity via satellite -image\\nmeasurements. SusPartMatter provides a measure of suspended particulate\\nmatter, particularly in coastal waters, and is also satelli te derived.\\nThe goal of this analysis is to estimate the probability of ﬁn ding Black\\nOreo in a trawl, as well as the expected catch size, standardi zed to take\\ninto account the eﬀects of variation in trawl speed and dista nce, as well\\nas the mesh size of the trawl net. The authors used logistic re gression\\nfor estimating the probability. For the catch size, it might seem natural\\nto assume a Poisson distribution and model the log of the mean count,\\nbut this is often not appropriate because of the excessive nu mber of zeros.\\nAlthough specialized approaches have been developed, such as thezero-\\ninﬂatedPoisson (Lambert, 1992), they chose a simpler approach. If Yis\\nthe (non-negative) catch size,\\nE(Y|X) = E(Y|Y >0,X)·Pr(Y >0|X). (10.54)\\nThe second term is estimated by the logistic regression, and the ﬁrst term\\ncan be estimated using only the 2353 trawls with a positive ca tch.\\nFor the logistic regression the authors used a gradient boos ted model\\n(GBM)4with binomial deviance loss function, depth-10 trees, and a shrink-\\nage factor ν= 0.025. For the positive-catch regression, they modeled\\nlog(Y) using a GBM with squared-error loss (also depth-10 trees, b ut\\nν= 0.01), and un-logged the predictions. In both cases they used 1 0-fold\\ncross-validation for selecting the number of terms, as well as the shrinkage\\nfactor.\\n3The models, data, and maps shown here were kindly provided by Dr John Leathwick\\nof the National Institute of Water and Atmospheric Research in New Zeal and, and Dr\\nJane Elith, School of Botany, University of Melbourne. The collectio n of the research\\ntrawl data took place from 1979–2005, and was funded by the New Zea land Ministry of\\nFisheries.\\n4Version 1.5-7 of package gbmin R, ver. 2.2.0.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 394}),\n",
       " Document(page_content='10.14 Illustrations 377\\nFIGURE 10.18. Map of New Zealand and its surrounding exclusive economic\\nzone, showing the locations of 17,000 trawls (small blue dots) t aken between 1979\\nand 2005. The red points indicate trawls for which the species Black Oreo Dory\\nwere present.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 395}),\n",
       " Document(page_content='378 10. Boosting and Additive Trees\\n0 500 1000 15000.24 0.26 0.28 0.30 0.32 0.34\\nNumber of TreesMean DevianceGBM Test\\nGBM CV\\nGAM Test\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\\nSpecificitySensitivity\\nAUC\\nGAM 0.97\\nGBM 0.98\\nFIGURE 10.19. The left panel shows the mean deviance as a function of the\\nnumber of trees for the GBM logistic regression model ﬁt to the p resence/absence\\ndata. Shown are 10-fold cross-validation on the training data (and1×s.e. bars),\\nand test deviance on the test data. Also shown for comparison i s the test deviance\\nusing a GAM model with 8dffor each term. The right panel shows ROC curves\\non the test data for the chosen GBM model (vertical line in left plo t) and the\\nGAM model.\\nFigure 10.19 (left panel) shows the mean binomial deviance f or the se-\\nquence of GBM models, both for 10-fold CV and test data. There is a mod-\\nestimprovementovertheperformanceofaGAMmodel,ﬁtusing smoothing\\nsplines with 8 degrees-of-freedom (df) per term. The right p anel shows the\\nROC curves (see Section 9.2.5) for both models, which measur es predictive\\nperformance. From this point of view, the performance looks very simi-\\nlar, with GBM perhaps having a slight edge as summarized by th e AUC\\n(area under the curve). At the point of equal sensitivity/sp eciﬁcity, GBM\\nachieves 91%, and GAM 90%.\\nFigure 10.20 summarizes the contributions of the variables in the logistic\\nGBM ﬁt. We see that there is a well-deﬁned depth range over whi ch Black\\nOreo are caught, with much more frequent capture in colder wa ters. We do\\nnot give details of the quantitative catch model; the import ant variables\\nwere much the same.\\nAll the predictors used in these models are available on a ﬁne geographi-\\ncal grid; in fact they were derived from environmental atlas es, satellite im-\\nages and the like—see Leathwick et al. (2006) for details. Thi s also means\\nthat predictions can be made on this grid, and imported into G IS mapping\\nsystems. Figure 10.21 shows prediction maps for both presen ce and catch\\nsize, with both standardized to a common set of trawl conditi ons; since the\\npredictors vary in a continuous fashion with geographical l ocation, so do\\nthe predictions.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 396}),\n",
       " Document(page_content='10.14 Illustrations 379\\nOrbVelSpeedDistanceDisOrgMatterCodendSizePentadeTidalCurrSlopeChlaCase2SSTGradSalResidSusPartMatterAvgDepthTempResid\\nRelative influence0 10 25 −4 0 2 4 6−7 −5 −3 −1\\nTempResidf(TempResid)\\n0 500 1000 2000−6 −4 −2\\nAvgDepthf(AvgDepth)\\n0 5 10 15−7 −5 −3\\nSusPartMatterf(SusPartMatter)\\n−0.8 −0.4 0.0 0.4−7 −5 −3 −1\\nSalResidf(SalResid)\\n0.00 0.05 0.10 0.15−7 −5 −3 −1\\nSSTGradf(SSTGrad)\\nFIGURE 10.20. The top-left panel shows the relative inﬂuence computed from\\nthe GBM logistic regression model. The remaining panels show the partial de-\\npendence plots for the leading ﬁve variables, all plotted on the s ame scale for\\ncomparison.\\nBecause of their ability to model interactions and automati cally select\\nvariables, as well as robustness to outliers and missing dat a, GBM models\\narerapidlygainingpopularityinthisdata-richandenthus iasticcommunity.\\n10.14.3 Demographics Data\\nIn this section we illustrate gradient boosting on a multicl ass classiﬁca-\\ntion problem, using MART. The data come from 9243 questionna ires ﬁlled\\nout by shopping mall customers in the San Francisco Bay Area ( Impact\\nResources, Inc., Columbus, OH). Among the questions are 14 c oncerning\\ndemographics. For this illustration the goal is to predict o ccupation us-\\ning the other 13 variables as predictors, and hence identify demographic\\nvariables that discriminate between diﬀerent occupationa l categories. We\\nrandomly divided the data into a training set (80%) and test s et (20%),\\nand usedJ= 6 node trees with a learning rate ν= 0.1.\\nFigure 10.22 shows the K= 9 occupation class values along with their\\ncorresponding error rates. The overall error rate is 42.5%, which can be\\ncomparedtothenullrateof69%obtainedbypredictingthemo stnumerous', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 397}),\n",
       " Document(page_content='380 10. Boosting and Additive Trees\\nFIGURE 10.21. Geological prediction maps of the presence probability (left\\nmap) and catch size (right map) obtained from the gradient bo osted models.\\nclassProf/Man (Professional/Managerial). The four best predicted class es\\nare seen to be Retired,Student,Prof/Man , andHomemaker .\\nFigure 10.23 shows the relative predictor variable importa nces as aver-\\naged over all classes (10.46). Figure 10.24 displays the ind ividual relative\\nimportance distributions (10.45) for each of the four best p redicted classes.\\nOne sees that the most relevant predictors are generally diﬀ erent for each\\nrespective class. An exception is agewhich is among the threemost relevant\\nfor predicting Retired,Student, andProf/Man .\\nFigure 10.25 shows the partial dependence of the log-odds (1 0.52) on age\\nfor these three classes. The abscissa values are ordered cod es for respective\\nequally spaced age intervals. One sees that after accountin g for the contri-\\nbutions of the other variables, the odds of being retired are higher for older\\npeople, whereas the opposite is the case for being a student. The odds of\\nbeing professional/managerial are highest for middle-age d people. These\\nresults are of course not surprising. They illustrate that i nspecting partial\\ndependences separately for each class can lead to sensible r esults.\\nBibliographic Notes\\nSchapire (1990) developed the ﬁrst simple boosting procedu re in the PAC\\nlearning framework (Valiant, 1984; Kearns and Vazirani, 19 94). Schapire', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 398}),\n",
       " Document(page_content='10.14 Illustrations 381\\nSalesUnemployedMilitaryClericalLaborHomemakerProf/ManRetiredStudent\\n0.0 0.2 0.4 0.6 0.8 1.0\\nError RateOverall Error Rate = 0.425\\nFIGURE 10.22. Error rate for each occupation in the demographics data.\\nageincomeeduhsld-statmar-dlincsexethnicmar-stattyp-homelangnum-hsldchildrenyrs-BA\\n0 20 40 60 80 100\\nRelative Importance\\nFIGURE 10.23. Relative importance of the predictors as averaged over all\\nclasses for the demographics data.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 399}),\n",
       " Document(page_content='382 10. Boosting and Additive Trees\\nagemar-dlincsexethnicincomehsld-statmar-statlangtyp-homechildrenedunum-hsldyrs-BA\\n0 20 40 60 80 100\\nRelative ImportanceClass =  Retired\\nhsld-statageincomemar-stateduethnicnum-hsldtyp-homesexmar-dlinclangyrs-BAchildren\\n0 20 40 60 80 100\\nRelative ImportanceClass =  Student\\neduincomeagemar-dlincethnichsld-stattyp-homesexnum-hsldlangmar-statyrs-BAchildren\\n0 20 40 60 80 100\\nRelative ImportanceClass =  Prof/Man\\nsexmar-dlincchildrenethnicnum-hsldedumar-statlangtyp-homeincomeagehsld-statyrs-BA\\n0 20 40 60 80 100\\nRelative ImportanceClass =  Homemaker\\nFIGURE 10.24. Predictor variable importances separately for each of the fou r\\nclasses with lowest error rate for the demographics data.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 400}),\n",
       " Document(page_content='10.14 Illustrations 383\\nagePartial Dependence\\n1 2 3 4 5 6 70 1 2 3 4Retired\\nagePartial Dependence\\n1 2 3 4 5 6 7-2 -1 0 1 2Student\\nagePartial Dependence\\n1 2 3 4 5 6 7-2 -1 0 1 2Prof/Man\\nFIGURE 10.25. Partial dependence of the odds of three diﬀerent occupation s\\non age, for the demographics data.\\nshowed that a weak learner could always improve its performance by train-\\ning two additional classiﬁers on ﬁltered versions of the inp ut data stream.\\nA weak learner is an algorithm for producing a two-class clas siﬁer with\\nperformance guaranteed (with high probability) to be signi ﬁcantly better\\nthan a coin-ﬂip. After learning an initial classiﬁer G1on the ﬁrst Ntraining\\npoints,\\n•G2is learned on a new sample of Npoints, half of which are misclas-\\nsiﬁed byG1;\\n•G3is learned on Npoints for which G1andG2disagree;\\n•the boosted classiﬁer is GB=majority vote (G1,G2,G3).\\nSchapire’s “Strength of Weak Learnability” theorem proves thatGBhas\\nimproved performance over G1.\\nFreund (1995) proposed a “boost by majority” variation whic h combined\\nmany weak learners simultaneously and improved the perform ance of the\\nsimple boosting algorithm of Schapire. The theory supporti ng both of these', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 401}),\n",
       " Document(page_content='384 10. Boosting and Additive Trees\\nalgorithms requires the weak learner to produce a classiﬁer with a ﬁxed\\nerror rate. This led to the more adaptive and realistic AdaBo ost (Freund\\nand Schapire, 1996a) and its oﬀspring, where this assumptio n was dropped.\\nFreund and Schapire (1996a) and Schapire and Singer (1999) p rovide\\nsome theory to support their algorithms, in the form of upper bounds on\\ngeneralization error. This theory has evolved in the comput ational learning\\ncommunity, initially based on the concepts of PAC learning. Other theo-\\nries attempting to explain boosting come from game theory (F reund and\\nSchapire, 1996b; Breiman, 1999; Breiman, 1998), and VC theo ry (Schapire\\net al., 1998). The bounds and the theory associated with the A daBoost\\nalgorithms are interesting, but tend to be too loose to be of p ractical im-\\nportance. In practice, boosting achieves results far more i mpressive than\\nthe bounds would imply. Schapire (2002) and Meir and R¨ atsch (2003) give\\nuseful overviews more recent than the ﬁrst edition of this bo ok.\\nFriedman et al. (2000) and Friedman (2001) form the basis for our expo-\\nsitioninthischapter.Friedmanetal.(2000)analyzeAdaBo oststatistically,\\nderive the exponential criterion, and show that it estimate s the log-odds\\nof the class probability. They propose additive tree models , the right-sized\\ntrees and ANOVA representation of Section 10.11, and the mul ticlass logit\\nformulation. Friedman (2001) developed gradient boosting and shrinkage\\nfor classiﬁcation and regression, while Friedman (1999) ex plored stochastic\\nvariants ofboosting.Masonetal.(2000) alsoembracedagra dientapproach\\nto boosting. As the published discussions of Friedman et al. (2000) shows,\\nthere is some controversy about how and why boosting works.\\nSince the publication of the ﬁrst edition of this book, these debates have\\ncontinued, and spread into the statistical community with a series of papers\\non consistency of boosting (Jiang, 2004; Lugosi and Vayatis , 2004; Zhang\\nand Yu, 2005; Bartlett and Traskin, 2007). Mease and Wyner (2 008),\\nthrough a series of simulation examples, challenge some of o ur interpre-\\ntations of boosting; our response (Friedman et al., 2008a) p uts most of\\nthese objections to rest. A recent survey by B¨ uhlmann and Ho thorn (2007)\\nsupports our approach to boosting.\\nExercises\\nEx. 10.1 Derive expression (10.12) for the update parameter in AdaBo ost.\\nEx. 10.2 Prove result (10.16), that is, the minimizer of the populati on\\nversion of the AdaBoost criterion, is one-half of the log odd s.\\nEx. 10.3 Show that the marginal average (10.47) recovers additive an d\\nmultiplicative functions (10.50) and (10.51), while the co nditional expec-\\ntation (10.49) does not.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 402}),\n",
       " Document(page_content='Exercises 385\\nEx. 10.4\\n(a) Write a program implementing AdaBoost with trees.\\n(b) Redo the computations for the example of Figure 10.2. Plo t the train-\\ning error as well as test error, and discuss its behavior.\\n(c) Investigate the number of iterations needed to make the t est error\\nﬁnally start to rise.\\n(d) Change the setup of this example as follows: deﬁne two cla sses, with\\nthe features in Class 1 being X1,X2,...,X 10, standard indepen-\\ndent Gaussian variates. In Class 2, the features X1,X2,...,X 10are\\nalso standard independent Gaussian, but conditioned on the event∑\\njX2\\nj>12.Nowtheclasseshavesigniﬁcantoverlapinfeaturespace .\\nRepeat the AdaBoost experiments as in Figure 10.2 and discus s the\\nresults.\\nEx. 10.5 Multiclass exponential loss (Zhu et al., 2005). For a K-class clas-\\nsiﬁcation problem, consider the coding Y= (Y1,...,Y K)Twith\\nYk={1, ifG=Gk\\n−1\\nK−1,otherwise.(10.55)\\nLetf= (f1,...,f K)Twith∑K\\nk=1fk= 0, and deﬁne\\nL(Y,f) = exp(\\n−1\\nKYTf)\\n. (10.56)\\n(a) Using Lagrange multipliers, derive the population mini mizerf∗of\\nL(Y,f), subject to the zero-sum constraint, and relate these to th e\\nclass probabilities.\\n(b) Show that a multiclass boosting using this loss function leads to a\\nreweighting algorithm similar to Adaboost, as in Section 10 .4.\\nEx. 10.6 McNemar test (Agresti, 1996). We report the test error rates on\\nthe spam data to be 5.5% for a generalized additive model (GAM ), and\\n4.5% for gradient boosting (GBM), with a test sample of size 1 536.\\n(a) Show that the standard error of these estimates is about 0 .6%.\\nSince the same test data are used for both methods, the error r ates are\\ncorrelated, and we cannot perform a two-sample t-test. We ca n compare\\nthe methods directly on each test observation, leading to th e summary\\nGBM\\nGAM Correct Error\\nCorrect 1434 18\\nError 33 51', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 403}),\n",
       " Document(page_content='386 10. Boosting and Additive Trees\\nThe McNemar test focuses on the discordant errors, 33 vs. 18.\\n(b) Conduct a test to show that GAM makes signiﬁcantly more er rors\\nthan gradient boosting, with a two-sided p-value of 0 .036.\\nEx. 10.7 Derive expression (10.32).\\nEx. 10.8 Consider a K-class problem where the targets yikare coded as\\n1 if observation iis in class kand zero otherwise. Suppose we have a\\ncurrent model fk(x), k= 1,...,K, with∑K\\nk=1fk(x) = 0 (see (10.21) in\\nSection 10.6). We wish to update the model for observations i n a regionR\\nin predictor space, by adding constants fk(x)+γk, withγK= 0.\\n(a) Write down the multinomial log-likelihood for this prob lem, and its\\nﬁrst and second derivatives.\\n(b) Using only the diagonal of the Hessian matrix in (1), and s tarting\\nfromγk= 0∀k, show that a one-step approximate Newton update\\nforγkis\\nγ1\\nk=∑\\nxi∈R(yik−pik)∑\\nxi∈Rpik(1−pik), k= 1,...,K−1, (10.57)\\nwherepik= exp(fk(xi))/exp(∑K\\nℓ=1fℓ(xi)).\\n(c) We prefer our update to sum to zero, as the current model do es. Using\\nsymmetry arguments, show that\\nˆγk=K−1\\nK(γ1\\nk−1\\nKK∑\\nℓ=1γ1\\nℓ), k= 1,...,K (10.58)\\nis an appropriate update, where γ1\\nkis deﬁned as in (10.57) for all\\nk= 1,...,K.\\nEx. 10.9 Consider a K-class problem where the targets yikare coded as\\n1 if observation iis in classkand zero otherwise. Using the multinomial\\ndeviance loss function (10.22) and the symmetric logistic t ransform, use\\nthe arguments leading to the gradient boosting Algorithm 10 .3 to derive\\nAlgorithm 10.4. Hint: See exercise 10.8 for step 2(b)iii.\\nEx. 10.10 Show that for K= 2 class classiﬁcation, only one tree needs to\\nbe grown at each gradient-boosting iteration.\\nEx. 10.11 Show how to compute the partial dependence function fS(XS)\\nin (10.47) eﬃciently.\\nEx. 10.12 Referring to (10.49), let S={1}andC={2}, withf(X1,X2) =\\nX1. AssumeX1andX2are bivariate Gaussian, each with mean zero, vari-\\nance one, and E( X1X2) =ρ. Show that E( f(X1,X2)|X2) =ρX2, even\\nthoughfis not a function of X2.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 404}),\n",
       " Document(page_content='Exercises 387\\nAlgorithm 10.4 Gradient Boosting for K-class Classiﬁcation.\\n1. Initialize fk0(x) = 0, k= 1,2,...,K.\\n2. Form=1 toM:\\n(a) Set\\npk(x) =efk(x)\\n∑K\\nℓ=1efℓ(x), k= 1,2,...,K.\\n(b) Fork= 1 toK:\\ni. Compute rikm=yik−pk(xi), i= 1,2,...,N.\\nii. Fit a regression tree to the targets rikm, i= 1,2,...,N,\\ngiving terminal regions Rjkm, j= 1,2,...,J m.\\niii. Compute\\nγjkm=K−1\\nK∑\\nxi∈Rjkmrikm∑\\nxi∈Rjkm|rikm|(1−|rikm|), j= 1,2,...,J m.\\niv. Update fkm(x) =fk,m−1(x)+∑Jm\\nj=1γjkmI(x∈Rjkm).\\n3. Output ˆfk(x) =fkM(x), k= 1,2,...,K.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 405}),\n",
       " Document(page_content='388 10. Boosting and Additive Trees', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 406}),\n",
       " Document(page_content='This is page 389\\nPrinter: Opaque this\\n11\\nNeural Networks\\n11.1 Introduction\\nIn this chapter we describe a class of learning methods that w as developed\\nseparately in diﬀerent ﬁelds—statistics and artiﬁcial inte lligence—based\\non essentially identical models. The central idea is to extr act linear com-\\nbinations of the inputs as derived features, and then model t he target as\\na nonlinear function of these features. The result is a power ful learning\\nmethod, with widespread applications in many ﬁelds. We ﬁrst discuss the\\nprojection pursuit model, which evolved in the domain of sem iparamet-\\nric statistics and smoothing. The rest of the chapter is devo ted to neural\\nnetwork models.\\n11.2 Projection Pursuit Regression\\nAs in our generic supervised learning problem, assume we hav e an input\\nvectorXwithpcomponents, and a target Y. Letωm, m= 1,2,...,M, be\\nunitp-vectors of unknown parameters. The projection pursuit reg ression\\n(PPR) model has the form\\nf(X) =M∑\\nm=1gm(ωT\\nmX). (11.1)\\nThis is an additive model, but in the derived features Vm=ωT\\nmXrather\\nthan the inputs themselves. The functions gmare unspeciﬁed and are esti-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 407}),\n",
       " Document(page_content='390 Neural Networks\\ng(V)\\nX1X2g(V)\\nX1X2\\nFIGURE 11.1. Perspective plots of two ridge functions.\\n(Left:)g(V) = 1/[1+exp( −5(V−0.5))], whereV= (X1+X2)/√\\n2.\\n(Right:)g(V) = (V+0.1)sin(1/(V/3+0.1)), whereV=X1.\\nmated along with the directions ωmusing some ﬂexible smoothing method\\n(see below).\\nThe function gm(ωT\\nmX) is called a ridge function in IRp. It varies only\\nin the direction deﬁned by the vector ωm. The scalar variable Vm=ωT\\nmX\\nis the projection of Xonto the unit vector ωm, and we seek ωmso that\\nthe model ﬁts well, hence the name “projection pursuit.” Fig ure 11.1 shows\\nsomeexamplesofridgefunctions.Intheexampleontheleft ω= (1/√\\n2)(1,1)T,\\nso that the function only varies in the direction X1+X2. In the example\\non the right, ω= (1,0).\\nThe PPR model (11.1) is very general, since the operation of f orming\\nnonlinear functions of linear combinations generates a sur prisingly large\\nclass of models. For example, the product X1·X2can be written as [( X1+\\nX2)2−(X1−X2)2]/4, and higher-order products can be represented simi-\\nlarly.\\nIn fact, ifMis taken arbitrarily large, for appropriate choice of gmthe\\nPPR model can approximate any continuous function in IRparbitrarily\\nwell. Such a class of models is called a universal approximator . However\\nthis generality comes at a price. Interpretation of the ﬁtte d model is usually\\ndiﬃcult, because each input enters into the model in a comple x and multi-\\nfaceted way. As a result, the PPR model is most useful for pred iction, and\\nnot very useful for producing an understandable model for th e data. The\\nM= 1 model, known as the single index model in econometrics, is an\\nexception. It is slightly more general than the linear regre ssion model, and\\noﬀers a similar interpretation.\\nHow do we ﬁt a PPR model, given training data ( xi,yi),i= 1,2,...,N?\\nWe seek the approximate minimizers of the error function\\nN∑\\ni=1[\\nyi−M∑\\nm=1gm(ωT\\nmxi)]2\\n(11.2)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 408}),\n",
       " Document(page_content='11.2 Projection Pursuit Regression 391\\nover functions gmand direction vectors ωm,m= 1,2,...,M. As in other\\nsmoothing problems, we need either explicitly or implicitl y to impose com-\\nplexity constraints on the gm, to avoid overﬁt solutions.\\nConsider just one term ( M= 1, and drop the subscript). Given the\\ndirection vector ω, we form the derived variables vi=ωTxi. Then we have\\na one-dimensional smoothing problem, and we can apply any sc atterplot\\nsmoother, such as a smoothing spline, to obtain an estimate o fg.\\nOn the other hand, given g, we want to minimize (11.2) over ω. A Gauss–\\nNewton search is convenient for this task. This is a quasi-Ne wton method,\\nin which the part of the Hessian involving the second derivat ive ofgis\\ndiscarded. It can be simply derived as follows. Let ωoldbe the current\\nestimate for ω. We write\\ng(ωTxi)≈g(ωT\\noldxi)+g′(ωT\\noldxi)(ω−ωold)Txi (11.3)\\nto give\\nN∑\\ni=1[\\nyi−g(ωTxi)]2≈N∑\\ni=1g′(ωT\\noldxi)2[(\\nωT\\noldxi+yi−g(ωT\\noldxi)\\ng′(ωT\\noldxi))\\n−ωTxi]2\\n.\\n(11.4)\\nTo minimize the right-hand side, we carry out a least squares regression\\nwith target ωT\\noldxi+(yi−g(ωT\\noldxi))/g′(ωT\\noldxi) on the input xi, with weights\\ng′(ωT\\noldxi)2and no intercept (bias) term. This produces the updated coef -\\nﬁcient vector ωnew.\\nThese two steps, estimation of gandω, are iterated until convergence.\\nWith more than one term in the PPR model, the model is built in a forward\\nstage-wise manner, adding a pair ( ωm,gm) at each stage.\\nThere are a number of implementation details.\\n•Although any smoothing method can in principle be used, it is conve-\\nnientifthemethodprovidesderivatives.Localregression andsmooth-\\ning splines are convenient.\\n•After each step the gm’s from previous steps can be readjusted using\\nthe backﬁtting procedure described in Chapter 9. While this may\\nlead ultimately to fewer terms, it is not clear whether it imp roves\\nprediction performance.\\n•Usually the ωmare not readjusted (partly to avoid excessive compu-\\ntation), although in principle they could be as well.\\n•The number of terms Mis usually estimated as part of the forward\\nstage-wise strategy. The model building stops when the next term\\ndoes not appreciably improve the ﬁt of the model. Cross-vali dation\\ncan also be used to determine M.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 409}),\n",
       " Document(page_content='392 Neural Networks\\nThere are many other applications, such as density estimati on (Friedman\\netal.,1984;Friedman,1987),wheretheprojectionpursuit ideacanbeused.\\nIn particular, see the discussion of ICA in Section 14.7 and i ts relationship\\nwith exploratory projection pursuit. However the projecti on pursuit re-\\ngression model has not been widely used in the ﬁeld of statist ics, perhaps\\nbecause at the time of its introduction (1981), its computat ional demands\\nexceeded the capabilities of most readily available comput ers. But it does\\nrepresent an important intellectual advance, one that has b lossomed in its\\nreincarnation in the ﬁeld of neural networks, the topic of th e rest of this\\nchapter.\\n11.3 Neural Networks\\nThe term neural network has evolved to encompass a large class of models\\nandlearningmethods.Herewedescribethemostwidelyused“ vanilla”neu-\\nral net, sometimes called the single hidden layer back-prop agation network,\\nor single layer perceptron. There has been a great deal of hypesurrounding\\nneural networks, making them seem magical and mysterious. A s we make\\nclear in this section, they are just nonlinear statistical m odels, much like\\nthe projection pursuit regression model discussed above.\\nA neural network is a two-stage regression or classiﬁcation model, typ-\\nically represented by a network diagram as in Figure 11.2. This network\\napplies both to regression or classiﬁcation. For regressio n, typically K= 1\\nand there is only one output unit Y1at the top. However, these networks\\ncan handle multiple quantitative responses in a seamless fa shion, so we will\\ndeal with the general case.\\nForK-class classiﬁcation, there are Kunits at the top, with the kth\\nunit modeling the probability of class k. There are Ktarget measurements\\nYk, k= 1,...,K, each being coded as a 0 −1 variable for the kth class.\\nDerived features Zmare created from linear combinations of the inputs,\\nand then the target Ykis modeled as a function of linear combinations of\\ntheZm,\\nZm=σ(α0m+αT\\nmX), m= 1,...,M,\\nTk=β0k+βT\\nkZ, k= 1,...,K,\\nfk(X) =gk(T), k= 1,...,K,(11.5)\\nwhereZ= (Z1,Z2,...,Z M), andT= (T1,T2,...,T K).\\nThe activation function σ(v) is usually chosen to be the sigmoidσ(v) =\\n1/(1+e−v); see Figure 11.3 for a plot of 1 /(1+e−v). Sometimes Gaussian\\nradial basis functions (Chapter 6) are used for the σ(v), producing what is\\nknown as a radial basis function network .\\nNeural network diagrams like Figure 11.2 are sometimes draw n with an\\nadditional biasunit feeding into every unit in the hidden and output layers.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 410}),\n",
       " Document(page_content='11.3 Neural Networks 393\\n Y  Y Y 2 1 K\\n Z  Z  Z1 Z2 3 m\\n X  X Z  Z1 Z2 3\\n1  Xp  X p-1  X2  X3M\\n X p-1 3 X 2 X 1p Z Y  Y Y\\n XK 1 2\\n                                                                                                                                                /0/0/1/1/0/0/1/1/0/0/1/1/0/0/1/1/0/0/1/1/0/0/1/1\\nFIGURE 11.2. Schematic of a single hidden layer, feed-forward neural netwo rk.\\nThinking of the constant “1” as an additional input feature, this bias unit\\ncaptures the intercepts α0mandβ0kin model (11.5).\\nThe output function gk(T) allows a ﬁnal transformation of the vector of\\noutputsT. For regression we typically choose the identity function gk(T) =\\nTk. Early work in K-class classiﬁcation also used the identity function, but\\nthis was later abandoned in favor of the softmaxfunction\\ngk(T) =eTk\\n∑K\\nℓ=1eTℓ. (11.6)\\nThis is of course exactly the transformation used in the mult ilogit model\\n(Section 4.4), and produces positive estimates that sum to o ne. In Sec-\\ntion 4.2 we discuss other problems with linear activation fu nctions, in par-\\nticular potentially severe masking eﬀects.\\nThe units in the middle of the network, computing the derived features\\nZm, are called hidden units because the values Zmare not directly ob-\\nserved. In general there can be more than one hidden layer, as illustrated\\nin the example at the end of this chapter. We can think of the Zmas a\\nbasis expansion of the original inputs X; the neural network is then a stan-\\ndard linear model, or linear multilogit model, using these t ransformations\\nas inputs. There is, however, an important enhancement over the basis-\\nexpansion techniques discussed in Chapter 5; here the param eters of the\\nbasis functions are learned from the data.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 411}),\n",
       " Document(page_content='394 Neural Networks\\n-10 -5 0 5 100.0 0.5 1.01/(1+e−v)\\nv\\nFIGURE 11.3. Plot of the sigmoid function σ(v) = 1/(1+exp( −v))(red curve),\\ncommonly used in the hidden layer of a neural network. Included a reσ(sv)for\\ns=1\\n2(blue curve) and s= 10(purple curve). The scale parameter scontrols\\nthe activation rate, and we can see that large samounts to a hard activation at\\nv= 0. Note that σ(s(v−v0))shifts the activation threshold from 0tov0.\\nNotice that if σis the identity function, then the entire model collapses\\nto a linear model in the inputs. Hence a neural network can be t hought of\\nas a nonlinear generalization of the linear model, both for r egression and\\nclassiﬁcation. By introducing the nonlinear transformati onσ, it greatly\\nenlarges the class of linear models. In Figure 11.3 we see tha t the rate of\\nactivation of the sigmoid depends on the norm of αm, and if∥αm∥is very\\nsmall, the unit will indeed be operating in the linear part of its activation\\nfunction.\\nNotice also that the neural network model with one hidden lay er has\\nexactly the same form as the projection pursuit model descri bed above.\\nThe diﬀerence is that the PPR model uses nonparametric funct ionsgm(v),\\nwhile the neural network uses a far simpler function based on σ(v), with\\nthree free parameters in its argument. In detail, viewing th e neural network\\nmodel as a PPR model, we identify\\ngm(ωT\\nmX) =βmσ(α0m+αT\\nmX)\\n=βmσ(α0m+∥αm∥(ωT\\nmX)), (11.7)\\nwhereωm=αm/∥αm∥is themth unit-vector. Since σβ,α0,s(v) =βσ(α0+\\nsv) has lower complexity than a more general nonparametric g(v), it is not\\nsurprising that a neural network might use 20 or 100 such func tions, while\\nthe PPR model typically uses fewer terms ( M= 5 or 10, for example).\\nFinally, we note that the name “neural networks” derives fro m the fact\\nthat they were ﬁrst developed as models for the human brain. E ach unit\\nrepresents a neuron, and the connections (links in Figure 11 .2) represent\\nsynapses. In early models, the neurons ﬁred when the total si gnal passed to\\nthatunitexceededacertainthreshold.Inthemodelabove,t hiscorresponds', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 412}),\n",
       " Document(page_content='11.4 Fitting Neural Networks 395\\nto use of a step function for σ(Z) andgm(T). Later the neural network was\\nrecognized as a useful tool for nonlinear statistical model ing, and for this\\npurpose the step function is not smooth enough for optimizat ion. Hence the\\nstep function was replaced by a smoother threshold function , the sigmoid\\nin Figure 11.3.\\n11.4 Fitting Neural Networks\\nThe neural network model has unknown parameters, often call edweights,\\nand we seek values for them that make the model ﬁt the training data well.\\nWe denote the complete set of weights by θ, which consists of\\n{α0m,αm;m= 1,2,...,M}M(p+1) weights ,\\n{β0k,βk;k= 1,2,...,K}K(M+1) weights .(11.8)\\nFor regression, we use sum-of-squared errors as our measure of ﬁt (error\\nfunction)\\nR(θ) =K∑\\nk=1N∑\\ni=1(yik−fk(xi))2. (11.9)\\nFor classiﬁcation we use either squared error or cross-entr opy (deviance):\\nR(θ) =−N∑\\ni=1K∑\\nk=1yiklogfk(xi), (11.10)\\nand the corresponding classiﬁer is G(x) = argmaxkfk(x). With the softmax\\nactivationfunctionandthecross-entropyerrorfunction, theneuralnetwork\\nmodel is exactly a linear logistic regression model in the hi dden units, and\\nall the parameters are estimated by maximum likelihood.\\nTypically we don’t want the global minimizer of R(θ), as this is likely\\nto be an overﬁt solution. Instead some regularization is nee ded: this is\\nachieved directly through a penalty term, or indirectly by e arly stopping.\\nDetails are given in the next section.\\nThe generic approach to minimizing R(θ) is by gradient descent, called\\nback-propagation in this setting. Because of the compositional form of the\\nmodel, the gradient can be easily derived using the chain rul e for diﬀeren-\\ntiation. This can be computed by a forward and backward sweep over the\\nnetwork, keeping track only of quantities local to each unit .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 413}),\n",
       " Document(page_content='396 Neural Networks\\nHere is back-propagation in detail for squared error loss. L etzmi=\\nσ(α0m+αT\\nmxi), from (11.5) and let zi= (z1i,z2i,...,z Mi). Then we have\\nR(θ)≡N∑\\ni=1Ri\\n=N∑\\ni=1K∑\\nk=1(yik−fk(xi))2, (11.11)\\nwith derivatives\\n∂Ri\\n∂βkm=−2(yik−fk(xi))g′\\nk(βT\\nkzi)zmi,\\n∂Ri\\n∂αmℓ=−K∑\\nk=12(yik−fk(xi))g′\\nk(βT\\nkzi)βkmσ′(αT\\nmxi)xiℓ.(11.12)\\nGiven these derivatives, a gradient descent update at the ( r+1)st iter-\\nation has the form\\nβ(r+1)\\nkm=β(r)\\nkm−γrN∑\\ni=1∂Ri\\n∂β(r)\\nkm,\\nα(r+1)\\nmℓ=α(r)\\nmℓ−γrN∑\\ni=1∂Ri\\n∂α(r)\\nmℓ,(11.13)\\nwhereγris thelearning rate , discussed below.\\nNow write (11.12) as\\n∂Ri\\n∂βkm=δkizmi,\\n∂Ri\\n∂αmℓ=smixiℓ.(11.14)\\nThe quantities δkiandsmiare “errors” from the current model at the\\noutput and hidden layer units, respectively. From their deﬁ nitions, these\\nerrors satisfy\\nsmi=σ′(αT\\nmxi)K∑\\nk=1βkmδki, (11.15)\\nknown as the back-propagation equations . Using this, the updates in (11.13)\\ncan be implemented with a two-pass algorithm. In the forward pass , the\\ncurrent weights are ﬁxed and the predicted values ˆfk(xi) are computed\\nfrom formula (11.5). In the backward pass , the errors δkiare computed,\\nand then back-propagated via (11.15) to give the errors smi. Both sets of\\nerrors are then used to compute the gradients for the updates in (11.13),\\nvia (11.14).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 414}),\n",
       " Document(page_content='11.5 Some Issues in Training Neural Networks 397\\nThis two-pass procedure is what is known as back-propagatio n. It has\\nalso been called the delta rule (Widrow and Hoﬀ, 1960). The computational\\ncomponents for cross-entropy have the same form as those for the sum of\\nsquares error function, and are derived in Exercise 11.3.\\nThe advantages of back-propagation are its simple, local na ture. In the\\nback propagation algorithm, each hidden unit passes and rec eives infor-\\nmation only to and from units that share a connection. Hence i t can be\\nimplemented eﬃciently on a parallel architecture computer .\\nThe updates in (11.13) are a kind of batch learning , with the parame-\\nter updates being a sum over all of the training cases. Learni ng can also\\nbe carried out online—processing each observation one at a ti me, updat-\\ning the gradient after each training case, and cycling throu gh the training\\ncases many times. In this case, the sums in equations (11.13) are replaced\\nby a single summand. A training epoch refers to one sweep through the\\nentire training set. Online training allows the network to h andle very large\\ntraining sets, and also to update the weights as new observat ions come in.\\nThe learning rate γrfor batch learning is usually taken to be a con-\\nstant, and can also be optimized by a line search that minimiz es the error\\nfunction at each update. With online learning γrshould decrease to zero\\nas the iteration r→∞. This learning is a form of stochastic approxima-\\ntion(Robbins and Munro, 1951); results in this ﬁeld ensure conve rgence if\\nγr→0,∑\\nrγr=∞, and∑\\nrγ2\\nr<∞(satisﬁed, for example, by γr= 1/r).\\nBack-propagation can be very slow, and for that reason is usu ally not\\nthe method of choice. Second-order techniques such as Newto n’s method\\nare not attractive here, because the second derivative matr ix ofR(the\\nHessian) can be very large. Better approaches to ﬁtting incl ude conjugate\\ngradients and variable metric methods. These avoid explici t computation\\nof the second derivative matrix while still providing faste r convergence.\\n11.5 Some Issues in Training Neural Networks\\nThere is quite an art in training neural networks. The model i s generally\\noverparametrized, and the optimization problem is nonconv ex and unstable\\nunless certain guidelines are followed. In this section we s ummarize some\\nof the important issues.\\n11.5.1 Starting Values\\nNotethatiftheweightsarenearzero,thentheoperativepar tofthesigmoid\\n(Figure 11.3) is roughly linear, and hence the neural networ k collapses into\\nan approximately linear model (Exercise 11.2). Usually sta rting values for\\nweights are chosen to be random values near zero. Hence the mo del starts\\nout nearly linear, and becomes nonlinear as the weights incr ease. Individual', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 415}),\n",
       " Document(page_content='398 Neural Networks\\nunits localize to directions and introduce nonlinearities where needed. Use\\nof exact zero weights leads to zero derivatives and perfect s ymmetry, and\\nthe algorithm never moves. Starting instead with large weig hts often leads\\nto poor solutions.\\n11.5.2 Overﬁtting\\nOften neural networks have too many weights and will overﬁt t he data at\\nthe global minimum of R. In early developments of neural networks, either\\nby design or by accident, an early stopping rule was used to av oid over-\\nﬁtting. Here we train the model only for a while, and stop well before we\\napproach the global minimum. Since the weights start at a hig hly regular-\\nized (linear) solution, this has the eﬀect of shrinking the ﬁ nal model toward\\na linear model. A validation dataset is useful for determini ng when to stop,\\nsince we expect the validation error to start increasing.\\nA more explicit method for regularization is weight decay , which is anal-\\nogous to ridge regression used for linear models (Section 3. 4.1). We add a\\npenalty to the error function R(θ)+λJ(θ), where\\nJ(θ) =∑\\nk,mβ2\\nkm+∑\\nm,ℓα2\\nmℓ (11.16)\\nandλ≥0 is a tuning parameter. Larger values of λwill tend to shrink\\nthe weights toward zero: typically cross-validation is use d to estimate λ.\\nThe eﬀect of the penalty is to simply add terms 2 βkmand 2αmℓto the\\nrespective gradient expressions (11.13). Other forms for t he penalty have\\nbeen proposed, for example,\\nJ(θ) =∑\\nk,mβ2\\nkm\\n1+β2\\nkm+∑\\nm,ℓα2\\nmℓ\\n1+α2\\nmℓ, (11.17)\\nknown as the weight elimination penalty. This has the eﬀect of shrinking\\nsmaller weights more than (11.16) does.\\nFigure 11.4 shows the result of training a neural network wit h ten hidden\\nunits, without weight decay (upper panel) and with weight de cay (lower\\npanel), to the mixture example of Chapter 2. Weight decay has clearly\\nimproved the prediction. Figure 11.5 shows heat maps of the e stimated\\nweights from the training (grayscale versions of these are c alledHinton\\ndiagrams. ) We see that weight decay has dampened the weights in both\\nlayers: the resulting weights are spread fairly evenly over the ten hidden\\nunits.\\n11.5.3 Scaling of the Inputs\\nSincethescalingoftheinputsdeterminestheeﬀectivescal ingoftheweights\\nin the bottom layer, it can have a large eﬀect on the quality of the ﬁnal', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 416}),\n",
       " Document(page_content='11.5 Some Issues in Training Neural Networks 399\\nNeural Network - 10 Units, No Weight Decay', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 417}),\n",
       " Document(page_content='. . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 417}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 417}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 417}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . .. . .. . .. . .. . .. . .. . . .. . . .. . . .. . . .. . . .. . . .. . . .. . . . .. . . . .. . . . .. . . . .. . . . .. . . . .. . . .. . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 417}),\n",
       " Document(page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\nooooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.100\\nTest Error:       0.259\\nBayes Error:    0.210\\nNeural Network - 10 Units, Weight Decay=0.02', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 417}),\n",
       " Document(page_content='. .. .. .. .. . .. . .. . .. . .. . . .. . . .. . . .. . . .. . . . .. . . . .. . . . .. . . . . .. . . . . .. . . . . .. . . . . . .. . . . . . .. . . . . . .. . . . . . . .. . . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 417}),\n",
       " Document(page_content='. . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 417}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 417}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 417}),\n",
       " Document(page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\nooooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.160\\nTest Error:       0.223\\nBayes Error:    0.210\\nFIGURE 11.4. A neural network on the mixture example of Chapter 2. The\\nupper panel uses no weight decay, and overﬁts the training da ta. The lower panel\\nuses weight decay, and achieves close to the Bayes error rate ( broken purple\\nboundary). Both use the softmax activation function and cro ss-entropy error.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 417}),\n",
       " Document(page_content='400 Neural Networks\\n1 1\\n11\\nx1 x1x2 x2y1 y1y2 y2\\nz1z1\\nz1z1\\nz2z2\\nz2z2\\nz3z3\\nz3z3\\nz1z1\\nz1z1\\nz5z5\\nz5z5\\nz6z6\\nz6z6\\nz7z7\\nz7z7\\nz8z8\\nz8z8\\nz9z9\\nz9z9\\nz10z10\\nz10z10No weight decay Weight decay\\nFIGURE 11.5. Heat maps of the estimated weights from the training of neura l\\nnetworks from Figure 11.4. The display ranges from bright gre en (negative) to\\nbright red (positive).\\nsolution. At the outset it is best to standardize all inputs t o have mean zero\\nand standard deviation one. This ensures all inputs are trea ted equally in\\nthe regularization process, and allows one to choose a meani ngful range for\\nthe random starting weights. With standardized inputs, it i s typical to take\\nrandom uniform weights over the range [ −0.7,+0.7].\\n11.5.4 Number of Hidden Units and Layers\\nGenerally speaking it is better to have too many hidden units than too few.\\nWith too few hidden units, the model might not have enough ﬂex ibility to\\ncapture the nonlinearities in the data; with too many hidden units, the\\nextra weights can be shrunk toward zero if appropriate regul arization is\\nused. Typically the number of hidden units is somewhere in th e range of\\n5 to 100, with the number increasing with the number of inputs and num-\\nber of training cases. It is most common to put down a reasonab ly large\\nnumber of units and train them with regularization. Some res earchers use\\ncross-validation to estimate the optimal number, but this s eems unneces-\\nsary if cross-validation is used to estimate the regulariza tion parameter.\\nChoice of the number of hidden layers is guided by background knowledge\\nand experimentation. Each layer extracts features of the in put for regres-\\nsion or classiﬁcation. Use of multiple hidden layers allows construction of\\nhierarchical features at diﬀerent levels of resolution. An example of the\\neﬀective use of multiple layers is given in Section 11.6.\\n11.5.5 Multiple Minima\\nThe error function R(θ) is nonconvex, possessing many local minima. As a\\nresult, the ﬁnal solution obtained is quite dependent on the choice of start-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 418}),\n",
       " Document(page_content='11.6 Example: Simulated Data 401\\ning weights. One must at least try a number of random starting conﬁgura-\\ntions, and choose the solution giving lowest (penalized) er ror. Probably a\\nbetter approach is to use the average predictions over the co llection of net-\\nworks as the ﬁnal prediction (Ripley, 1996). This is prefera ble to averaging\\nthe weights, since the nonlinearity of the model implies tha t this averaged\\nsolution could be quite poor. Another approach is via bagging, which aver-\\nages the predictions of networks training from randomly per turbed versions\\nof the training data. This is described in Section 8.7.\\n11.6 Example: Simulated Data\\nWe generated data from two additive error models Y=f(X)+ε:\\nSum of sigmoids: Y=σ(aT\\n1X)+σ(aT\\n2X)+ε1;\\nRadial:Y=10∏\\nm=1φ(Xm)+ε2.\\nHereXT= (X1,X2,...,X p), eachXjbeing a standard Gaussian variate,\\nwithp= 2 in the ﬁrst model, and p= 10 in the second.\\nFor the sigmoid model, a1= (3,3), a2= (3,−3); for the radial model,\\nφ(t) = (1/2π)1/2exp(−t2/2). Bothε1andε2are Gaussian errors, with\\nvariance chosen so that the signal-to-noise ratio\\nVar(E(Y|X))\\nVar(Y−E(Y|X))=Var(f(X))\\nVar(ε)(11.18)\\nis 4 in both models. We took a training sample of size 100 and a t est sample\\nof size 10,000. We ﬁt neural networks with weight decay and various num-\\nbers of hidden units, and recorded the average test error E Test(Y−ˆf(X))2\\nfor each of 10 random starting weights. Only one training set was gen-\\nerated, but the results are typical for an “average” trainin g set. The test\\nerrors are shown in Figure 11.6. Note that the zero hidden uni t model refers\\nto linear least squares regression. The neural network is pe rfectly suited to\\nthe sum of sigmoids model, and the two-unit model does perfor m the best,\\nachieving an error close to the Bayes rate. (Recall that the B ayes rate for\\nregression with squared error is the error variance; in the ﬁ gures, we report\\ntest error relative to the Bayes error). Notice, however, th at with more hid-\\nden units, overﬁtting quickly creeps in, and with some start ing weights the\\nmodel does worse than the linear model (zero hidden unit) mod el. Even\\nwith two hidden units, two of the ten starting weight conﬁgur ations pro-\\nduced results no better than the linear model, conﬁrming the importance\\nof multiple starting values.\\nA radial function is in a sense the most diﬃcult for the neural net, as it is\\nspherically symmetric and with no preferred directions. We see in the right', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 419}),\n",
       " Document(page_content='402 Neural Networks\\n1.0 1.5 2.0 2.5 3.0\\n0 1 2 3 4 5 6 7 8 9 10\\nNumber of Hidden UnitsTest ErrorSum of Sigmoids\\n0 5 10 15 20 25 30\\n0 1 2 3 4 5 6 7 8 9 10\\nNumber of Hidden UnitsTest ErrorRadial\\nFIGURE 11.6. Boxplots of test error, for simulated data example, relative to\\nthe Bayes error (broken horizontal line). True function is a s um of two sigmoids\\non the left, and a radial function is on the right. The test erro r is displayed for\\n10diﬀerent starting weights, for a single hidden layer neural ne twork with the\\nnumber of units as indicated.\\npanel of Figure 11.6 that it does poorly in this case, with the test error\\nstaying well above the Bayes error (note the diﬀerent vertic al scale from\\nthe left panel). In fact, since a constant ﬁt (such as the samp le average)\\nachieves a relative error of 5 (when the SNR is 4), we see that t he neural\\nnetworks perform increasingly worse than the mean.\\nIn this example we used a ﬁxed weight decay parameter of 0 .0005, rep-\\nresenting a mild amount of regularization. The results in th e left panel of\\nFigure 11.6 suggest that more regularization is needed with greater num-\\nbers of hidden units.\\nInFigure11.7werepeatedtheexperimentforthesumofsigmo idsmodel,\\nwith no weight decay in the left panel, and stronger weight de cay (λ= 0.1)\\nin the right panel. With no weight decay, overﬁtting becomes even more\\nsevere for larger numbers of hidden units. The weight decay v alueλ= 0.1\\nproduces good results for all numbers of hidden units, and th ere does not\\nappeartobeoverﬁttingasthenumberofunitsincrease.Fina lly,Figure11.8\\nshows the test error for a ten hidden unit network, varying th e weight decay\\nparameter over a wide range. The value 0 .1 is approximately optimal.\\nIn summary, there are two free parameters to select: the weig ht decayλ\\nand number of hidden units M. As a learning strategy, one could ﬁx either\\nparameter at the value corresponding to the least constrain ed model, to\\nensure that the model is rich enough, and use cross-validati on to choose\\ntheotherparameter.Heretheleastconstrainedvaluesarez eroweightdecay\\nand ten hidden units. Comparing the left panel of Figure 11.7 to Figure\\n11.8, we see that the test error is less sensitive to the value of the weight', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 420}),\n",
       " Document(page_content='11.6 Example: Simulated Data 4031.0 1.5 2.0 2.5 3.0\\n0 1 2 3 4 5 6 7 8 9 10\\nNumber of Hidden UnitsTest ErrorNo Weight Decay\\n1.0 1.5 2.0 2.5 3.0\\n0 1 2 3 4 5 6 7 8 9 10\\nNumber of Hidden UnitsTest ErrorWeight Decay=0.1\\nFIGURE 11.7. Boxplots of test error, for simulated data example, relative to t he\\nBayes error. True function is a sum of two sigmoids. The test e rror is displayed\\nfor ten diﬀerent starting weights, for a single hidden layer ne ural network with\\nthe number units as indicated. The two panels represent no wei ght decay (left)\\nand strong weight decay λ= 0.1(right).1.0 1.2 1.4 1.6 1.8 2.0 2.2\\n0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14\\nWeight Decay ParameterTest ErrorSum of Sigmoids, 10 Hidden Unit Model\\nFIGURE 11.8. Boxplots of test error, for simulated data example. True functi on\\nis a sum of two sigmoids. The test error is displayed for ten diﬀ erent starting\\nweights, for a single hidden layer neural network with ten hidd en units and weight\\ndecay parameter value as indicated.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 421}),\n",
       " Document(page_content='404 Neural Networks\\nFIGURE 11.9. Examples of training cases from ZIP code data. Each image is\\na16×16 8-bit grayscale representation of a handwritten digit.\\ndecay parameter, and hence cross-validation of this parame ter would be\\npreferred.\\n11.7 Example: ZIP Code Data\\nThis example is a character recognition task: classiﬁcatio n of handwritten\\nnumerals. This problem captured the attention of the machin e learning and\\nneural network community for many years, and has remained a b enchmark\\nproblem in the ﬁeld. Figure 11.9 shows some examples of norma lized hand-\\nwritten digits, automatically scanned from envelopes by th e U.S. Postal\\nService. The original scanned digits are binary and of diﬀer ent sizes and\\norientations; the images shown here have been deslanted and size normal-\\nized, resulting in 16 ×16 grayscale images (Le Cun et al., 1990). These 256\\npixel values are used as inputs to the neural network classiﬁ er.\\nAblack box neural network is not ideally suited to this pattern recogni -\\ntion task, partly because the pixel representation of the im ages lack certain\\ninvariances (such as small rotations of the image). Consequ ently early at-\\ntempts with neural networks yielded misclassiﬁcation rate s around 4.5%\\non various examples of the problem. In this section we show so me of the\\npioneering eﬀorts to handcraft the neural network to overco me some these\\ndeﬁciencies (Le Cun, 1989), which ultimately led to the stat e of the art in\\nneural network performance(Le Cun et al., 1998)1.\\nAlthough current digit datasets have tens of thousands of tr aining and\\ntest examples, the sample size here is deliberately modest i n order to em-\\n1The ﬁgures and tables in this example were recreated from Le Cun (1989) .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 422}),\n",
       " Document(page_content='11.7 Example: ZIP Code Data 405\\n16x168x8x2\\n16x1610\\n4x44x4\\n8x8x210\\nShared WeightsNet-5Net-4Net-1\\n4x4x4Local Connectivity10\\n1010\\nNet-3Net-28x812\\n16x1616x1616x16\\nFIGURE 11.10. Architecture of the ﬁve networks used in the ZIP code example.\\nphasize the eﬀects. The examples were obtained by scanning s ome actual\\nhand-drawn digits, and then generating additional images b y random hor-\\nizontal shifts. Details may be found in Le Cun (1989). There a re 320 digits\\nin the training set, and 160 in the test set.\\nFive diﬀerent networks were ﬁt to the data:\\nNet-1:No hidden layer, equivalent to multinomial logistic regres sion.\\nNet-2:One hidden layer, 12 hidden units fully connected.\\nNet-3:Two hidden layers locally connected.\\nNet-4:Two hidden layers, locally connected with weight sharing.\\nNet-5:Two hidden layers, locally connected, two levels of weight s haring.\\nThese are depicted in Figure 11.10. Net-1 for example has 256 inputs, one\\neach for the 16×16 input pixels, and ten output units for each of the digits\\n0–9. The predicted value ˆfk(x) represents the estimated probability that\\nan imagexhas digit class k, fork= 0,1,2,...,9.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 423}),\n",
       " Document(page_content='406 Neural Networks\\nTraining Epochs% Correct on Test Data\\n0 5 10 15 20 25 3060708090100\\nNet-1Net-2Net-3Net-4Net-5\\nFIGURE 11.11. Test performance curves, as a function of the number of train -\\ning epochs, for the ﬁve networks of Table 11.1 applied to the ZIP code data.\\n(Le Cun, 1989)\\nThe networks all have sigmoidal output units, and were all ﬁt with the\\nsum-of-squares error function. The ﬁrst network has no hidd en layer, and\\nhence is nearly equivalent to a linear multinomial regressi on model (Exer-\\ncise 11.4). Net-2 is a single hidden layer network with 12 hid den units, of\\nthe kind described above.\\nThe training set error for all of the networks was 0%, since in all cases\\nthere are more parameters than training observations. The e volution of the\\ntest error during the training epochs is shown in Figure 11.1 1. The linear\\nnetwork (Net-1) starts to overﬁt fairly quickly, while test performance of\\nthe others level oﬀ at successively superior values.\\nThe other three networks have additional features which dem onstrate\\nthe power and ﬂexibility of the neural network paradigm. The y introduce\\nconstraints on the network, natural for the problem at hand, which allow\\nfor more complex connectivity but fewer parameters.\\nNet-3 uses local connectivity: this means that each hidden u nit is con-\\nnected to only a small patch of units in the layer below. In the ﬁrst hidden\\nlayer (an 8×8 array), each unit takes inputs from a 3 ×3 patch of the input\\nlayer; for units in the ﬁrst hidden layer that are one unit apa rt, their recep-\\ntive ﬁelds overlap by one row or column, and hence are two pixe ls apart.\\nIn the second hidden layer, inputs are from a 5 ×5 patch, and again units\\nthat are one unit apart have receptive ﬁelds that are two unit s apart. The\\nweights for all other connections are set to zero. Local conn ectivity makes\\neach unit responsible for extracting local features from th e layer below, and', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 424}),\n",
       " Document(page_content='11.7 Example: ZIP Code Data 407\\nTABLE 11.1. Test set performance of ﬁve diﬀerent neural networks on a han d-\\nwritten digit classiﬁcation example (Le Cun, 1989).\\nNetwork Architecture LinksWeights % Correct\\nNet-1: Single layer network 2570 2570 80.0%\\nNet-2: Two layer network 3214 3214 87.0%\\nNet-3: Locally connected 1226 1226 88.5%\\nNet-4: Constrained network 1 2266 1132 94.0%\\nNet-5: Constrained network 2 5194 1060 98.4%\\nreduces considerably the total number of weights. With many more hidden\\nunits than Net-2, Net-3 has fewer links and hence weights (12 26 vs. 3214),\\nand achieves similar performance.\\nNet-4 and Net-5 have local connectivity with shared weights . All units\\nin a local feature map perform the sameoperation on diﬀerent parts of the\\nimage, achieved by sharing the same weights. The ﬁrst hidden layer of Net-\\n4 has two 8×8 arrays, and each unit takes input from a 3 ×3 patch just like\\nin Net-3. However, each of the units in a single 8 ×8 feature map share the\\nsame set of nine weights (but have their own bias parameter). This forces\\nthe extracted features in diﬀerent parts of the image to be co mputed by\\nthe same linear functional, and consequently these network s are sometimes\\nknown as convolutional networks . The second hidden layer of Net-4 has\\nno weight sharing, and is the same as in Net-3. The gradient of the error\\nfunctionRwith respect to a shared weight is the sum of the gradients of\\nRwith respect to each connection controlled by the weights in question.\\nTable 11.1 gives the number of links, the number of weights an d the\\noptimal test performance for each of the networks. We see tha t Net-4 has\\nmore links but fewer weights than Net-3, and superior test pe rformance.\\nNet-5 has four 4 ×4 feature maps in the second hidden layer, each unit\\nconnected to a 5 ×5 local patch in the layer below. Weights are shared\\nin each of these feature maps. We see that Net-5 does the best, having\\nerrors of only 1.6%, compared to 13% for the “vanilla” networ k Net-2.\\nThe clever design of network Net-5, motivated by the fact tha t features of\\nhandwriting style should appear in more than one part of a dig it, was the\\nresult of many person years of experimentation. This and sim ilar networks\\ngave better performance on ZIP code problems than any other l earning\\nmethod at that time (early 1990s). This example also shows th at neural\\nnetworks are not a fully automatic tool, as they are sometime s advertised.\\nAs with all statistical models, subject matter knowledge ca n and should be\\nused to improve their performance.\\nThis network was later outperformed by the tangent distance approach\\n(Simard et al., 1993) described in Section 13.3.3, which exp licitly incorpo-\\nrates natural aﬃne invariances. At this point the digit reco gnition datasets\\nbecome test beds for every new learning procedure, and resea rchers worked', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 425}),\n",
       " Document(page_content='408 Neural Networks\\nhardtodrivedowntheerrorrates.Asofthiswriting,thebes terrorrateson\\na large database (60 ,000 training, 10 ,000 test observations), derived from\\nstandard NIST2databases, were reported to be the following: (Le Cun et\\nal., 1998):\\n•1.1% for tangent distance with a 1-nearest neighbor classiﬁer (Sec-\\ntion 13.3.3);\\n•0.8% for a degree-9 polynomial SVM (Section 12.3);\\n•0.8% forLeNet-5, a more complex version of the convolutional net-\\nwork described here;\\n•0.7% for boosted LeNet-4. Boosting is described in Chapter 8. LeNet-\\n4is a predecessor of LeNet-5.\\nLe Cun et al. (1998) report a much larger table of performance results, and\\nit is evident that many groups have been working very hard to b ring these\\ntest error rates down. They report a standard error of 0 .1% on the error\\nestimates, which is based on a binomial average with N= 10,000 and\\np≈0.01. This implies that error rates within 0 .1—0.2% of one another\\nare statistically equivalent. Realistically the standard error is even higher,\\nsince the test data has been implicitly used in the tuning of t he various\\nprocedures.\\n11.8 Discussion\\nBothprojectionpursuitregressionandneuralnetworkstak enonlinearfunc-\\ntions of linear combinations (“derived features”) of the in puts. This is a\\npowerful and very general approach for regression and class iﬁcation, and\\nhas been shown to compete well with the best learning methods on many\\nproblems.\\nThesetoolsareespeciallyeﬀectiveinproblemswithahighs ignal-to-noise\\nratio and settings where prediction without interpretatio n is the goal. They\\nare less eﬀective for problems where the goal is to describe t he physical pro-\\ncess that generated the data and the roles of individual inpu ts. Each input\\nenters into the model in many places, in a nonlinear fashion. Some authors\\n(Hinton, 1989) plot a diagram of the estimated weights into e ach hidden\\nunit, to try to understand the feature that each unit is extra cting. This\\nis limited however by the lack of identiﬁability of the param eter vectors\\nαm, m= 1,...,M. Often there are solutions with αmspanning the same\\nlinear space as the ones found during training, giving predi cted values that\\n2The National Institute of Standards and Technology maintain l arge databases, in-\\ncluding handwritten character databases; http://www.nist.gov/srd/ .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 426}),\n",
       " Document(page_content='11.9 Bayesian Neural Nets and the NIPS 2003 Challenge 409\\nare roughly the same. Some authors suggest carrying out a pri ncipal com-\\nponent analysis of these weights, to try to ﬁnd an interpreta ble solution. In\\ngeneral, the diﬃculty of interpreting these models has limi ted their use in\\nﬁelds like medicine, where interpretation of the model is ve ry important.\\nThere has been a great deal of research on the training of neur al net-\\nworks. Unlike methods like CART and MARS, neural networks ar e smooth\\nfunctions of real-valued parameters. This facilitates the development of\\nBayesian inference for these models. The next sections disc usses a success-\\nful Bayesian implementation of neural networks.\\n11.9 Bayesian Neural Nets and the NIPS 2003\\nChallenge\\nA classiﬁcation competition was held in 2003, in which ﬁve la beled train-\\ning datasets were provided to participants. It was organize d for a Neural\\nInformation Processing Systems (NIPS) workshop. Each of th e data sets\\nconstitutedatwo-classclassiﬁcationproblems,withdiﬀe rentsizesandfrom\\na variety of domains (see Table 11.2). Feature measurements for a valida-\\ntion dataset were also available.\\nParticipants developed and applied statistical learning p rocedures to\\nmake predictions on the datasets, and could submit predicti ons to a web-\\nsite on the validation set for a period of 12 weeks. With this f eedback,\\nparticipants were then asked to submit predictions for a sep arate test set\\nand they received their results. Finally, the class labels f or the validation\\nset were released and participants had one week to train thei r algorithms\\non the combined training and validation sets, and submit the ir ﬁnal pre-\\ndictions to the competition website. A total of 75 groups par ticipated, with\\n20 and 16 eventually making submissions on the validation an d test sets,\\nrespectively.\\nThere was an emphasis on feature extraction in the competiti on. Arti-\\nﬁcial “probes” were added to the data: these are noise featur es with dis-\\ntributions resembling the real features but independent of the class labels.\\nThe percentage of probes that were added to each dataset, rel ative to the\\ntotal set of features, is shown on Table 11.2. Thus each learn ing algorithm\\nhad to ﬁgure out a way of identifying the probes and downweigh ting or\\neliminating them.\\nA number of metrics were used to evaluate the entries, includ ing the\\npercentage correct on the test set, the area under the ROC cur ve, and a\\ncombined score that compared each pair of classiﬁers head-t o-head. The\\nresults of the competition are very interesting and are deta iled in Guyon et\\nal. (2006). The most notable result: the entries of Neal and Z hang (2006)\\nwere the clear overall winners. In the ﬁnal competition they ﬁnished ﬁrst', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 427}),\n",
       " Document(page_content='410 Neural Networks\\nTABLE 11.2. NIPS 2003 challenge data sets. The column labeled pis the number\\nof features. For the Dorothea dataset the features are binary. Ntr,NvalandNte\\nare the number of training, validation and test cases, respec tively\\nDataset Domain Feature p Percent NtrNvalNte\\nType Probes\\nArcene Mass spectrometry Dense 10,000 30 100 100 700\\nDexter Text classiﬁcation Sparse 20,000 50 300 300 2000\\nDorothea Drug discovery Sparse 100,000 50 800 350 800\\nGisette Digit recognition Dense 5000 30 6000 1000 6500\\nMadelon Artiﬁcial Dense 500 96 2000 600 1800\\nin three of the ﬁve datasets, and were 5th and 7th on the remain ing two\\ndatasets.\\nIn their winning entries, Neal and Zhang (2006) used a series of pre-\\nprocessing feature-selection steps, followed by Bayesian neural networks,\\nDirichlet diﬀusion trees, and combinations of these method s. Here we focus\\nonly on the Bayesian neural network approach, and try to disc ern which\\naspects of their approach were important for its success. We rerun their\\nprograms and compare the results to boosted neural networks and boosted\\ntrees, and other related methods.\\n11.9.1 Bayes, Boosting and Bagging\\nLet us ﬁrst review brieﬂy the Bayesian approach to inference and its appli-\\ncation to neural networks. Given training data Xtr,ytr, we assume a sam-\\npling model with parameters θ; Neal and Zhang (2006) use a two-hidden-\\nlayer neural network, with output nodes the class probabili ties Pr(Y|X,θ)\\nfor the binary outcomes. Given a prior distribution Pr( θ), the posterior\\ndistribution for the parameters is\\nPr(θ|Xtr,ytr) =Pr(θ)Pr(ytr|Xtr,θ)∫\\nPr(θ)Pr(ytr|Xtr,θ)dθ(11.19)\\nFor a test case with features Xnew, the predictive distribution for the\\nlabelYnewis\\nPr(Ynew|Xnew,Xtr,ytr) =∫\\nPr(Ynew|Xnew,θ)Pr(θ|Xtr,ytr)dθ(11.20)\\n(c.f.equation8.24).Sincetheintegralin(11.20)isintra ctable,sophisticated\\nMarkov Chain Monte Carlo (MCMC) methods are used to sample fr om the\\nposterior distribution Pr( Ynew|Xnew,Xtr,ytr). A few hundred values θare\\ngenerated and then a simple average of these values estimate s the integral.\\nNeal and Zhang (2006) use diﬀuse Gaussian priors for all of th e parame-\\nters. The particular MCMC approach that was used is called hybrid Monte\\nCarlo, and may be important for the success of the method. It includ es\\nan auxiliary momentum vector and implements Hamiltonian dy namics in\\nwhich the potential function is the target density. This is d one to avoid', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 428}),\n",
       " Document(page_content='11.9 Bayesian Neural Nets and the NIPS 2003 Challenge 411\\nrandom walk behavior; the successive candidates move acros s the sample\\nspace in larger steps. They tend to be less correlated and hen ce converge\\nto the target distribution more rapidly.\\nNeal and Zhang (2006) also tried diﬀerent forms of pre-proce ssing of the\\nfeatures:\\n1. univariate screening using t-tests, and\\n2. automatic relevance determination.\\nIn the latter method (ARD), the weights (coeﬃcients) for the jth feature\\nto each of the ﬁrst hidden layer units all share a common prior variance\\nσ2\\nj, and prior mean zero. The posterior distributions for each v arianceσ2\\nj\\nare computed, and the features whose posterior variance con centrates on\\nsmall values are discarded.\\nThere are thus three main features of this approach that coul d be im-\\nportant for its success:\\n(a) the feature selection and pre-processing,\\n(b) the neural network model, and\\n(c) the Bayesian inference for the model using MCMC.\\nAccording to Neal and Zhang (2006), feature screening in (a) is carried\\nout purely for computational eﬃciency; the MCMC procedure i s slow with\\nalargenumberoffeatures.Thereisnoneedtousefeaturesel ectiontoavoid\\noverﬁtting. The posterior average (11.20) takes care of thi s automatically.\\nWe would like to understand the reasons for the success of the Bayesian\\nmethod. In our view, power of modern Bayesian methods does no t lie in\\ntheir use as a formal inference procedure; most people would not believe\\nthat the priors in a high-dimensional, complex neural netwo rk model are\\nactually correct. Rather the Bayesian/MCMC approach gives an eﬃcient\\nway of sampling the relevant parts of model space, and then av eraging the\\npredictions for the high-probability models.\\nBagging and boosting are non-Bayesian procedures that have some simi-\\nlarity toMCMCinaBayesian model. TheBayesian approach ﬁxe sthedata\\nand perturbs the parameters, according to current estimate of the poste-\\nrior distribution. Bagging perturbs the data in an i.i.d fas hion and then\\nre-estimates the model to give a new set of model parameters. At the end,\\na simple average of the model predictions from diﬀerent bagg ed samples is\\ncomputed. Boosting is similar to bagging, but ﬁts a model tha t is additive\\nin the models of each individual base learner, which are lear ned using non\\ni.i.d. samples. We can write all of these models in the form\\nˆf(xnew) =L∑\\nℓ=1wℓE(Ynew|xnew,ˆθℓ) (11.21)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 429}),\n",
       " Document(page_content='412 Neural Networks\\nIn all cases the ˆθℓare a large collection of model parameters. For the\\nBayesian model the wℓ= 1/L, and the average estimates the posterior\\nmean (11.21) by sampling θℓfrom the posterior distribution. For bagging,\\nwℓ= 1/Las well, and the ˆθℓare the parameters reﬁt to bootstrap re-\\nsamples of the training data. For boosting, the weights are a ll equal to\\n1, but the ˆθℓare typically chosen in a nonrandom sequential fashion to\\nconstantly improve the ﬁt.\\n11.9.2 Performance Comparisons\\nBased on the similarities above, we decided to compare Bayes ian neural\\nnetworks to boosted trees, boosted neural networks, random forests and\\nbagged neural networks on the ﬁve datasets in Table 11.2. Bag ging and\\nboosting of neural networks are not methods that we have prev iously used\\ninourwork.Wedecidedtotrythemhere,becauseofthesucces sofBayesian\\nneural networks in this competition, and the good performan ce of bagging\\nand boosting with trees. We also felt that by bagging and boos ting neural\\nnets, we could assess both the choice of model as well as the mo del search\\nstrategy.\\nHere are the details of the learning methods that were compar ed:\\nBayesian neural nets. The results here are taken from Neal and Zhang\\n(2006), using their Bayesian approach to ﬁtting neural netw orks. The\\nmodels had two hidden layers of 20 and 8 units. We re-ran some\\nnetworks for timing purposes only.\\nBoosted trees. We used the gbmpackage (version 1.5-7) in the R language.\\nTree depth and shrinkage factors varied from dataset to data set. We\\nconsistently bagged 80% of the data at each boosting iterati on (the\\ndefault is 50%). Shrinkage was between 0.001 and 0.1. Tree de pth was\\nbetween 2 and 9.\\nBoosted neural networks. Since boosting is typically most eﬀective with\\n“weak”learners,weboostedasinglehiddenlayerneuralnet workwith\\ntwo or four units, ﬁt with the nnetpackage (version 7.2-36) in R.\\nRandom forests. We used the R package randomForest (version 4.5-16)\\nwith default settings for the parameters.\\nBagged neural networks. Weusedthe same architecture as in the Bayesian\\nneural network above (two hidden layers of 20 and 8 units), ﬁt using\\nboth Neal’s C language package “Flexible Bayesian Modeling ” (2004-\\n11-10 release), and Matlab neural-net toolbox (version 5.1 ).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 430}),\n",
       " Document(page_content='11.9 Bayesian Neural Nets and the NIPS 2003 Challenge 413Test Error (%)\\nArcene Dexter Dorothea Gisette Madelon5 15 25Univariate Screened Features\\nBayesian neural nets\\nboosted trees \\nboosted neural nets\\nrandom forests\\nbagged neural networks \\nTest Error (%)\\nArcene Dexter Dorothea Gisette Madelon5 15 25ARD Reduced Features\\nFIGURE 11.12. Performance of diﬀerent learning methods on ﬁve problems,\\nusing both univariate screening of features (top panel) and a reduced feature set\\nfrom automatic relevance determination. The error bars at th e top of each plot\\nhave width equal to one standard error of the diﬀerence betwe en two error rates.\\nOn most of the problems several competitors are within this er ror bound.\\nThis analysis was carried out by Nicholas Johnson, and full d etails may\\nbe found in Johnson (2008)3. The results are shown in Figure 11.12 and\\nTable 11.3.\\nTheﬁgureandtableshowBayesian,boostedandbaggedneural networks,\\nboosted trees, and random forests, using both the screened a nd reduced\\nfeatures sets. The error bars at the top of each plot indicate one standard\\nerror of the diﬀerence between two error rates. Bayesian neu ral networks\\nagain emerge as the winner, although for some datasets the di ﬀerences\\nbetween the test error rates is not statistically signiﬁcan t. Random forests\\nperforms the best among the competitors using the selected f eature set,\\nwhile the boosted neural networks perform best with the redu ced feature\\nset, and nearly match the Bayesian neural net.\\nThe superiority of boosted neural networks over boosted tre es suggest\\nthat the neural network model is better suited to these parti cular prob-\\nlems. Speciﬁcally, individual features might not be good pr edictors here\\n3We also thank Isabelle Guyon for help in preparing the results of this section.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 431}),\n",
       " Document(page_content='414 Neural Networks\\nTABLE 11.3. Performance of diﬀerent methods. Values are average rank of t est\\nerror across the ﬁve problems (low is good), and mean computati on time and\\nstandard error of the mean, in minutes.\\nScreened Features ARD Reduced Features\\nMethod Average Average Average Average\\nRank Time Rank Time\\nBayesian neural networks 1.5384(138) 1.6 600(186)\\nBoosted trees 3.43.03(2.5) 4.0 34.1(32.4)\\nBoosted neural networks 3.89.4(8.6) 2.2 35.6(33.5)\\nRandom forests 2.71.9(1.7) 3.2 11.2(9.3)\\nBagged neural networks 3.63.5(1.1) 4.0 6.4(4.4)\\nand linear combinations of features work better. However th e impressive\\nperformance of random forests is at odds with this explanati on, and came\\nas a surprise to us.\\nSince the reduced feature sets come from the Bayesian neural network\\napproach, only the methods that use the screened features ar e legitimate,\\nself-contained procedures. However, this does suggest tha t better methods\\nfor internal feature selection might help the overall perfo rmance of boosted\\nneural networks.\\nThe table also shows the approximate training time required for each\\nmethod. Here the non-Bayesian methods show a clear advantag e.\\nOverall, the superior performance of Bayesian neural netwo rks here may\\nbe due to the fact that\\n(a) the neural network model is well suited to these ﬁve probl ems, and\\n(b) the MCMC approach provides an eﬃcient way of exploring th e im-\\nportant part of the parameter space, and then averaging the r esulting\\nmodels according to their quality.\\nThe Bayesian approach works well for smoothly parametrized models like\\nneural nets; it is not yet clear that it works as well for non-s mooth models\\nlike trees.\\n11.10 Computational Considerations\\nWithNobservations, ppredictors,Mhiddenunitsand Ltrainingepochs,a\\nneuralnetwork ﬁttypically requires O(NpML)operations. Therearemany\\npackages available for ﬁtting neural networks, probably ma ny more than\\nexist for mainstream statistical methods. Because the avai lable software\\nvaries widely in quality, and the learning problem for neura l networks is\\nsensitive to issues such as input scaling, such software sho uld be carefully\\nchosen and tested.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 432}),\n",
       " Document(page_content='Exercises 415\\nBibliographic Notes\\nProjection pursuit was proposed by Friedman and Tukey (1974 ), and spe-\\ncialized to regression by Friedman and Stuetzle (1981). Hub er (1985) gives\\na scholarly overview, and Roosen and Hastie (1994) present a formulation\\nusing smoothing splines. The motivation for neural network s dates back\\nto McCulloch and Pitts (1943), Widrow and Hoﬀ (1960) (reprin ted in An-\\nderson and Rosenfeld (1988)) and Rosenblatt (1962). Hebb (1 949) heavily\\ninﬂuencedthedevelopmentoflearningalgorithms.Theresu rgenceofneural\\nnetworks in the mid 1980s was due to Werbos (1974), Parker (19 85) and\\nRumelhart et al. (1986), who proposed the back-propagation algorithm.\\nToday there are many books written on the topic, for a broad ra nge of\\naudiences. For readers of this book, Hertz et al. (1991), Bis hop (1995) and\\nRipley (1996) may be the most informative. Bayesian learnin g for neural\\nnetworksisdescribedinNeal(1996). TheZIPcodeexamplewa stakenfrom\\nLe Cun (1989); see also Le Cun et al. (1990) and Le Cun et al. (19 98).\\nWe do not discuss theoretical topics such as approximation p roperties of\\nneural networks, such as the work of Barron (1993), Girosi et al. (1995)\\nand Jones (1992). Some of these results are summarized by Rip ley (1996).\\nExercises\\nEx. 11.1 Establish the exact correspondence between the projection pur-\\nsuit regression model (11.1) and the neural network (11.5). In particular,\\nshow that the single-layer regression network is equivalen t to a PPR model\\nwithgm(ωT\\nmx) =βmσ(α0m+sm(ωT\\nmx)), whereωmis themth unit vector.\\nEstablish a similar equivalence for a classiﬁcation networ k.\\nEx. 11.2 Consider a neural network for a quantitative outcome as in (1 1.5),\\nusing squared-error loss and identity output function gk(t) =t. Suppose\\nthat the weights αmfrom the input to hidden layer are nearly zero. Show\\nthat the resulting model is nearly linear in the inputs.\\nEx. 11.3 Derive the forward and backward propagation equations for t he\\ncross-entropy loss function.\\nEx. 11.4 Consider a neural network for a Kclass outcome that uses cross-\\nentropy loss. If the network has no hidden layer, show that th e model is\\nequivalent to the multinomial logistic model described in C hapter 4.\\nEx. 11.5\\n(a) Write a program to ﬁt a single hidden layer neural network (ten hidden\\nunits) via back-propagation and weight decay.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 433}),\n",
       " Document(page_content='416 Neural Networks\\n(b) Apply it to 100 observations from the model\\nY=σ(aT\\n1X)+(aT\\n2X)2+0.30·Z,\\nwhereσisthesigmoidfunction, Zisstandardnormal, XT= (X1,X2),\\neachXjbeing independent standard normal, and a1= (3,3),a2=\\n(3,−3). Generate a test sample of size 1000, and plot the training and\\ntest error curves as a function of the number of training epoc hs, for\\ndiﬀerent values of the weight decay parameter. Discuss the o verﬁtting\\nbehavior in each case.\\n(c) Vary the number of hidden units in the network, from 1 up to 10, and\\ndetermine the minimum number needed to perform well for this task.\\nEx. 11.6 Write a program to carry out projection pursuit regression, using\\ncubic smoothing splines with ﬁxed degrees of freedom. Fit it to the data\\nfrom the previous exercise, for various values of the smooth ing parameter\\nand number of model terms. Find the minimum number of model te rms\\nnecessary for the model to perform well and compare this to th e number\\nof hidden units from the previous exercise.\\nEx.11.7 Fitaneuralnetworktothe spamdataofSection9.1.2,andcompare\\nthe results to those for the additive model given in that chap ter. Compare\\nboth the classiﬁcation performance and interpretability o f the ﬁnal model.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 434}),\n",
       " Document(page_content='This is page 417\\nPrinter: Opaque this\\n12\\nSupport Vector Machines and\\nFlexible Discriminants\\n12.1 Introduction\\nIn this chapter we describe generalizations of linear decis ion boundaries\\nfor classiﬁcation. Optimal separating hyperplanes are int roduced in Chap-\\nter 4 for the case when two classes are linearly separable. He re we cover\\nextensions to the nonseparable case, where the classes over lap. These tech-\\nniquesarethengeneralizedtowhatisknownasthe support vector machine ,\\nwhich produces nonlinear boundaries by constructing a line ar boundary in\\nalarge, transformed versionof the featurespace. Thesecon dsetof methods\\ngeneralize Fisher’s linear discriminant analysis (LDA). T he generalizations\\nincludeﬂexible discriminant analysis which facilitates construction of non-\\nlinear boundaries in a manner very similar to the support vec tor machines,\\npenalized discriminant analysis for problems such as signal and image clas-\\nsiﬁcation where the large number of features are highly corr elated, and\\nmixture discriminant analysis for irregularly shaped classes.\\n12.2 The Support Vector Classiﬁer\\nIn Chapter 4 we discussed a technique for constructing an optimalseparat-\\ning hyperplane between two perfectly separated classes. We review this and\\ngeneralize to the nonseparable case, where the classes may n ot be separable\\nby a linear boundary.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 435}),\n",
       " Document(page_content='418 12. Flexible Discriminants\\n••\\n•\\n••\\n•••\\n•\\n••\\n•••\\n••\\n••\\n•\\n•\\nmarginM=1\\n∥β∥\\nM=1\\n∥β∥xTβ+β0= 0\\n••\\n•\\n••\\n•••\\n•\\n••\\n••\\n•••\\n••\\n••\\n•\\n• •\\nmarginξ∗\\n1ξ∗\\n1ξ∗\\n1\\nξ∗\\n2ξ∗\\n2ξ∗\\n2ξ∗\\n3ξ∗\\n3ξ∗\\n4ξ∗\\n4ξ∗\\n4ξ∗\\n5\\nM=1\\n∥β∥\\nM=1\\n∥β∥xTβ+β0= 0\\nFIGURE 12.1. Support vector classiﬁers. The left panel shows the separable\\ncase. The decision boundary is the solid line, while broken lines bound the shaded\\nmaximal margin of width 2M= 2/∥β∥. The right panel shows the nonseparable\\n(overlap) case. The points labeled ξ∗\\njare on the wrong side of their margin by\\nan amount ξ∗\\nj=Mξj; points on the correct side have ξ∗\\nj= 0. The margin is\\nmaximized subject to a total budget∑ξi≤constant. Hence∑ξ∗\\njis the total\\ndistance of points on the wrong side of their margin.\\nOur training data consists of Npairs (x1,y1),(x2,y2),...,(xN,yN), with\\nxi∈IRpandyi∈ {−1,1}. Deﬁne a hyperplane by\\n{x:f(x) =xTβ+β0= 0}, (12.1)\\nwhereβis a unit vector:∥β∥= 1. A classiﬁcation rule induced by f(x) is\\nG(x) = sign[xTβ+β0]. (12.2)\\nThegeometryofhyperplanesisreviewedinSection4.5,wher eweshowthat\\nf(x) in (12.1) gives the signed distance from a point xto the hyperplane\\nf(x) =xTβ+β0= 0. Since the classes are separable, we can ﬁnd a function\\nf(x) =xTβ+β0withyif(xi)>0∀i. Hence we are able to ﬁnd the\\nhyperplane that creates the biggest marginbetween the training points for\\nclass 1 and−1 (see Figure 12.1). The optimization problem\\nmax\\nβ,β0,∥β∥=1M\\nsubject toyi(xT\\niβ+β0)≥M, i= 1,...,N,(12.3)\\ncaptures this concept. The band in the ﬁgure is Munits away from the\\nhyperplane on either side, and hence 2 Munits wide. It is called the margin.\\nWe showed that this problem can be more conveniently rephras ed as\\nmin\\nβ,β0∥β∥\\nsubject toyi(xT\\niβ+β0)≥1, i= 1,...,N,(12.4)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 436}),\n",
       " Document(page_content='12.2 The Support Vector Classiﬁer 419\\nwhere we have dropped the norm constraint on β. Note that M= 1/∥β∥.\\nExpression (12.4) is the usual way of writing the support vec tor criterion\\nfor separated data. This is a convex optimization problem (q uadratic cri-\\nterion, linear inequality constraints), and the solution i s characterized in\\nSection 4.5.2.\\nSuppose now that the classes overlap in feature space. One wa y to deal\\nwith the overlap is to still maximize M, but allow for some points to be on\\nthe wrong side of the margin. Deﬁne the slack variables ξ= (ξ1,ξ2,...,ξ N).\\nThere are two natural ways to modify the constraint in (12.3) :\\nyi(xT\\niβ+β0)≥M−ξi, (12.5)\\nor\\nyi(xT\\niβ+β0)≥M(1−ξi), (12.6)\\n∀i, ξi≥0,∑N\\ni=1ξi≤constant. The two choices lead to diﬀerent solutions.\\nThe ﬁrst choice seems more natural, since it measures overla p in actual\\ndistancefromthemargin;thesecondchoicemeasurestheove rlapinrelative\\ndistance, which changes with the width of the margin M. However, the ﬁrst\\nchoice results in a nonconvex optimization problem, while t he second is\\nconvex; thus (12.6) leads to the “standard” support vector c lassiﬁer, which\\nwe use from here on.\\nHereistheideaoftheformulation.Thevalue ξiintheconstraint yi(xT\\niβ+\\nβ0)≥M(1−ξi) is the proportional amount by which the prediction\\nf(xi) =xT\\niβ+β0is on the wrong side of its margin. Hence by bounding the\\nsum∑ξi, we bound the total proportional amount by which prediction s\\nfall on the wrong side of their margin. Misclassiﬁcations oc cur whenξi>1,\\nso bounding∑ξiat a valueKsay, bounds the total number of training\\nmisclassiﬁcations at K.\\nAs in (4.48) in Section 4.5.2, we can drop the norm constraint onβ,\\ndeﬁneM= 1/∥β∥, and write (12.4) in the equivalent form\\nmin∥β∥subject to{\\nyi(xT\\niβ+β0)≥1−ξi∀i,\\nξi≥0,∑ξi≤constant.(12.7)\\nThis is the usual way the support vector classiﬁer is deﬁned f or the non-\\nseparable case. However we ﬁnd confusing the presence of the ﬁxed scale\\n“1” in the constraint yi(xT\\niβ+β0)≥1−ξi, and prefer to start with (12.6).\\nThe right panel of Figure 12.1 illustrates this overlapping case.\\nBy the nature of the criterion (12.7), we see that points well inside their\\nclass boundary do not play a big role in shaping the boundary. This seems\\nlike an attractive property, and one that diﬀerentiates it f rom linear dis-\\ncriminant analysis (Section 4.3). In LDA, the decision boun dary is deter-\\nmined by the covariance of the class distributions and the po sitions of the\\nclass centroids. We will see in Section 12.3.3 that logistic regression is more\\nsimilar to the support vector classiﬁer in this regard.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 437}),\n",
       " Document(page_content='420 12. Flexible Discriminants\\n12.2.1 Computing the Support Vector Classiﬁer\\nThe problem (12.7) is quadratic with linear inequality cons traints, hence it\\nis a convex optimization problem. We describe a quadratic pr ogramming\\nsolution using Lagrange multipliers. Computationally it i s convenient to\\nre-express (12.7) in the equivalent form\\nmin\\nβ,β01\\n2∥β∥2+CN∑\\ni=1ξi\\nsubject to ξi≥0, yi(xT\\niβ+β0)≥1−ξi∀i,(12.8)\\nwhere the “cost” parameter Creplaces the constant in (12.7); the separable\\ncase corresponds to C=∞.\\nThe Lagrange (primal) function is\\nLP=1\\n2∥β∥2+CN∑\\ni=1ξi−N∑\\ni=1αi[yi(xT\\niβ+β0)−(1−ξi)]−N∑\\ni=1µiξi,(12.9)\\nwhich we minimize w.r.t β,β0andξi. Setting the respective derivatives to\\nzero, we get\\nβ=N∑\\ni=1αiyixi, (12.10)\\n0 =N∑\\ni=1αiyi, (12.11)\\nαi=C−µi,∀i, (12.12)\\nas well as the positivity constraints αi, µi, ξi≥0∀i. By substituting\\n(12.10)–(12.12) into (12.9), we obtain the Lagrangian (Wol fe) dual objec-\\ntive function\\nLD=N∑\\ni=1αi−1\\n2N∑\\ni=1N∑\\ni′=1αiαi′yiyi′xT\\nixi′, (12.13)\\nwhich gives a lower bound on the objective function (12.8) fo r any feasible\\npoint. We maximize LDsubject to 0≤αi≤Cand∑N\\ni=1αiyi= 0. In\\naddition to (12.10)–(12.12), the Karush–Kuhn–Tucker cond itions include\\nthe constraints\\nαi[yi(xT\\niβ+β0)−(1−ξi)] = 0, (12.14)\\nµiξi= 0, (12.15)\\nyi(xT\\niβ+β0)−(1−ξi)≥0, (12.16)\\nfori= 1,...,N. Together these equations (12.10)–(12.16) uniquely char-\\nacterize the solution to the primal and dual problem.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 438}),\n",
       " Document(page_content='12.2 The Support Vector Classiﬁer 421\\nFrom (12.10) we see that the solution for βhas the form\\nˆβ=N∑\\ni=1ˆαiyixi, (12.17)\\nwith nonzero coeﬃcients ˆ αionly for those observations ifor which the\\nconstraints in (12.16) are exactly met (due to (12.14)). The se observations\\nare called the support vectors , sinceˆβis represented in terms of them\\nalone. Among these support points, some will lie on the edge o f the margin\\n(ˆξi= 0), and hence from (12.15) and (12.12) will be characterize d by\\n0<ˆαi< C; the remainder ( ˆξi>0) have ˆαi=C. From (12.14) we can\\nsee that any of these margin points (0 <ˆαi,ˆξi= 0) can be used to solve\\nforβ0, and we typically use an average of all the solutions for nume rical\\nstability.\\nMaximizing the dual (12.13) is a simpler convex quadratic pr ogramming\\nproblemthantheprimal(12.9),andcanbesolvedwithstanda rdtechniques\\n(Murray et al., 1981, for example).\\nGiven the solutions ˆβ0andˆβ, the decision function can be written as\\nˆG(x) = sign[ ˆf(x)]\\n= sign[xTˆβ+ˆβ0]. (12.18)\\nThe tuning parameter of this procedure is the cost parameter C.\\n12.2.2 Mixture Example (Continued)\\nFigure 12.2 shows the support vector boundary for the mixtur e example\\nof Figure 2.5 on page 21, with two overlapping classes, for tw o diﬀerent\\nvalues of the cost parameter C. The classiﬁers are rather similar in their\\nperformance. Points on the wrong side of the boundary are sup port vectors.\\nIn addition, points on the correct side of the boundary but cl ose to it (in\\nthe margin), are also support vectors. The margin is larger f orC= 0.01\\nthan it is for C= 10,000. Hence larger values of Cfocus attention more\\non (correctly classiﬁed) points near the decision boundary , while smaller\\nvalues involve data further away. Either way, misclassiﬁed points are given\\nweight, no matter how far away. In this example the procedure is not very\\nsensitive to choices of C, because of the rigidity of a linear boundary.\\nThe optimal value for Ccan be estimated by cross-validation, as dis-\\ncussed in Chapter 7. Interestingly, the leave-one-out cros s-validation error\\ncan be bounded above by the proportion of support points in th e data. The\\nreason is that leaving out an observation that is not a suppor t vector will\\nnot change the solution. Hence these observations, being cl assiﬁed correctly\\nby the original boundary, will be classiﬁed correctly in the cross-validation\\nprocess. However this bound tends to be too high, and not gene rally useful\\nfor choosing C(62% and 85%, respectively, in our examples).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 439}),\n",
       " Document(page_content='422 12. Flexible Discriminants', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 440}),\n",
       " Document(page_content='.. . . .. . . . . . .. . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 440}),\n",
       " Document(page_content='. . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 440}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 440}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . .. . . . . . . .. . . . ..', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 440}),\n",
       " Document(page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\n••\\n•\\nTraining Error: 0.270\\nTest Error:       0.288\\nBayes Error:    0.210\\nC= 10000', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 440}),\n",
       " Document(page_content='.. .. . . .. . . . .. . . . . . .. . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 440}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 440}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 440}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . .. . . . . . . .. . . . . . .. . . . .. . . .. ..', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 440}),\n",
       " Document(page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\nooooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no•\\nTraining Error: 0.26\\nTest Error:       0.30\\nBayes Error:    0.21\\nC= 0.01\\nFIGURE 12.2. The linear support vector boundary for the mixture data exam-\\nple with two overlapping classes, for two diﬀerent values of C. The broken lines\\nindicate the margins, where f(x) =±1. The support points ( αi>0) are all the\\npoints on the wrong side of their margin. The black solid dots ar e those support\\npoints falling exactly on the margin ( ξi= 0, αi>0). In the upper panel 62%of\\nthe observations are support points, while in the lower panel 85%are. The broken\\npurple curve in the background is the Bayes decision boundary .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 440}),\n",
       " Document(page_content='12.3 Support Vector Machines and Kernels 423\\n12.3 Support Vector Machines and Kernels\\nThe support vector classiﬁer described so far ﬁnds linear bo undaries in the\\ninput feature space. As with other linear methods, we can mak e the pro-\\ncedure more ﬂexible by enlarging the feature space using bas is expansions\\nsuch as polynomials or splines (Chapter 5). Generally linea r boundaries\\nin the enlarged space achieve better training-class separa tion, and trans-\\nlate to nonlinear boundaries in the original space. Once the basis functions\\nhm(x), m= 1,...,Mare selected, the procedure is the same as before. We\\nﬁttheSVclassiﬁerusinginputfeatures h(xi) = (h1(xi),h2(xi),...,h M(xi)),\\ni= 1,...,N, and produce the (nonlinear) function ˆf(x) =h(x)Tˆβ+ˆβ0.\\nThe classiﬁer is ˆG(x) = sign( ˆf(x)) as before.\\nThesupport vector machine classiﬁer is an extension of this idea, where\\nthe dimension of the enlarged space is allowed to get very lar ge, inﬁnite\\nin some cases. It might seem that the computations would beco me pro-\\nhibitive. It would also seem that with suﬃcient basis functi ons, the data\\nwould be separable, and overﬁtting would occur. We ﬁrst show how the\\nSVM technology deals with these issues. We then see that in fa ct the SVM\\nclassiﬁer is solving a function-ﬁtting problem using a part icular criterion\\nand form of regularization, and is part of a much bigger class of problems\\nthat includes the smoothing splines of Chapter 5. The reader may wish\\nto consult Section 5.8, which provides background material and overlaps\\nsomewhat with the next two sections.\\n12.3.1 Computing the SVM for Classiﬁcation\\nWe can represent the optimization problem (12.9) and its sol ution in a\\nspecial way that only involves the input features via inner p roducts. We do\\nthis directly for the transformed feature vectors h(xi). We then see that for\\nparticular choices of h, these inner products can be computed very cheaply.\\nThe Lagrange dual function (12.13) has the form\\nLD=N∑\\ni=1αi−1\\n2N∑\\ni=1N∑\\ni′=1αiαi′yiyi′⟨h(xi),h(xi′)⟩. (12.19)\\nFrom (12.10) we see that the solution function f(x) can be written\\nf(x) =h(x)Tβ+β0\\n=N∑\\ni=1αiyi⟨h(x),h(xi)⟩+β0. (12.20)\\nAs before, given αi,β0can be determined by solving yif(xi) = 1 in (12.20)\\nfor any (or all) xifor which 0 <αi<C.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 441}),\n",
       " Document(page_content='424 12. Flexible Discriminants\\nSo both (12.19) and (12.20) involve h(x) only through inner products. In\\nfact, we need not specify the transformation h(x) at all, but require only\\nknowledge of the kernel function\\nK(x,x′) =⟨h(x),h(x′)⟩ (12.21)\\nthat computes inner products in the transformed space. Kshould be a\\nsymmetric positive (semi-) deﬁnite function; see Section 5 .8.1.\\nThree popular choices for Kin the SVM literature are\\ndth-Degree polynomial: K(x,x′) = (1+⟨x,x′⟩)d,\\nRadial basis: K(x,x′) = exp(−γ∥x−x′∥2),\\nNeural network: K(x,x′) = tanh(κ1⟨x,x′⟩+κ2).(12.22)\\nConsider for example a feature space with two inputs X1andX2, and a\\npolynomial kernel of degree 2. Then\\nK(X,X′) = (1+⟨X,X′⟩)2\\n= (1+X1X′\\n1+X2X′\\n2)2\\n= 1+2X1X′\\n1+2X2X′\\n2+(X1X′\\n1)2+(X2X′\\n2)2+2X1X′\\n1X2X′\\n2.\\n(12.23)\\nThenM= 6, and if we choose h1(X) = 1,h2(X) =√\\n2X1,h3(X) =√\\n2X2,h4(X) =X2\\n1,h5(X) =X2\\n2,andh6(X) =√\\n2X1X2,thenK(X,X′) =\\n⟨h(X),h(X′)⟩. From (12.20) we see that the solution can be written\\nˆf(x) =N∑\\ni=1ˆαiyiK(x,xi)+ˆβ0. (12.24)\\nThe role of the parameter Cis clearer in an enlarged feature space,\\nsince perfect separation is often achievable there. A large value ofCwill\\ndiscourage any positive ξi, and lead to an overﬁt wiggly boundary in the\\noriginal feature space; a small value of Cwill encourage a small value of\\n∥β∥, which in turn causes f(x) and hence the boundary to be smoother.\\nFigure 12.3 show two nonlinear support vector machines appl ied to the\\nmixture example of Chapter 2. The regularization parameter was chosen\\nin both cases to achieve good test error. The radial basis ker nel produces\\na boundary quite similar to the Bayes optimal boundary for th is example;\\ncompare Figure 2.5.\\nIn the early literature on support vectors, there were claim s that the\\nkernel property of the support vector machine is unique to it and allows\\none to ﬁnesse the curse of dimensionality. Neither of these c laims is true,\\nand we go into both of these issues in the next three subsectio ns.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 442}),\n",
       " Document(page_content='12.3 Support Vector Machines and Kernels 425\\nSVM - Degree-4 Polynomial in Feature Space', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 443}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 443}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 443}),\n",
       " Document(page_content='. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 443}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 443}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . .. . . . . . . . .. . . . . . .. . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 443}),\n",
       " Document(page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\n•••••••\\n•\\n• ••\\n•\\n•••\\nTraining Error: 0.180\\nTest Error:       0.245\\nBayes Error:    0.210\\nSVM - Radial Kernel in Feature Space', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 443}),\n",
       " Document(page_content='. . . . . . .. . . . . . .. . . . . .. . . . . .. . . . .. . . . .. . . .. . . .. . . .. . . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . . .. . . .. . . .. . . .. . . . .. . . . .. . . . . .. . . . . .. . . . . . .. . . . . . .. . . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 443}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 443}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 443}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 443}),\n",
       " Document(page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\nooooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\n•\\n•••\\n•\\n••••\\n••••\\n••••\\n••\\n••\\n••••\\n••\\n•\\n••\\n•\\nTraining Error: 0.160\\nTest Error:       0.218\\nBayes Error:    0.210\\nFIGURE 12.3. Two nonlinear SVMs for the mixture data. The upper plot uses\\na4th degree polynomial kernel, the lower a radial basis kernel (wi thγ= 1). In\\neach caseCwas tuned to approximately achieve the best test error perfor mance,\\nandC= 1worked well in both cases. The radial basis kernel performs t he best\\n(close to Bayes optimal), as might be expected given the data ar ise from mixtures\\nof Gaussians. The broken purple curve in the background is the B ayes decision\\nboundary.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 443}),\n",
       " Document(page_content='426 12. Flexible Discriminants\\n−3 −2 −1 0 1 2 30.0 0.5 1.0 1.5 2.0 2.5 3.0Hinge Loss\\nBinomial Deviance\\nSquared Error\\nClass HuberLoss\\nyf\\nFIGURE 12.4. The support vector loss function (hinge loss), compared to the\\nnegative log-likelihood loss (binomial deviance) for logistic r egression, squared-er-\\nror loss, and a “Huberized” version of the squared hinge loss. A ll are shown as a\\nfunction of yfrather than f, because of the symmetry between the y= +1and\\ny=−1case. The deviance and Huber have the same asymptotes as the S VM\\nloss, but are rounded in the interior. All are scaled to have the limiting left-tail\\nslope of−1.\\n12.3.2 The SVM as a Penalization Method\\nWithf(x) =h(x)Tβ+β0, consider the optimization problem\\nmin\\nβ0, βN∑\\ni=1[1−yif(xi)]++λ\\n2∥β∥2(12.25)\\nwhere the subscript “+” indicates positive part. This has th e formloss+\\npenalty, which is a familiar paradigm in function estimation. It is e asy to\\nshow (Exercise 12.1) that the solution to (12.25), with λ= 1/C, is the\\nsame as that for (12.8).\\nExamination of the “hinge” loss function L(y,f) = [1−yf]+shows that\\nit is reasonable for two-class classiﬁcation, when compare d to other more\\ntraditional loss functions. Figure 12.4 compares it to the l og-likelihood loss\\nfor logistic regression, as well as squared-error loss and a variant thereof.\\nThe (negative) log-likelihood or binomial deviance has sim ilar tails as the\\nSVM loss, giving zero penalty to points well inside their mar gin, and a', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 444}),\n",
       " Document(page_content='12.3 Support Vector Machines and Kernels 427\\nTABLE 12.1. The population minimizers for the diﬀerent loss functions in F ig-\\nure 12.4. Logistic regression uses the binomial log-likelihoo d or deviance. Linear\\ndiscriminant analysis (Exercise 4.2) uses squared-error los s. The SVM hinge loss\\nestimates the mode of the posterior class probabilities, wher eas the others estimate\\na linear transformation of these probabilities.\\nLoss Function L[y,f(x)] Minimizing Function\\nBinomial\\nDeviance log[1+e−yf(x)]f(x) = logPr(Y= +1|x)\\nPr(Y= -1|x)\\nSVM Hinge\\nLoss[1−yf(x)]+ f(x) = sign[Pr( Y= +1|x)−1\\n2]\\nSquared\\nError[y−f(x)]2= [1−yf(x)]2f(x) = 2Pr(Y= +1|x)−1\\n“Huberised”\\nSquare\\nHinge Loss−4yf(x), yf (x)<-1\\n[1−yf(x)]2\\n+otherwisef(x) = 2Pr(Y= +1|x)−1\\nlinear penalty to points on the wrong side and far away. Squar ed-error, on\\nthe other hand gives a quadratic penalty, and points well ins ide their own\\nmargin have a strong inﬂuence on the model as well. The square d hinge\\nlossL(y,f) = [1−yf]2\\n+is like the quadratic, except it is zero for points\\ninside their margin. It still rises quadratically in the lef t tail, and will be\\nless robust than hinge or deviance to misclassiﬁed observat ions. Recently\\nRossetandZhu(2007)proposeda“Huberized”versionofthes quaredhinge\\nloss, which converts smoothly to a linear loss at yf=−1.\\nWe can characterize these loss functions in terms of what the y are es-\\ntimating at the population level. We consider minimizing E L(Y,f(X)).\\nTable 12.1 summarizes the results. Whereas the hinge loss es timates the\\nclassiﬁerG(x) itself, all the others estimate a transformation of the cla ss\\nposterior probabilities. The “Huberized” square hinge los s shares attractive\\nproperties of logistic regression (smooth loss function, e stimates probabili-\\nties), as well as the SVM hinge loss (support points).\\nFormulation (12.25) casts the SVM as a regularized function estimation\\nproblem, where the coeﬃcients of the linear expansion f(x) =β0+h(x)Tβ\\nareshrunktowardzero(excludingtheconstant).If h(x)representsahierar-\\nchical basis having some ordered structure (such as ordered in roughness),', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 445}),\n",
       " Document(page_content='428 12. Flexible Discriminants\\nthen the uniform shrinkage makes more sense if the rougher el ementshjin\\nthe vectorhhave smaller norm.\\nAll the loss-functions in Table 12.1 except squared-error a re so called\\n“margin maximizing loss-functions” (Rosset et al., 2004b) . This means that\\nif the data are separable, then the limit of ˆβλin (12.25) as λ→0 deﬁnes\\nthe optimal separating hyperplane1.\\n12.3.3 Function Estimation and Reproducing Kernels\\nHere we describe SVMs in terms of function estimation in repr oducing\\nkernel Hilbert spaces, where the kernel property abounds. T his material is\\ndiscussed in some detail in Section 5.8. This provides anoth er view of the\\nsupport vector classiﬁer, and helps to clarify how it works.\\nSuppose the basis harises from the (possibly ﬁnite) eigen-expansion of\\na positive deﬁnite kernel K,\\nK(x,x′) =∞∑\\nm=1φm(x)φm(x′)δm (12.26)\\nandhm(x) =√δmφm(x). Then with θm=√δmβm, we can write (12.25)\\nas\\nmin\\nβ0, θN∑\\ni=1[\\n1−yi(β0+∞∑\\nm=1θmφm(xi))]\\n++λ\\n2∞∑\\nm=1θ2\\nm\\nδm.(12.27)\\nNow (12.27) is identical in form to (5.49) on page 169 in Secti on 5.8, and\\nthe theory of reproducing kernel Hilbert spaces described t here guarantees\\na ﬁnite-dimensional solution of the form\\nf(x) =β0+N∑\\ni=1αiK(x,xi). (12.28)\\nIn particular we see there an equivalent version of the optim ization crite-\\nrion (12.19) [Equation (5.67) in Section 5.8.2; see also Wah ba et al. (2000)],\\nmin\\nβ0,αN∑\\ni=1(1−yif(xi))++λ\\n2αTKα, (12.29)\\nwhereKis theN×Nmatrix of kernel evaluations for all pairs of training\\nfeatures (Exercise 12.2).\\nThese models are quite general, and include, for example, th e entire fam-\\nily of smoothing splines, additive and interaction spline m odels discussed\\n1For logistic regression with separable data, ˆβλdiverges, but ˆβλ/∥ˆβλ∥converges to\\nthe optimal separating direction.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 446}),\n",
       " Document(page_content='12.3 Support Vector Machines and Kernels 429\\nin Chapters 5 and 9, and in more detail in Wahba (1990) and Hast ie and\\nTibshirani (1990). They can be expressed more generally as\\nmin\\nf∈HN∑\\ni=1[1−yif(xi)]++λJ(f), (12.30)\\nwhereHis the structured space of functions, and J(f) an appropriate reg-\\nularizer on that space. For example, suppose His the space of additive\\nfunctionsf(x) =∑p\\nj=1fj(xj), andJ(f) =∑\\nj∫\\n{f′′\\nj(xj)}2dxj. Then the\\nsolution to (12.30) is an additive cubic spline, and has a ker nel representa-\\ntion (12.28) with K(x,x′) =∑p\\nj=1Kj(xj,x′\\nj). Each of the Kjis the kernel\\nappropriate for the univariate smoothing spline in xj(Wahba, 1990).\\nConverselythisdiscussionalsoshowsthat,forexample, anyofthekernels\\ndescribed in (12.22) above can be used with anyconvex loss function, and\\nwill also lead to a ﬁnite-dimensional representation of the form (12.28).\\nFigure 12.5 uses the same kernel functions as in Figure 12.3, except using\\nthe binomial log-likelihood as a loss function2. The ﬁtted function is hence\\nan estimate of the log-odds,\\nˆf(x) = logˆPr(Y= +1|x)\\nˆPr(Y=−1|x)\\n=ˆβ0+N∑\\ni=1ˆαiK(x,xi), (12.31)\\nor conversely we get an estimate of the class probabilities\\nˆPr(Y= +1|x) =1\\n1+e−ˆβ0−∑N\\ni=1ˆαiK(x,xi). (12.32)\\nThe ﬁtted models are quite similar in shape and performance. Examples\\nand more details are given in Section 5.8.\\nIt does happen that for SVMs, a sizable fraction of the Nvalues ofαi\\ncan be zero (the nonsupport points). In the two examples in Fi gure 12.3,\\nthese fractions are 42% and 45%, respectively. This is a cons equence of the\\npiecewise linear nature of the ﬁrst part of the criterion (12 .25). The lower\\nthe class overlap (on the training data), the greater this fr action will be.\\nReducingλwill generally reduce the overlap (allowing a more ﬂexible f).\\nA small number of support points means that ˆf(x) can be evaluated more\\nquickly, which is important at lookup time. Of course, reduc ing the overlap\\ntoo much can lead to poor generalization.\\n2Ji Zhu assisted in the preparation of these examples.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 447}),\n",
       " Document(page_content='430 12. Flexible Discriminants\\nLR - Degree-4 Polynomial in Feature Space', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 448}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 448}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 448}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 448}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 448}),\n",
       " Document(page_content='. . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . .. . . . . . . .. . . . . . .. . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 448}),\n",
       " Document(page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\nooooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.190\\nTest Error:       0.263\\nBayes Error:    0.210\\nLR - Radial Kernel in Feature Space', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 448}),\n",
       " Document(page_content='.. . . .. . . . .. . . . . .. . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 448}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 448}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 448}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 448}),\n",
       " Document(page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\nooooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.150\\nTest Error:       0.221\\nBayes Error:    0.210\\nFIGURE 12.5. The logistic regression versions of the SVM models in Fig-\\nure 12.3, using the identical kernels and hence penalties, but the log-likelihood\\nloss instead of the SVM loss function. The two broken contours c orrespond to\\nposterior probabilities of 0.75and0.25for the +1 class (or vice versa). The bro-\\nken purple curve in the background is the Bayes decision bound ary.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 448}),\n",
       " Document(page_content='12.3 Support Vector Machines and Kernels 431\\nTABLE 12.2. Skin of the orange: Shown are mean (standard error of the mean )\\nof the test error over 50simulations. BRUTO ﬁts an additive spline model adap-\\ntively, while MARS ﬁts a low-order interaction model adaptively .\\nTest Error (SE)\\nMethod No Noise Features Six Noise Features\\n1 SV Classiﬁer 0.450 (0.003) 0.472 (0.003)\\n2 SVM/poly 2 0.078 (0.003) 0.152 (0.004)\\n3 SVM/poly 5 0.180 (0.004) 0.370 (0.004)\\n4 SVM/poly 10 0.230 (0.003) 0.434 (0.002)\\n5 BRUTO 0.084 (0.003) 0.090 (0.003)\\n6 MARS 0.156 (0.004) 0.173 (0.005)\\nBayes 0.029 0.029\\n12.3.4 SVMs and the Curse of Dimensionality\\nIn this section, we address the question of whether SVMs have some edge\\non the curse of dimensionality. Notice that in expression (1 2.23) we are not\\nallowed a fully general inner product in the space of powers a nd products.\\nFor example, all terms of the form 2 XjX′\\njare given equal weight, and the\\nkernel cannot adapt itself to concentrate on subspaces. If t he number of\\nfeaturespwere large, but the class separation occurred only in the lin ear\\nsubspace spanned by say X1andX2, this kernel would not easily ﬁnd the\\nstructure and would suﬀer from having many dimensions to sea rch over.\\nOne would have to build knowledge about the subspace into the kernel;\\nthat is, tell it to ignore all but the ﬁrst two inputs. If such k nowledge were\\navailable a priori, much of statistical learning would be ma de much easier.\\nA major goal of adaptive methods is to discover such structur e.\\nWe support these statements with an illustrative example. W e generated\\n100 observations in each of two classes. The ﬁrst class has fo ur standard\\nnormal independent features X1,X2,X3,X4. The second class also has four\\nstandard normal independent features, but conditioned on 9 ≤∑X2\\nj≤16.\\nThis is a relatively easy problem. As a second harder problem , we aug-\\nmented the features with an additional six standard Gaussia n noise fea-\\ntures. Hence the second class almost completely surrounds t he ﬁrst, like the\\nskin surrounding the orange, in a four-dimensional subspac e. The Bayes er-\\nror rate for this problem is 0 .029 (irrespective of dimension). We generated\\n1000 test observations to compare diﬀerent procedures. The average test\\nerrors over 50 simulations, with and without noise features , are shown in\\nTable 12.2.\\nLine 1 uses the support vector classiﬁer in the original feat ure space.\\nLines2–4refertothesupportvectormachinewitha2-,5-and 10-dimension-\\nal polynomial kernel. For all support vector procedures, we chose the cost\\nparameterCto minimize the test error, to be as fair as possible to the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 449}),\n",
       " Document(page_content='432 12. Flexible Discriminants\\n1e−01 1e+01 1e+030.20 0.25 0.30 0.35\\n1e−01 1e+01 1e+03 1e−01 1e+01 1e+03 1e−01 1e+01 1e+03Test Error\\nCTest Error Curves − SVM with Radial Kernel\\nγ= 5 γ= 1 γ= 0.5 γ= 0.1\\nFIGURE 12.6. Test-error curves as a function of the cost parameter Cfor\\nthe radial-kernel SVM classiﬁer on the mixture data. At the top of each plot is\\nthe scale parameter γfor the radial kernel: Kγ(x,y) = exp( −γ||x−y||2). The\\noptimal value for Cdepends quite strongly on the scale of the kernel. The Bayes\\nerror rate is indicated by the broken horizontal lines.\\nmethod. Line 5 ﬁts an additive spline model to the ( −1,+1) response by\\nleast squares, using the BRUTO algorithm for additive model s, described\\nin Hastie and Tibshirani (1990). Line 6 uses MARS (multivari ate adaptive\\nregression splines) allowing interaction of all orders, as described in Chap-\\nter 9; as such it is comparable with the SVM/poly 10. Both BRUT O and\\nMARS have the ability to ignore redundant variables. Test er ror was not\\nused to choose the smoothing parameters in either of lines 5 o r 6.\\nIn the original feature space, a hyperplane cannot separate the classes,\\nand the support vector classiﬁer (line 1) does poorly. The po lynomial sup-\\nport vector machine makes a substantial improvement in test error rate,\\nbut is adversely aﬀected by the six noise features. It is also very sensitive to\\nthe choice of kernel: the second degree polynomial kernel (l ine 2) does best,\\nsince the true decision boundary is a second-degree polynom ial. However,\\nhigher-degree polynomial kernels (lines 3 and 4) do much wor se. BRUTO\\nperforms well, since the boundary is additive. BRUTO and MAR S adapt\\nwell: their performance does not deteriorate much in the pre sence of noise.\\n12.3.5 A Path Algorithm for the SVM Classiﬁer\\nThe regularization parameter for the SVM classiﬁer is the co st parameter\\nC, or its inverse λin (12.25). Common usage is to set Chigh, leading often\\nto somewhat overﬁt classiﬁers.\\nFigure 12.6 shows the test error on the mixture data as a funct ion of\\nC, using diﬀerent radial-kernel parameters γ. Whenγ= 5 (narrow peaked\\nkernels), the heaviest regularization (small C) is called for. With γ= 1', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 450}),\n",
       " Document(page_content='12.3 Support Vector Machines and Kernels 433\\n−0.5 0.0 0.5 1.0 1.5 2.0−1.0 −0.5 0.0 0.5 1.0 1.5789\\n1011\\n12\\n123\\n45\\n61/||β|| f(x) = 0f(x) = +1\\nf(x) =−1\\n0.0 0.2 0.4 0.6 0.8 1.00 2 4 6 8 101\\n2\\n34\\n56\\n789\\n1011\\n12\\nαi(λ)λ\\nFIGURE 12.7. A simple example illustrates the SVM path algorithm. (left\\npanel:) This plot illustrates the state of the model at λ= 1/2. The ‘‘+1” points\\nare orange, the “ −1” blue. The width of the soft margin is 2/||β||= 2×0.587.\\nTwo blue points {3,5}are misclassiﬁed, while the two orange points {10,12}are\\ncorrectly classiﬁed, but on the wrong side of their margin f(x) = +1; each of\\nthese hasyif(xi)<1. The three square shaped points {2,6,7}are exactly on\\ntheir margins. (right panel:) This plot shows the piecewise lin ear proﬁles αi(λ).\\nThe horizontal broken line at λ= 1/2indicates the state of the αifor the model\\nin the left plot.\\n(the value used in Figure 12.3), an intermediate value of Cis required.\\nClearly in situations such as these, we need to determine a go od choice\\nforC, perhaps by cross-validation. Here we describe a path algor ithm (in\\nthe spirit of Section 3.8) for eﬃciently ﬁtting the entire se quence of SVM\\nmodels obtained by varying C.\\nIt is convenient to use the loss+penalty formulation (12.25 ), along with\\nFigure 12.4. This leads to a solution for βat a given value of λ:\\nβλ=1\\nλN∑\\ni=1αiyixi. (12.33)\\nTheαiare again Lagrange multipliers, but in this case they all lie in [0,1].\\nFigure 12.7 illustrates the setup. It can be shown that the KK T optimal-\\nity conditions imply that the labeled points ( xi,yi) fall into three distinct\\ngroups:', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 451}),\n",
       " Document(page_content='434 12. Flexible Discriminants\\n•Observationscorrectlyclassiﬁedandoutsidetheirmargin s.Theyhave\\nyif(xi)>1, and Lagrange multipliers αi= 0. Examples are the\\norange points 8, 9 and 11, and the blue points 1 and 4.\\n•Observationssittingontheirmarginswith yif(xi) = 1,withLagrange\\nmultipliers αi∈[0,1]. Examples are the orange 7 and the blue 2 and\\n6.\\n•Observations inside their margins have yif(xi)<1, withαi= 1.\\nExamples are the blue 3 and 5, and the orange 10 and 12.\\nThe idea for the path algorithm is as follows. Initially λis large, the\\nmargin 1/||βλ||is wide, and all points are inside their margin and have\\nαi= 1. Asλdecreases, 1 /||βλ||decreases, and the margin gets narrower.\\nSome points will move from inside their margins to outside th eir margins,\\nandtheirαiwillchangefrom1to0.Bycontinuity ofthe αi(λ),thesepoints\\nwilllingeron the margin during this transition. From (12.33) we see tha t\\nthe points with αi= 1 make ﬁxed contributions to β(λ), and those with\\nαi= 0 make no contribution. So all that changes as λdecreases are the\\nαi∈[0,1] of those (small number) of points on the margin. Since all t hese\\npoints have yif(xi) = 1, this results in a small set of linear equations that\\nprescribe how αi(λ) and hence βλchanges during these transitions. This\\nresults in piecewise linear paths for each of the αi(λ). The breaks occur\\nwhen points cross the margin. Figure 12.7 (right panel) show s theαi(λ)\\nproﬁles for the small example in the left panel.\\nAlthough we have described this for linear SVMs, exactly the same idea\\nworks for nonlinear models, in which (12.33) is replaced by\\nfλ(x) =1\\nλN∑\\ni=1αiyiK(x,xi). (12.34)\\nDetails can be found in Hastie et al. (2004). An Rpackagesvmpath is\\navailable on CRAN for ﬁtting these models.\\n12.3.6 Support Vector Machines for Regression\\nIn this section we show how SVMs can be adapted for regression with a\\nquantitative response, in ways that inherit some of the prop erties of the\\nSVM classiﬁer. We ﬁrst discuss the linear regression model\\nf(x) =xTβ+β0, (12.35)\\nand then handle nonlinear generalizations. To estimate β, we consider min-\\nimization of\\nH(β,β0) =N∑\\ni=1V(yi−f(xi))+λ\\n2∥β∥2, (12.36)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 452}),\n",
       " Document(page_content='12.3 Support Vector Machines and Kernels 435\\n-4 -2 0 2 4-1 0 1 2 3 4\\n-4 -2 0 2 40 2 4 6 8 10 12ǫ−ǫ c −cVH(r)Vǫ(r)\\nr r\\nFIGURE 12.8. The left panel shows the ǫ-insensitive error function used by the\\nsupport vector regression machine. The right panel shows th e error function used\\nin Huber’s robust regression (blue curve). Beyond |c|, the function changes from\\nquadratic to linear.\\nwhere\\nVǫ(r) ={\\n0 if|r|<ǫ,\\n|r|−ǫ,otherwise.(12.37)\\nThis is an “ ǫ-insensitive” error measure, ignoring errors of size less t han\\nǫ(left panel of Figure 12.8). There is a rough analogy with the support\\nvector classiﬁcation setup, where points on the correct sid e of the deci-\\nsion boundary and far away from it, are ignored in the optimiz ation. In\\nregression, these “low error” points are the ones with small residuals.\\nIt is interesting to contrast this with error measures used i n robust re-\\ngression in statistics. The most popular, due to Huber (1964 ), has the form\\nVH(r) ={\\nr2/2 if|r|≤c,\\nc|r|−c2/2,|r|>c,(12.38)\\nshownintherightpanelofFigure12.8.Thisfunctionreduce sfromquadratic\\nto linear the contributions of observations with absolute r esidual greater\\nthan a prechosen constant c. This makes the ﬁtting less sensitive to out-\\nliers. The support vector error measure (12.37) also has lin ear tails (beyond\\nǫ), but in addition it ﬂattens the contributions of those case s with small\\nresiduals.\\nIfˆβ,ˆβ0are the minimizers of H, the solution function can be shown to\\nhave the form\\nˆβ=N∑\\ni=1(ˆα∗\\ni−ˆαi)xi, (12.39)\\nˆf(x) =N∑\\ni=1(ˆα∗\\ni−ˆαi)⟨x,xi⟩+β0, (12.40)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 453}),\n",
       " Document(page_content='436 12. Flexible Discriminants\\nwhere ˆαi,ˆα∗\\niare positive and solve the quadratic programming problem\\nmin\\nαi,α∗\\niǫN∑\\ni=1(α∗\\ni+αi)−N∑\\ni=1yi(α∗\\ni−αi)+1\\n2N∑\\ni,i′=1(α∗\\ni−αi)(α∗\\ni′−αi′)⟨xi,xi′⟩\\nsubject to the constraints\\n0≤αi, α∗\\ni≤1/λ,\\nN∑\\ni=1(α∗\\ni−αi) = 0, (12.41)\\nαiα∗\\ni= 0.\\nDuetothenatureoftheseconstraints,typicallyonlyasubs etofthesolution\\nvalues (ˆα∗\\ni−ˆαi) are nonzero, and the associated data values are called the\\nsupport vectors. As was the case in the classiﬁcation settin g, the solution\\ndepends on the input values only through the inner products ⟨xi,xi′⟩. Thus\\nwe can generalize the methods to richer spaces by deﬁning an a ppropriate\\ninner product, for example, one of those deﬁned in (12.22).\\nNote that there are parameters, ǫandλ, associated with the criterion\\n(12.36). These seem to play diﬀerent roles. ǫis a parameter of the loss\\nfunctionVǫ, just likecis forVH. Note that both VǫandVHdepend on the\\nscale ofyand hencer. If we scale our response (and hence use VH(r/σ) and\\nVǫ(r/σ)instead),thenwemightconsiderusingpresetvaluesfor candǫ(the\\nvaluec= 1.345 achieves 95% eﬃciency for the Gaussian). The quantity λ\\nis a more traditional regularization parameter, and can be e stimated for\\nexample by cross-validation.\\n12.3.7 Regression and Kernels\\nAs discussed in Section 12.3.3, this kernel property is not u nique to sup-\\nport vector machines. Suppose we consider approximation of the regression\\nfunction in terms of a set of basis functions {hm(x)},m= 1,2,...,M:\\nf(x) =M∑\\nm=1βmhm(x)+β0. (12.42)\\nTo estimate βandβ0we minimize\\nH(β,β0) =N∑\\ni=1V(yi−f(xi))+λ\\n2∑\\nβ2\\nm (12.43)\\nfor some general error measure V(r). For any choice of V(r), the solution\\nˆf(x) =∑ˆβmhm(x)+ˆβ0has the form\\nˆf(x) =N∑\\ni=1ˆaiK(x,xi) (12.44)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 454}),\n",
       " Document(page_content='12.3 Support Vector Machines and Kernels 437\\nwithK(x,y) =∑M\\nm=1hm(x)hm(y). Notice that this has the same form\\nas both the radial basis function expansion and a regulariza tion estimate,\\ndiscussed in Chapters 5 and 6.\\nFor concreteness, let’s work out the case V(r) =r2. LetHbe theN×M\\nbasis matrix with imth element hm(xi), and suppose that M >Nis large.\\nFor simplicity we assume that β0= 0, or that the constant is absorbed in\\nh; see Exercise 12.3 for an alternative.\\nWe estimate βby minimizing the penalized least squares criterion\\nH(β) = (y−Hβ)T(y−Hβ)+λ∥β∥2. (12.45)\\nThe solution is\\nˆy=Hˆβ (12.46)\\nwithˆβdetermined by\\n−HT(y−Hˆβ)+λˆβ= 0. (12.47)\\nFrom this it appears that we need to evaluate the M×Mmatrix of inner\\nproducts in the transformed space. However, we can premulti ply byHto\\ngive\\nHˆβ= (HHT+λI)−1HHTy. (12.48)\\nTheN×NmatrixHHTconsists of inner products between pairs of obser-\\nvationsi,i′; that is, the evaluation of an inner product kernel {HHT}i,i′=\\nK(xi,xi′). It is easy to show (12.44) directly in this case, that the pr edicted\\nvalues at an arbitrary xsatisfy\\nˆf(x) =h(x)Tˆβ\\n=N∑\\ni=1ˆαiK(x,xi), (12.49)\\nwhere ˆα= (HHT+λI)−1y. As in the support vector machine, we need not\\nspecify or evaluate the large set of functions h1(x),h2(x),...,h M(x). Only\\nthe inner product kernel K(xi,xi′) need be evaluated, at the Ntraining\\npoints for each i,i′and at points xfor predictions there. Careful choice\\nofhm(such as the eigenfunctions of particular, easy-to-evalua te kernels\\nK) means, for example, that HHTcan be computed at a cost of N2/2\\nevaluations of K, rather than the direct cost N2M.\\nNote, however, that this property depends on the choice of sq uared norm\\n∥β∥2in the penalty. It does not hold, for example, for the L1norm|β|,\\nwhich may lead to a superior model.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 455}),\n",
       " Document(page_content='438 12. Flexible Discriminants\\n12.3.8 Discussion\\nThe support vector machine can be extended to multiclass pro blems, es-\\nsentially by solving many two-class problems. A classiﬁer i s built for each\\npair of classes, and the ﬁnal classiﬁer is the one that domina tes the most\\n(Kressel,1999;Friedman,1996;HastieandTibshirani,199 8).Alternatively,\\none could use the multinomial loss function along with a suit able kernel,\\nas in Section 12.3.3. SVMs have applications in many other su pervised\\nand unsupervised learning problems. At the time of this writ ing, empirical\\nevidence suggests that it performs well in many real learnin g problems.\\nFinally, we mention the connection of the support vector mac hine and\\nstructural risk minimization (7.9). Suppose the training p oints (or their\\nbasis expansion) are contained in a sphere of radius R, and letG(x) =\\nsign[f(x)] = sign[βTx+β0] as in (12.2). Then one can show that the class\\nof functions{G(x),∥β∥≤A}has VC-dimension hsatisfying\\nh≤R2A2. (12.50)\\nIff(x) separates the training data, optimally for ∥β∥ ≤A, then with\\nprobability at least 1 −ηover training sets (Vapnik, 1996, page 139):\\nErrorTest≤4h[log(2N/h)+1]−log(η/4)\\nN. (12.51)\\nThe support vector classiﬁer was one of the ﬁrst practical le arning pro-\\ncedures for which useful bounds on the VC dimension could be o btained,\\nand hence the SRM program could be carried out. However in the deriva-\\ntion, balls are put around the data points—a process that depe nds on the\\nobserved values of the features. Hence in a strict sense, the VC complexity\\nof the class is not ﬁxed a priori, before seeing the features.\\nThe regularization parameter Ccontrols an upper bound on the VC\\ndimensionoftheclassiﬁer.FollowingtheSRMparadigm,wec ouldchoose C\\nbyminimizingtheupperboundonthetesterror,givenin(12. 51). However,\\nit is not clear that this has any advantage over the use of cros s-validation\\nfor choice of C.\\n12.4 Generalizing Linear Discriminant Analysis\\nIn Section 4.3 we discussed linear discriminant analysis (L DA), a funda-\\nmental tool for classiﬁcation. For the remainder of this cha pter we discuss\\na class of techniques that produce better classiﬁers than LD A by directly\\ngeneralizing LDA.\\nSome of the virtues of LDA are as follows:\\n•Itisasimpleprototypeclassiﬁer.Anewobservationisclas siﬁedtothe\\nclass with closest centroid. A slight twist is that distance is measured\\nin the Mahalanobis metric, using a pooled covariance estima te.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 456}),\n",
       " Document(page_content='12.4 Generalizing Linear Discriminant Analysis 439\\n•LDA is the estimated Bayes classiﬁer if the observations are multi-\\nvariate Gaussian in each class, with a common covariance mat rix.\\nSince this assumption is unlikely to be true, this might not s eem to\\nbe much of a virtue.\\n•The decision boundaries created by LDA are linear, leading t o deci-\\nsion rules that are simple to describe and implement.\\n•LDA provides natural low-dimensional views of the data. For exam-\\nple, Figure 12.12 is an informative two-dimensional view of data in\\n256 dimensions with ten classes.\\n•Often LDA produces the best classiﬁcation results, because of its\\nsimplicity and low variance. LDA was among the top three clas siﬁers\\nfor 7 of the 22 datasets studied in the STATLOG project (Michi e et\\nal., 1994)3.\\nUnfortunately the simplicity of LDA causes it to fail in a num ber of situa-\\ntions as well:\\n•Oftenlineardecisionboundariesdonotadequatelyseparat etheclasses.\\nWhenNis large, it is possible to estimate more complex decision\\nboundaries. Quadratic discriminant analysis (QDA) is ofte n useful\\nhere, and allows for quadratic decision boundaries. More ge nerally\\nwe would like to be able to model irregular decision boundari es.\\n•The aforementioned shortcoming of LDA can often be paraphra sed\\nby saying that a single prototype per class is insuﬃcient. LD A uses\\na single prototype (class centroid) plus a common covarianc e matrix\\nto describe the spread of the data in each class. In many situa tions,\\nseveral prototypes are more appropriate.\\n•At the other end of the spectrum, we may have way too many (corr e-\\nlated)predictors,forexample,inthecaseofdigitizedana loguesignals\\nand images. In this case LDA uses too many parameters, which a re\\nestimated with high variance, and its performance suﬀers. I n cases\\nsuch as this we need to restrict or regularize LDA even furthe r.\\nIn the remainder of this chapter we describe a class of techni ques that\\nattend to all these issues by generalizing the LDA model. Thi s is achieved\\nlargely by three diﬀerent ideas.\\nTheﬁrstideaistorecasttheLDAproblemasalinearregressi onproblem.\\nMany techniques exist for generalizing linear regression t o more ﬂexible,\\nnonparametric forms of regression. This in turn leads to mor e ﬂexible forms\\nof discriminant analysis, which we call FDA. In most cases of interest, the\\n3This study predated the emergence of SVMs.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 457}),\n",
       " Document(page_content='440 12. Flexible Discriminants\\nregression procedures can be seen to identify an enlarged se t of predictors\\nvia basis expansions. FDA amounts to LDA in this enlarged spa ce, the\\nsame paradigm used in SVMs.\\nIn the case of too many predictors, such as the pixels of a digi tized image,\\nwe do not want to expand the set: it is already too large. The se cond idea is\\nto ﬁt an LDA model, but penalize its coeﬃcients to be smooth or otherwise\\ncoherent in the spatial domain, that is, as an image. We call t his procedure\\npenalized discriminant analysis or PDA. With FDA itself, the expanded\\nbasis set is often so large that regularization is also requi red (again as in\\nSVMs). Both of these can be achieved via a suitably regulariz ed regression\\nin the context of the FDA model.\\nThe third idea is to model each class by a mixture of two or more Gaus-\\nsians with diﬀerent centroids, but with every component Gau ssian, both\\nwithinandbetweenclasses,sharingthesamecovariancemat rix.Thisallows\\nfor more complex decision boundaries, and allows for subspa ce reduction\\nas in LDA. We call this extension mixture discriminant analysis or MDA.\\nAll three of these generalizations use a common framework by exploiting\\ntheir connection with LDA.\\n12.5 Flexible Discriminant Analysis\\nIn this section we describe a method for performing LDA using linear re-\\ngression on derived responses. This in turn leads to nonpara metric and ﬂex-\\nible alternatives to LDA. As in Chapter 4, we assume we have ob servations\\nwith a quantitative response Gfalling into one of KclassesG={1,...,K},\\neach having measured features X. Supposeθ:G↦→IR1is a function that\\nassigns scores to the classes, such that the transformed cla ss labels are op-\\ntimally predicted by linear regression on X: If our training sample has the\\nform (gi,xi), i= 1,2,...,N, then we solve\\nmin\\nβ,θN∑\\ni=1(\\nθ(gi)−xT\\niβ)2, (12.52)\\nwith restrictions on θto avoid a trivial solution (mean zero and unit vari-\\nance over the training data). This produces a one-dimension al separation\\nbetween the classes.\\nMore generally, we can ﬁnd up to L≤K−1 sets of independent scorings\\nfortheclasslabels, θ1,θ2,...,θ L,andLcorrespondinglinearmaps ηℓ(X) =\\nXTβℓ,ℓ= 1,...,L, chosen to be optimal for multiple regression in IRp. The\\nscoresθℓ(g) and the maps βℓare chosen to minimize the average squared\\nresidual,\\nASR=1\\nNL∑\\nℓ=1[N∑\\ni=1(\\nθℓ(gi)−xT\\niβℓ)2]\\n. (12.53)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 458}),\n",
       " Document(page_content='12.5 Flexible Discriminant Analysis 441\\nThe set of scores are assumed to be mutually orthogonal and no rmalized\\nwith respect to an appropriate inner product to prevent triv ial zero\\nsolutions.\\nWhy are we going down this road? It can be shown that the sequen ce\\nof discriminant (canonical) vectors νℓderived in Section 4.3.3 are identical\\nto the sequence βℓup to a constant (Mardia et al., 1979; Hastie et al.,\\n1995). Moreover, the Mahalanobis distance of a test point xto thekth\\nclass centroid ˆ µkis given by\\nδJ(x,ˆµk) =K−1∑\\nℓ=1wℓ(ˆηℓ(x)−¯ηk\\nℓ)2+D(x), (12.54)\\nwhere ¯ηk\\nℓis the mean of the ˆ ηℓ(xi) in thekth class, and D(x) does not\\ndepend on k. Herewℓare coordinate weights that are deﬁned in terms of\\nthe mean squared residual r2\\nℓof theℓth optimally scored ﬁt\\nwℓ=1\\nr2\\nℓ(1−r2\\nℓ). (12.55)\\nIn Section 4.3.2 we saw that these canonical distances are al l that is needed\\nforclassiﬁcationintheGaussiansetup,withequalcovaria ncesineachclass.\\nTo summarize:\\nLDA can be performed by a sequence of linear regressions, fol -\\nlowed by classiﬁcation to the closest class centroid in the s pace\\nof ﬁts. The analogy applies both to the reduced rank version,\\nor the full rank case when L=K−1.\\nThe real power of this result is in the generalizations that i t invites. We\\ncan replace the linear regression ﬁts ηℓ(x) =xTβℓby far more ﬂexible,\\nnonparametric ﬁts, and by analogy achieve a more ﬂexible cla ssiﬁer than\\nLDA. We have in mind generalized additive ﬁts, spline functi ons, MARS\\nmodels and the like. In this more general form the regression problems are\\ndeﬁned via the criterion\\nASR({θℓ,ηℓ}L\\nℓ=1) =1\\nNL∑\\nℓ=1[N∑\\ni=1(θℓ(gi)−ηℓ(xi))2+λJ(ηℓ)]\\n,(12.56)\\nwhereJisaregularizerappropriateforsomeformsofnonparametri cregres-\\nsion, such as smoothing splines, additive splines and lower -order ANOVA\\nspline models. Also included are the classes of functions an d associated\\npenalties generated by kernels, as in Section 12.3.3.\\nBefore we describe the computations involved in this genera lization, let\\nus consider a very simple example. Suppose we use degree-2 po lynomial\\nregression for each ηℓ. The decision boundaries implied by the (12.54) will\\nbe quadratic surfaces, since each of the ﬁtted functions is q uadratic, and as', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 459}),\n",
       " Document(page_content='442 12. Flexible Discriminants\\n-2 0 2-2 0 2o\\nooo\\noo\\nooo\\noo\\no o\\no\\noooo\\noo\\noo oo\\no\\no\\nooo\\noooo\\noo\\nooo\\noo\\noo\\noo\\noo\\nooo\\no\\nooo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\nooo\\noo\\no\\noo\\no\\noooo\\nooo\\noo\\noo\\nooo\\noo o\\noo\\nooo\\noo\\no\\nFIGURE 12.9. The data consist of 50points generated from each of N(0,I)and\\nN(0,9\\n4I). The solid black ellipse is the decision boundary found by FDA us ing\\ndegree-two polynomial regression. The dashed purple circle is the Bayes decision\\nboundary.\\nin LDA their squares cancel out when comparing distances. We could have\\nachieved identical quadratic boundaries in a more conventional way, by\\naugmenting our original predictors with their squares and c ross-products.\\nIn the enlarged space one performs an LDA, and the linear boun daries in\\nthe enlarged space map down to quadratic boundaries in the or iginal space.\\nA classic example is a pair of multivariate Gaussians center ed at the origin,\\none having covariance matrix I, and the other cIforc >1; Figure 12.9\\nillustrates. The Bayes decision boundary is the sphere ∥x∥=pclogc\\n2(c−1), which\\nis a linear boundary in the enlarged space.\\nMany nonparametric regression procedures operate by gener ating a basis\\nexpansion of derived variables, and then performing a linea r regression in\\nthe enlarged space. The MARS procedure (Chapter 9) is exactl y of this\\nform. Smoothing splines and additive spline models generat e an extremely\\nlarge basis set ( N×pbasis functions for additive splines), but then perform\\na penalized regression ﬁt in the enlarged space. SVMs do as we ll; see also\\nthe kernel-based regression example in Section 12.3.7. FDA in this case can\\nbeshowntoperforma penalized linear discriminant analysis intheenlarged\\nspace.WeelaborateinSection12.6.Linearboundariesinth eenlargedspace\\nmap down to nonlinear boundaries in the reduced space. This i s exactly the\\nsame paradigm that is used with support vector machines (Sec tion 12.3).\\nWe illustrate FDA on the speech recognition example used in C hapter 4,\\nwithK= 11 classes and p= 10 predictors. The classes correspond to', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 460}),\n",
       " Document(page_content='12.5 Flexible Discriminant Analysis 443\\noooo oo\\noooooooo\\noooo\\noo\\no\\no\\no\\no\\noooooooooooo\\nooo\\no\\noooooooo\\nooooooo\\no\\noo\\no\\nooooo\\no\\nooooooo\\noooooo\\no\\no\\nooooooooooooo\\nooooooooooo\\noooo\\noooo\\no\\nooooo\\noooooo o\\no\\noooooooooooo\\noo\\no\\noo\\nooooooooooooo\\noooooooooooooooooooooooooooooo\\no\\no\\nooooooo\\no\\no\\nooooooo\\noooooooooooo\\no\\no\\noooooooooooooooo\\noo\\no\\no\\noooooooo\\nooooooooooooooooo\\noooo\\no\\no\\nooooo\\no\\nooooo\\no\\noo\\no\\no\\no\\no\\nooooooooooooo\\noo\\noooooo\\nooooooooo\\nooo\\nooooooooo\\no\\noo\\no\\nooo\\nooooooooooooooo\\nooo\\noooooooo\\noooo\\nooooooooooooo\\no\\noooo\\nooooo\\noooooo\\noo\\no\\no\\no\\no\\noooooo\\no\\noooooo\\noooo\\no\\nooooooooooooo\\no ooooooooooooooooo\\no\\no\\nooooo\\nooooooooooo\\no\\no\\nooo\\nooooooo\\noooooo\\noooooo\\noooooooooo\\no\\nooooooo\\noooooooooooo\\noo\\no\\nooo\\nCoordinate 1 for Training DataCoordinate 2 for Training DataLinear Discriminant Analysis\\noo oooooo\\noooo\\noooooooooooo\\noooooo\\no\\nooo\\no\\noooooo\\noooo\\nooo\\nooooooo\\nooooo\\noooooo\\nooooooooooo\\nooooooo\\nooooooo\\no\\noooo\\noooooo\\nooo ooooooooo\\noooooooo\\noooo\\noooooo\\no\\noooo\\nooooooo\\noooooo\\noooooooooooooooooo\\noooooooooooooooooooooooooooo\\nooooooooooooo\\nooo\\no\\no oooooooooooooooooooo\\noooo\\no\\nooooooo\\noo\\noooo\\nooo\\noo\\nooooo\\nooo\\no\\noooo ooooooooo\\no\\no\\noooo\\noooooooooo\\nooo\\noo\\noo\\nooo\\nooo\\no\\no\\no\\noooooo\\no\\noooo\\no\\noooooo\\nooo\\nooooooo\\nooo\\nooooooooo\\noooo\\no\\nooo\\nooooo\\no\\nooooooo\\no\\noooooo\\no\\noooo\\noooo\\no\\nooo\\noooo\\no\\noooooo\\no\\noooooooooooo\\noo\\no\\nooo\\no\\no\\no\\noooooo\\no\\nooooooooooooo\\nooooo\\noooooooo\\noooo\\noooooo\\nooooooooo\\no\\noo\\nooo\\noooo\\noooooo\\no\\noooo\\nooooooooooo\\nooooo oo\\noooo\\noooo\\noooo\\nCoordinate 1 for Training DataCoordinate 2 for Training DataFlexible Discriminant Analysis -- Bruto\\nFIGURE 12.10. The left plot shows the ﬁrst two LDA canonical variates for\\nthe vowel training data. The right plot shows the correspondi ng projection when\\nFDA/BRUTO is used to ﬁt the model; plotted are the ﬁtted regress ion functions\\nˆη1(xi)andˆη2(xi). Notice the improved separation. The colors represent the ele ven\\ndiﬀerent vowel sounds.\\n11 vowel sounds, each contained in 11 diﬀerent words. Here ar e the words,\\npreceded by the symbols that represent them:\\nVowel Word Vowel Word Vowel Word Vowel Word\\ni: heed O hod I hid C: hoard\\nE head U hood A had u: who’d\\na: hard 3: heard Y hud\\nEach of eight speakers spoke each word six times in the traini ng set, and\\nlikewise seven speakers in the test set. The ten predictors a re derived from\\nthedigitizedspeechinarathercomplicatedway,butstanda rdinthespeech\\nrecognition world. There are thus 528 training observation s, and 462 test\\nobservations. Figure 12.10 shows two-dimensional project ions produced by\\nLDA and FDA. The FDA model used adaptive additive-spline reg ression\\nfunctions to model the ηℓ(x), and the points plotted in the right plot have\\ncoordinates ˆ η1(xi) and ˆη2(xi). The routine used in S-PLUS is called bruto,\\nhence the heading on the plot and in Table 12.3. We see that ﬂex ible model-\\ning has helped to separate the classes in this case. Table 12. 3 shows training\\nand test error rates for a number of classiﬁcation technique s. FDA/MARS\\nrefers to Friedman’s multivariate adaptive regression spl ines; degree = 2\\nmeans pairwise products are permitted. Notice that for FDA/ MARS, the\\nbest classiﬁcation results are obtained in a reduced-rank s ubspace.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 461}),\n",
       " Document(page_content='444 12. Flexible Discriminants\\nTABLE 12.3. Vowel recognition data performance results. The results for n eural\\nnetworks are the best among a much larger set, taken from a neur al network\\narchive. The notation FDA/BRUTO refers to the regression me thod used with\\nFDA.\\nTechnique Error Rates\\nTraining Test\\n(1) LDA 0.32 0.56\\nSoftmax 0.48 0.67\\n(2) QDA 0.01 0.53\\n(3) CART 0.05 0.56\\n(4) CART (linear combination splits) 0.05 0.54\\n(5) Single-layer perceptron 0.67\\n(6) Multi-layer perceptron (88 hidden units) 0.49\\n(7) Gaussian node network (528 hidden units) 0.45\\n(8) Nearest neighbor 0.44\\n(9) FDA/BRUTO 0.06 0.44\\nSoftmax 0.11 0.50\\n(10) FDA/MARS (degree = 1) 0.09 0.45\\nBest reduced dimension (=2) 0.18 0.42\\nSoftmax 0.14 0.48\\n(11) FDA/MARS (degree = 2) 0.02 0.42\\nBest reduced dimension (=6) 0.13 0.39\\nSoftmax 0.10 0.50\\n12.5.1 Computing the FDA Estimates\\nThe computations for the FDA coordinates can be simpliﬁed in many im-\\nportant cases, in particular when the nonparametric regres sion procedure\\ncan be represented as a linear operator. We will denote this o perator by\\nSλ; that is, ˆy=Sλy, whereyis the vector of responses and ˆythe vector\\nof ﬁts. Additive splines have this property, if the smoothin g parameters are\\nﬁxed, as does MARS once the basis functions are selected. The subscriptλ\\ndenotes the entire set of smoothing parameters. In this case optimal scoring\\nis equivalent to a canonical correlation problem, and the so lution can be\\ncomputedbyasingleeigen-decomposition.Thisispursuedi nExercise12.6,\\nand the resulting algorithm is presented here.\\nWe create an N×Kindicator response matrix Yfrom the responses gi,\\nsuch thatyik= 1 ifgi=k, otherwise yik= 0. For a ﬁve-class problem Y\\nmight look like the following:', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 462}),\n",
       " Document(page_content='12.5 Flexible Discriminant Analysis 445\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8edC1C2C3C4C5\\ng1= 2 0 1 0 0 0\\ng2= 1 1 0 0 0 0\\ng3= 1 1 0 0 0 0\\ng4= 5 0 0 0 0 1\\ng5= 4 0 0 0 1 0\\n......\\ngN= 3 0 0 1 0 0\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\nHere are the computational steps:\\n1.Multivariate nonparametric regression. Fit a multiresponse, adaptive\\nnonparametric regression of YonX, giving ﬁtted values ˆY. LetSλ\\nbe the linear operator that ﬁts the ﬁnal chosen model, and η∗(x) be\\nthe vector of ﬁtted regression functions.\\n2.Optimal scores. Compute the eigen-decomposition of YTˆY=YTSλY,\\nwhere the eigenvectors Θare normalized: ΘTDπΘ=I. HereDπ=\\nYTY/Nis a diagonal matrix of the estimated class prior\\nprobabilities.\\n3.Updatethemodelfromstep1usingtheoptimalscores: η(x) =ΘTη∗(x).\\nThe ﬁrst of the Kfunctions in η(x) is the constant function— a trivial\\nsolution; the remaining K−1 functions are the discriminant functions. The\\nconstant function, along with the normalization, causes al l the remaining\\nfunctions to be centered.\\nAgainSλcan correspond to any regression method. When Sλ=HX, the\\nlinear regression projection operator, then FDA is linear d iscriminant anal-\\nysis. The software that we reference in the Computational Considerations\\nsection on page 455 makes good use of this modularity; the fdafunction\\nhas amethod=argument that allows one to supply anyregression function,\\nas long as it follows some natural conventions. The regressi on functions\\nwe provide allow for polynomial regression, adaptive addit ive models and\\nMARS. They all eﬃciently handle multiple responses, so step (1) is a single\\ncall to a regression routine. The eigen-decomposition in st ep (2) simulta-\\nneously computes all the optimal scoring functions.\\nIn Section 4.2 we discussed the pitfalls of using linear regr ession on an\\nindicator response matrix as a method for classiﬁcation. In particular, se-\\nvere masking can occur with three or more classes. FDA uses th e ﬁts from\\nsuch a regression in step (1), but then transforms them furth er to produce\\nuseful discriminant functions that are devoid of these pitf alls. Exercise 12.9\\ntakes another view of this phenomenon.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 463}),\n",
       " Document(page_content='446 12. Flexible Discriminants\\n12.6 Penalized Discriminant Analysis\\nAlthough FDA is motivated by generalizing optimal scoring, it can also be\\nviewed directly as a form of regularized discriminant analy sis. Suppose the\\nregression procedure used in FDA amounts to a linear regress ion onto a\\nbasis expansion h(X), with a quadratic penalty on the coeﬃcients:\\nASR({θℓ,βℓ}L\\nℓ=1) =1\\nNL∑\\nℓ=1[N∑\\ni=1(θℓ(gi)−hT(xi)βℓ)2+λβT\\nℓΩβℓ]\\n.(12.57)\\nThe choice of Ωdepends on the problem. If ηℓ(x) =h(x)βℓis an expansion\\non spline basis functions, Ωmight constrain ηℓto be smooth over IRp. In\\nthe case of additive splines, there are Nspline basis functions for each\\ncoordinate, resulting in a total of Npbasis functions in h(x);Ωin this case\\nisNp×Npand block diagonal.\\nThe steps in FDA can then be viewed as a generalized form of LDA ,\\nwhich we call penalized discriminant analysis , or PDA:\\n•Enlarge the set of predictors Xvia a basis expansion h(X).\\n•Use (penalized) LDA in the enlarged space, where the penaliz ed\\nMahalanobis distance is given by\\nD(x,µ) = (h(x)−h(µ))T(ΣW+λΩ)−1(h(x)−h(µ)),(12.58)\\nwhereΣWis the within-class covariance matrix of the derived vari-\\nablesh(xi).\\n•Decompose the classiﬁcation subspace using a penalized met ric:\\nmaxuTΣBetusubject touT(ΣW+λΩ)u= 1.\\nLoosely speaking, the penalized Mahalanobis distance tend s to give less\\nweight to “rough” coordinates, and more weight to “smooth” o nes; since\\nthe penalty is not diagonal, the same applies to linear combi nations that\\nare rough or smooth.\\nForsomeclassesofproblems,theﬁrststep,involvingtheba sisexpansion,\\nis not needed; we already have far too many (correlated) pred ictors. A\\nleading example is when the objects to be classiﬁed are digit ized analog\\nsignals:\\n•the log-periodogram of a fragment of spoken speech, sampled at a set\\nof 256 frequencies; see Figure 5.5 on page 149.\\n•the grayscale pixel values in a digitized image of a handwrit ten digit.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 464}),\n",
       " Document(page_content='12.6 Penalized Discriminant Analysis 447\\nLDA: Coefficient 1 PDA: Coefficient 1 LDA: Coefficient 2 PDA: Coefficient 2 LDA: Coefficient 3 PDA: Coefficient 3\\nLDA: Coefficient 4 PDA: Coefficient 4 LDA: Coefficient 5 PDA: Coefficient 5 LDA: Coefficient 6 PDA: Coefficient 6\\nLDA: Coefficient 7 PDA: Coefficient 7 LDA: Coefficient 8 PDA: Coefficient 8 LDA: Coefficient 9 PDA: Coefficient 9\\nFIGURE 12.11. The images appear in pairs, and represent the nine discrim-\\ninant coeﬃcient functions for the digit recognition problem . The left member of\\neach pair is the LDA coeﬃcient, while the right member is the PD A coeﬃcient,\\nregularized to enforce spatial smoothness.\\nIt is also intuitively clear in these cases why regularizati on is needed.\\nTake the digitized image as an example. Neighboring pixel va lues will tend\\nto be correlated, being often almost the same. This implies t hat the pair\\nof corresponding LDA coeﬃcients for these pixels can be wild ly diﬀerent\\nand opposite in sign, and thus cancel when applied to similar pixel values.\\nPositively correlated predictors lead to noisy, negativel y correlated coeﬃ-\\ncient estimates, and this noise results in unwanted samplin g variance. A\\nreasonable strategy is to regularize the coeﬃcients to be smooth over the\\nspatial domain, as with images. This is what PDA does. The com putations\\nproceed just as for FDA, except that an appropriate penalize d regression\\nmethod is used. Here hT(X)βℓ=Xβℓ, andΩis chosen so that βT\\nℓΩβℓ\\npenalizes roughness in βℓwhen viewed as an image. Figure 1.2 on page 4\\nshows some examples of handwritten digits. Figure 12.11 sho ws the dis-\\ncriminant variates using LDA and PDA. Those produced by LDA a ppear\\nassalt-and-pepper images, while those produced by PDA are smooth im-\\nages. The ﬁrst smooth image can be seen as the coeﬃcients of a l inear\\ncontrast functional for separating images with a dark centr al vertical strip\\n(ones, possibly sevens) from images that are hollow in the mi ddle (zeros,\\nsome fours). Figure 12.12 supports this interpretation, an d with more dif-\\nﬁculty allows an interpretation of the second coordinate. T his and other', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 465}),\n",
       " Document(page_content='448 12. Flexible Discriminants\\n-5 0 5-6 -4 -2 0 2 4 60\\n0\\n00\\n0000\\n0\\n0000\\n00\\n00\\n00000\\n00\\n00000\\n0\\n0000\\n000\\n00\\n0\\n00\\n00000\\n0\\n0000\\n0\\n0\\n0000\\n0\\n00000\\n0\\n000\\n00\\n00\\n000\\n000\\n0000\\n00\\n00\\n000\\n00\\n000\\n00\\n000\\n0\\n0000\\n0000\\n000\\n0\\n00\\n00\\n00\\n00\\n00\\n00\\n000\\n00000 000\\n0000\\n00\\n0\\n000 0\\n0000\\n000\\n000\\n00 0\\n0\\n0000\\n0\\n00\\n0 000\\n0\\n000\\n00\\n0\\n0\\n000\\n00 00\\n000\\n00\\n0\\n000\\n00\\n0\\n00\\n00 0 0\\n000\\n0\\n00\\n0\\n000000\\n0\\n0 0\\n000\\n00\\n0 00\\n000 0\\n00\\n000\\n00\\n000\\n000\\n0\\n000\\n000\\n000\\n000\\n000\\n00\\n000\\n00\\n00\\n00\\n000\\n0\\n000\\n0000\\n0\\n00 00\\n0000\\n00\\n0\\n000\\n00\\n00\\n0\\n00\\n0\\n0000\\n0\\n000000\\n00\\n000\\n0\\n000\\n000\\n00\\n000\\n000\\n00\\n000\\n00000\\n00\\n0\\n0000\\n00\\n0\\n0\\n011\\n11\\n11\\n11\\n1\\n1111\\n111\\n111\\n11\\n1\\n11\\n11\\n111\\n11\\n111\\n11\\n111\\n11\\n1\\n111\\n1\\n11\\n111\\n1\\n111\\n111\\n111\\n11\\n11\\n1111\\n111\\n11\\n1\\n1\\n111\\n1\\n11\\n1111\\n1\\n111\\n111\\n111\\n11\\n1\\n11111\\n11\\n1\\n11\\n111\\n11111\\n111\\n1\\n11111\\n1\\n111\\n1111\\n11\\n11\\n11\\n11111\\n1111\\n11\\n1\\n11\\n11\\n1\\n1\\n1 1\\n11\\n1\\n111\\n11\\n1\\n11\\n1\\n1\\n111\\n1\\n11\\n111\\n111\\n11111\\n111\\n1\\n11\\n111\\n111111\\n111\\n1\\n11\\n11\\n1111\\n111\\n11\\n11\\n1 1\\n111\\n11\\n1111111\\n1\\n11\\n111\\n111\\n1\\n11111111\\n1\\n111\\n111\\n1\\n22\\n22\\n2222\\n222\\n22\\n2\\n222\\n22\\n2\\n22\\n2\\n22\\n2222\\n222\\n2\\n22\\n2\\n2\\n2\\n2222\\n2222\\n22222\\n22\\n2\\n22\\n22\\n2\\n22\\n222\\n222\\n2\\n2\\n22\\n22\\n2\\n222\\n2\\n2\\n22\\n22\\n2\\n2\\n22\\n22\\n222222\\n222\\n2\\n2\\n2222222\\n22222\\n222\\n222\\n2\\n2222\\n2\\n2\\n2\\n2222\\n222\\n22\\n222\\n222\\n2\\n22\\n2\\n2222\\n2\\n222\\n2222\\n22\\n222\\n22\\n222\\n2\\n2\\n222 22\\n2\\n22\\n22\\n22\\n2222\\n22\\n222\\n22\\n22\\n2222\\n333\\n3\\n33\\n3\\n3 3\\n333\\n333\\n333\\n33\\n33\\n3\\n33\\n3\\n3\\n333\\n33333\\n33\\n33\\n3\\n33333\\n33\\n3\\n3\\n333\\n3333\\n33\\n3333\\n33\\n33\\n3\\n33\\n33\\n33\\n3\\n333\\n3\\n3\\n333\\n33\\n333\\n333\\n333\\n3\\n3\\n33 3\\n33333\\n3\\n3333\\n33\\n3\\n3\\n33\\n3\\n33\\n3333\\n33\\n3333\\n33\\n33\\n333\\n333\\n333\\n33\\n3\\n333333\\n3\\n3\\n33 3\\n333\\n333\\n33\\n33\\n33\\n4\\n4444\\n4\\n4 44444\\n44\\n4\\n44\\n444\\n4\\n444\\n44\\n44\\n4\\n44\\n444\\n44\\n4\\n4\\n44\\n44\\n44\\n44\\n444\\n44444\\n44\\n444\\n4\\n44\\n44\\n4\\n44\\n4\\n44444\\n4\\n444\\n44\\n44\\n4 44\\n4\\n4\\n444\\n4\\n444\\n44\\n4444\\n4\\n4444\\n4444\\n4\\n444\\n44\\n44\\n444\\n44\\n444\\n44\\n44\\n4\\n4\\n44444\\n44\\n4\\n4\\n444\\n44 44\\n44\\n444\\n4\\n44\\n4\\n44\\n4\\n444\\n444444\\n444\\n4444\\n44\\n44\\n4\\n44\\n4\\n444\\n444\\n44\\n444\\n44\\n444\\n4455\\n55\\n5\\n55\\n555\\n5555\\n5\\n5 5\\n555\\n5\\n555\\n55\\n55\\n55\\n5\\n5555\\n555\\n55\\n5555\\n5555\\n5\\n55\\n55\\n55\\n5\\n555\\n55\\n55\\n55555\\n555\\n5 55\\n5\\n5555\\n555\\n5\\n5\\n55\\n5\\n555\\n555\\n55\\n55 5\\n55\\n5\\n555\\n55\\n55\\n555\\n555\\n55\\n55\\n555\\n55\\n5 555\\n55\\n5\\n5\\n5\\n55\\n5\\n555555\\n5\\n55\\n55\\n55\\n55\\n55\\n55\\n555\\n5\\n55\\n6\\n6666\\n66\\n66\\n6\\n66\\n66\\n666\\n6\\n66\\n66\\n6\\n666\\n666\\n6\\n6\\n6\\n666\\n6\\n66\\n66666\\n66\\n6\\n66\\n6\\n666\\n66\\n666\\n66\\n66\\n6\\n66\\n6\\n66\\n666\\n6\\n6\\n66\\n666\\n6\\n6666\\n6\\n666\\n66\\n66\\n666\\n66 6\\n666\\n66\\n6\\n666\\n666\\n6\\n6\\n66\\n666\\n666\\n6666\\n6\\n6 66\\n6\\n66\\n6\\n66\\n66\\n6666\\n666\\n66\\n6666\\n666\\n6666\\n66\\n666\\n666\\n66\\n666\\n6\\n6\\n6\\n7\\n7777\\n77\\n777\\n77\\n77\\n7\\n777\\n77777\\n777\\n7\\n77 7\\n77\\n77\\n7\\n777\\n777777\\n7\\n777\\n77\\n777\\n7\\n77\\n77\\n77777\\n7\\n77\\n77 7\\n777\\n77\\n7\\n77\\n777\\n77777\\n77\\n77\\n7\\n77777\\n777\\n77\\n7\\n77\\n77\\n777\\n7\\n7\\n7\\n77\\n77\\n77\\n77\\n77\\n7777\\n7\\n77\\n77\\n777\\n77\\n7777\\n77777\\n7\\n7\\n788\\n8\\n8\\n88\\n8888\\n88\\n8\\n8\\n888\\n88\\n8\\n888\\n88\\n88\\n8\\n888\\n8\\n888\\n88\\n88\\n888\\n88\\n888\\n88 888\\n8888\\n88\\n8888\\n88 88\\n88\\n88\\n8888\\n88 8\\n8\\n8 8\\n88\\n888\\n888\\n88\\n8\\n888\\n88\\n888\\n8\\n8\\n88\\n8888\\n888\\n888\\n88\\n8\\n888\\n88\\n88\\n8\\n8\\n88888\\n88\\n8\\n8888888 8\\n8\\n8\\n8\\n88\\n888 888\\n888\\n88\\n8\\n888\\n8\\n8 888\\n9999\\n999\\n999\\n99999\\n999\\n99\\n999\\n999\\n9\\n9\\n99\\n999\\n9\\n9\\n999\\n999\\n999\\n9999\\n9\\n99\\n999\\n99\\n999\\n99\\n9\\n9\\n9999\\n99\\n999\\n99\\n9\\n99\\n9\\n9\\n99\\n99\\n9\\n9\\n9\\n9\\n999\\n999\\n9\\n9\\n99\\n9\\n99\\n9\\n99\\n999\\n99\\n9999\\n99999\\n9\\n999\\n99\\n999\\n99\\n9\\n999\\n9\\n99\\n999\\n9\\n99\\n99\\n99\\n9\\n99\\n99\\n99\\n9\\n99\\n9\\n999\\n99\\n999\\n9\\n9999\\n9\\n99\\n999\\n9901\\n2\\n3\\n456\\n78\\n9\\nPDA: Discriminant Coordinate 1PDA: Discriminant Coordinate 2\\nFIGURE 12.12. The ﬁrst two penalized canonical variates, evaluated for the\\ntest data. The circles indicate the class centroids. The ﬁrst c oordinate contrasts\\nmainly0’s and1’s, while the second contrasts 6’s and7/9’s.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 466}),\n",
       " Document(page_content='12.7 Mixture Discriminant Analysis 449\\nexamples are discussed in more detail in Hastie et al. (1995) , who also show\\nthat the regularization improves the classiﬁcation perfor mance of LDA on\\nindependent test data by a factor of around 25% in the cases th ey tried.\\n12.7 Mixture Discriminant Analysis\\nLinear discriminant analysis can be viewed as a prototype classiﬁer. Each\\nclass is represented by its centroid, and we classify to the c losest using an\\nappropriate metric. In many situations a single prototype i s not suﬃcient\\nto represent inhomogeneous classes, and mixture models are more appro-\\npriate. In this section we review Gaussian mixture models an d show how\\nthey can be generalized via the FDA and PDA methods discussed earlier.\\nA Gaussian mixture model for the kth class has density\\nP(X|G=k) =Rk∑\\nr=1πkrφ(X;µkr,Σ), (12.59)\\nwhere the mixing proportions πkrsum to one. This has Rkprototypes for\\nthekth class, and in our speciﬁcation, the same covariance matri xΣis\\nused as the metric throughout. Given such a model for each cla ss, the class\\nposterior probabilities are given by\\nP(G=k|X=x) =∑Rk\\nr=1πkrφ(X;µkr,Σ)Πk∑K\\nℓ=1∑Rℓ\\nr=1πℓrφ(X;µℓr,Σ)Πℓ, (12.60)\\nwhere Π krepresent the class prior probabilities.\\nWe saw these calculations for the special case of two compone nts in\\nChapter 8. As in LDA, we estimate the parameters by maximum li kelihood,\\nusing the joint log-likelihood based on P(G,X):\\nK∑\\nk=1∑\\ngi=klog[Rk∑\\nr=1πkrφ(xi;µkr,Σ)Πk]\\n. (12.61)\\nThe sum within the log makes this a rather messy optimization problem\\nif tackled directly. The classical and natural method for co mputing the\\nmaximum-likelihood estimates (MLEs) for mixture distribu tions is the EM\\nalgorithm (Dempster et al., 1977), which is known to possess good conver-\\ngence properties. EM alternates between the two steps:', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 467}),\n",
       " Document(page_content='450 12. Flexible Discriminants\\nE-step:Given the current parameters, compute the responsibility of sub-\\nclassckrwithin class kfor each of the class- kobservations ( gi=k):\\nW(ckr|xi,gi) =πkrφ(xi;µkr,Σ)∑Rk\\nℓ=1πkℓφ(xi;µkℓ,Σ).(12.62)\\nM-step:Compute the weighted MLEs for the parameters of each of the\\ncomponent Gaussians within each of the classes, using the we ights\\nfrom the E-step.\\nIn the E-step, the algorithm apportions the unit weight of an observation\\nin classkto the various subclasses assigned to that class. If it is clo se to the\\ncentroid of a particular subclass, and far from the others, i t will receive a\\nmassclosetooneforthatsubclass.Ontheotherhand,observ ationshalfway\\nbetween two subclasses will get approximately equal weight for both.\\nIn the M-step, an observation in class kis usedRktimes, to estimate the\\nparameters in each of the Rkcomponent densities, with a diﬀerent weight\\nforeach.TheEMalgorithm isstudiedindetailinChapter8.T healgorithm\\nrequires initialization, which can have an impact, since mi xture likelihoods\\nare generally multimodal. Our software (referenced in the Computational\\nConsiderations on page 455) allows several strategies; here we describe the\\ndefault. The user supplies the number Rkof subclasses per class. Within\\nclassk, ak-means clustering model, with multiple random starts, is ﬁt ted\\nto the data. This partitions the observations into Rkdisjoint groups, from\\nwhich an initial weight matrix, consisting of zeros and ones , is created.\\nOur assumption of an equal component covariance matrix Σthroughout\\nbuys an additional simplicity; we can incorporate rank rest rictions in the\\nmixtureformulationjustlikeinLDA.Tounderstandthis,we reviewalittle-\\nknown fact about LDA. The rank- LLDA ﬁt (Section 4.3.3) is equivalent to\\nthe maximum-likelihood ﬁt of a Gaussian model,where the diﬀ erent mean\\nvectors in each class are conﬁned to a rank- Lsubspace of IRp(Exercise 4.8).\\nWe can inherit this property for the mixture model, and maxim ize the log-\\nlikelihood (12.61) subject to rank constraints on allthe∑\\nkRkcentroids:\\nrank{µkℓ}=L.\\nAgain the EM algorithm is available, and the M-step turns out to be\\na weighted version of LDA, with R=∑K\\nk=1Rk“classes.” Furthermore,\\nwe can use optimal scoring as before to solve the weighted LDA problem,\\nwhich allows us to use a weighted version of FDA or PDA at this s tage.\\nOne would expect, in addition to an increase in the number of “ classes,” a\\nsimilar increase in the number of “observations” in the kth class by a factor\\nofRk. It turns out that this is not the case if linear operators are used for\\nthe optimal scoring regression. The enlarged indicator Ymatrix collapses\\nin this case to a blurredresponse matrix Z, which is intuitively pleasing.\\nFor example, suppose there are K= 3 classes, and Rk= 3 subclasses per\\nclass. Then Zmight be', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 468}),\n",
       " Document(page_content='12.7 Mixture Discriminant Analysis 451\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8edc11c12c13c21c22c23c31c32c33\\ng1= 2 0 0 0 0 .3 0.5 0.2 0 0 0\\ng2= 1 0.9 0.1 0.0 0 0 0 0 0 0\\ng3= 1 0.1 0.8 0.1 0 0 0 0 0 0\\ng4= 3 0 0 0 0 0 0 0 .5 0.4 0.1\\ng5= 2 0 0 0 0 .7 0.1 0.2 0 0 0\\n......\\ngN= 3 0 0 0 0 0 0 0 .1 0.1 0.8\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8,(12.63)\\nwhere the entries in a class- krow correspond to W(ckr|x,gi).\\nThe remaining steps are the same:\\nˆZ=SZ\\nZTˆZ=ΘDΘT\\nUpdateπs and Πs\\uf8fc\\n\\uf8fd\\n\\uf8feM-step of MDA .\\nThese simple modiﬁcations add considerable ﬂexibility to t he mixture\\nmodel:\\n•The dimension reduction step in LDA, FDA or PDA is limited by\\nthe number of classes; in particular, for K= 2 classes no reduction is\\npossible. MDA substitutes subclasses for classes, and then allows us\\nto look at low-dimensional views of the subspace spanned by t hese\\nsubclass centroids. This subspace will often be an importan t one for\\ndiscrimination.\\n•By using FDA or PDA in the M-step, we can adapt even more to par-\\nticular situations. For example, we can ﬁt MDA models to digi tized\\nanalog signals and images, with smoothness constraints bui lt in.\\nFigure 12.13 compares FDA and MDA on the mixture example.\\n12.7.1 Example: Waveform Data\\nWe now illustrate some of these ideas on a popular simulated e xample,\\ntaken from Breiman et al. (1984, pages 49–55), and used in Has tie and\\nTibshirani (1996b) and elsewhere. It is a three-class probl em with 21 vari-\\nables, and is considered to be a diﬃcult pattern recognition problem. The\\npredictors are deﬁned by\\nXj=Uh1(j)+(1−U)h2(j)+ǫjClass 1,\\nXj=Uh1(j)+(1−U)h3(j)+ǫjClass 2,(12.64)\\nXj=Uh2(j)+(1−U)h3(j)+ǫjClass 3,\\nwherej= 1,2,...,21,Uis uniform on (0 ,1),ǫjare standard normal vari-\\nates, and the hℓare the shifted triangular waveforms: h1(j) = max(6−', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 469}),\n",
       " Document(page_content='452 12. Flexible Discriminants\\nFDA / MARS - Degree 2', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 470}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 470}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 470}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 470}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 470}),\n",
       " Document(page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\nooooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.185\\nTest Error:       0.235\\nBayes Error:    0.210\\nMDA - 5 Subclasses per Class', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 470}),\n",
       " Document(page_content='.... .. .. . .. . .. . .. . . . .. . . . .. . . . . .. . . . . . . .. . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 470}),\n",
       " Document(page_content='. . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 470}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 470}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 470}),\n",
       " Document(page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\nooooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\n•••\\n••\\n•••\\n••••\\n••\\n•••\\n••\\n•\\nTraining Error: 0.17\\nTest Error:       0.22\\nBayes Error:    0.21\\nFIGURE 12.13. FDA and MDA on the mixture data. The upper plot uses\\nFDA with MARS as the regression procedure. The lower plot uses M DA with\\nﬁve mixture centers per class (indicated). The MDA solution is close to Bayes\\noptimal, as might be expected given the data arise from mixtur es of Gaussians.\\nThe broken purple curve in the background is the Bayes decisio n boundary.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 470}),\n",
       " Document(page_content='12.7 Mixture Discriminant Analysis 453\\n1 1 1 1 1 1111111111\\n1\\n1\\n1\\n1\\n1\\n1 2 2 2 2 22222222222\\n2\\n2\\n2\\n2\\n2\\n2 3 3 3 3 3333333\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 4 4 4 4 4444444\\n4\\n4\\n4\\n4\\n4\\n44444 5 5 5 5 55555555555\\n5\\n5\\n5\\n5\\n5\\n5Class 1\\n11111111111\\n1\\n1\\n1\\n1\\n1\\n1 1 1 1 1 22222222222\\n2\\n2\\n2\\n2\\n2\\n2 2 2 2 2 33333333333\\n3\\n3\\n3\\n3\\n3\\n3 3 3 3 3 4444444\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4 4 4 4 4 5555555\\n5\\n5\\n5\\n5\\n5\\n5\\n5\\n5\\n5\\n5 5 5 5 5Class 2\\n1111111\\n1\\n1\\n1\\n1\\n1\\n111111111 2222222\\n2\\n2\\n2\\n2\\n2\\n222222222 3333333\\n3\\n3333333\\n3\\n3\\n3\\n3\\n3\\n3 4444444\\n4\\n4\\n4\\n4\\n4\\n444444444 5555555\\n5\\n5\\n5\\n5\\n5\\n555555555Class 3\\nFIGURE 12.14. Some examples of the waveforms generated from model (12.64)\\nbefore the Gaussian noise is added.\\n|j−11|,0),h2(j) =h1(j−4) andh3(j) =h1(j+ 4). Figure 12.14 shows\\nsome example waveforms from each class.\\nTable 12.4 shows the results of MDA applied to the waveform da ta, as\\nwell as several other methods from this and other chapters. E ach train-\\ning sample has 300 observations, and equal priors were used, so there are\\nroughly 100 observations in each class. We used test samples of size 500.\\nThe two MDA models are described in the caption.\\nFigure 12.15 shows the leading canonical variates for the pe nalized MDA\\nmodel, evaluated at the test data. As we might have guessed, t he classes\\nappear to lie on the edges of a triangle. This is because the hj(i) are repre-\\nsented by three points in 21-space, thereby forming vertice s of a triangle,\\nand each class is represented as a convex combination of a pai r of vertices,\\nand hence lie on an edge. Also it is clear visually that all the information\\nlies in the ﬁrst two dimensions; the percentage of variance e xplained by the\\nﬁrst two coordinates is 99 .8%, and we would lose nothing by truncating the\\nsolution there. The Bayes risk for this problem has been esti mated to be\\nabout 0.14 (Breiman et al., 1984). MDA comes close to the optimal rate ,\\nwhich is not surprising since the structure of the MDA model i s similar to\\nthe generating model.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 471}),\n",
       " Document(page_content='454 12. Flexible Discriminants\\nTABLE 12.4. Results for waveform data. The values are averages over ten sim -\\nulations, with the standard error of the average in parenthes es. The ﬁve entries\\nabove the line are taken from Hastie et al. (1994). The ﬁrst mode l below the line\\nis MDA with three subclasses per class. The next line is the same, except that the\\ndiscriminant coeﬃcients are penalized via a roughness penalt y to eﬀectively 4df.\\nThe third is the corresponding penalized LDA or PDA model.\\nTechnique Error Rates\\nTraining Test\\nLDA 0.121(0.006) 0.191(0.006)\\nQDA 0.039(0.004) 0.205(0.006)\\nCART 0.072(0.003) 0.289(0.004)\\nFDA/MARS (degree = 1) 0.100(0.006) 0.191(0.006)\\nFDA/MARS (degree = 2) 0.068(0.004) 0.215(0.002)\\nMDA (3 subclasses) 0.087(0.005) 0.169(0.006)\\nMDA (3 subclasses, penalized 4 df) 0.137(0.006) 0.157(0.00 5)\\nPDA (penalized 4 df) 0.150(0.005) 0.171(0.005)\\nBayes 0.140\\nDiscriminant Var 1Discriminant Var 2\\n-6 -4 -2 0 2 4 6-6 -4 -2 0 2 411\\n111\\n11\\n111\\n11\\n11\\n111\\n111\\n1\\n1\\n11\\n1\\n111\\n1111\\n111\\n11\\n11\\n11\\n11\\n11\\n111\\n11\\n111\\n11\\n1\\n11\\n1\\n1\\n1\\n11\\n1\\n111\\n111111\\n111\\n11\\n11\\n11\\n11\\n11\\n11\\n11\\n11\\n1\\n1111\\n1\\n11\\n111\\n111\\n11\\n11\\n111\\n1\\n11\\n1\\n11111\\n11\\n1\\n11111\\n1\\n11\\n11\\n111\\n11\\n1111\\n111\\n111\\n1\\n1\\n1\\n11\\n11\\n1\\n11\\n111\\n11\\n1111\\n11\\n11111\\n11\\n22\\n22\\n2\\n22222\\n2 22\\n2\\n222\\n22\\n2\\n222\\n22\\n2\\n222\\n22\\n222\\n22\\n22222\\n2\\n2222\\n22\\n22\\n22\\n222\\n2\\n222\\n22\\n222\\n2\\n2\\n2\\n22\\n222\\n22\\n22\\n2\\n222\\n2\\n22\\n2\\n2\\n222\\n222\\n2\\n2\\n22\\n2\\n222\\n22\\n22\\n222\\n22\\n22\\n22\\n2\\n222222\\n22\\n2\\n22\\n2\\n2\\n22\\n222\\n2\\n22\\n22\\n22\\n2\\n2\\n2\\n22\\n2\\n22\\n22\\n2222\\n2\\n22\\n33\\n3 333\\n3333 33\\n33\\n3\\n3 3333\\n33\\n333\\n333\\n333\\n3\\n33\\n33\\n333\\n33\\n33\\n3333333\\n3\\n3\\n333\\n3333\\n333\\n33\\n333333\\n333\\n33\\n3\\n33\\n333\\n3\\n333 3\\n33\\n3\\n33\\n3\\n3\\n33\\n3\\n3\\n333\\n3\\n333 3\\n3 3\\n333\\n33\\n33\\n33\\n3\\n333\\n3\\n3\\n333\\n33\\n333\\n333\\n3\\n33\\n333\\n333\\n33\\n333\\n3\\n33\\n333\\n33 333\\n33\\n3\\n3 33\\n3\\n3\\n33\\n3\\n33333\\n333 Subclasses, Penalized 4 df\\nDiscriminant Var 3Discriminant Var 4\\n-2 -1 0 1 2-1.0 0.0 0.5 1.01\\n1\\n11\\n111\\n11\\n11\\n1\\n11\\n11\\n11\\n1\\n111\\n11\\n11\\n11\\n11\\n11\\n11 111\\n111\\n11\\n1\\n11\\n11\\n11\\n11\\n1\\n111\\n11\\n11111\\n11111\\n1\\n1111\\n1\\n11\\n1 1\\n11\\n1 111\\n11\\n1\\n11\\n1111\\n11\\n11\\n11\\n1\\n11\\n1\\n11\\n11\\n11\\n11\\n111\\n111\\n1\\n11\\n11\\n1\\n11\\n11\\n11\\n1\\n11111\\n1\\n111\\n11\\n111\\n1\\n1\\n1\\n111\\n11\\n11\\n11\\n111\\n1\\n1\\n111\\n1111\\n11\\n111\\n11\\n1111\\n22\\n22\\n22\\n222\\n22\\n222\\n22 2\\n22\\n22\\n22\\n22\\n22\\n22\\n22\\n222\\n22\\n2\\n22222\\n2\\n222\\n22\\n2\\n22222\\n22\\n22\\n22\\n22\\n2\\n2 22\\n222\\n2\\n222\\n22\\n2\\n22222\\n222\\n22\\n222\\n222\\n2222\\n222\\n22\\n2222\\n2\\n22\\n2\\n2\\n22\\n2\\n222\\n2\\n222\\n2\\n22\\n22\\n22\\n2\\n22\\n22 2\\n22\\n22\\n2\\n2\\n2\\n22\\n2222\\n2\\n22\\n222\\n222\\n3\\n333\\n3\\n33\\n3 33\\n3\\n33\\n33\\n33\\n333\\n3\\n3\\n3\\n33\\n3333\\n3\\n333\\n3\\n33\\n333\\n33\\n3\\n3\\n33\\n33\\n33\\n3\\n333\\n3\\n33333\\n3\\n3\\n33\\n33\\n33\\n3\\n3\\n3333\\n33\\n3333\\n3 3\\n33\\n3333\\n3\\n3333\\n33\\n33\\n3333\\n33\\n33\\n333\\n3\\n33 3\\n33\\n3333\\n333\\n33\\n3\\n33\\n3\\n3\\n333\\n33\\n33\\n333\\n3333333\\n33\\n3333\\n3\\n33\\n333\\n3\\n33\\n333\\n3\\n33\\n3\\n33\\n3\\n333\\n33333 Subclasses, Penalized 4 df\\nFIGURE 12.15. Some two-dimensional views of the MDA model ﬁtted to a\\nsample of the waveform model. The points are independent test d ata, projected\\non to the leading two canonical coordinates (left panel), and th e third and fourth\\n(right panel). The subclass centers are indicated.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 472}),\n",
       " Document(page_content='Exercises 455\\nComputational Considerations\\nWithNtraining cases, ppredictors, and msupport vectors, the support\\nvector machine requires m3+mN+mpNoperations, assuming m≈N.\\nThey do not scale well with N, although computational shortcuts are avail-\\nable (Platt, 1999). Since these are evolving rapidly, the re ader is urged to\\nsearch the web for the latest technology.\\nLDA requires Np2+p3operations, as does PDA. The complexity of\\nFDA depends on the regression method used. Many techniques a re linear\\ninN, such as additive models and MARS. General splines and kerne l-based\\nregression methods will typically require N3operations.\\nSoftware is available for ﬁtting FDA, PDA and MDA models in th eR\\npackagemda, which is also available in S-PLUS.\\nBibliographic Notes\\nThe theory behind support vector machines is due to Vapnik an d is de-\\nscribed in Vapnik (1996). There is a burgeoning literature o n SVMs; an\\nonline bibliography, created and maintained by Alex Smola a nd Bernhard\\nSch¨ olkopf, can be found at:\\nhttp://www.kernel-machines.org .\\nOur treatment is based on Wahba et al. (2000) and Evgeniou et a l. (2000),\\nand the tutorial by Burges (Burges, 1998).\\nLinear discriminant analysis is due to Fisher (1936) and Rao (1973). The\\nconnection with optimal scoring dates back at least to Breim an and Ihaka\\n(1984), and in a simple form to Fisher (1936). There are stron g connections\\nwithcorrespondenceanalysis(Greenacre,1984).Thedescr iptionofﬂexible,\\npenalized and mixture discriminant analysis is taken from H astie et al.\\n(1994), Hastie et al. (1995) and Hastie and Tibshirani (1996 b), and all\\nthree are summarized in Hastie et al. (2000); see also Ripley (1996).\\nExercises\\nEx. 12.1 Show that the criteria (12.25) and (12.8) are equivalent.\\nEx. 12.2 Show that the solution to (12.29) is the same as the solution t o\\n(12.25) for a particular kernel.\\nEx. 12.3 Consider a modiﬁcation to (12.43) where you do not penalize t he\\nconstant. Formulate the problem, and characterize its solu tion.\\nEx. 12.4 Suppose you perform a reduced-subspace linear discriminan t anal-\\nysis for aK-group problem. You compute the canonical variables of di-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 473}),\n",
       " Document(page_content='456 12. Flexible Discriminants\\nmensionL≤K−1 given by z=UTx, whereUis thep×Lmatrix of\\ndiscriminant coeﬃcients, and p>Kis the dimension of x.\\n(a) IfL=K−1 show that\\n∥z−¯zk∥2−∥z−¯zk′∥2=∥x−¯xk∥2\\nW−∥x−¯xk′∥2\\nW,\\nwhere∥·∥WdenotesMahalanobis distance with respect to the covari-\\nanceW.\\n(b) IfL < K−1, show that the same expression on the left measures\\nthe diﬀerence in Mahalanobis squared distances for the dist ributions\\nprojected onto the subspace spanned by U.\\nEx. 12.5 The data in phoneme.subset , available from this book’s website\\nhttp://www-stat.stanford.edu/ElemStatLearn\\nconsists of digitized log-periodograms for phonemes utter ed by 60 speakers,\\neach speaker having produced phonemes from each of ﬁve class es. It is\\nappropriate to plot each vector of 256 “features” against th e frequencies\\n0–255.\\n(a) Produce a separate plot of all the phoneme curves against frequency\\nfor each class.\\n(b) You plan to use a nearest prototype classiﬁcation scheme to classify\\nthecurvesintophonemeclasses.Inparticular,youwilluse aK-means\\nclustering algorithm in each class ( kmeans() inR), and then classify\\nobservations to the class of the closest cluster center. The curves are\\nhigh-dimensionalandyouhavearathersmallsample-size-t o-variables\\nratio. You decide to restrict all the prototypes to be smooth functions\\nof frequency. In particular, you decide to represent each pr ototypem\\nasm=BθwhereBis a 256×Jmatrix of natural spline basis\\nfunctions with Jknots uniformly chosen in (0 ,255) and boundary\\nknots at 0 and 255. Describe how to proceed analytically, and in\\nparticular, how to avoid costly high-dimensional ﬁtting pr ocedures.\\n(Hint:It may help to restrict Bto be orthogonal.)\\n(c) Implement your procedure on the phoneme data, and try it o ut. Divide\\nthe data into a training set and a test set (50-50), making sur e that\\nspeakers are not split across sets (why?). Use K= 1,3,5,7 centers\\nper class, and for each use J= 5,10,15 knots (taking care to start\\ntheK-means procedure at the same starting values for each value o f\\nJ), and compare the results.\\nEx.12.6SupposethattheregressionprocedureusedinFDA(Section1 2.5.1)\\nis a linear expansion of basis functions hm(x), m= 1,...,M. LetDπ=\\nYTY/Nbe the diagonal matrix of class proportions.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 474}),\n",
       " Document(page_content='Exercises 457\\n(a)Showthattheoptimalscoringproblem(12.52)canbewrit teninvector\\nnotation as\\nmin\\nθ,β∥Yθ−Hβ∥2, (12.65)\\nwhereθis a vector of Kreal numbers, and His theN×Mmatrix\\nof evaluations hj(xi).\\n(b) Suppose that the normalization on θisθTDπ1 = 0 andθTDπθ= 1.\\nInterpret these normalizations in terms of the original sco redθ(gi).\\n(c) Show that, with this normalization, (12.65) can be parti ally optimized\\nw.r.t.β, and leads to\\nmax\\nθθTYTSYθ, (12.66)\\nsubject to the normalization constraints, where Sis the projection\\noperator corresponding to the basis matrix H.\\n(d) Suppose that the hjinclude the constant function. Show that the\\nlargest eigenvalue of Sis 1.\\n(e) LetΘbe aK×Kmatrix of scores (in columns), and suppose the\\nnormalization is ΘTDπΘ=I. Show that the solution to (12.53) is\\ngiven by the complete set of eigenvectors of S; the ﬁrst eigenvector is\\ntrivial, and takes care of the centering of the scores. The re mainder\\ncharacterize the optimal scoring solution.\\nEx. 12.7 Derive the solution to the penalized optimal scoring proble m\\n(12.57).\\nEx.12.8Showthatcoeﬃcients βℓfoundbyoptimalscoringareproportional\\nto the discriminant directions νℓfound by linear discriminant analysis.\\nEx. 12.9 LetˆY=XˆBbe the ﬁtted N×Kindicator response matrix after\\nlinearregressiononthe N×pmatrixX,wherep>K.Considerthereduced\\nfeaturesx∗\\ni=ˆBTxi. Show that LDA using x∗\\niis equivalent to LDA in the\\noriginal space.\\nEx. 12.10 Kernels and linear discriminant analysis . Suppose you wish to\\ncarry out a linear discriminant analysis (two classes) usin g a vector of\\ntransformations of the input variables h(x). Sinceh(x) is high-dimensional,\\nyou will use a regularized within-class covariance matrix Wh+γI. Show\\nthat the model can be estimated using only the inner products K(xi,xi′) =\\n⟨h(xi),h(xi′)⟩. Hence the kernel property of support vector machines is als o\\nshared by regularized linear discriminant analysis.\\nEx. 12.11 TheMDAproceduremodels eachclassas amixtureofGaussians .\\nHence each mixture center belongs to one and only one class. A more\\ngeneral model allows each mixture center to be shared by all c lasses. We\\ntake the joint density of labels and features to be', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 475}),\n",
       " Document(page_content='458 12. Flexible Discriminants\\nP(G,X) =R∑\\nr=1πrPr(G,X), (12.67)\\na mixture of joint densities. Furthermore we assume\\nPr(G,X) =Pr(G)φ(X;µr,Σ). (12.68)\\nThis model consists of regions centered at µr, and for each there is a class\\nproﬁlePr(G). The posterior class distribution is given by\\nP(G=k|X=x) =∑R\\nr=1πrPr(G=k)φ(x;µr,Σ)\\n∑R\\nr=1πrφ(x;µr,Σ), (12.69)\\nwhere the denominator is the marginal distribution P(X).\\n(a) Show that this model (called MDA2) can be viewed as a gener alization\\nof MDA since\\nP(X|G=k) =∑R\\nr=1πrPr(G=k)φ(x;µr,Σ)\\n∑R\\nr=1πrPr(G=k),(12.70)\\nwhereπrk=πrPr(G=k)/∑R\\nr=1πrPr(G=k) corresponds to the\\nmixing proportions for the kth class.\\n(b) Derive the EM algorithm for MDA2.\\n(c) Show that if the initial weight matrix is constructed as i n MDA, in-\\nvolving separate k-means clustering in each class, then the algorithm\\nfor MDA2 is identical to the original MDA procedure.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 476}),\n",
       " Document(page_content='This is page 459\\nPrinter: Opaque this\\n13\\nPrototype Methods and\\nNearest-Neighbors\\n13.1 Introduction\\nIn this chapter we discuss some simple and essentially model -free methods\\nfor classiﬁcation and pattern recognition. Because they ar e highly unstruc-\\ntured, they typically are not useful for understanding the n ature of the\\nrelationship between the features and class outcome. Howev er, asblack box\\nprediction engines, they can be very eﬀective, and are often among the best\\nperformers in real data problems. The nearest-neighbor tec hnique can also\\nbe used in regression; this was touched on in Chapter 2 and wor ks reason-\\nably well for low-dimensional problems. However, with high -dimensional\\nfeatures, the bias–variance tradeoﬀ does not work as favora bly for nearest-\\nneighbor regression as it does for classiﬁcation.\\n13.2 Prototype Methods\\nThroughout this chapter, our training data consists of the Npairs (x1,g1),\\n...,(xn,gN) wheregiis a class label taking values in {1,2,...,K}. Pro-\\ntotype methods represent the training data by a set of points in feature\\nspace. These prototypes are typically not examples from the training sam-\\nple, except in the case of 1-nearest-neighbor classiﬁcatio n discussed later.\\nEach prototype has an associated class label, and classiﬁca tion of a query\\npointxis made to the class of the closest prototype. “Closest” is us ually\\ndeﬁned by Euclidean distance in the feature space, after eac h feature has', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 477}),\n",
       " Document(page_content='460 13. Prototypes and Nearest-Neighbors\\nbeen standardized to have overall mean 0 and variance 1 in the training\\nsample. Euclidean distance is appropriate for quantitativ e features. We\\ndiscuss distance measures between qualitative and other ki nds of feature\\nvalues in Chapter 14.\\nThese methods can be very eﬀective if the prototypes are well positioned\\nto capture the distribution of each class. Irregular class b oundaries can be\\nrepresented, with enough prototypes in the right places in f eature space.\\nThe main challenge is to ﬁgure out how many prototypes to use a nd where\\nto put them. Methods diﬀer according to the number and way in w hich\\nprototypes are selected.\\n13.2.1K-means Clustering\\nK-means clustering is a method for ﬁnding clusters and cluste r centers in a\\nsetofunlabeleddata.Onechoosesthedesirednumberofclus tercenters,say\\nR, and theK-means procedure iteratively moves the centers to minimize\\nthe total within cluster variance.1Given an initial set of centers, the K-\\nmeans algorithm alternates the two steps:\\n•for each center we identify the subset of training points (it s cluster)\\nthat is closer to it than any other center;\\n•the means of each feature for the data points in each cluster a re\\ncomputed, and this mean vector becomes the new center for tha t\\ncluster.\\nThese two steps are iterated until convergence. Typically t he initial centers\\nareRrandomly chosen observations from the training data. Detai ls of the\\nK-meansprocedure,aswellasgeneralizationsallowingford iﬀerentvariable\\ntypes and more general distance measures, are given in Chapt er 14.\\nTo useK-means clustering for classiﬁcation of labeled data, the st eps\\nare:\\n•applyK-means clustering to the training data in each class sepa-\\nrately, using Rprototypes per class;\\n•assign a class label to each of the K×Rprototypes;\\n•classify a new feature xto the class of the closest prototype.\\nFigure 13.1 (upper panel) shows a simulated example with thr ee classes\\nand two features. We used R= 5 prototypes per class, and show the clas-\\nsiﬁcation regions and the decision boundary. Notice that a n umber of the\\n1The “K” inK-means refers to the number of cluster centers. Since we have already\\nreserved Kto denote the number of classes, we denote the number of clusters by R.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 478}),\n",
       " Document(page_content='13.2 Prototype Methods 461\\nK-means - 5 Prototypes  per Class\\n................................................................................................................................................................................................................................................................................................................................................................................................................................. ........................ ................................ ..................................... ............................................................................................................................................................................. .................................. ....................................... ......................................... ............................................ .............................................. .................................................. ......................... .............................. ........................ ................................... ......................... ..................................... .......................... ...................................... ............................. .................................. .................................. ....................... ....................................... ............. ............................................... ...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 479}),\n",
       " Document(page_content='................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ........... ........................................ ............................................................. ........ ........................................... ........................................................ ..... ................................................................................................. .. .............................................................................................. . .......................................................................................... .... .................................................................................... ....... .............................................................................. .......... ............................................................................ ......... ............................................................................. ......... ............................................................................. ......... ............................................................................. ......... ............................................................................... ....... .................................................................................... ..... ........................................................................................ ... ............................................................................................ . ............................................................................................... . ................................................................................................ ... ................................................................................................ ..... ................................................................................................ ....... ............................................ ................................ .................... .... ....................................................................... ..................... .... ........................................................... ................. ............. .......................................... ............. ...................... ........................ .............. ....', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 479}),\n",
       " Document(page_content='................................................................................ ......................... ......................................... .................... .................................................... ................ ........................................................... ............... ............................................................. ............... ............................................................... ............... ............................................................... ............... ........................................................... ................ ....................................................... ................ .................................................... ................ ................................................... ............... ................................................. .............. .............................................. .............. ......................................... .................................................... ............................................... ............................................... ........... .......................................... .......... ................................................ .......... ...................................................... ......... .................................................................... .......................................................................... ........................................................................... ............................................................................ ............................................................................. ............................................................................... ................................................................................ ................................................................................. .................................................................................. .................................................................................... ..................................................................................... ...................................................................................... ....................................................................................... ......................................................................................... .......................................................................................... ........................................................................................... ............................................................................................ .............................................................................................. ............................................................................................... ................................................................................................ ................................................................................................. ................................................\\noo\\noo\\no\\nooo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\nooo\\nooo\\nooo\\noo\\noooo\\nooooo\\noo\\nooo\\no\\nooo\\noo\\nooo\\no\\noo\\noo\\noo\\nooo\\noo\\no\\noo\\nooo\\nooo\\noo\\noo\\no\\noo\\no\\no\\noo\\noo\\nooo\\nooo\\noo\\noo\\no\\nooo\\nooo\\no\\noo\\noo\\no\\noo\\noooo\\nooooo\\noo\\noooo\\nooo\\no\\nooo\\nooo\\noo oo\\noo\\noo\\nooo\\noo\\noooo\\noo\\noo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\noo\\noooo\\no\\no\\noo\\noo oo\\no\\nooo\\nooo\\noo\\noooooo\\no\\nooo\\noooo\\noo\\noooo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo\\noooo\\no o o\\nooo\\noo\\no\\nooo\\nooooo\\noo\\noo\\no\\no\\noooo\\no\\noo\\noo\\noooo\\nooooo\\no\\n•\\n•••\\n••\\n•••\\n•\\n••\\n••\\n•• •\\n••\\n•••\\n•••\\n••\\n•••\\nLVQ - 5 Prototypes per Class', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 479}),\n",
       " Document(page_content='ooo\\noo\\no\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo\\noooo\\no o o\\nooo\\noo\\no\\nooo\\nooooo\\noo\\noo\\no\\no\\noooo\\no\\noo\\noo\\noooo\\nooooo\\no\\n•\\n•••\\n••\\n•••\\n•\\n••\\n••\\n•• •\\n••\\n•••\\n•••\\n••\\n•••\\nLVQ - 5 Prototypes per Class\\n......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................... ................................... ........................................ .............................................. .................................................. .................... .................................. ..................... ...................................... ..................... ......................................... ....................... ..................................... .............................. ............................ ..................................... .............. ............................................ .......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 479}),\n",
       " Document(page_content='................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... .... ............................................... ............................................................ .... ............................................... ........................................................... .... ............................................... .......................................................... ... ................................................ ........................................................ ... ................................................ ...................................................... ... ................................................ ..................................................... .. .................................................................................................... .. .................................................................................................. . ................................................................................................. . ................................................................................................ . ............................................................................................... ............................................................................................. ........................................................................................... . ......................................................................................... . ............................................................ .................... ......... ........................................... ........................ ..............', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 479}),\n",
       " Document(page_content='.......................... ................................. ............................... ....................... ................................................. ................ .......................................................... ............... ............................................................ ............... ............................................................. ............... .............................................................. ............... .............................................................. ............... ................................................................ .............. .................................................................. .............. ................................................................... .............. ................................................................ .............. ........................................................... ............... ..................................................... ................ ................................................ ............... ............................................. .............. ......................................... ................................................... ................................................ .................................................. ........... .............................................. ........... ...................................................... .......... ....................................................................... .......................................................................... ........................................................................... ............................................................................ ............................................................................. .............................................................................. ............................................................................... ................................................................................ ................................................................................. .................................................................................. ................................................................................... .................................................................................... ..................................................................................... ...................................................................................... ...................................................................................... ....................................................................................... ........................................................................................ ......................................................................................... .......................................................................................... ........................................................................................... ............................................................................................ ..........................................\\noo\\noo\\no\\nooo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\nooo\\nooo\\nooo\\noo\\noooo\\nooooo\\noo\\nooo\\no\\nooo\\noo\\nooo\\no\\noo\\noo\\noo\\nooo\\noo\\no\\noo\\nooo\\nooo\\noo\\noo\\no\\noo\\no\\no\\noo\\noo\\nooo\\nooo\\noo\\noo\\no\\nooo\\nooo\\no\\noo\\noo\\no\\noo\\noooo\\nooooo\\noo\\noooo\\nooo\\no\\nooo\\nooo\\noo oo\\noo\\noo\\nooo\\noo\\noooo\\noo\\noo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\noo\\noooo\\no\\no\\noo\\noo oo\\no\\nooo\\nooo\\noo\\noooooo\\no\\nooo\\noooo\\noo\\noooo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo\\noooo\\no o o\\nooo\\noo\\no\\nooo\\nooooo\\noo\\noo\\no\\no\\noooo\\no\\noo\\noo\\noooo\\nooooo\\no\\n••••\\n••\\n•••\\n•\\n• •\\n••\\n•• •\\n••\\n•••\\n•••\\n••\\n•••\\nFIGURE 13.1. Simulated example with three classes and ﬁve prototypes per\\nclass. The data in each class are generated from a mixture of Gaus sians. In the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 479}),\n",
       " Document(page_content='o\\n••••\\n••\\n•••\\n•\\n• •\\n••\\n•• •\\n••\\n•••\\n•••\\n••\\n•••\\nFIGURE 13.1. Simulated example with three classes and ﬁve prototypes per\\nclass. The data in each class are generated from a mixture of Gaus sians. In the\\nupper panel, the prototypes were found by applying the K-means clustering algo-\\nrithm separately in each class. In the lower panel, the LVQ algorit hm (starting\\nfrom theK-means solution) moves the prototypes away from the decision bound-\\nary. The broken purple curve in the background is the Bayes dec ision boundary.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 479}),\n",
       " Document(page_content='462 13. Prototypes and Nearest-Neighbors\\nAlgorithm 13.1 Learning Vector Quantization—LVQ.\\n1. ChooseRinitial prototypes for each class: m1(k),m2(k),...,m R(k),\\nk= 1,2,...,K,forexample,bysampling Rtrainingpointsatrandom\\nfrom each class.\\n2. Sampleatrainingpoint xirandomly(withreplacement),andlet( j,k)\\nindex the closest prototype mj(k) toxi.\\n(a) Ifgi=k(i.e., they are in the same class), move the prototype\\ntowards the training point:\\nmj(k)←mj(k)+ǫ(xi−mj(k)),\\nwhereǫis thelearning rate .\\n(b) Ifgi̸=k(i.e., they are in diﬀerent classes), move the prototype\\naway from the training point:\\nmj(k)←mj(k)−ǫ(xi−mj(k)).\\n3. Repeat step 2, decreasing the learning rate ǫwith each iteration to-\\nwards zero.\\nprototypes are near the class boundaries, leading to potent ial misclassiﬁca-\\ntion errors for points near these boundaries. This results f rom an obvious\\nshortcoming with this method: for each class, the other clas ses do not have\\na say in the positioning of the prototypes for that class. A be tter approach,\\ndiscussed next, uses all of the data to position all prototyp es.\\n13.2.2 Learning Vector Quantization\\nInthistechniqueduetoKohonen(1989),prototypesareplac edstrategically\\nwith respect to the decision boundaries in an ad-hoc way. LVQ is anonline\\nalgorithm—observations are processed one at a time.\\nTheideaisthatthetrainingpointsattractprototypesofth ecorrectclass,\\nand repel other prototypes. When the iterations settle down , prototypes\\nshould be close to the training points in their class. The lea rning rateǫis\\ndecreased to zero with each iteration, following the guidel ines for stochastic\\napproximation learning rates (Section 11.4.)\\nFigure 13.1 (lower panel) shows the result of LVQ, using the K-means\\nsolution as starting values. The prototypes have tended to m ove away from\\nthe decision boundaries, and away from prototypes of compet ing classes.\\nThe procedure just described is actually called LVQ1. Modiﬁ cations\\n(LVQ2, LVQ3, etc.) have been proposed, that can sometimes im prove per-\\nformance. A drawback of learning vector quantization metho ds is the fact', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 480}),\n",
       " Document(page_content='13.3k-Nearest-Neighbor Classiﬁers 463\\nthat they are deﬁned by algorithms, rather than optimizatio n of some ﬁxed\\ncriteria; this makes it diﬃcult to understand their propert ies.\\n13.2.3 Gaussian Mixtures\\nThe Gaussian mixture model can also bethought of as aprototy pe method,\\nsimilar in spirit to K-means and LVQ. We discuss Gaussian mixtures in\\nsome detail in Sections 6.8, 8.5 and 12.7. Each cluster is des cribed in terms\\nof a Gaussian density, which has a centroid (as in K-means), and a covari-\\nance matrix. The comparison becomes crisper if we restrict t he component\\nGaussians to have a scalar covariance matrix (Exercise 13.1 ). The two steps\\nof the alternating EM algorithm are very similar to the two st eps inK-\\nmeans:\\n•In the E-step, each observation is assigned a responsibility or weight\\nfor each cluster, based on the likelihood of each of the corre spond-\\ning Gaussians. Observations close to the center of a cluster will most\\nlikely get weight 1 for that cluster, and weight 0 for every ot her clus-\\nter. Observations half-way between two clusters divide the ir weight\\naccordingly.\\n•In the M-step, each observation contributes to the weighted means\\n(and covariances) for everycluster.\\nAs a consequence, the Gaussian mixture model is often referr ed to as a soft\\nclustering method, while K-means is hard.\\nSimilarly, when Gaussian mixture models are used to represe nt the fea-\\nture density in each class, it produces smooth posterior pro babilities ˆp(x) =\\n{ˆp1(x),...,ˆpK(x)}for classifying x(see (12.60) on page 449.) Often this\\nis interpreted as a soft classiﬁcation, while in fact the cla ssiﬁcation rule is\\nˆG(x) = argmax kˆpk(x). Figure 13.2 compares the results of K-means and\\nGaussian mixtures on the simulated mixture problem of Chapt er 2. We\\nsee that although the decision boundaries are roughly simil ar, those for the\\nmixture model are smoother (although the prototypes are in a pproximately\\nthe same positions.) We also see that while both procedures d evote a blue\\nprototype (incorrectly) to a region in the northwest, the Ga ussian mixture\\nclassiﬁer can ultimately ignore this region, while K-means cannot. LVQ\\ngave very similar results to K-means on this example, and is not shown.\\n13.3k-Nearest-Neighbor Classiﬁers\\nThese classiﬁers are memory-based , and require no model to be ﬁt. Given\\na query point x0, we ﬁnd the ktraining points x(r),r= 1,...,kclosest in\\ndistanceto x0,andthenclassifyusingmajorityvoteamongthe kneighbors.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 481}),\n",
       " Document(page_content='464 13. Prototypes and Nearest-Neighbors\\nK-means - 5 Prototypes per Class', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 482}),\n",
       " Document(page_content='... .. . . .. . . .. . . . .. . . . . . .. . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 482}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 482}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 482}),\n",
       " Document(page_content='. . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 482}),\n",
       " Document(page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\nooooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no••\\n••\\n•••\\n••\\n•\\n•••\\n••\\n•••\\n••\\nTraining Error: 0.170\\nTest Error:       0.243\\nBayes Error:    0.210\\nGaussian Mixtures - 5 Subclasses per Class', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 482}),\n",
       " Document(page_content='.... .. .. . .. . .. . .. . . . .. . . . .. . . . . .. . . . . . . .. . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 482}),\n",
       " Document(page_content='. . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 482}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 482}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 482}),\n",
       " Document(page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\nooooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\n•••\\n••\\n•••\\n••••\\n••\\n•••\\n••\\n•\\nTraining Error: 0.17\\nTest Error:       0.22\\nBayes Error:    0.21\\nFIGURE 13.2. The upper panel shows the K-means classiﬁer applied to the\\nmixture data example. The decision boundary is piecewise line ar. The lower panel\\nshows a Gaussian mixture model with a common covariance for al l component\\nGaussians. The EM algorithm for the mixture model was started a t theK-means\\nsolution. The broken purple curve in the background is the Baye s decision\\nboundary.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 482}),\n",
       " Document(page_content='13.3k-Nearest-Neighbor Classiﬁers 465\\nTies are broken at random. For simplicity we will assume that the features\\nare real-valued, and we use Euclidean distance in feature sp ace:\\nd(i)=||x(i)−x0||. (13.1)\\nTypically we ﬁrst standardize each of the features to have me an zero and\\nvariance 1, since it is possible that they are measured in diﬀ erent units. In\\nChapter 14 we discuss distance measures appropriate for qua litative and\\nordinal features, and how to combine them for mixed data. Ada ptively\\nchosen distance metrics are discussed later in this chapter .\\nDespite its simplicity, k-nearest-neighbors has been successful in a large\\nnumber of classiﬁcation problems, including handwritten d igits, satellite\\nimage scenes and EKG patterns. It is often successful where e ach class\\nhas many possible prototypes, and the decision boundary is v ery irregular.\\nFigure 13.3 (upper panel) shows the decision boundary of a 15 -nearest-\\nneighbor classiﬁer applied to the three-class simulated da ta. The decision\\nboundary is fairly smooth compared to the lower panel, where a 1-nearest-\\nneighbor classiﬁer was used. There is a close relationship b etween nearest-\\nneighbor and prototype methods: in 1-nearest-neighbor cla ssiﬁcation, each\\ntraining point is a prototype.\\nFigure 13.4 shows the training, test and tenfold cross-vali dation errors\\nas a function of the neighborhood size, for the two-class mix ture problem.\\nSince the tenfold CV errors are averages of ten numbers, we ca n estimate\\na standard error.\\nBecauseitusesonly thetrainingpoint closest tothequeryp oint, thebias\\nof the 1-nearest-neighbor estimate is often low, but the var iance is high.\\nA famous result of Cover and Hart (1967) shows that asymptoti cally the\\nerror rate of the 1-nearest-neighbor classiﬁer is never mor e than twice the\\nBayes rate. The rough idea of the proof is as follows (using sq uared-error\\nloss). We assume that the query point coincides with one of th e training\\npoints, so that the bias is zero. This is true asymptotically if the dimension\\nof the feature space is ﬁxed and the training data ﬁlls up the s pace in a\\ndense fashion. Then the error of the Bayes rule is just the var iance of a\\nBernoulli random variate (the target at the query point), wh ile the error of\\n1-nearest-neighbor rule is twicethe variance of a Bernoulli random variate,\\none contribution each for the training and query targets.\\nWe now give more detail for misclassiﬁcation loss. At xletk∗be the\\ndominant class, and pk(x) the true conditional probability for class k. Then\\nBayes error = 1 −pk∗(x), (13.2)\\n1-nearest-neighbor error =K∑\\nk=1pk(x)(1−pk(x)),(13.3)\\n≥1−pk∗(x). (13.4)\\nThe asymptotic 1-nearest-neighbor error rate is that of a ra ndom rule; we\\npick both the classiﬁcation and the test point at random with probabili-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 483}),\n",
       " Document(page_content='466 13. Prototypes and Nearest-Neighbors\\n15-Nearest Neighbors\\n. ........ ......... ..... ...... ......... ......................................................................................... ............................................................................................................................................................................................................................................................................ .............................................................................................................. .. .. . ...................................... .. . .... .... ......................................... ........... ....................................................... ......... ............................................................ ...... .......................................................................... ............................................................................... ...................... .................................................. ................................................................... ........................... ...................................... ........ .. .................................................................................... . .....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 484}),\n",
       " Document(page_content='................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................. . . ................................................... ............................................................. . .................................................. ............................................................ ................................................... ............................................................... .. ................................................. .................................................... ...... ... ................................................... ............................................... ..... ... .. ................................................. .............................................. .... .... ................................................... ............................................... ...... ................................................... ..................................................... .... ............................................... ........................................................ .... ............................................... ........................ ............................... ... .............................................................. .... . . ........................ ....... ...................................................... ........................ ................. ................................... ................. .............................. ............... ....... ... ........................................ ................................................................ .............................. . ...................... ......', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 484}),\n",
       " Document(page_content='.... .. . ..................... .............................. .................................................................................................. .......................................... .. ......... ............................................... ........... ..................................................... .............. ............................................................. .............. .................................................................. ........... .................................................................. . ............. .............................................................. ................ .......................................................... .. . . ............ .................... .................................... . . ............... .. ......... .. ................................... .................................................... ............................................... ................................................................................................................................ .................................................................................................... . ....................................................... . ........ ............................................................. ......................................................................... ................ ... ..................................................... ........................................................................... ............................................................................ .............................................................................. ............................................................................... ................................................................................ ................................................................................. .................................................................................. ................................................................................... ................................................................................... .................................................................................... ..................................................................................... ...................................................................................... ....................................................................................... ....................................................................................... ........................................................................................ ......................................................................................... .......................................................................................... ........................................................................................... ............................................................................................. ...........................................\\noo\\noo\\no\\nooo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\nooo\\nooo\\nooo\\noo\\noooo\\nooooo\\noo\\nooo\\no\\nooo\\noo\\nooo\\no\\noo\\noo\\noo\\nooo\\noo\\no\\noo\\nooo\\nooo\\noo\\noo\\no\\noo\\no\\no\\noo\\noo\\nooo\\nooo\\noo\\noo\\no\\nooo\\nooo\\no\\noo\\noo\\no\\noo\\noooo\\nooooo\\noo\\noooo\\nooo\\no\\nooo\\nooo\\noo oo\\noo\\noo\\nooo\\noo\\noooo\\noo\\noo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\noo\\noooo\\no\\no\\noo\\noo oo\\no\\nooo\\nooo\\noo\\noooooo\\no\\nooo\\noooo\\noo\\noooo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo\\noooo\\no o o\\nooo\\noo\\no\\nooo\\nooooo\\noo\\noo\\no\\no\\noooo\\no\\noo\\noo\\noooo\\nooooo\\no\\n1-Nearest Neighbor', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 484}),\n",
       " Document(page_content='oooo\\no\\no\\noo\\noo oo\\no\\nooo\\nooo\\noo\\noooooo\\no\\nooo\\noooo\\noo\\noooo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo\\noooo\\no o o\\nooo\\noo\\no\\nooo\\nooooo\\noo\\noo\\no\\no\\noooo\\no\\noo\\noo\\noooo\\nooooo\\no\\n1-Nearest Neighbor\\n................. ................. ............ .............. .................. ....................... ... ................ ..... ............... ..... ............. .... ................ ......... ..... ..... ...... ................... ..... ...... ...... .... ....... ... .......... ... .... .. .... . ........ ............ ............ ...... ............ .......... ........ .............. ........ ........ ............. .......... ...... ............... ....... ............. ............. ......... .......................... ............. ......................... . ..... .. .. ............................. ....... .. . ...... .......................... ....... ............ .............. ................ ............... .............................. ............. ........... ............ ....................... ............. ........... ....................... .......................................... ...... ..................... ..................... ..... ......................... .......... ............... ........ ....... ... .... ....................... .... ... ................ ...................... . ........................ . ......... ..... .... .............. ............... .............. ...... ... .................................. ......... .......... ... .............................. ........... ...... .......... ............................. ......... .......... ...................................... ... ................................................... ...................................... ..................................... .......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 484}),\n",
       " Document(page_content='................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................ ................................... .............................................................. ....... ............................................ .............................................. ...... .... ......................................................................................... ........ ... ................................................ ............................................ ........ ............... .............................................................................. ........ .......... ............................................................................... ....... ........ ......................................................... .............. ......... ... .. .......................................................... ............ ....... ... ........ ...................................................... .......... ...... .... .......... .................................................. ...... .. ....... ................ .................................................... . .. .............. .............. ..................................................... ... ............ .... ... ........... ............................................. .. ............. ... .................. ....................................... .............. ..... ... ..... ..... ....... .......................................... ............. ...... . ....... ... ......... ....................................... ............... .. .......... ...... ..... ... .............................................. .................. .......... ........ . ............................................ ..... .......... ......... ............. .................................. ... ..... ......... ..................... ........................... . ... ... ........ ........................ ....................... ..... ... ....... ............ ............. ................ .. .... ..... ............. ... .... .................... .... ..... . ........... ..... ...... ...........', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 484}),\n",
       " Document(page_content='................................................. ............ ................. ................................................................. .................. ..................... ...................... ............................ ......................................... ................................................ .. ... .......... .......................... ...... . ...... ...................................... ...... . ..... ............ .................................... .. .............. .................................... .. . ................ ................................................ ... ....... .................................................. ...... .......... ........................................... ..... ........ ......... ........................................... ... . .. .. ... ................................... .. .. .. .. .................................. ........... .... . ...... . .... ............................................... ..... ..... ................................................... ...... . .................................................. .... .. . ......................................................... ..... ..... .................................................... ...... ............................................................ .................. ........................................................ ................... ......................................................... ................... .......................................................... ............................................................................... .............................................................................. .............................................................................. ................................................................................ .................................................................................. .................................................................................. ................................................................................... .................................................................................... ..................................................................................... ..................................................................................... ...................................................................................... ....................................................................................... ........................................................................................ ........................................................................................ ......................................................................................... .......................................................................................... ........................................................................................... ........................................................................................... .........................................\\noo\\noo\\no\\nooo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\nooo\\nooo\\nooo\\noo\\noooo\\nooooo\\noo\\nooo\\no\\nooo\\noo\\nooo\\no\\noo\\noo\\noo\\nooo\\noo\\no\\noo\\nooo\\nooo\\noo\\noo\\no\\noo\\no\\no\\noo\\noo\\nooo\\nooo\\noo\\noo\\no\\nooo\\nooo\\no\\noo\\noo\\no\\noo\\noooo\\nooooo\\noo\\noooo\\nooo\\no\\nooo\\nooo\\noo oo\\noo\\noo\\nooo\\noo\\noooo\\noo\\noo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\noo\\noooo\\no\\no\\noo\\noo oo\\no\\nooo\\nooo\\noo\\noooooo\\no\\nooo\\noooo\\noo\\noooo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo\\noooo\\no o o\\nooo\\noo\\no\\nooo\\nooooo\\noo\\noo\\no\\no\\noooo\\no\\noo\\noo\\noooo\\nooooo\\no\\nFIGURE 13.3. k-nearest-neighbor classiﬁers applied to the simulation data o f\\nFigure 13.1. The broken purple curve in the background is the B ayes decision\\nboundary.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 484}),\n",
       " Document(page_content='13.3k-Nearest-Neighbor Classiﬁers 467\\nNumber of NeighborsMisclassification Errors\\n0 5 10 15 20 25 300.0 0.05 0.10 0.15 0.20 0.25 0.30•\\n••••••• • ••• •••\\n•• ••••\\n••••• •• • ••\\n•••••• •••• ••••\\nTest Error\\n10-fold CV\\nTraining Error\\nBayes Error\\n7-Nearest Neighbors', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 485}),\n",
       " Document(page_content='.. .. . .. . . . .. . . . . . .. . . . . . .. . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 485}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 485}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 485}),\n",
       " Document(page_content='. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 485}),\n",
       " Document(page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.145\\nTest Error:       0.225\\nBayes Error:    0.210\\nFIGURE 13.4. k-nearest-neighbors on the two-class mixture data. The upper\\npanel shows the misclassiﬁcation errors as a function of neig hborhood size. Stan-\\ndard error bars are included for 10-fold cross validation. The lo wer panel shows\\nthe decision boundary for 7-nearest-neighbors, which appe ars to be optimal for\\nminimizing test error. The broken purple curve in the backgro und is the Bayes\\ndecision boundary.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 485}),\n",
       " Document(page_content='468 13. Prototypes and Nearest-Neighbors\\ntiespk(x), k= 1,...,K. ForK= 2 the 1-nearest-neighbor error rate is\\n2pk∗(x)(1−pk∗(x))≤2(1−pk∗(x)) (twice the Bayes error rate). More\\ngenerally, one can show (Exercise 13.3)\\nK∑\\nk=1pk(x)(1−pk(x))≤2(1−pk∗(x))−K\\nK−1(1−pk∗(x))2.(13.5)\\nMany additional results of this kind have been derived; Ripl ey (1996) sum-\\nmarizes a number of them.\\nThis result can provide a rough idea about the best performan ce that\\nis possible in a given problem. For example, if the 1-nearest -neighbor rule\\nhas a 10% error rate, then asymptotically the Bayes error rat e is at least\\n5%. The kicker here is the asymptotic part, which assumes the bias of the\\nnearest-neighbor rule is zero. In real problems the bias can be substantial.\\nThe adaptive nearest-neighbor rules, described later in th is chapter, are an\\nattempt to alleviate this bias. For simple nearest-neighbo rs, the bias and\\nvariance characteristics can dictate the optimal number of near neighbors\\nfor a given problem. This is illustrated in the next example.\\n13.3.1 Example: A Comparative Study\\nWe tested the nearest-neighbors, K-means and LVQ classiﬁers on two sim-\\nulated problems. There are 10 independent features Xj, each uniformly\\ndistributed on [0 ,1]. The two-class 0-1 target variable is deﬁned as follows:\\nY=I(\\nX1>1\\n2)\\n; problem 1: “easy”,\\nY=I\\uf8eb\\n\\uf8edsign\\uf8f1\\n\\uf8f2\\n\\uf8f33∏\\nj=1(\\nXj−1\\n2)\\uf8fc\\n\\uf8fd\\n\\uf8fe>0\\uf8f6\\n\\uf8f8; problem 2: “diﬃcult.”(13.6)\\nHence in the ﬁrst problem the two classes are separated by the hyperplane\\nX1= 1/2; in the second problem, the two classes form a checkerboard\\npattern in the hypercube deﬁned by the ﬁrst three features. T he Bayes\\nerror rate is zero in both problems. There were 100 training a nd 1000 test\\nobservations.\\nFigure 13.5 shows the mean and standard error of the misclass iﬁcation\\nerror for nearest-neighbors, K-means and LVQ over ten realizations, as\\nthe tuning parameters are varied. We see that K-means and LVQ give\\nnearly identical results. For the best choices of their tuni ng parameters,\\nK-means and LVQ outperform nearest-neighbors for the ﬁrst pr oblem, and\\nthey perform similarly for the second problem. Notice that t he best value\\nof each tuning parameter is clearly situation dependent. Fo r example 25-\\nnearest-neighbors outperforms 1-nearest-neighbor by a fa ctor of 70% in the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 486}),\n",
       " Document(page_content='13.3k-Nearest-Neighbor Classiﬁers 469\\nNumber of NeighborsMisclassification Error\\n0 20 40 600.1 0.2 0.3 0.4 0.5Nearest Neighbors / Easy\\nNumber of Prototypes per ClassMisclassification Error\\n0 5 10 15 20 25 300.1 0.2 0.3 0.4 0.5K-means & LVQ / Easy\\nNumber of NeighborsMisclassification Error\\n0 20 40 600.40 0.45 0.50 0.55 0.60Nearest Neighbors / Difficult\\nNumber of Prototypes per ClassMisclassification Error\\n0 5 10 15 20 25 300.40 0.45 0.50 0.55 0.60K-means & LVQ / Difficult\\nFIGURE 13.5. Mean±one standard error of misclassiﬁcation error for near-\\nest-neighbors, K-means (blue) and LVQ (red) over ten realizations for two sim-\\nulated problems: “easy” and “diﬃcult,” described in the text.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 487}),\n",
       " Document(page_content='470 13. Prototypes and Nearest-Neighbors\\nSpectral Band 1 Spectral Band 2 Spectral Band 3\\nSpectral Band 4 Land Usage Predicted Land Usage\\nFIGURE 13.6. The ﬁrst four panels are LANDSAT images for an agricultural\\narea in four spectral bands, depicted by heatmap shading. Th e remaining two\\npanels give the actual land usage (color coded) and the predicte d land usage using\\na ﬁve-nearest-neighbor rule described in the text.\\nﬁrst problem, while 1-nearest-neighbor is best in the secon d problem by a\\nfactor of 18%. These results underline the importance of usi ng an objective,\\ndata-based method like cross-validation to estimate the be st value of a\\ntuning parameter (see Figure 13.4 and Chapter 7).\\n13.3.2 Example: k-Nearest-Neighbors and Image Scene\\nClassiﬁcation\\nThe STATLOG project (Michie et al., 1994) used part of a LANDS AT\\nimage as a benchmark for classiﬁcation (82 ×100 pixels). Figure 13.6 shows\\nfour heat-map images, two in the visible spectrum and two in t he infrared,\\nfor an area of agricultural land in Australia. Each pixel has a class label\\nfrom the 7-element set G={red soil, cotton, vegetation stubble, mixture,\\ngray soil, damp gray soil, very damp gray soil }, determined manually by\\nresearch assistants surveying the area. The lower middle pa nel shows the\\nactual land usage, shaded by diﬀerent colors to indicate the classes. The\\nobjective is to classify the land usage at a pixel, based on th e information\\nin the four spectral bands.\\nFive-nearest-neighbors produced the predicted map shown i n the bot-\\ntom right panel, and was computed as follows. For each pixel w e extracted\\nan 8-neighbor feature map—the pixel itself and its 8 immediat e neighbors', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 488}),\n",
       " Document(page_content='13.3k-Nearest-Neighbor Classiﬁers 471\\nN\\nN\\nN\\n N\\nX\\nN\\nN\\nN\\nN\\nFIGURE 13.7. A pixel and its 8-neighbor feature map.\\n(see Figure 13.7). This is done separately in the four spectr al bands, giving\\n(1+8)×4 = 36 input features per pixel. Then ﬁve-nearest-neighbors classi-\\nﬁcation was carried out in this 36-dimensional feature spac e. The resulting\\ntest error rate was about 9 .5% (see Figure 13.8). Of all the methods used\\nin the STATLOG project, including LVQ, CART, neural network s, linear\\ndiscriminant analysis and many others, k-nearest-neighbors performed best\\non this task. Hence it is likely that the decision boundaries in IR36are quite\\nirregular.\\n13.3.3 Invariant Metrics and Tangent Distance\\nIn some problems, the training features are invariant under certain natural\\ntransformations. The nearest-neighbor classiﬁer can expl oit such invari-\\nances by incorporating them into the metric used to measure t he distances\\nbetween objects. Here we give an example where this idea was u sed with\\ngreat success, and the resulting classiﬁer outperformed al l others at the\\ntime of its development (Simard et al., 1993).\\nThe problem is handwritten digit recognition, as discussed is Chapter 1\\nand Section 11.7. The inputs are grayscale images with 16 ×16 = 256\\npixels; some examples are shown in Figure 13.9. At the top of F igure 13.10,\\na “3” is shown, in its actual orientation (middle) and rotate d 7.5◦and 15◦\\nin either direction. Such rotations can often occur in real h andwriting, and\\nit is obvious to our eye that this “3” is still a “3” after small rotations.\\nHence we want our nearest-neighbor classiﬁer to consider th ese two “3”s\\nto be close together (similar). However the 256 grayscale pi xel values for a\\nrotated “3” will look quite diﬀerent from those in the origin al image, and\\nhence the two objects can be far apart in Euclidean distance i n IR256.\\nWe wish to remove the eﬀect of rotation in measuring distance s between\\ntwo digits of the same class. Consider the set of pixel values consisting of\\nthe original “3” and its rotated versions. This is a one-dime nsional curve in\\nIR256, depicted by the green curve passing through the “3” in Figur e 13.10.\\nFigure 13.11 shows a stylized version of IR256, with two images indicated by\\nxiandxi′. These might be two diﬀerent “3”s, for example. Through each\\nimage we have drawn the curve of rotated versions of that imag e, called', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 489}),\n",
       " Document(page_content='472 13. Prototypes and Nearest-Neighbors\\nSTATLOG results\\nMethodTest Misclassification Error\\n2 4 6 8 10 12 140.0 0.05 0.10 0.15LVQRBFALLOC80CART NeuralNewID C4.5QDASMARTLogisticLDA\\nDANNK-NN\\nFIGURE 13.8. Test-error performance for a number of classiﬁers, as report ed\\nby the STATLOG project. The entry DANN is a variant of k-nearest neighbors,\\nusing an adaptive metric (Section 13.4.2).\\nFIGURE 13.9. Examples of grayscale images of handwritten digits.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 490}),\n",
       " Document(page_content='13.3k-Nearest-Neighbor Classiﬁers 473\\nTangent\\n+ α.Transformations of 30 7.5 −15 −7.5\\n3\\nα=0 α=0.1 α=− 0.2 α=− 0.1 α=0.2\\nLinear equation for \\nimages above15\\nPixel space\\nFIGURE 13.10. The top row shows a “ 3” in its original orientation (middle)\\nand rotated versions of it. The green curve in the middle of the ﬁgure depicts\\nthis set of rotated “ 3” in256-dimensional space. The red line is the tangent line\\nto the curve at the original image, with some “ 3”s on this tangent line, and its\\nequation shown at the bottom of the ﬁgure.\\ninvariance manifolds in this context. Now, rather than using the usual\\nEuclidean distance between the two images, we use the shorte st distance\\nbetween the two curves. In other words, the distance between the two\\nimages is taken to be the shortest Euclidean distance betwee n any rotated\\nversion of ﬁrst image, and any rotated version of the second i mage. This\\ndistance is called an invariant metric .\\nIn principle one could carry out 1-nearest-neighbor classi ﬁcation using\\nthis invariant metric. However there are two problems with i t. First, it is\\nvery diﬃcult to calculate for real images. Second, it allows large trans-\\nformations that can lead to poor performance. For example a “ 6” would\\nbe considered close to a “9” after a rotation of 180◦. We need to restrict\\nattention to small rotations.\\nThe use of tangent distance solves both of these problems. As shown in\\nFigure 13.10, we can approximate the invariance manifold of the image\\n“3” by its tangent at the original image. This tangent can be c omputed\\nby estimating the direction vector from small rotations of t he image, or by\\nmore sophisticated spatial smoothing methods (Exercise 13 .4.) For large\\nrotations, the tangent image no longer looks like a “3,” so th e problem\\nwith large transformations is alleviated.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 491}),\n",
       " Document(page_content='474 13. Prototypes and Nearest-Neighbors\\nTransformationsTransformations\\nxixi′ofxi\\nofxi′Tangent distance\\nEuclidean distance\\nbetween xiandxi′Distance between\\ntransformed\\nxiandxi′\\nFIGURE 13.11. Tangent distance computation for two images xiandxi′.\\nRather than using the Euclidean distance between xiandxi′, or the shortest\\ndistance between the two curves, we use the shortest distanc e between the two\\ntangent lines.\\nThe idea then is to compute the invariant tangent line for eac h training\\nimage. For a query image to be classiﬁed, we compute its invar iant tangent\\nline, and ﬁnd the closest line to it among the lines in the trai ning set. The\\nclass (digit) corresponding to this closest line is our pred icted class for the\\nquery image. In Figure 13.11 the two tangent lines intersect , but this is only\\nbecause we have been forced to draw a two-dimensional repres entation of\\nthe actual 256-dimensional situation. In IR256the probability of two such\\nlines intersecting is eﬀectively zero.\\nNow a simpler way to achieve this invariance would be to add in to the\\ntraining set a number of rotated versions of each training im age, and then\\njust use a standard nearest-neighbor classiﬁer. This idea i s called “hints” in\\nAbu-Mostafa (1995), and works well when the space of invaria nces is small.\\nSo far we have presented a simpliﬁed version of the problem. I n addition to\\nrotation, there are six other types of transformations unde r which we would\\nlike our classiﬁer to be invariant. There are translation (t wo directions),\\nscaling (two directions), sheer, and character thickness. Hence the curves\\nand tangent lines in Figures 13.10 and 13.11 are actually 7-d imensional\\nmanifolds and hyperplanes. It is infeasible to add transfor med versions\\nof each training image to capture all of these possibilities . The tangent\\nmanifolds provide an elegant way of capturing the invarianc es.\\nTable 13.1 shows the test misclassiﬁcation error for a probl em with 7291\\ntraining images and 2007 test digits (the U.S. Postal Servic es database), for\\na carefully constructed neural network, and simple 1-neare st-neighbor and', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 492}),\n",
       " Document(page_content='13.4 Adaptive Nearest-Neighbor Methods 475\\nTABLE 13.1. Test error rates for the handwritten ZIP code problem.\\nMethod Error rate\\nNeural-net 0.049\\n1-nearest-neighbor/Euclidean distance 0.055\\n1-nearest-neighbor/tangent distance 0.026\\ntangent distance 1-nearest-neighbor rules. The tangent di stance nearest-\\nneighbor classiﬁer works remarkably well, with test error r ates near those\\nfor the human eye (this is a notoriously diﬃcult test set). In practice,\\nit turned out that nearest-neighbors are too slow for online classiﬁcation\\nin this application (see Section 13.5), and neural network c lassiﬁers were\\nsubsequently developed to mimic it.\\n13.4 Adaptive Nearest-Neighbor Methods\\nWhen nearest-neighbor classiﬁcation is carried out in a hig h-dimensional\\nfeature space, the nearest neighbors of a point can be very fa r away, causing\\nbias and degrading the performance of the rule.\\nToquantifythis,consider Ndatapointsuniformlydistributedintheunit\\ncube [−1\\n2,1\\n2]p. LetRbe the radius of a 1-nearest-neighborhood centered at\\nthe origin. Then\\nmedian(R) =v−1/p\\np(\\n1−1\\n21/N)1/p\\n, (13.7)\\nwherevprpis the volume of the sphere of radius rinpdimensions. Fig-\\nure 13.12 shows the median radius for various training sampl e sizes and\\ndimensions. We see that median radius quickly approaches 0 .5, the dis-\\ntance to the edge of the cube.\\nWhat can be done about this problem? Consider the two-class s ituation\\nin Figure 13.13. There are two features, and a nearest-neigh borhood at\\na query point is depicted by the circular region. Implicit in near-neighbor\\nclassiﬁcation is the assumption that the class probabiliti es are roughly con-\\nstant in the neighborhood, and hence simple averages give go od estimates.\\nHowever, in this example the class probabilities vary only i n the horizontal\\ndirection. If we knew this, we would stretch the neighborhoo d in the verti-\\ncal direction, as shown by the tall rectangular region. This will reduce the\\nbias of our estimate and leave the variance the same.\\nIn general, this calls for adapting the metric used in neares t-neighbor\\nclassiﬁcation, so that the resulting neighborhoods stretc h out in directions\\nfor which the class probabilities don’t change much. In high -dimensional\\nfeature space, the class probabilities might change only a l ow-dimensional\\nsubspace and hence there can be considerable advantage to ad apting the\\nmetric.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 493}),\n",
       " Document(page_content='476 13. Prototypes and Nearest-Neighbors\\nDimensionMedian Radius\\n0 5 10 150.0 0.1 0.2 0.3 0.4 0.5 0.6N=100N=1,000\\nN=10,000\\nFIGURE 13.12. Median radius of a 1-nearest-neighborhood, for uniform dat a\\nwithNobservations in pdimensions.\\no\\noo\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noo\\no\\noo\\noo\\no\\n•5-Nearest Neighborhoods\\nFIGURE 13.13. The points are uniform in the cube, with the vertical line sepa -\\nrating class red and green. The vertical strip denotes the 5-nearest-neighbor region\\nusing only the horizontal coordinate to ﬁnd the nearest-neig hbors for the target\\npoint (solid dot). The sphere shows the 5-nearest-neighbor region using both co-\\nordinates, and we see in this case it has extended into the clas s-red region (and\\nis dominated by the wrong class in this instance).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 494}),\n",
       " Document(page_content='13.4 Adaptive Nearest-Neighbor Methods 477\\nFriedman (1994a) proposed a method in which rectangular nei ghbor-\\nhoods are found adaptively by successively carving away edg es of a box\\ncontaining the training data. Here we describe the discriminant adaptive\\nnearest-neighbor (DANN) rule of Hastie and Tibshirani (1996a). Earlier,\\nrelated proposals appear in Short and Fukunaga (1981) and My les and\\nHand (1990).\\nAt each query point a neighborhood of say 50 points is formed, and the\\nclass distribution among the points is used to decide how to d eform the\\nneighborhood—that is, to adapt the metric. The adapted metri c is then\\nused in a nearest-neighbor rule at the query point. Thus at ea ch query\\npoint a potentially diﬀerent metric is used.\\nIn Figure 13.13 it is clear that the neighborhood should be st retched in\\nthe direction orthogonal to line joining the class centroid s. This direction\\nalso coincides with the linear discriminant boundary, and i s the direction\\nin which the class probabilities change the least. In genera l this direction\\nof maximum change will not be orthogonal to the line joining t he class cen-\\ntroids (see Figure 4.9 on page 116.) Assuming a local discrim inant model,\\nthe information contained in the local within- and between- class covari-\\nance matrices is all that is needed to determine the optimal s hape of the\\nneighborhood.\\nThediscriminant adaptive nearest-neighbor (DANN) metric at a query\\npointx0is deﬁned by\\nD(x,x0) = (x−x0)TΣ(x−x0), (13.8)\\nwhere\\nΣ=W−1/2[W−1/2BW−1/2+ǫI]W−1/2\\n=W−1/2[B∗+ǫI]W−1/2. (13.9)\\nHereWis the pooled within-class covariance matrix∑K\\nk=1πkWkandB\\nis the between class covariance matrix∑K\\nk=1πk(¯xk−¯x)(¯xk−¯x)T, with\\nWandBcomputed using only the 50 nearest neighbors around x0. After\\ncomputation of the metric, it is used in a nearest-neighbor r ule atx0.\\nThis complicated formula is actually quite simple in its ope ration. It ﬁrst\\nspheres the data with respect to W, and then stretches the neighborhood\\nin the zero-eigenvalue directions of B∗(the between-matrix for the sphered\\ndata ). This makes sense, since locally the observed class me ans do not dif-\\nfer in these directions. The ǫparameter rounds the neighborhood, from an\\ninﬁnite strip to an ellipsoid, to avoid using points far away from the query\\npoint. The value of ǫ= 1 seems to work well in general. Figure 13.14 shows\\nthe resulting neighborhoods for a problem where the classes form two con-\\ncentric circles. Notice how the neighborhoods stretch out o rthogonally to\\nthe decision boundaries when both classes are present in the neighborhood.\\nIn the pure regions with only one class, the neighborhoods re main circular;', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 495}),\n",
       " Document(page_content='478 13. Prototypes and Nearest-Neighbors\\no\\noo ooo\\noo\\noo\\noo\\noo\\no\\noooo\\no\\no\\nooo\\no\\noo\\nooo\\nooooo\\noo\\no\\nooo\\noo ooo\\noo\\no\\nooo\\nooo\\noo\\no oo\\nooo\\no\\nooo\\nooo\\noooooo\\no\\no\\noo\\noo\\nooooo\\noo\\no\\noooo\\noo\\no\\no\\no\\noo\\noo\\no\\nooo\\noooo\\nooo\\noo\\noo\\noo\\no\\nooo\\noo\\nooo\\nooo\\no\\noo\\noooo\\noo o\\noo\\noooo\\noo\\noooo\\noo\\no\\no\\noo\\no\\nooo\\nooo\\noo\\no\\no\\noo\\noo\\nooo\\noo\\noo\\nooo\\noo\\noooo\\noo\\no\\noo\\no\\nooo\\no\\noooo\\noo\\nooo\\noooo\\nooo\\no\\nooo\\nooo\\noo\\noo\\noo\\noo\\noooo\\noooo\\no\\noo\\nooooo\\noo\\no\\noo\\nooo\\noo\\no o\\nooo\\nooo\\no\\noooo\\nooo\\noo\\no\\nooo\\noo\\noo\\noo\\noo\\noo\\no\\noo\\no o\\noooo\\noo\\noo\\noo\\noooo\\no\\nooo\\nooo\\noo\\noo oo\\no\\noo\\noooo\\noo\\nooo\\no\\noooo\\no\\noo\\nooo o\\no ooo\\nooooo\\noo\\noo\\no\\no\\nooo\\noo\\nooo\\no\\noo\\nooo\\noo\\nooo\\noo\\noooo ooo\\noo\\noo\\no\\no\\noo\\nooo\\nFIGURE 13.14. Neighborhoods found by the DANN procedure, at various query\\npoints (centers of the crosses). There are two classes in the d ata, with one class\\nsurrounding the other. 50nearest-neighbors were used to estimate the local met-\\nrics. Shown are the resulting metrics used to form 15-nearest-neighborhoods.\\nin these cases the between matrix B= 0, and the Σin (13.8) is the identity\\nmatrix.\\n13.4.1 Example\\nHere we generate two-class data in ten dimensions, analogou s to the two-\\ndimensional example of Figure 13.14. All ten predictors in c lass 1 are inde-\\npendent standard normal, conditioned on the squared radius being greater\\nthan 22.4 and less than 40, while the predictors in class 2 are independent\\nstandardnormalwithouttherestriction.Thereare250obse rvationsineach\\nclass. Hence the ﬁrst class almost completely surrounds the second class in\\nthe full ten-dimensional space.\\nIn this example there are no pure noise variables, the kind th at a nearest-\\nneighbor subset selection rule might be able to weed out. At a ny given\\npoint in the feature space, the class discrimination occurs along only one\\ndirection. However, this direction changes as we move acros s the feature\\nspace and all variables are important somewhere in the space .\\nFigure 13.15 shows boxplots of the test error rates over ten r ealiza-\\ntions, for standard 5-nearest-neighbors, LVQ, and discrim inant adaptive\\n5-nearest-neighbors. We used 50 prototypes per class for LV Q, to make\\nit comparable to 5 nearest-neighbors (since 250 /5 = 50). The adaptive\\nmetric signiﬁcantly reduces the error rate, compared to LVQ or standard\\nnearest-neighbors.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 496}),\n",
       " Document(page_content='13.4 Adaptive Nearest-Neighbor Methods 4790.0 0.1 0.2 0.3 0.4\\n5NN LVQ DANNTest Error\\nFIGURE 13.15. Ten-dimensional simulated example: boxplots of the test error\\nrates over ten realizations, for standard 5-nearest-neighbors, LVQ with 50centers,\\nand discriminant-adaptive 5-nearest-neighbors\\n13.4.2 Global Dimension Reduction for Nearest-Neighbors\\nThe discriminant-adaptive nearest-neighbor method carri es out local di-\\nmension reduction—that is, dimension reduction separately at each query\\npoint. In many problems we can also beneﬁt from global dimens ion re-\\nduction, that is, apply a nearest-neighbor rule in some opti mally chosen\\nsubspace of the original feature space. For example, suppos e that the two\\nclasses form two nested spheres in four dimensions of featur e space, and\\nthere are an additional six noise features whose distributi on is independent\\nof class. Then we would like to discover the important four-d imensional\\nsubspace, and carry out nearest-neighbor classiﬁcation in that reduced sub-\\nspace.HastieandTibshirani(1996a)discussavariationof thediscriminant-\\nadaptive nearest-neighbor method for this purpose. At each training point\\nxi, the between-centroids sum of squares matrix Biis computed, and then\\nthese matrices are averaged over all training points:\\n¯B=1\\nNN∑\\ni=1Bi. (13.10)\\nLete1,e2,...,e pbe the eigenvectors of the matrix ¯B, ordered from largest\\nto smallest eigenvalue θk. Then these eigenvectors span the optimal sub-\\nspaces for global subspace reduction. The derivation is bas ed on the fact\\nthat the best rank- Lapproximation to ¯B,¯B[L]=∑L\\nℓ=1θℓeℓeT\\nℓ, solves the\\nleast squares problem\\nmin\\nrank(M)=LN∑\\ni=1trace[(Bi−M)2]. (13.11)\\nSince each Bicontains information on (a) the local discriminant subspac e,\\nand (b) the strength of discrimination in that subspace, (13 .11) can be seen', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 497}),\n",
       " Document(page_content='480 13. Prototypes and Nearest-Neighbors\\nas a way of ﬁnding the best approximating subspace of dimensi onLto a\\nseries ofNsubspaces by weighted least squares (Exercise 13.5.)\\nIn the four-dimensional sphere example mentioned above and examined\\nin Hastie and Tibshirani (1996a), four of the eigenvalues θℓturn out to be\\nlarge (having eigenvectors nearly spanning the interestin g subspace), and\\nthe remaining six are near zero. Operationally, we project t he data into\\nthe leading four-dimensional subspace, and then carry out n earest neighbor\\nclassiﬁcation. In the satellite image classiﬁcation examp le in Section 13.3.2,\\nthe technique labeled DANNin Figure 13.8 used 5-nearest-neighbors in a\\nglobally reduced subspace. There are also connections of th is technique\\nwith the sliced inverse regression proposal of Duan and Li (1991). These\\nauthors use similar ideas in the regression setting, but do g lobal rather\\nthan local computations. They assume and exploit spherical symmetry of\\nthe feature distribution to estimate interesting subspace s.\\n13.5 Computational Considerations\\nOne drawback of nearest-neighbor rules in general is the com putational\\nload, both in ﬁnding the neighbors and storing the entire tra ining set. With\\nNobservationsand ppredictors,nearest-neighborclassiﬁcationrequires Np\\noperations to ﬁnd the neighbors per query point. There are fa st algorithms\\nfor ﬁnding nearest-neighbors (Friedman et al., 1975; Fried man et al., 1977)\\nwhich can reduce this load somewhat. Hastie and Simard (1998 ) reduce\\nthe computations for tangent distance by developing analog s ofK-means\\nclustering in the context of this invariant metric.\\nReducing the storage requirements is more diﬃcult, and vari ousediting\\nandcondensing procedures have been proposed. The idea is to isolate a\\nsubset of the training set that suﬃces for nearest-neighbor predictions, and\\nthrow away the remaining training data. Intuitively, it see ms important to\\nkeep the training points that are near the decision boundari es and on the\\ncorrect side of those boundaries, while some points far from the boundaries\\ncould be discarded.\\nThemulti-edit algorithm of Devijver and Kittler (1982) divides the data\\ncyclically into training and test sets, computing a nearest neighbor rule on\\nthe training set and deleting test points that are misclassi ﬁed. The idea is\\nto keep homogeneous clusters of training observations.\\nThecondensing procedure of Hart (1968) goes further, trying to keep\\nonly important exterior points of these clusters. Starting with a single ran-\\ndomly chosen observation as the training set, each addition al data item is\\nprocessed one at a time, adding it to the training set only if i t is misclas-\\nsiﬁed by a nearest-neighbor rule computed on the current tra ining set.\\nThese procedures are surveyed in Dasarathy (1991) and Riple y (1996).\\nThey can also be applied to other learning procedures beside s nearest-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 498}),\n",
       " Document(page_content='Exercises 481\\nneighbors. While such methods are sometimes useful, we have not had\\nmuch practical experience with them, nor have we found any sy stematic\\ncomparison of their performance in the literature.\\nBibliographic Notes\\nThe nearest-neighbor method goes back at least to Fix and Hod ges (1951).\\nThe extensive literature on the topic is reviewed by Dasarat hy (1991);\\nChapter 6 of Ripley (1996) contains a good summary. K-means cluster-\\ning is due to Lloyd (1957) and MacQueen (1967). Kohonen (1989 ) intro-\\nduced learning vector quantization. The tangent distance m ethod is due to\\nSimard et al. (1993). Hastie and Tibshirani (1996a) propose d the discrim-\\ninant adaptive nearest-neighbor technique.\\nExercises\\nEx. 13.1 Consider a Gaussian mixture model where the covariance matr ices\\nare assumed to be scalar: Σr=σI∀r= 1,...,R, andσis a ﬁxed param-\\neter. Discuss the analogy between the K-means clustering algorithm and\\nthe EM algorithm for ﬁtting this mixture model in detail. Sho w that in the\\nlimitσ→0 the two methods coincide.\\nEx. 13.2 Derive formula (13.7) for the median radius of the 1-nearest -\\nneighborhood.\\nEx. 13.3 LetE∗be the error rate of the Bayes rule in a K-class problem,\\nwhere the true class probabilities are given by pk(x), k= 1,...,K. As-\\nsuming the test point and training point have identical feat uresx, prove\\n(13.5)\\nK∑\\nk=1pk(x)(1−pk(x))≤2(1−pk∗(x))−K\\nK−1(1−pk∗(x))2.\\nwherek∗= argmax kpk(x). Hence argue that the error rate of the 1-\\nnearest-neighbor rule converges in L1, as the size of the training set in-\\ncreases, to a value E1, bounded above by\\nE∗(\\n2−E∗K\\nK−1)\\n. (13.12)\\n[This statement of the theorem of Cover and Hart (1967) is tak en from\\nChapter 6 of Ripley (1996), where a short proof is also given] .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 499}),\n",
       " Document(page_content='482 13. Prototypes and Nearest-Neighbors\\nEx. 13.4 Consider an image to be a function F(x) : IR2↦→IR1over the two-\\ndimensionalspatialdomain(papercoordinates).Then F(c+x0+A(x−x0))\\nrepresents an aﬃne transformation of the image F, whereAis a 2×2\\nmatrix.\\n1. Decompose A(via Q-R) in such a way that parameters identifying\\nthe four aﬃne transformations (two scale, shear and rotatio n) are\\nclearly identiﬁed.\\n2. Using the chain rule, show that the derivative of F(c+x0+A(x−x0))\\nw.r.t. each of these parameters can be represented in terms o f the two\\nspatial derivatives of F.\\n3. Using a two-dimensional kernel smoother (Chapter 6), des cribe how\\nto implement this procedure when the images are quantized to 16×16\\npixels.\\nEx. 13.5 LetBi,i= 1,2,...,Nbe squarep×ppositive semi-deﬁnite ma-\\ntrices and let ¯B= (1/N)∑Bi. Write the eigen-decomposition of ¯Bas∑p\\nℓ=1θℓeℓeT\\nℓwithθℓ≥θℓ−1≥···≥θ1. Show that the best rank- Lapprox-\\nimation for the Bi,\\nmin\\nrank(M)=LN∑\\ni=1trace[(Bi−M)2],\\nis given by ¯B[L]=∑L\\nℓ=1θℓeℓeT\\nℓ. (Hint:Write∑N\\ni=1trace[(Bi−M)2] as\\nN∑\\ni=1trace[(Bi−¯B)2]+N∑\\ni=1trace[(M−¯B)2]).\\nEx. 13.6 Here we consider the problem of shape averaging . In particular,\\nLi, i= 1,...,Mare eachN×2 matrices of points in IR2, each sampled\\nfrom corresponding positions of handwritten (cursive) let ters. We seek an\\naﬃne invariant average V, alsoN×2,VTV=I, of theMlettersLiwith\\nthe following property: Vminimizes\\nM∑\\nj=1min\\nAj∥Lj−VAj∥2.\\nCharacterize the solution.\\nThis solution can suﬀer if some of the letters are bigand dominate the\\naverage. An alternative approach is to minimize instead:\\nM∑\\nj=1min\\nAj\\ued79\\ued79LjA∗\\nj−V\\ued79\\ued792.\\nDerive the solution to this problem. How do the criteria diﬀe r? Use the\\nSVD of the Ljto simplify the comparison of the two approaches.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 500}),\n",
       " Document(page_content='Exercises 483\\nEx. 13.7 Consider the application of nearest-neighbors to the “easy ” and\\n“hard” problems in the left panel of Figure 13.5.\\n1. Replicate the results in the left panel of Figure 13.5.\\n2. Estimate the misclassiﬁcation errors using ﬁvefold cros s-validation,\\nand compare the error rate curves to those in 1.\\n3. Consider an “AIC-like” penalization of the training set m isclassiﬁca-\\ntion error. Speciﬁcally, add 2 t/Nto the training set misclassiﬁcation\\nerror, where tis the approximate number of parameters N/r,rbe-\\ning the number of nearest-neighbors. Compare plots of the re sulting\\npenalized misclassiﬁcation error to those in 1 and 2. Which m ethod\\ngives a better estimate of the optimal number of nearest-nei ghbors:\\ncross-validation or AIC?\\nEx. 13.8 Generate data in two classes, with two features. These featu res\\nare all independent Gaussian variates with standard deviat ion 1. Their\\nmean vectors are ( −1,−1) in class 1 and (1 ,1) in class 2. To each feature\\nvector apply a random rotation of angle θ,θchosen uniformly from 0 to\\n2π. Generate 50 observations from each class to form the traini ng set, and\\n500 in each class as the test set. Apply four diﬀerent classiﬁ ers:\\n1. Nearest-neighbors.\\n2. Nearest-neighbors with hints: ten randomly rotated vers ions of each\\ndata point are added to the training set before applying near est-\\nneighbors.\\n3. Invariant metric nearest-neighbors, using Euclidean di stance invari-\\nant to rotations about the origin.\\n4. Tangent distance nearest-neighbors.\\nIn each case choose the number of neighbors by tenfold cross- validation.\\nCompare the results.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 501}),\n",
       " Document(page_content='484 13. Prototypes and Nearest-Neighbors', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 502}),\n",
       " Document(page_content='This is page 485\\nPrinter: Opaque this\\n14\\nUnsupervised Learning\\n14.1 Introduction\\nThe previous chapters have been concerned with predicting t he values\\nof one or more outputs or response variables Y= (Y1,...,Y m) for a\\ngiven set of input or predictor variables XT= (X1,...,X p). Denote by\\nxT\\ni= (xi1,...,x ip) the inputs for the ith training case, and let yibe a\\nresponse measurement. The predictions are based on the trai ning sample\\n(x1,y1),...,(xN,yN) of previously solved cases, where the joint values of\\nall of the variables are known. This is called supervised learning or “learn-\\ning with a teacher.” Under this metaphor the “student” prese nts an an-\\nswer ˆyifor eachxiin the training sample, and the supervisor or “teacher”\\nprovides either the correct answer and/or an error associat ed with the stu-\\ndent’s answer. This is usually characterized by some loss fu nctionL(y,ˆy),\\nfor example, L(y,ˆy) = (y−ˆy)2.\\nIf one supposes that ( X,Y) are random variables represented by some\\njointprobabilitydensityPr( X,Y),thensupervisedlearningcanbeformally\\ncharacterized as a density estimation problem where one is c oncerned with\\ndetermining properties of the conditional density Pr( Y|X). Usually the\\nproperties of interest are the “location” parameters µthat minimize the\\nexpected error at each x,\\nµ(x) = argmin\\nθEY|XL(Y,θ). (14.1)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 503}),\n",
       " Document(page_content='486 14. Unsupervised Learning\\nConditioning one has\\nPr(X,Y) = Pr(Y|X)·Pr(X),\\nwhere Pr(X) is the joint marginal density of the Xvalues alone. In su-\\npervised learning Pr( X) is typically of no direct concern. One is interested\\nmainly in the properties of the conditional density Pr( Y|X). SinceYis of-\\nten of low dimension (usually one), and only its location µ(x) is of interest,\\nthe problem is greatly simpliﬁed. As discussed in the previo us chapters,\\nthere are many approaches for successfully addressing supe rvised learning\\nin a variety of contexts.\\nIn this chapter we address unsupervised learning or “learning without a\\nteacher.” In this case one has a set of Nobservations ( x1,x2,...,x N) of a\\nrandomp-vectorXhaving joint density Pr( X). The goal is to directly infer\\nthe properties of this probability density without the help of a supervisor or\\nteacher providing correct answers or degree-of-error for e ach observation.\\nThe dimension of Xis sometimes much higher than in supervised learn-\\ning, and the properties of interest are often more complicat ed than simple\\nlocation estimates. These factors are somewhat mitigated b y the fact that\\nXrepresents all of the variables under consideration; one is not required\\nto infer how the properties of Pr( X) change, conditioned on the changing\\nvalues of another set of variables.\\nIn low-dimensional problems (say p≤3), there are a variety of eﬀective\\nnonparametric methods for directly estimating the density Pr(X) itself at\\nallX-values, and representing it graphically (Silverman, 1986 , e.g.). Owing\\nto the curse of dimensionality, these methods fail in high di mensions. One\\nmust settle for estimating rather crude global models, such as Gaussian\\nmixtures or various simple descriptive statistics that cha racterize Pr( X).\\nGenerally, these descriptive statistics attempt to charac terizeX-values,\\nor collections of such values, where Pr( X) is relatively large. Principal\\ncomponents, multidimensional scaling, self-organizing m aps, and principal\\ncurves, for example, attempt to identify low-dimensional m anifolds within\\ntheX-space that represent high data density. This provides info rmation\\nabout the associations among the variables and whether or no t they can be\\nconsidered as functions of a smaller set of “latent” variabl es. Cluster anal-\\nysis attempts to ﬁnd multiple convex regions of the X-space that contain\\nmodes of Pr( X). This can tell whether or not Pr( X) can be represented by\\na mixture of simpler densities representing distinct types or classes of ob-\\nservations. Mixture modeling has a similar goal. Associati on rules attempt\\nto construct simple descriptions (conjunctive rules) that describe regions\\nof high density in the special case of very high dimensional b inary-valued\\ndata.\\nWith supervised learning there is a clear measure of success , or lack\\nthereof, that can be used to judge adequacy in particular sit uations and\\nto compare the eﬀectiveness of diﬀerent methods over variou s situations.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 504}),\n",
       " Document(page_content='14.2 Association Rules 487\\nLack of success is directly measured by expected loss over th e joint dis-\\ntribution Pr( X,Y). This can be estimated in a variety of ways including\\ncross-validation. In the context of unsupervised learning , there is no such\\ndirectmeasureofsuccess.Itisdiﬃculttoascertaintheval idityofinferences\\ndrawnfromtheoutputofmostunsupervisedlearningalgorit hms.Onemust\\nresort to heuristic arguments not only for motivating the al gorithms, as is\\noften the case in supervised learning as well, but also for ju dgments as to\\nthe quality of the results. This uncomfortable situation ha s led to heavy\\nproliferation of proposed methods, since eﬀectiveness is a matter of opinion\\nand cannot be veriﬁed directly.\\nIn this chapter we present those unsupervised learning tech niques that\\nare among the most commonly used in practice, and additional ly, a few\\nothers that are favored by the authors.\\n14.2 Association Rules\\nAssociation rule analysis has emerged as a popular tool for m ining com-\\nmercial data bases. The goal is to ﬁnd joint values of the vari ablesX=\\n(X1,X2,...,X p) that appear most frequently in the data base. It is most\\noften applied to binary-valued data Xj∈{0,1}, where it is referred to as\\n“market basket” analysis. In this context the observations are sales trans-\\nactions, such as those occurring at the checkout counter of a store. The\\nvariables represent all of the items sold in the store. For ob servationi, each\\nvariableXjis assigned one of two values; xij= 1 if thejth item is pur-\\nchased as part of the transaction, whereas xij= 0 if it was not purchased.\\nThosevariablesthatfrequentlyhavejointvaluesofonerep resentitemsthat\\nare frequently purchased together. This information can be quite useful for\\nstocking shelves, cross-marketing in sales promotions, ca talog design, and\\nconsumer segmentation based on buying patterns.\\nMore generally, the basic goal of association rule analysis is to ﬁnd a\\ncollection of prototype X-valuesv1,...,v Lfor the feature vector X, such\\nthat the probability density Pr( vl) evaluated at each of those values is rela-\\ntivelylarge.Inthisgeneralframework,theproblemcanbev iewedas“mode\\nﬁnding” or “bump hunting.” As formulated, this problem is im possibly dif-\\nﬁcult. A natural estimator for each Pr( vl) is the fraction of observations\\nfor whichX=vl. For problems that involve more than a small number\\nof variables, each of which can assume more than a small numbe r of val-\\nues, the number of observations for which X=vlwill nearly always be too\\nsmall for reliable estimation. In order to have a tractable p roblem, both the\\ngoals of the analysis and the generality of the data to which i t is applied\\nmust be greatly simpliﬁed.\\nThe ﬁrst simpliﬁcation modiﬁes the goal. Instead of seeking valuesx\\nwhere Pr(x) is large, one seeks regionsof theX-space with high probability', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 505}),\n",
       " Document(page_content='488 14. Unsupervised Learning\\ncontent relative to their size or support. Let Sjrepresent the set of all\\npossible values of the jth variable (its support), and letsj⊆Sjbe a subset\\nof these values. The modiﬁed goal can be stated as attempting to ﬁnd\\nsubsets of variable values s1,...,s psuch that the probability of each of the\\nvariables simultaneously assuming a value within its respe ctive subset,\\nPr\\uf8ee\\n\\uf8f0p⋂\\nj=1(Xj∈sj)\\uf8f9\\n\\uf8fb, (14.2)\\nis relatively large. The intersection of subsets ∩p\\nj=1(Xj∈sj) is called a\\nconjunctive rule . For quantitative variables the subsets sjare contiguous\\nintervals;forcategoricalvariablesthesubsetsaredelin eatedexplicitly.Note\\nthat if the subset sjis in fact the entire set of values sj=Sj, as is often\\nthe case, the variable Xjis saidnotto appear in the rule (14.2).\\n14.2.1 Market Basket Analysis\\nGeneral approaches to solving (14.2) are discussed in Secti on 14.2.5. These\\ncan be quite useful in many applications. However, they are n ot feasible\\nfor the very large ( p≈104,N≈108) commercial data bases to which\\nmarket basket analysis is often applied. Several further si mpliﬁcations of\\n(14.2) are required. First, only two types of subsets are con sidered; either\\nsjconsists of a singlevalue ofXj,sj=v0j, or it consists of the entire set\\nof values that Xjcan assume, sj=Sj. This simpliﬁes the problem (14.2)\\nto ﬁnding subsets of the integers J⊂{1,...,p}, and corresponding values\\nv0j, j∈J, such that\\nPr\\uf8ee\\n\\uf8f0⋂\\nj∈J(Xj=v0j)\\uf8f9\\n\\uf8fb (14.3)\\nis large. Figure 14.1 illustrates this assumption.\\nOne can apply the technique of dummy variables to turn (14.3) into\\na problem involving only binary-valued variables. Here we a ssume that\\nthe supportSjis ﬁnite for each variable Xj. Speciﬁcally, a new set of\\nvariablesZ1,...,Z Kis created, one such variable for each of the values\\nvljattainable by each of the original variables X1,...,X p. The number of\\ndummy variables Kis\\nK=p∑\\nj=1|Sj|,\\nwhere|Sj|is the number of distinct values attainable by Xj. Each dummy\\nvariable is assigned the value Zk= 1 if the variable with which it is as-\\nsociated takes on the corresponding value to which Zkis assigned, and\\nZk= 0 otherwise. This transforms (14.3) to ﬁnding a subset of th e integers\\nK⊂{1,...,K}such that', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 506}),\n",
       " Document(page_content='14.2 Association Rules 489\\nX1 X1 X1\\nX2X2X2\\nFIGURE 14.1. Simpliﬁcations for association rules. Here there are two inpu ts\\nX1andX2, taking four and six distinct values, respectively. The red sq uares\\nindicate areas of high density. To simplify the computations , we assume that the\\nderived subset corresponds to either a single value of an input or all values. With\\nthis assumption we could ﬁnd either the middle or right pattern , but not the left\\none.\\nPr[⋂\\nk∈K(Zk= 1)]\\n= Pr[∏\\nk∈KZk= 1]\\n(14.4)\\nis large. This is the standard formulation of the market bask et problem.\\nThe setKis called an “item set.” The number of variables Zkin the item\\nset is called its “size” (note that the size is no bigger than p). The estimated\\nvalue of (14.4) is taken to be the fraction of observations in the data base\\nfor which the conjunction in (14.4) is true:\\nˆPr[∏\\nk∈K(Zk= 1)]\\n=1\\nNN∑\\ni=1∏\\nk∈Kzik. (14.5)\\nHerezikis the value of Zkfor thisith case. This is called the “support” or\\n“prevalence” T(K)oftheitemsetK.Anobservation iforwhich∏\\nk∈Kzik=\\n1 is said to “contain” the item set K.\\nIn association rule mining a lower support bound tis speciﬁed, and one\\nseeksallitem setsKlthat can be formed from the variables Z1,...,Z K\\nwith support in the data base greater than this lower bound t\\n{Kl|T(Kl)>t}. (14.6)\\n14.2.2 The Apriori Algorithm\\nThe solution to this problem (14.6) can be obtained with feas ible compu-\\ntation for very large data bases provided the threshold tis adjusted so that\\n(14.6) consists of only a small fraction of all 2Kpossible item sets. The\\n“Apriori” algorithm (Agrawal et al., 1995) exploits severa l aspects of the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 507}),\n",
       " Document(page_content='490 14. Unsupervised Learning\\ncurse of dimensionality to solve (14.6) with a small number o f passes over\\nthe data. Speciﬁcally, for a given support threshold t:\\n•The cardinality|{K|T(K)>t}|is relatively small.\\n•Any item setLconsisting of a subset of the items in Kmust have\\nsupport greater than or equal to that of K,L⊆K⇒T(L)≥T(K).\\nThe ﬁrst pass over the data computes the support of all single -item sets.\\nThose whose support is less than the threshold are discarded . The second\\npass computes the support of all item sets of size two that can be formed\\nfrom pairs of the single items surviving the ﬁrst pass. In oth er words, to\\ngenerate all frequent itemsets with |K|=m, we need to consider only\\ncandidates such that allof theirmancestral item sets of size m−1 are\\nfrequent. Those size-two item sets with support less than th e threshold are\\ndiscarded. Each successive pass over the data considers onl y those item\\nsets that can be formed by combining those that survived the p revious\\npass with those retained from the ﬁrst pass. Passes over the d ata continue\\nuntil all candidate rules from the previous pass have suppor t less than the\\nspeciﬁed threshold. The Apriori algorithm requires only on e pass over the\\ndata for each value of |K|, which is crucial since we assume the data cannot\\nbe ﬁtted into a computer’s main memory. If the data are suﬃcie ntly sparse\\n(or if the threshold tis high enough), then the process will terminate in\\nreasonable time even for huge data sets.\\nThere are many additional tricks that can be used as part of th is strat-\\negy to increase speed and convergence (Agrawal et al., 1995) . The Apriori\\nalgorithm represents one of the major advances in data minin g technology.\\nEach high support item set K(14.6) returned by the Apriori algorithm is\\ncast into a set of “association rules.” The items Zk,k∈K, are partitioned\\ninto two disjoint subsets, A∪B=K, and written\\nA⇒B. (14.7)\\nThe ﬁrst item subset Ais called the “antecedent” and the second Bthe\\n“consequent.”Associationrulesaredeﬁnedtohaveseveral propertiesbased\\non the prevalence of the antecedent and consequent item sets in the data\\nbase. The “support” of the rule T(A⇒B) is the fraction of observations\\nin the union of the antecedent and consequent, which is just t he support\\nof the item setKfrom which they were derived. It can be viewed as an\\nestimate (14.5) of the probability of simultaneously obser ving both item\\nsets Pr(AandB) in a randomly selected market basket. The “conﬁdence”\\nor “predictability” C(A⇒B) of the rule is its support divided by the\\nsupport of the antecedent\\nC(A⇒B) =T(A⇒B)\\nT(A), (14.8)\\nwhich can be viewed as an estimate of Pr( B|A). The notation Pr( A), the\\nprobability of an item set Aoccurring in a basket, is an abbreviation for', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 508}),\n",
       " Document(page_content='14.2 Association Rules 491\\nPr(∏\\nk∈AZk= 1). The “expected conﬁdence” is deﬁned as the support of\\nthe consequent T(B), which is an estimate of the unconditional probability\\nPr(B). Finally, the “lift” of the rule is deﬁned as the conﬁdence d ivided by\\nthe expected conﬁdence\\nL(A⇒B) =C(A⇒B)\\nT(B).\\nThis is an estimate of the association measure Pr( AandB)/Pr(A)Pr(B).\\nAsanexample,supposetheitemset K={peanut butter, jelly, bread }\\nand consider the rule {peanut butter, jelly }⇒{bread}. A support value\\nof 0.03 for this rule means that peanut butter ,jelly, andbreadappeared\\ntogether in 3% of the market baskets. A conﬁdence of 0.82 for t his rule im-\\nplies that when peanut butter andjellywere purchased, 82% of the time\\nbreadwas also purchased. If bread appeared in 43% of all market bas kets\\nthen the rule{peanut butter, jelly }⇒{bread}would have a lift of 1 .95.\\nThe goal of this analysis is to produce association rules (14 .7) with both\\nhigh values of support and conﬁdence (14.8). The Apriori alg orithm returns\\nall item sets with high support as deﬁned by the support thres holdt(14.6).\\nA conﬁdence threshold cis set, and all rules that can be formed from those\\nitem sets (14.6) with conﬁdence greater than this value\\n{A⇒B|C(A⇒B)>c} (14.9)\\nare reported. For each item set Kof size|K|there are 2|K|−1−1 rules of\\nthe formA⇒(K−A),A⊂K. Agrawal et al. (1995) present a variant of\\nthe Apriori algorithm that can rapidly determine which rule s survive the\\nconﬁdence threshold (14.9) from all possible rules that can be formed from\\nthe solution item sets (14.6).\\nThe output of the entire analysis is a collection of associat ion rules (14.7)\\nthat satisfy the constraints\\nT(A⇒B)>tandC(A⇒B)>c.\\nThese are generally stored in a data base that can be queried b y the user.\\nTypical requests might be to display the rules in sorted orde r of conﬁdence,\\nlift or support. More speciﬁcally, one might request such a l ist conditioned\\non particular items in the antecedent or especially the cons equent. For\\nexample, a request might be the following:\\nDisplay all transactions in which ice skates are the consequ ent\\nthat have conﬁdence over 80%and support of more than 2%.\\nThis could provide information on those items (antecedent) that predicate\\nsales of ice skates. Focusing on a particular consequent cas ts the problem\\ninto the framework of supervised learning.\\nAssociation rules have become a popular tool for analyzing v ery large\\ncommercial data bases in settings where market basket is rel evant. That is', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 509}),\n",
       " Document(page_content='492 14. Unsupervised Learning\\nwhen the data can be cast in the form of a multidimensional con tingency\\ntable. The output is in the form of conjunctive rules (14.4) t hat are easily\\nunderstood and interpreted. The Apriori algorithm allows t his analysis to\\nbeappliedtohugedatabases,muchlargerthatareamenablet oothertypes\\nof analyses. Association rules are among data mining’s bigg est successes.\\nBesides the restrictive form of the data to which they can be a pplied, as-\\nsociation rules have other limitations. Critical to comput ational feasibility\\nis the support threshold (14.6). The number of solution item sets, their size,\\nand the number of passes required over the data can grow expon entially\\nwith decreasing size of this lower bound. Thus, rules with hi gh conﬁdence\\nor lift, but low support, will not be discovered. For example , a high conﬁ-\\ndence rule such as vodka⇒caviarwill not be uncovered owing to the low\\nsales volume of the consequent caviar.\\n14.2.3 Example: Market Basket Analysis\\nWe illustrate the use of Apriori on a moderately sized demogr aphics data\\nbase. This data set consists of N= 9409 questionnaires ﬁlled out by shop-\\npingmallcustomersintheSanFranciscoBayArea(ImpactRes ources,Inc.,\\nColumbus OH, 1987). Here we use answers to the ﬁrst 14 questio ns, relat-\\ning to demographics, for illustration. These questions are listed in Table\\n14.1. The data are seen to consist of a mixture of ordinal and ( unordered)\\ncategorical variables, many of the latter having more than a few values.\\nThere are many missing values.\\nWeusedafreewareimplementationoftheApriorialgorithmd uetoChris-\\ntianBorgelt1.Afterremovingobservationswithmissingvalues,eachord inal\\npredictor was cut at its median and coded by two dummy variabl es; each\\ncategorical predictor with kcategories was coded by kdummy variables.\\nThis resulted in a 6876 ×50 matrix of 6876 observations on 50 dummy\\nvariables.\\nThe algorithm found a total of 6288 association rules, invol ving≤5\\npredictors, with support of at least 10%. Understanding thi s large set of\\nrulesisitselfachallengingdataanalysistask.Wewillnot attemptthishere,\\nbut only illustrate in Figure 14.2 the relative frequency of each dummy\\nvariable in the data (top) and the association rules (bottom ). Prevalent\\ncategories tend to appear more often in the rules, for exampl e, the ﬁrst\\ncategory in language (English). However, others such as occ upation are\\nunder-represented, with the exception of the ﬁrst and ﬁfth l evel.\\nHere are three examples of association rules found by the Apr iori algo-\\nrithm:\\nAssociation rule 1: Support 25%, conﬁdence 99.7% and lift 1.03.\\n1Seehttp://fuzzy.cs.uni-magdeburg.de/ ∼borgelt.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 510}),\n",
       " Document(page_content='14.2 Association Rules 493\\n0 10 20 30 40 500.0 0.02 0.04 0.06\\nAttributeRelative Frequency in Data\\nincomesexmarstatageeduc occup yrs−baydualincperhousperyounghousetypehome ethnic language\\n0 10 20 30 40 500.0 0.04 0.08 0.12\\nAttributeRelative Frequency in Association Rules\\nincomesexmarstatageeduc occup yrs−baydualincperhousperyounghousetypehome ethnic languageFIGURE 14.2. Market basket analysis: relative frequency of each dummy vari -\\nable (coding an input category) in the data (top), and the asso ciation rules found\\nby the Apriori algorithm (bottom).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 511}),\n",
       " Document(page_content='494 14. Unsupervised Learning\\nTABLE 14.1. Inputs for the demographic data.\\nFeature Demographic # Values Type\\n1 Sex 2 Categorical\\n2 Marital status 5 Categorical\\n3 Age 7 Ordinal\\n4 Education 6 Ordinal\\n5 Occupation 9 Categorical\\n6 Income 9 Ordinal\\n7 Years in Bay Area 5 Ordinal\\n8 Dual incomes 3 Categorical\\n9 Number in household 9 Ordinal\\n10 Number of children 9 Ordinal\\n11 Householder status 3 Categorical\\n12 Type of home 5 Categorical\\n13 Ethnic classiﬁcation 8 Categorical\\n14 Language in home 3 Categorical\\n[number in household = 1\\nnumber of children = 0]\\n⇓\\nlanguage in home = English\\nAssociation rule 2: Support 13.4%, conﬁdence 80.8%, and lift 2.13.\\n\\uf8ee\\n\\uf8f0language in home = English\\nhouseholder status = own\\noccupation = {professional/managerial }\\uf8f9\\n\\uf8fb\\n⇓\\nincome≥$40,000\\nAssociation rule 3: Support 26.5%, conﬁdence 82.8% and lift 2.15.\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0language in home = English\\nincome<$40,000\\nmarital status = not married\\nnumber of children = 0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n⇓\\neducation/∈{college graduate, graduate study }', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 512}),\n",
       " Document(page_content='14.2 Association Rules 495\\nWe chose the ﬁrst and third rules based on their high support. The second\\nrule is an association rule with a high-income consequent, a nd could be\\nused to try to target high-income individuals.\\nAs stated above, we created dummy variables for each categor y of the\\ninput predictors, for example, Z1=I(income<$40,000) andZ2=\\nI(income≥$40,000) for below and above the median income. If we were\\ninterested only in ﬁnding associations with the high-incom e category, we\\nwould include Z2but notZ1. This is often the case in actual market basket\\nproblems, where we are interested in ﬁnding associations wi th the presence\\nof a relatively rare item, but not associations with its abse nce.\\n14.2.4 Unsupervised as Supervised Learning\\nHere we discuss a technique for transforming the density est imation prob-\\nlem into one of supervised function approximation. This for ms the basis\\nfor the generalized association rules described in the next section.\\nLetg(x) be the unknown data probability density to be estimated, an d\\ng0(x) be a speciﬁed probability density function used for refere nce. For ex-\\nample,g0(x) might be the uniform density over the range of the variables .\\nOther possibilities are discussed below. The data set x1,x2,...,x Nis pre-\\nsumed to be an i.i.d.random sample drawn from g(x). A sample of size N0\\ncan be drawn from g0(x) using Monte Carlo methods. Pooling these two\\ndata sets, and assigning mass w=N0/(N+N0) to those drawn from g(x),\\nandw0=N/(N+N0) to those drawn from g0(x), results in a random\\nsample drawn from the mixture density ( g(x)+g0(x))/2. If one assigns\\nthe valueY= 1 to each sample point drawn from g(x) andY= 0 those\\ndrawn from g0(x), then\\nµ(x) =E(Y|x) =g(x)\\ng(x)+g0(x)\\n=g(x)/g0(x)\\n1+g(x)/g0(x)(14.10)\\ncan be estimated by supervised learning using the combined s ample\\n(y1,x1),(y2,x2),...,(yN+N0,xN+N0) (14.11)\\nas training data. The resulting estimate ˆ µ(x) can be inverted to provide an\\nestimate for g(x)\\nˆg(x) =g0(x)ˆµ(x)\\n1−ˆµ(x). (14.12)\\nGeneralizedversionsoflogistic regression(Section 4.4) areespeciallywell\\nsuited for this application since the log-odds,\\nf(x) = logg(x)\\ng0(x), (14.13)\\nare estimated directly. In this case one has', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 513}),\n",
       " Document(page_content='496 14. Unsupervised Learning\\n-1 0 1 2-2 0 2 4 6••\\n••••\\n••\\n•\\n•••\\n••\\n••\\n••\\n••\\n•\\n••••\\n••••\\n••\\n•\\n••\\n••\\n••\\n••\\n•\\n••••\\n•••\\n•\\n••\\n••••••\\n••••\\n• •\\n••\\n••••\\n••••\\n•\\n•••\\n•••••\\n•\\n• • •\\n••\\n••\\n••\\n•\\n••\\n•••\\n••\\n••\\n••\\n•••\\n•••\\n••\\n••\\n••\\n••\\n•••\\n••\\n••\\n••\\n•••\\n••••\\n•\\n••••\\n••••\\n•••••\\n•••\\n••••••••\\n•\\n••\\n•••\\n••\\n••\\n•••\\n•••\\n• •\\n••\\n••\\n•\\n•••\\n•\\n••\\n•••\\n•\\n••\\n•••••\\n•\\n-1 0 1 2-2 0 2 4 6••\\n••••\\n••\\n•\\n•••\\n••\\n••\\n••\\n••\\n•\\n••••\\n••••\\n••\\n•\\n••\\n••\\n••\\n••\\n•\\n••••\\n•••\\n•\\n••\\n••••••\\n••••\\n• •\\n••\\n••••\\n••••\\n•\\n•••\\n•••••\\n•\\n• • •\\n••\\n••\\n••\\n•\\n••\\n•••\\n••\\n••\\n••\\n•••\\n•••\\n••\\n••\\n••\\n••\\n•••\\n••\\n••\\n••\\n•••\\n••••\\n•\\n••••\\n••••\\n•••••\\n•••\\n••••••••\\n•\\n••\\n•••\\n••\\n••\\n•••\\n•••\\n• •\\n••\\n••\\n•\\n•••\\n•\\n••\\n•••\\n•\\n••\\n•••••\\n••\\n• ••\\n•\\n•••\\n••\\n•\\n••••\\n••\\n••••\\n•\\n••\\n••••\\n•\\n••\\n••\\n••\\n••\\n•••••• •\\n••••\\n•\\n•••\\n•\\n••\\n•••\\n••\\n•\\n••\\n•••••\\n•\\n•••••\\n•\\n•• ••\\n••\\n••\\n••\\n•••\\n••\\n•\\n•\\n••\\n•\\n••\\n•••\\n•\\n••••\\n•\\n••\\n••\\n••\\n•••\\n•\\n•• •••\\n•••••\\n••\\n•••\\n•••\\n•\\n••\\n••\\n•••\\n••\\n•\\n••\\n••\\n•\\n••\\n••\\n••\\n••••\\n•••\\n•••\\n•\\n••\\n••\\n•\\n••\\n••\\n•••\\n••\\n•\\n••\\n•••\\n•\\n• •\\n••\\n••••\\n••\\n• •\\nX1 X1\\nX2X2\\nFIGURE 14.3. Density estimation via classiﬁcation. (Left panel:) Trainin g set\\nof200data points. (Right panel:) Training set plus 200reference data points,\\ngenerated uniformly over the rectangle containing the traini ng data. The training\\nsample was labeled as class 1, and the reference sample class 0, and a semipara-\\nmetric logistic regression model was ﬁt to the data. Some cont ours for ˆg(x)are\\nshown.\\nˆg(x) =g0(x)eˆf(x). (14.14)\\nAn example is shown in Figure 14.3. We generated a training se t of size\\n200 shown in the left panel. The right panel shows the referen ce data (blue)\\ngenerated uniformly over the rectangle containing the trai ning data. The\\ntraining sample was labeled as class 1, and the reference sam ple class 0,\\nand a logistic regression model, using a tensor product of na tural splines\\n(Section 5.2.1), was ﬁt to the data. Some probability contou rs of ˆµ(x) are\\nshown in the right panel; these are also the contours of the de nsity estimate\\nˆg(x), since ˆg(x) = ˆµ(x)/(1−ˆµ(x)), is a monotone function. The contours\\nroughly capture the data density.\\nIn principle any reference density can be used for g0(x) in (14.14). In\\npractice the accuracy of the estimate ˆ g(x) can depend greatly on partic-\\nular choices. Good choices will depend on the data density g(x) and the\\nprocedure used to estimate (14.10) or (14.13). If accuracy i s the goal,g0(x)\\nshould be chosen so that the resulting functions µ(x) orf(x) are approx-\\nimated easily by the method being used. However, accuracy is not always\\nthe primary goal. Both µ(x) andf(x) are monotonic functions of the den-\\nsity ratiog(x)/g0(x). They can thus be viewed as “contrast” statistics that\\nprovide information concerning departures of the data dens ityg(x) from\\nthe chosen reference density g0(x). Therefore, in data analytic settings, a\\nchoice forg0(x) is dictated by types of departures that are deemed most\\ninteresting in the context of the speciﬁc problem at hand. Fo r example, if\\ndepartures from uniformity are of interest, g0(x) might be the a uniform\\ndensity over the range of the variables. If departures from j oint normality', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 514}),\n",
       " Document(page_content='14.2 Association Rules 497\\nare of interest, a good choice for g0(x) would be a Gaussian distribution\\nwith the same mean vector and covariance matrix as the data. D epartures\\nfrom independence could be investigated by using\\ng0(x) =p∏\\nj=1gj(xj), (14.15)\\nwheregj(xj) is the marginal data density of Xj, thejth coordinate of X.\\nA sample from this independent density (14.15) is easily gen erated from the\\ndata itself by applying a diﬀerent random permutation to the data values\\nof each of the variables.\\nAs discussed above, unsupervised learning is concerned wit h revealing\\nproperties of the data density g(x). Each technique focuses on a particu-\\nlar property or set of properties. Although this approach of transforming\\nthe problem to one of supervised learning (14.10)–(14.14) s eems to have\\nbeen part of the statistics folklore for some time, it does no t appear to\\nhave had much impact despite its potential to bring well-dev eloped su-\\npervised learning methodology to bear on unsupervised lear ning problems.\\nOne reason may be that the problem must be enlarged with a simu lated\\ndata set generated by Monte Carlo techniques. Since the size of this data\\nset should be at least as large as the data sample N0≥N, the compu-\\ntation and memory requirements of the estimation procedure are at least\\ndoubled. Also, substantial computation may be required to g enerate the\\nMonte Carlo sample itself. Although perhaps a deterrent in t he past, these\\nincreased computational requirements are becoming much le ss of a burden\\nas increased resources become routinely available. We illu strate the use of\\nsupervised learning methods for unsupervised learning in t he next section.\\n14.2.5 Generalized Association Rules\\nThe more general problem (14.2) of ﬁnding high-density regi ons in the data\\nspace can be addressed using the supervised learning approa ch described\\nabove. Although not applicable to the huge data bases for whi ch market\\nbasket analysis is feasible, useful information can be obta ined from mod-\\nerately sized data sets. The problem (14.2) can be formulate d as ﬁnding\\nsubsets of the integers J ⊂{1,2,...,p}and corresponding value subsets\\nsj, j∈Jfor the corresponding variables Xj, such that\\nˆPr\\uf8eb\\n\\uf8ed⋂\\nj∈J(Xj∈sj)\\uf8f6\\n\\uf8f8=1\\nNN∑\\ni=1I\\uf8eb\\n\\uf8ed⋂\\nj∈J(xij∈sj)\\uf8f6\\n\\uf8f8 (14.16)\\nis large. Following the nomenclature of association rule an alysis,{(Xj∈\\nsj)}j∈Jwill be called a “generalized” item set. The subsets sjcorrespond-\\ning to quantitative variables are taken to be contiguous int ervals within', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 515}),\n",
       " Document(page_content='498 14. Unsupervised Learning\\ntheir range of values, and subsets for categorical variable s can involve more\\nthan a single value. The ambitious nature of this formulatio n precludes a\\nthorough search for all generalized item sets with support ( 14.16) greater\\nthan a speciﬁed minimum threshold, as was possible in the mor e restric-\\ntive setting of market basket analysis. Heuristic search me thods must be\\nemployed, and the most one can hope for is to ﬁnd a useful colle ction of\\nsuch generalized item sets.\\nBothmarketbasketanalysis(14.5)andthegeneralizedform ulation(14.16)\\nimplicitly reference the uniform probability distributio n. One seeks item\\nsets that are more frequent than would be expected if all join t data values\\n(x1,x2,...,x N) were uniformly distributed. This favors the discovery of\\nitem sets whose marginal constituents ( Xj∈sj) areindividually frequent,\\nthat is, the quantity\\n1\\nNN∑\\ni=1I(xij∈sj) (14.17)\\nis large. Conjunctions of frequent subsets (14.17) will ten d to appear more\\noften among item sets of high support (14.16) than conjuncti ons of margin-\\nally less frequent subsets. This is why the rule vodka⇒caviaris not likely\\nto be discovered in spite of a high association (lift); neith er item has high\\nmarginal support, so that their joint support is especially small. Reference\\nto the uniform distribution can cause highly frequent item s ets with low\\nassociations among their constituents to dominate the coll ection of highest\\nsupport item sets.\\nHighly frequent subsets sjare formed as disjunctions of the most fre-\\nquentXj-values. Using the product of the variable marginal data den sities\\n(14.15) as a reference distribution removes the preference for highly fre-\\nquent values of the individual variables in the discovered i tem sets. This is\\nbecause the density ratio g(x)/g0(x) is uniform if there are no associations\\namong the variables (complete independence), regardless o f the frequency\\ndistribution of the individual variable values. Rules like vodka⇒caviar\\nwould have a chance to emerge. It is not clear however, how to i ncorporate\\nreference distributions other than the uniform into the Apr iori algorithm.\\nAs explained in Section 14.2.4, it is straightforward to gen erate a sample\\nfrom the product density (14.15), given the original data se t.\\nAfter choosing a reference distribution, and drawing a samp le from it\\nas in (14.11), one has a supervised learning problem with a bi nary-valued\\noutput variable Y∈{0,1}. The goal is to use this training data to ﬁnd\\nregions\\nR=⋂\\nj∈J(Xj∈sj) (14.18)\\nforwhichthetargetfunction µ(x) =E(Y|x)isrelativelylarge.Inaddition,\\none might wish to require that the datasupport of these regions', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 516}),\n",
       " Document(page_content='14.2 Association Rules 499\\nT(R) =∫\\nx∈Rg(x)dx (14.19)\\nnot be too small.\\n14.2.6 Choice of Supervised Learning Method\\nThe regions (14.18) are deﬁned by conjunctive rules. Hence s upervised\\nmethods that learn such rules would be most appropriate in th is context.\\nThe terminal nodes of a CART decision tree are deﬁned by rules precisely\\nof the form (14.18). Applying CART to the pooled data (14.11) will pro-\\nduce a decision tree that attempts to model the target (14.10 ) over the\\nentire data space by a disjoint set of regions (terminal node s). Each region\\nis deﬁned by a rule of the form (14.18). Those terminal nodes twith high\\naveragey-values\\n¯yt= ave(yi|xi∈t)\\nare candidates for high-support generalized item sets (14. 16). The actual\\n(data) support is given by\\nT(R) = ¯yt·Nt\\nN+N0,\\nwhereNtis the number of (pooled) observations within the region rep re-\\nsented by the terminal node. By examining the resulting deci sion tree, one\\nmight discover interesting generalized item sets of relati vely high-support.\\nThese can then be partitioned into antecedents and conseque nts in a search\\nfor generalized association rules of high conﬁdence and/or lift.\\nAnother natural learning method for this purpose is the pati ent rule\\ninduction method PRIM described in Section 9.3. PRIM also pr oduces\\nrules precisely of the form (14.18), but it is especially des igned for ﬁnding\\nhigh-support regions that maximize the average target (14. 10) value within\\nthem, rather than trying to model the target function over th e entire data\\nspace. It also provides more control over the support/avera ge-target-value\\ntradeoﬀ.\\nExercise 14.3 addresses an issue that arises with either of t hese methods\\nwhen we generate random data from the product of the marginal distribu-\\ntions.\\n14.2.7 Example: Market Basket Analysis (Continued)\\nWe illustrate the use of PRIM on the demographics data of Tabl e 14.1.\\nThree of the high-support generalized item sets emerging fr om the PRIM\\nanalysis were the following:\\nItem set 1: Support= 24%.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 517}),\n",
       " Document(page_content='500 14. Unsupervised Learning\\n\\uf8ee\\n\\uf8f0marital status = married\\nhouseholder status = own\\ntype of home̸=apartment\\uf8f9\\n\\uf8fb\\nItem set 2: Support= 24%.\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0age≤24\\nmarital status∈ {living together-not married, single }\\noccupation /∈ {professional, homemaker, retired }\\nhouseholder status ∈ {rent, live with family }\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\nItem set 3: Support= 15%.\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0householder status = rent\\ntype of home̸=house\\nnumber in household ≤2\\nnumber of children = 0\\noccupation /∈ {homemaker, student, unemployed }\\nincome∈[$20,000,$150,000]\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nGeneralized association rules derived from these item sets with conﬁdence\\n(14.8) greater than 95% are the following:\\nAssociation rule 1: Support 25%, conﬁdence 99.7% and lift 1.35.\\n[\\nmarital status = married\\nhouseholder status = own]\\n⇓\\ntype of home̸=apartment\\nAssociation rule 2: Support 25%, conﬁdence 98.7% and lift 1.97.\\n\\uf8ee\\n\\uf8f0age≤24\\noccupation /∈ {professional, homemaker, retired }\\nhouseholder status ∈ {rent, live with family }\\uf8f9\\n\\uf8fb\\n⇓\\nmarital status∈{single, living together-not married }\\nAssociation rule 3: Support 25%, conﬁdence 95.9% and lift 2.61.\\n[householder status = own\\ntype of home̸=apartment]\\n⇓\\nmarital status = married', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 518}),\n",
       " Document(page_content='14.3 Cluster Analysis 501\\nAssociation rule 4: Support 15%, conﬁdence 95.4% and lift 1.50.\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0householder status = rent\\ntype of home̸=house\\nnumber in household ≤2\\noccupation /∈ {homemaker, student, unemployed }\\nincome∈[$20,000,$150,000]\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n⇓\\nnumber of children = 0\\nThere are no great surprises among these particular rules. F or the most\\npart they verify intuition. In other contexts where there is less prior in-\\nformation available, unexpected results have a greater cha nce to emerge.\\nThese results do illustrate the type of information general ized association\\nrules can provide, and that the supervised learning approac h, coupled with\\na ruled induction method such as CART or PRIM, can uncover ite m sets\\nexhibiting high associations among their constituents.\\nHowdothesegeneralizedassociationrulescomparetothose foundearlier\\nby the Apriori algorithm? Since the Apriori procedure gives thousands of\\nrules, it is diﬃcult to compare them. However some general po ints can be\\nmade. The Apriori algorithm is exhaustive—it ﬁnds allrules with support\\ngreater than a speciﬁed amount. In contrast, PRIM is a greedy algorithm\\nand is not guaranteed to give an “optimal” set of rules. On the other hand,\\nthe Apriori algorithm can deal only with dummy variables and hence could\\nnot ﬁnd some of the above rules. For example, since type of home is a\\ncategorical input, with a dummy variable for each level, Apr iori could not\\nﬁnd a rule involving the set\\ntype of home̸=apartment.\\nTo ﬁnd this set, we would have to code a dummy variable for apartment\\nversus the other categories of type of home. It will not gener ally be feasible\\nto precode all such potentially interesting comparisons.\\n14.3 Cluster Analysis\\nCluster analysis, also called data segmentation, has a vari ety of goals. All\\nrelate to grouping or segmenting a collection of objects int o subsets or\\n“clusters,” such that those within each cluster are more clo sely related to\\none another than objects assigned to diﬀerent clusters. An o bject can be\\ndescribed by a set of measurements, or by its relation to othe r objects.\\nIn addition, the goal is sometimes to arrange the clusters in to a natural\\nhierarchy. This involves successively grouping the cluste rs themselves so', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 519}),\n",
       " Document(page_content='502 14. Unsupervised Learning\\n• •••\\n••••\\n• •••\\n••\\n•••••• • ••\\n•••\\n••\\n••\\n••\\n•\\n•••••\\n••••\\n••\\n•••\\n••\\n••••\\n••\\n••••\\n•••\\n••\\n••\\n••\\n•••\\n••\\n••• ••\\n•\\n•••\\n•••••\\n••••••\\n•\\n••••\\n•••••\\n•••••\\n•••••\\n••••\\n•• •\\n•••••\\n•••\\n••\\n•••\\n••\\n•••\\n••••\\n•••\\n••• ••\\nX1X2\\nFIGURE 14.4. Simulated data in the plane, clustered into three classes (repre -\\nsented by orange, blue and green) by the K-means clustering algorithm\\nthat at each level of the hierarchy, clusters within the same group are more\\nsimilar to each other than those in diﬀerent groups.\\nCluster analysis is also used to form descriptive statistic s to ascertain\\nwhether or not the data consists of a set distinct subgroups, each group\\nrepresenting objects with substantially diﬀerent propert ies. This latter goal\\nrequires an assessment of the degree of diﬀerence between th e objects as-\\nsigned to the respective clusters.\\nCentral to all of the goals of cluster analysis is the notion o f the degree of\\nsimilarity (or dissimilarity) between the individual obje cts being clustered.\\nA clustering method attempts to group the objects based on th e deﬁnition\\nof similarity supplied to it. This can only come from subject matter consid-\\nerations. The situation is somewhat similar to the speciﬁca tion of a loss or\\ncost function in prediction problems (supervised learning ). There the cost\\nassociated with an inaccurate prediction depends on consid erations outside\\nthe data.\\nFigure 14.4 shows some simulated data clustered into three g roups via\\nthe popular K-means algorithm. In this case two of the clusters are not\\nwell separated, so that “segmentation” more accurately des cribes the part\\nof this process than “clustering.” K-means clustering starts with guesses\\nfor the three cluster centers. Then it alternates the follow ing steps until\\nconvergence:\\n•for each data point, the closest cluster center (in Euclidea n distance)\\nis identiﬁed;', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 520}),\n",
       " Document(page_content='14.3 Cluster Analysis 503\\n•each cluster center is replaced by the coordinate-wise aver age of all\\ndata points that are closest to it.\\nWe describe K-means clustering in more detail later, including the prob-\\nlem of how to choose the number of clusters (three in this exam ple).K-\\nmeans clustering is a top-down procedure, while other cluster approaches\\nthat we discuss are bottom-up . Fundamental to all clustering techniques is\\nthe choice of distance or dissimilarity measure between two objects. We\\nﬁrst discuss distance measures before describing a variety of algorithms for\\nclustering.\\n14.3.1 Proximity Matrices\\nSometimes the data is represented directly in terms of the pr oximity (alike-\\nness or aﬃnity) between pairs of objects. These can be either similarities or\\ndissimilarities (diﬀerence or lack of aﬃnity). For example, in social scienc e\\nexperiments, participants are asked to judge by how much cer tain objects\\ndiﬀer from one another. Dissimilarities can then be compute d by averaging\\nover the collection of such judgments. This type of data can b e represented\\nby anN×NmatrixD, whereNis the number of objects, and each element\\ndii′records the proximity between the ith andi′th objects. This matrix is\\nthen provided as input to the clustering algorithm.\\nMost algorithms presume a matrix of dissimilarities with no nnegative\\nentries and zero diagonal elements: dii= 0, i= 1,2,...,N.If the original\\ndata were collected as similarities, a suitable monotone-d ecreasing function\\ncan be used to convert them to dissimilarities. Also, most al gorithms as-\\nsume symmetric dissimilarity matrices, so if the original m atrixDis not\\nsymmetric it must be replaced by ( D+DT)/2. Subjectively judged dissimi-\\nlarities are seldom distances in the strict sense, since the triangle inequality\\ndii′≤dik+di′k,forallk∈{1,...,N}doesnothold.Thus,somealgorithms\\nthat assume distances cannot be used with such data.\\n14.3.2 Dissimilarities Based on Attributes\\nMost often we have measurements xijfori= 1,2,...,N, on variables\\nj= 1,2,...,p(also called attributes ). Since most of the popular clustering\\nalgorithmstakeadissimilaritymatrixastheirinput,wemu stﬁrstconstruct\\npairwisedissimilaritiesbetweentheobservations.Inthe mostcommoncase,\\nwe deﬁne a dissimilarity dj(xij,xi′j) between values of the jth attribute,\\nand then deﬁne\\nD(xi,xi′) =p∑\\nj=1dj(xij,xi′j) (14.20)\\nas the dissimilarity between objects iandi′. By far the most common\\nchoice is squared distance', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 521}),\n",
       " Document(page_content='504 14. Unsupervised Learning\\ndj(xij,xi′j) = (xij−xi′j)2. (14.21)\\nHowever, other choices are possible, and can lead to potenti ally diﬀerent\\nresults. For nonquantitative attributes (e.g., categoric al data), squared dis-\\ntance may not be appropriate. In addition, it is sometimes de sirable to\\nweigh attributes diﬀerently rather than giving them equal w eight as in\\n(14.20).\\nWe ﬁrst discuss alternatives in terms of the attribute type:\\nQuantitative variables. Measurements of this type of variable or attribute\\nare represented by continuous real-valued numbers. It is na tural to\\ndeﬁne the “error” between them as a monotone-increasing fun ction\\nof their absolute diﬀerence\\nd(xi,xi′) =l(|xi−xi′|).\\nBesides squared-error loss ( xi−xi′)2, a common choice is the identity\\n(absolute error). The former places more emphasis on larger diﬀer-\\nences than smaller ones. Alternatively, clustering can be b ased on the\\ncorrelation\\nρ(xi,xi′) =∑\\nj(xij−¯xi)(xi′j−¯xi′)√∑\\nj(xij−¯xi)2∑\\nj(xi′j−¯xi′)2, (14.22)\\nwith ¯xi=∑\\njxij/p. Note that this is averaged over variables , not ob-\\nservations. If the observations are ﬁrst standardized, then∑\\nj(xij−\\nxi′j)2∝2(1−ρ(xi,xi′)). Hence clustering based on correlation (simi-\\nlarity) is equivalent to that based on squared distance (dis similarity).\\nOrdinal variables. The values of this type of variable are often represented\\nas contiguous integers, and the realizable values are consi dered to be\\nan ordered set. Examples are academic grades (A, B, C, D, F), d egree\\nof preference (can’t stand, dislike, OK, like, terriﬁc). Ra nk data are a\\nspecial kind of ordinal data. Error measures for ordinal var iables are\\ngenerally deﬁned by replacing their Moriginal values with\\ni−1/2\\nM, i= 1,...,M (14.23)\\nin the prescribed order of their original values. They are th en treated\\nas quantitative variables on this scale.\\nCategorical variables. With unordered categorical (also called nominal)\\nvariables, the degree-of-diﬀerence between pairs of value s must be\\ndelineated explicitly. If the variable assumes Mdistinct values, these\\ncan be arranged in a symmetric M×Mmatrix with elements Lrr′=\\nLr′r,Lrr= 0,Lrr′≥0. The most common choice is Lrr′= 1 for all\\nr̸=r′, while unequal losses can be used to emphasize some errors\\nmore than others.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 522}),\n",
       " Document(page_content='14.3 Cluster Analysis 505\\n14.3.3 Object Dissimilarity\\nNext we deﬁne a procedure for combining the p-individual attribute dissim-\\nilaritiesdj(xij,xi′j), j= 1,2,...,pinto a single overall measure of dissim-\\nilarityD(xi,xi′) between two objects or observations ( xi,xi′) possessing\\nthe respective attribute values. This is nearly always done by means of a\\nweighted average (convex combination)\\nD(xi,xi′) =p∑\\nj=1wj·dj(xij,xi′j);p∑\\nj=1wj= 1. (14.24)\\nHerewjis a weight assigned to the jth attribute regulating the relative\\ninﬂuence of that variable in determining the overall dissim ilarity between\\nobjects. This choice should be based on subject matter consi derations.\\nIt is important to realize that setting the weight wjto the same value\\nfor each variable (say, wj= 1∀j) doesnotnecessarily give all attributes\\nequalinﬂuence.Theinﬂuenceofthe jthattribute Xjonobjectdissimilarity\\nD(xi,xi′) (14.24) depends upon its relative contribution to the aver age\\nobject dissimilarity measure over all pairs of observation s in the data set\\n¯D=1\\nN2N∑\\ni=1N∑\\ni′=1D(xi,xi′) =p∑\\nj=1wj·¯dj,\\nwith\\n¯dj=1\\nN2N∑\\ni=1N∑\\ni′=1dj(xij,xi′j) (14.25)\\nbeing the average dissimilarity on the jth attribute. Thus, the relative in-\\nﬂuence of the jth variable is wj·¯dj, and setting wj∼1/¯djwould give all\\nattributesequalinﬂuenceincharacterizingoveralldissi milaritybetweenob-\\njects. For example, with pquantitative variables and squared-error distance\\nused for each coordinate, then (14.24) becomes the (weighte d) squared Eu-\\nclidean distance\\nDI(xi,xi′) =p∑\\nj=1wj·(xij−xi′j)2(14.26)\\nbetween pairs of points in an IRp, with the quantitative variables as axes.\\nIn this case (14.25) becomes\\n¯dj=1\\nN2N∑\\ni=1N∑\\ni′=1(xij−xi′j)2= 2·varj, (14.27)\\nwhere var jis the sample estimate of Var( Xj). Thus, the relative impor-\\ntance of each such variable is proportional to its variance o ver the data', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 523}),\n",
       " Document(page_content='506 14. Unsupervised Learning\\n-6 -4 -2 0 2 4-6 -4 -2 0 2 4•••\\n••\\n•••\\n••••\\n••\\n••••••\\n•\\n••\\n••\\n••\\n•\\n••••••••\\n••••••••\\n••\\n•••• •••\\n••••\\n•••\\n•••• •••\\n••\\n••••\\n••••\\n••\\n•••••\\n•••\\n•••\\n•••••\\n•••\\n••\\n-2 -1 0 1 2-2 -1 0 1 2••\\n••••\\n•\\n••••\\n•\\n••\\n•\\n•\\n••••\\n•\\n•••\\n•••\\n••\\n•••\\n•••\\n•••••\\n••\\n•••\\n•••\\n••\\n•\\n•\\n•••\\n•\\n•••\\n••\\n•••\\n••\\n••\\n•••\\n•\\n•••••\\n•••\\n•\\n•••\\n••\\n••••••••\\n••\\n••\\n••\\nX1 X1\\nX2X2\\nFIGURE 14.5. Simulated data: on the left, K-means clustering (with K=2) has\\nbeen applied to the raw data. The two colors indicate the cluster memberships. On\\nthe right, the features were ﬁrst standardized before cluste ring. This is equivalent\\nto using feature weights 1/[2·var(Xj)]. The standardization has obscured the two\\nwell-separated groups. Note that each plot uses the same units in the horizontal\\nand vertical axes.\\nset. In general, setting wj= 1/¯djfor all attributes, irrespective of type,\\nwill cause each one of them to equally inﬂuence the overall di ssimilarity\\nbetween pairs of objects ( xi,xi′). Although this may seem reasonable, and\\nis often recommended, it can be highly counterproductive. I f the goal is to\\nsegment the data into groups of similar objects, all attribu tes may not con-\\ntribute equally to the (problem-dependent) notion of dissi milarity between\\nobjects. Some attribute value diﬀerences may reﬂect greate r actual object\\ndissimilarity in the context of the problem domain.\\nIf the goal is to discover natural groupings in the data, some attributes\\nmay exhibit more of a grouping tendency than others. Variabl es that are\\nmore relevant in separating the groups should be assigned a h igher inﬂu-\\nence in deﬁning object dissimilarity. Giving all attribute s equal inﬂuence\\nin this case will tend to obscure the groups to the point where a clustering\\nalgorithm cannot uncover them. Figure 14.5 shows an example .\\nAlthough simple generic prescriptions for choosing the ind ividual at-\\ntribute dissimilarities dj(xij,xi′j) and their weights wjcan be comforting,\\nthere is no substitute for careful thought in the context of e ach individ-\\nual problem. Specifying an appropriate dissimilarity meas ure is far more\\nimportant in obtaining success with clustering than choice of clustering\\nalgorithm. This aspect of the problem is emphasized less in t he cluster-\\ning literature than the algorithms themselves, since it dep ends on domain\\nknowledge speciﬁcs and is less amenable to general research .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 524}),\n",
       " Document(page_content='14.3 Cluster Analysis 507\\nFinally, often observations have missing values in one or more of the\\nattributes. The most common method of incorporating missin g values in\\ndissimilarity calculations (14.24) is to omit each observa tion pairxij,xi′j\\nhaving at least one value missing, when computing the dissim ilarity be-\\ntween observations xiandx′\\ni. This method can fail in the circumstance\\nwhen both observations have no measured values in common. In this case\\nboth observations could be deleted from the analysis. Alter natively, the\\nmissingvaluescouldbeimputedusingthemeanormedianofea chattribute\\nover the nonmissing data. For categorical variables, one co uld consider the\\nvalue “missing” as just another categorical value, if it wer e reasonable to\\nconsider two objects as being similar if they both have missi ng values on\\nthe same variables.\\n14.3.4 Clustering Algorithms\\nThe goal of cluster analysis is to partition the observation s into groups\\n(“clusters”) so that the pairwise dissimilarities between those assigned to\\nthe same cluster tend to be smaller than those in diﬀerent clu sters. Clus-\\ntering algorithms fall into three distinct types: combinat orial algorithms,\\nmixture modeling, and mode seeking.\\nCombinatorial algorithms work directly on the observed data with no\\ndirect reference to an underlying probability model. Mixture modeling sup-\\nposes that the data is an i.i.dsample from some population described by a\\nprobability density function. This density function is cha racterized by a pa-\\nrameterized model taken to be a mixture of component density functions;\\neach component density describes one of the clusters. This m odel is then ﬁt\\nto the data by maximum likelihood or corresponding Bayesian approaches.\\nMode seekers (“bumphunters”)takeanonparametricperspective,attemp t-\\ning to directly estimate distinct modes of the probability d ensity function.\\nObservations “closest” to each respective mode then deﬁne t he individual\\nclusters.\\nMixture modeling is described in Section 6.8. The PRIM algor ithm, dis-\\ncussed in Sections 9.3 and 14.2.5, is an example of mode seeki ng or “bump\\nhunting.” We discuss combinatorial algorithms next.\\n14.3.5 Combinatorial Algorithms\\nThe most popular clustering algorithms directly assign eac h observation\\nto a group or cluster without regard to a probability model de scribing the\\ndata. Each observation is uniquely labeled by an integer i∈{1,···,N}.\\nA prespeciﬁed number of clusters K < N is postulated, and each one is\\nlabeled by an integer k∈{1,...,K}. Each observation is assigned to one\\nand only one cluster. These assignments can be characterize d by a many-\\nto-one mapping, or encoderk=C(i), that assigns the ith observation to\\nthekth cluster. One seeks the particular encoder C∗(i) that achieves the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 525}),\n",
       " Document(page_content='508 14. Unsupervised Learning\\nrequired goal (details below), based on the dissimilaritie sd(xi,xi′) between\\nevery pair of observations. These are speciﬁed by the user as described\\nabove. Generally, the encoder C(i) is explicitly delineated by giving its\\nvalue (cluster assignment) for each observation i. Thus, the “parameters”\\nof the procedure are the individual cluster assignments for each of the N\\nobservations. These are adjusted so as to minimize a “loss” function that\\ncharacterizes the degree to which the clustering goal is notmet.\\nOne approach is to directly specify a mathematical loss func tion and\\nattempttominimizeitthroughsomecombinatorialoptimiza tionalgorithm.\\nSince the goal is to assign close points to the same cluster, a natural loss\\n(or “energy”) function would be\\nW(C) =1\\n2K∑\\nk=1∑\\nC(i)=k∑\\nC(i′)=kd(xi,xi′). (14.28)\\nThis criterion characterizes the extent to which observati ons assigned to\\nthe same cluster tend to be close to one another. It is sometim es referred\\nto as the “within cluster” point scatter since\\nT=1\\n2N∑\\ni=1N∑\\ni′=1dii′=1\\n2K∑\\nk=1∑\\nC(i)=k\\uf8eb\\n\\uf8ed∑\\nC(i′)=kdii′+∑\\nC(i′)̸=kdii′\\uf8f6\\n\\uf8f8,\\nor\\nT=W(C)+B(C),\\nwheredii′=d(xi,xi′). HereTis thetotalpoint scatter, which is a constant\\ngiven the data, independent of cluster assignment. The quan tity\\nB(C) =1\\n2K∑\\nk=1∑\\nC(i)=k∑\\nC(i′)̸=kdii′ (14.29)\\nis thebetween-cluster point scatter. This will tend to be large when obser-\\nvations assigned to diﬀerent clusters are far apart. Thus on e has\\nW(C) =T−B(C)\\nand minimizing W(C) is equivalent to maximizing B(C).\\nClusteranalysisbycombinatorialoptimizationisstraigh tforwardinprin-\\nciple. One simply minimizes Wor equivalently maximizes Bover all pos-\\nsible assignments of the Ndata points to Kclusters. Unfortunately, such\\noptimization by complete enumeration is feasible only for v ery small data\\nsets. The number of distinct assignments is (Jain and Dubes, 1988)\\nS(N,K) =1\\nK!K∑\\nk=1(−1)K−k(K\\nk)\\nkN. (14.30)\\nFor example, S(10,4) = 34,105 which is quite feasible. But, S(N,K) grows\\nvery rapidly with increasing values of its arguments. Alrea dyS(19,4)≃', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 526}),\n",
       " Document(page_content='14.3 Cluster Analysis 509\\n1010, and most clustering problems involve much larger data sets than\\nN= 19. For this reason, practical clustering algorithms are a ble to examine\\nonly a very small fraction of all possible encoders k=C(i). The goal is to\\nidentify a small subset that is likely to contain the optimal one, or at least\\na good suboptimal partition.\\nSuch feasible strategies are based on iterative greedy desc ent. An initial\\npartition is speciﬁed. At each iterative step, the cluster a ssignments are\\nchanged in such a way that the value of the criterion is improv ed from\\nits previous value. Clustering algorithms of this type diﬀe r in their pre-\\nscriptions for modifying the cluster assignments at each it eration. When\\nthe prescription is unable to provide an improvement, the al gorithm ter-\\nminates with the current assignments as its solution. Since the assignment\\nof observations to clusters at any iteration is a perturbati on of that for the\\nprevious iteration, only a very small fraction of all possib le assignments\\n(14.30) are examined. However, these algorithms converge t olocaloptima\\nwhich may be highly suboptimal when compared to the global op timum.\\n14.3.6K-means\\nTheK-means algorithm is one of the most popular iterative descen t clus-\\ntering methods. It is intended for situations in which all va riables are of\\nthe quantitative type, and squared Euclidean distance\\nd(xi,xi′) =p∑\\nj=1(xij−xi′j)2=||xi−xi′||2\\nis chosen as the dissimilarity measure. Note that weighted E uclidean dis-\\ntance can be used by redeﬁning the xijvalues (Exercise 14.1).\\nThe within-point scatter (14.28) can be written as\\nW(C) =1\\n2K∑\\nk=1∑\\nC(i)=k∑\\nC(i′)=k||xi−xi′||2\\n=K∑\\nk=1Nk∑\\nC(i)=k||xi−¯xk||2, (14.31)\\nwhere ¯xk= (¯x1k,...,¯xpk) is the mean vector associated with the kth clus-\\nter, andNk=∑N\\ni=1I(C(i) =k). Thus, the criterion is minimized by\\nassigning the Nobservations to the Kclusters in such a way that within\\neach cluster the average dissimilarity of the observations from the cluster\\nmean, as deﬁned by the points in that cluster, is minimized.\\nAn iterative descent algorithm for solving', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 527}),\n",
       " Document(page_content='510 14. Unsupervised Learning\\nAlgorithm 14.1 K-means Clustering.\\n1. For a given cluster assignment C, the total cluster variance (14.33) is\\nminimized with respect to {m1,...,m K}yielding the means of the\\ncurrently assigned clusters (14.32).\\n2. Given a current set of means {m1,...,m K}, (14.33) is minimized by\\nassigning each observation to the closest (current) cluste r mean. That\\nis,\\nC(i) = argmin\\n1≤k≤K||xi−mk||2. (14.34)\\n3. Steps 1 and 2 are iterated until the assignments do not chan ge.\\nC∗= min\\nCK∑\\nk=1Nk∑\\nC(i)=k||xi−¯xk||2\\ncan be obtained by noting that for any set of observations S\\n¯xS= argmin\\nm∑\\ni∈S||xi−m||2. (14.32)\\nHence we can obtain C∗by solving the enlarged optimization problem\\nmin\\nC,{mk}K\\n1K∑\\nk=1Nk∑\\nC(i)=k||xi−mk||2. (14.33)\\nThis can be minimized by an alternating optimization proced ure given in\\nAlgorithm 14.1.\\nEach of steps 1 and 2 reduces the value of the criterion (14.33 ), so that\\nconvergence is assured. However, the result may represent a suboptimal\\nlocal minimum. The algorithm of Hartigan and Wong (1979) goe s further,\\nand ensures that there is no single switch of an observation f rom one group\\nto another group that will decrease the objective. In additi on, one should\\nstart the algorithm with many diﬀerent random choices for th e starting\\nmeans, and choose the solution having smallest value of the o bjective func-\\ntion.\\nFigure 14.6 shows some of the K-means iterations for the simulated data\\nof Figure 14.4. The centroids are depicted by “O”s. The strai ght lines show\\nthe partitioning of points, each sector being the set of poin ts closest to\\neach centroid. This partitioning is called the Voronoi tessellation . After 20\\niterations the procedure has converged.\\n14.3.7 Gaussian Mixtures as Soft K-means Clustering\\nTheK-means clustering procedure is closely related to the EM alg orithm\\nfor estimating a certain Gaussian mixture model. (Sections 6.8 and 8.5.1).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 528}),\n",
       " Document(page_content='14.3 Cluster Analysis 511\\n-4 -2 0 2 4 6-2 0 2 4 6Initial Centroids\\n••••\\n••••••••\\n••\\n•••••••••\\n••••\\n••\\n••\\n••\\n•\\n•••••\\n•••\\n••\\n•••\\n••\\n••••\\n••••\\n••\\n•••\\n••\\n•••\\n••\\n••• ••\\n•\\n••••••••\\n••••••\\n•\\n••••\\n••••••••\\n•••••\\n••••••••\\n•••\\n••••••\\n•••\\n••\\n•••\\n••\\n•••\\n•••\\n••\\n•••\\n•••••\\n•••\\n•••\\n••••\\n••••••••\\n••\\n•••••••••\\n••••\\n••\\n••\\n••\\n•\\n•••••\\n•••\\n••\\n•••\\n••\\n••••\\n••••\\n••\\n•••\\n••\\n•••\\n••\\n••• ••\\n•\\n••••••••\\n••••••\\n•\\n••••\\n••••••••\\n•••••\\n••••••••\\n•••\\n••••••\\n•••\\n••\\n•••\\n••\\n•••\\n•••\\n••\\n•••\\n•••••\\n•••\\n•••Initial Partition\\n••••\\n••••••••\\n••\\n•••••••••\\n•••\\n••\\n••\\n••\\n•\\n•••••\\n••••\\n••\\n•••\\n••\\n•••\\n••\\n••••\\n••\\n••\\n••\\n••\\n•••\\n••\\n••• ••\\n•\\n••••••••\\n••••••\\n•\\n••••\\n••••••\\n••\\n••••••\\n•••\\n••••\\n••\\n••\\n••••\\n••\\n•••\\n••\\n•••\\n••\\n••\\n•••\\n•••\\n•••••Iteration Number  2\\n•••\\n•••\\n••••\\n••••••••\\n••\\n•••••••••\\n•••\\n••\\n••\\n••\\n•\\n•••••\\n••••\\n••\\n•••\\n••\\n••••\\n••\\n••••\\n•••\\n••\\n••\\n••\\n•••\\n••\\n••• ••\\n•\\n••••••••\\n••••••\\n•\\n••••\\n•••••\\n•••••\\n•••••••••\\n•••\\n•••••\\n•••\\n••\\n•••\\n••\\n•••\\n••••\\n•••\\n•••••Iteration Number  20\\n•••\\n•••\\nFIGURE 14.6. Successive iterations of the K-means clustering algorithm for\\nthe simulated data of Figure 14.4.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 529}),\n",
       " Document(page_content='512 14. Unsupervised Learning\\n• •Responsibilities\\n0.0 0.2 0.4 0.6 0.8 1.0\\n• •Responsibilities\\n0.0 0.2 0.4 0.6 0.8 1.0σ= 1.0 σ= 1.0\\nσ= 0.2 σ= 0.2\\nFIGURE 14.7. (Left panels:) two Gaussian densities g0(x) andg1(x)(blue and\\norange) on the real line, and a single data point (green dot) at x= 0.5. The colored\\nsquares are plotted at x=−1.0andx= 1.0, the means of each density. (Right\\npanels:) the relative densities g0(x)/(g0(x) +g1(x))andg1(x)/(g0(x) +g1(x)),\\ncalled the “responsibilities” of each cluster, for this data po int. In the top panels,\\nthe Gaussian standard deviation σ= 1.0; in the bottom panels σ= 0.2. The\\nEM algorithm uses these responsibilities to make a “soft” assi gnment of each\\ndata point to each of the two clusters. When σis fairly large, the responsibilities\\ncan be near 0.5(they are 0.36and0.64 in the top right panel). As σ→0, the\\nresponsibilities →1, for the cluster center closest to the target point, and 0for\\nall other clusters. This “hard” assignment is seen in the bott om right panel.\\nThe E-step of the EM algorithm assigns “responsibilities” f or each data\\npoint based in its relative density under each mixture compo nent, while\\nthe M-step recomputes the component density parameters bas ed on the\\ncurrent responsibilities. Suppose we specify Kmixture components, each\\nwith a Gaussian density having scalar covariance matrix σ2I. Then the\\nrelative density under each mixture component is a monotone function of\\nthe Euclidean distance between the data point and the mixtur e center.\\nHence in this setup EM is a “soft” version of K-means clustering, making\\nprobabilistic (rather than deterministic) assignments of points to cluster\\ncenters. As the variance σ2→0, these probabilities become 0 and 1, and\\nthe two methods coincide. Details are given in Exercise 14.2 . Figure 14.7\\nillustrates this result for two clusters on the real line.\\n14.3.8 Example: Human Tumor Microarray Data\\nWe applyK-means clustering to the human tumor microarray data de-\\nscribed in Chapter 1. This is an example of high-dimensional clustering.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 530}),\n",
       " Document(page_content='14.3 Cluster Analysis 513\\nNumber of Clusters KSum of Squares\\n2 4 6 8 10160000 200000 240000•\\n•\\n•\\n•••••••\\nFIGURE 14.8. Total within-cluster sum of squares for K-means clustering ap-\\nplied to the human tumor microarray data.\\nTABLE 14.2. Human tumor data: number of cancer cases of each type, in each\\nof the three clusters from K-means clustering.\\nCluster Breast CNS Colon K562 Leukemia MCF7\\n1 3 5 0 0 0 0\\n2 2 0 0 2 6 2\\n3 2 0 7 0 0 0\\nCluster Melanoma NSCLC Ovarian Prostate Renal Unknown\\n1 1 7 6 2 9 1\\n2 7 2 0 0 0 0\\n3 0 0 0 0 0 0\\nThe data are a 6830 ×64 matrix of real numbers, each representing an\\nexpression measurement for a gene (row) and sample (column) . Here we\\ncluster the samples, each of which is a vector of length 6830, correspond-\\ning to expression values for the 6830 genes. Each sample has a label such\\nasbreast(for breast cancer), melanoma , and so on; we don’t use these la-\\nbels in the clustering, but will examine posthocwhich labels fall into which\\nclusters.\\nWe applied K-means clustering with Krunning from 1 to 10, and com-\\nputed the total within-sum of squares for each clustering, s hown in Fig-\\nure 14.8. Typically one looks for a kink in the sum of squares c urve (or its\\nlogarithm) to locate the optimal number of clusters (see Sec tion 14.3.11).\\nHere there is no clear indication: for illustration we chose K= 3 giving the\\nthree clusters shown in Table 14.2.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 531}),\n",
       " Document(page_content='514 14. Unsupervised Learning\\nFIGURE 14.9. Sir Ronald A. Fisher ( 1890−1962) was one of the founders\\nof modern day statistics, to whom we owe maximum-likelihood, s uﬃciency, and\\nmany other fundamental concepts. The image on the left is a 1024×1024grayscale\\nimage at 8bits per pixel. The center image is the result of 2×2block VQ, using\\n200code vectors, with a compression rate of 1.9bits/pixel. The right image uses\\nonly four code vectors, with a compression rate of 0.50bits/pixel\\nWe see that the procedure is successful at grouping together samples of\\nthe same cancer. In fact, the two breast cancers in the second cluster were\\nlater found to be misdiagnosed and were melanomas that had me tastasized.\\nHowever,K-meansclusteringhasshortcomingsinthisapplication. Fo rone,\\nit does not give a linear ordering of objects within a cluster : we have simply\\nlisted them in alphabetic order above. Secondly, as the numb er of clusters\\nKis changed, the cluster memberships can change in arbitrary ways. That\\nis, with say four clusters, the clusters need not be nested wi thin the three\\nclusters above. For these reasons, hierarchical clusterin g (described later),\\nis probably preferable for this application.\\n14.3.9 Vector Quantization\\nTheK-means clustering algorithm represents a key tool in the app arently\\nunrelatedareaofimageandsignalcompression,particular lyinvector quan-\\ntizationor VQ (Gersho and Gray, 1992). The left image in Figure 14.92is a\\ndigitized photograph of a famous statistician, Sir Ronald F isher. It consists\\nof 1024×1024 pixels, where each pixel is a grayscale value ranging fr om 0\\nto 255, and hence requires 8 bits of storage per pixel. The ent ire image oc-\\ncupies 1 megabyte of storage. The center image is a VQ-compre ssed version\\nof the left panel, and requires 0 .239 of the storage (at some loss in quality).\\nThe right image is compressed even more, and requires only 0 .0625 of the\\nstorage (at a considerable loss in quality).\\nThe version of VQ implemented here ﬁrst breaks the image into small\\nblocks, in this case 2 ×2 blocks of pixels. Each of the 512 ×512 blocks of four\\n2This example was prepared by Maya Gupta.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 532}),\n",
       " Document(page_content='14.3 Cluster Analysis 515\\nnumbers is regarded as a vector in IR4. AK-means clustering algorithm\\n(also known as Lloyd’s algorithm in this context) is run in th is space.\\nThe center image uses K= 200, while the right image K= 4. Each of\\nthe 512×512 pixel blocks (or points) is approximated by its closest c luster\\ncentroid,knownasacodeword.Theclusteringprocessiscal ledtheencoding\\nstep, and the collection of centroids is called the codebook.\\nTo represent the approximated image, we need to supply for ea ch block\\nthe identity of the codebook entry that approximates it. Thi s will require\\nlog2(K) bits per block. We also need to supply the codebook itself, w hich\\nisK×4 real numbers (typically negligible). Overall, the storag e for the\\ncompressed image amounts to log2(K)/(4·8) of the original (0 .239 for\\nK= 200, 0.063 forK= 4). This is typically expressed as a ratein bits\\nper pixel: log2(K)/4, which are 1 .91 and 0.50, respectively. The process\\nof constructing the approximate image from the centroids is called the\\ndecoding step.\\nWhy do we expect VQ to work at all? The reason is that for typica l\\neveryday images like photographs, many of the blocks look th e same. In\\nthis case there are many almost pure white blocks, and simila rly pure gray\\nblocks of various shades. These require only one block each t o represent\\nthem, and then multiple pointers to that block.\\nWhat we have described is known as lossycompression, since our im-\\nages are degraded versions of the original. The degradation ordistortion is\\nusually measured in terms of mean squared error. In this case D= 0.89\\nforK= 200 and D= 16.95 forK= 4. More generally a rate/distortion\\ncurve would be used to assess the tradeoﬀ. One can also perfor mlossless\\ncompression using block clustering, and still capitalize o n the repeated pat-\\nterns. If you took the original image and losslessly compres sed it, the best\\nyou would do is 4.48 bits per pixel.\\nWeclaimedabovethatlog2(K)bitswereneededtoidentifyeachofthe K\\ncodewords in the codebook. This uses a ﬁxed-length code, and is ineﬃcient\\nif some codewords occur many more times than others in the ima ge. Using\\nShannon coding theory, we know that in general a variable len gth code\\nwill do better, and the rate then becomes −∑K\\nℓ=1pℓlog2(pℓ)/4. The term\\nin the numerator is the entropy of the distribution pℓof the codewords\\nin the image. Using variable length coding our rates come dow n to 1.42\\nand 0.39, respectively. Finally, there are many generalizations of VQ that\\nhave been developed: for example, tree-structured VQ ﬁnds t he centroids\\nwith a top-down, 2-means style algorithm, as alluded to in Se ction 14.3.12.\\nThis allows successive reﬁnement of the compression. Furth er details may\\nbe found in Gersho and Gray (1992).\\n14.3.10 K-medoids\\nAs discussed above, the K-means algorithm is appropriate when the dis-\\nsimilarity measure is taken to be squared Euclidean distanc eD(xi,xi′)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 533}),\n",
       " Document(page_content='516 14. Unsupervised Learning\\nAlgorithm 14.2 K-medoids Clustering.\\n1. For a given cluster assignment Cﬁnd the observation in the cluster\\nminimizing total distance to other points in that cluster:\\ni∗\\nk= argmin\\n{i:C(i)=k}∑\\nC(i′)=kD(xi,xi′). (14.35)\\nThenmk=xi∗\\nk, k= 1,2,...,Kare the current estimates of the\\ncluster centers.\\n2. Given a current set of cluster centers {m1,...,m K}, minimize the to-\\ntal error by assigning each observation to the closest (curr ent) cluster\\ncenter:\\nC(i) = argmin\\n1≤k≤KD(xi,mk). (14.36)\\n3. Iterate steps 1 and 2 until the assignments do not change.\\n(14.112). This requires all of the variables to be of the quan titative type. In\\naddition, using squaredEuclidean distance places the highest inﬂuence on\\nthe largest distances. This causes the procedure to lack rob ustness against\\noutliers that produce very large distances. These restrict ions can be re-\\nmoved at the expense of computation.\\nThe only part of the K-means algorithm that assumes squared Eu-\\nclideandistanceistheminimizationstep(14.32);theclus terrepresentatives\\n{m1,...,m K}in(14.33)aretakentobethemeansofthecurrentlyassigned\\nclusters. The algorithm can be generalized for use with arbi trarily deﬁned\\ndissimilarities D(xi,xi′) by replacing this step by an explicit optimization\\nwith respect to{m1,...,m K}in (14.33). In the most common form, cen-\\nters for each cluster are restricted to be one of the observat ions assigned\\nto the cluster, as summarized in Algorithm 14.2. This algori thm assumes\\nattribute data, but the approach can also be applied to data d escribed\\nonlyby proximity matrices (Section 14.3.1). There is no need to e xplicitly\\ncompute cluster centers; rather we just keep track of the ind icesi∗\\nk.\\nSolving (14.32) for each provisional cluster krequires an amount of com-\\nputation proportional to the number of observations assign ed to it, whereas\\nfor solving (14.35) the computation increases to O(N2\\nk). Given a set of clus-\\nter “centers,”{i1,...,iK}, obtaining the new assignments\\nC(i) = argmin\\n1≤k≤Kdii∗\\nk(14.37)\\nrequires computation proportional to K·Nas before. Thus, K-medoids is\\nfar more computationally intensive than K-means.\\nAlternating between (14.35) and (14.37) represents a parti cular heuristic\\nsearch strategy for trying to solve', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 534}),\n",
       " Document(page_content='14.3 Cluster Analysis 517\\nTABLE 14.3. Data from a political science survey: values are average pairw ise\\ndissimilarities of countries from a questionnaire given to p olitical science students.\\nBEL BRA CHI CUB EGY FRA IND ISR USA USS YUG\\nBRA5.58\\nCHI7.00 6.50\\nCUB7.08 7.00 3.83\\nEGY4.83 5.08 8.17 5.83\\nFRA2.17 5.75 6.67 6.92 4.92\\nIND6.42 5.00 5.58 6.00 4.67 6.42\\nISR3.42 5.50 6.42 6.42 5.00 3.92 6.17\\nUSA2.50 4.92 6.25 7.33 4.50 2.25 6.33 2.75\\nUSS6.08 6.67 4.25 2.67 6.00 6.17 6.17 6.92 6.17\\nYUG5.25 6.83 4.50 3.75 5.75 5.42 6.08 5.83 6.67 3.67\\nZAI4.75 3.00 6.08 6.67 5.00 5.58 4.83 6.17 5.67 6.50 6.92\\nmin\\nC,{ik}K\\n1K∑\\nk=1∑\\nC(i)=kdiik. (14.38)\\nKaufmanandRousseeuw(1990)proposeanalternativestrate gyfordirectly\\nsolving (14.38) that provisionally exchanges each center ikwith an obser-\\nvation that is not currently a center, selecting the exchang e that produces\\nthe greatest reduction in the value of the criterion (14.38) . This is repeated\\nuntil no advantageous exchanges can be found. Massart et al. (1983) derive\\na branch-and-bound combinatorial method that ﬁnds the glob al minimum\\nof (14.38) that is practical only for very small data sets.\\nExample: Country Dissimilarities\\nThis example, taken from Kaufman and Rousseeuw (1990), come s from a\\nstudy in which political science students were asked to prov ide pairwise dis-\\nsimilarity measures for 12 countries: Belgium, Brazil, Chi le, Cuba, Egypt,\\nFrance, India, Israel, United States, Union of Soviet Socia list Republics,\\nYugoslavia and Zaire. The average dissimilarity scores are given in Ta-\\nble 14.3. We applied 3-medoid clustering to these dissimila rities. Note that\\nK-means clustering could not be applied because we have only d istances\\nrather than raw observations. The left panel of Figure 14.10 shows the\\ndissimilarities reordered and blocked according to the 3-m edoid clustering.\\nThe right panel is a two-dimensional multidimensional scal ing plot, with\\nthe 3-medoid clusters assignments indicated by colors (mul tidimensional\\nscaling is discussed in Section 14.8.) Both plots show three well-separated\\nclusters, but the MDS display indicates that “Egypt” falls a bout halfway\\nbetween two clusters.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 535}),\n",
       " Document(page_content='518 14. Unsupervised Learning\\nCHICUBUSSYUGBRAINDZAIBELEGYFRAISR\\nCUBUSSYUGBRAINDZAIBELEGYFRAISRUSA\\nReordered Dissimilarity Matrix First MDS CoordinateSecond MDS Coordinate\\n-2 0 2 4-2 -1 0 1 2 3CHI\\nCUB\\nUSS\\nYUGBRAIND ZAI\\nBELEGY\\nFRAISRUSA\\nFIGURE 14.10. Survey of country dissimilarities. (Left panel:) dissimilari ties\\nreordered and blocked according to 3-medoid clustering. Heat map is coded from\\nmost similar (dark red) to least similar (bright red). (Right pa nel:) two-dimen-\\nsional multidimensional scaling plot, with 3-medoid clusters indicated by diﬀerent\\ncolors.\\n14.3.11 Practical Issues\\nIn order to apply K-means orK-medoids one must select the number of\\nclustersK∗and an initialization. The latter can be deﬁned by specifyin g\\nan initial set of centers {m1,...,m K}or{i1,...,iK}or an initial encoder\\nC(i). Usually specifying the centers is more convenient. Sugge stions range\\nfrom simple random selection to a deliberate strategy based on forward\\nstepwise assignment. At each step a new center ikis chosen to minimize\\nthe criterion (14.33) or (14.38), given the centers i1,...,ik−1chosen at the\\nprevious steps. This continues for Ksteps, thereby producing Kinitial\\ncenters with which to begin the optimization algorithm.\\nA choice for the number of clusters Kdepends on the goal. For data\\nsegmentation Kis usually deﬁned as part of the problem. For example,\\na company may employ Ksales people, and the goal is to partition a\\ncustomerdatabaseinto Ksegments,oneforeachsalesperson,suchthatthe\\ncustomers assigned to each one are as similar as possible. Of ten, however,\\ncluster analysis is used to provide a descriptive statistic for ascertaining the\\nextent to which the observations comprising the data base fa ll into natural\\ndistinct groupings. Here the number of such groups K∗is unknown and\\none requires that it, as well as the groupings themselves, be estimated from\\nthe data.\\nData-based methods for estimating K∗typically examine the within-\\nclusterdissimilarity WKasafunctionofthenumberofclusters K.Separate\\nsolutions are obtained for K∈{1,2,...,K max}. The corresponding values', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 536}),\n",
       " Document(page_content='14.3 Cluster Analysis 519\\n{W1,W2,...,W Kmax}generally decrease with increasing K. This will be\\nthe case even when the criterion is evaluated on an independe nt test set,\\nsince a large number of cluster centers will tend to ﬁll the fe ature space\\ndensely and thus will be close to all data points. Thus cross- validation\\ntechniques, so useful for model selection in supervised lea rning, cannot be\\nutilized in this context.\\nThe intuition underlying the approach is that if there are ac tuallyK∗\\ndistinct groupings of the observations (as deﬁned by the dis similarity mea-\\nsure), then for K < K∗the clusters returned by the algorithm will each\\ncontain a subset of the true underlying groups. That is, the s olution will\\nnot assign observations in the same naturally occurring gro up to diﬀerent\\nestimated clusters. To the extent that this is the case, the s olution criterion\\nvalue will tend todecreasesubstantially with each success iveincrease inthe\\nnumber of speciﬁed clusters, WK+1≪WK, as the natural groups are suc-\\ncessively assigned to separate clusters. For K >K∗, one of the estimated\\nclusters must partition at least one of the natural groups in to two sub-\\ngroups. This will tend to provide a smaller decrease in the cr iterion asKis\\nfurther increased. Splitting a natural group, within which the observations\\nare all quite close to each other, reduces the criterion less than partitioning\\nthe union of two well-separated groups into their proper con stituents.\\nTo the extent this scenario is realized, there will be a sharp decrease in\\nsuccessive diﬀerences in criterion value, WK−WK+1, atK=K∗. That\\nis,{WK−WK+1|K < K∗}≫{WK−WK+1|K≥K∗}. An estimate\\nˆK∗forK∗is then obtained by identifying a “kink” in the plot of WKas a\\nfunctionofK.Aswithotheraspectsofclusteringprocedures,thisappro ach\\nis somewhat heuristic.\\nThe recently proposed Gap statistic (Tibshirani et al., 2001b) compares\\nthe curve log WKto the curve obtained from data uniformly distributed\\nover a rectangle containing the data. It estimates the optim al number of\\nclusters to be the place where the gap between the two curves i s largest.\\nEssentially this is an automatic way of locating the aforeme ntioned “kink.”\\nIt also works reasonably well when the data fall into a single cluster, and\\nin that case will tend to estimate the optimal number of clust ers to be one.\\nThis is the scenario where most other competing methods fail .\\nFigure 14.11 shows the result of the Gap statistic applied to simulated\\ndataofFigure14.4.Theleftpanelshowslog WKforK= 1,2,...,8clusters\\n(green curve) and the expected value of log WKover 20 simulations from\\nuniformdata(bluecurve).Therightpanelshowsthegapcurv e,whichisthe\\nexpected curve minus the observed curve. Shown also are erro r bars of half-\\nwidths′\\nK=sK√\\n1+1/20, wheresKis the standard deviation of log WK\\nover the 20 simulations. The Gap curve is maximized at K= 2 clusters. If\\nG(K) is the Gap curve at Kclusters, the formal rule for estimating K∗is\\nK∗= argmin\\nK{K|G(K)≥G(K+1)−s′\\nK+1}.(14.39)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 537}),\n",
       " Document(page_content='520 14. Unsupervised Learning\\nNumber of Clusters2 4 6 8-3.0 -2.5 -2.0 -1.5 -1.0 -0.5 0.0•\\n••\\n••\\n••••\\n•\\n•\\n•\\n•\\n•••\\nNumber of ClustersGap\\n2 4 6 8-0.5 0.0 0.5 1.0••\\n•••••\\n•logWK−logW1\\nFIGURE 14.11. (Left panel): observed (green) and expected (blue) values of\\nlogWKfor the simulated data of Figure 14.4. Both curves have been tr anslated\\nto equal zero at one cluster. (Right panel): Gap curve, equal to t he diﬀerence\\nbetween the observed and expected values of logWK. The Gap estimate K∗is the\\nsmallestKproducing a gap within one standard deviation of the gap at K+ 1;\\nhereK∗= 2.\\nThis givesK∗= 2, which looks reasonable from Figure 14.4.\\n14.3.12 Hierarchical Clustering\\nThe results of applying K-means orK-medoids clustering algorithms de-\\npend on the choice for the number of clusters to be searched an d a starting\\nconﬁguration assignment. In contrast, hierarchical clust ering methods do\\nnot require such speciﬁcations. Instead, they require the u ser to specify a\\nmeasure of dissimilarity between (disjoint) groupsof observations, based\\non the pairwise dissimilarities among the observations in t he two groups.\\nAs the name suggests, they produce hierarchical representa tions in which\\nthe clusters at each level of the hierarchy are created by mer ging clusters\\nat the next lower level. At the lowest level, each cluster con tains a single\\nobservation. At the highest level there is only one cluster c ontaining all of\\nthe data.\\nStrategies for hierarchical clustering divide into two bas ic paradigms: ag-\\nglomerative (bottom-up) and divisive(top-down). Agglomerative strategies\\nstart at the bottom and at each level recursively merge a sele cted pair of\\nclusters into a single cluster. This produces a grouping at t he next higher\\nlevel with one less cluster. The pair chosen for merging cons ist of the two\\ngroups with the smallest intergroup dissimilarity. Divisi ve methods start\\nat the top and at each level recursively split one of the exist ing clusters at', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 538}),\n",
       " Document(page_content='14.3 Cluster Analysis 521\\nthat level into two new clusters. The split is chosen to produ ce two new\\ngroups with the largest between-group dissimilarity. With both paradigms\\nthere areN−1 levels in the hierarchy.\\nEach level of the hierarchy represents a particular groupin g of the data\\ninto disjoint clusters of observations. The entire hierarc hy represents an\\nordered sequence of such groupings. It is up to the user to dec ide which\\nlevel (if any) actually represents a “natural” clustering i n the sense that\\nobservations within each of its groups are suﬃciently more s imilar to each\\nother than to observations assigned to diﬀerent groups at th at level. The\\nGap statistic described earlier can be used for this purpose .\\nRecursive binary splitting/agglomeration can be represen ted by a rooted\\nbinary tree. The nodes of the trees represent groups. The roo t node repre-\\nsents the entire data set. The Nterminal nodes each represent one of the\\nindividual observations (singleton clusters). Each nonte rminal node (“par-\\nent”) has two daughter nodes. For divisive clustering the tw o daughters\\nrepresent the two groups resulting from the split of the pare nt; for agglom-\\nerative clustering the daughters represent the two groups t hat were merged\\nto form the parent.\\nMost agglomerative and some divisive methods (when viewed b ottom-\\nup) possess a monotonicity property. That is, the dissimila rity between\\nmerged clusters is monotone increasing with the level of the merger. Thus\\nthebinarytreecanbeplottedsothattheheightofeachnodei sproportional\\nto the value of the intergroup dissimilarity between its two daughters. The\\nterminal nodes representing individual observations are a ll plotted at zero\\nheight. This type of graphical display is called a dendrogram .\\nA dendrogram provides a highly interpretable complete desc ription of\\nthe hierarchical clustering in a graphical format. This is o ne of the main\\nreasons for the popularity of hierarchical clustering meth ods.\\nFor the microarray data, Figure 14.12 shows the dendrogram r esulting\\nfrom agglomerative clustering with average linkage; agglo merative cluster-\\ning and this example are discussed in more detail later in thi s chapter.\\nCutting the dendrogram horizontally at a particular height partitions the\\ndata into disjoint clusters represented by the vertical lin es that intersect\\nit. These are the clusters that would be produced by terminat ing the pro-\\ncedure when the optimal intergroup dissimilarity exceeds t hat threshold\\ncut value. Groups that merge at high values, relative to the m erger values\\nof the subgroups contained within them lower in the tree, are candidates\\nfor natural clusters. Note that this may occur at several diﬀ erent levels,\\nindicating a clustering hierarchy: that is, clusters neste d within clusters.\\nSuch a dendrogram is often viewed as a graphical summary of th e data\\nitself, rather than a description of the results of the algor ithm. However,\\nsuch interpretations should be treated with caution. First , diﬀerent hierar-\\nchical methods (see below), as well as small changes in the da ta, can lead\\nto quite diﬀerent dendrograms. Also, such a summary will be v alid only to\\nthe extent that the pairwise observation dissimilarities possess the hierar-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 539}),\n",
       " Document(page_content='522 14. Unsupervised Learning\\nCNSCNSCNSRENAL\\nBREASTCNSCNS\\nBREASTNSCLC\\nNSCLCRENAL\\nRENALRENALRENAL\\nRENALRENALRENALBREAST\\nNSCLCRENAL\\nUNKNOWN\\nOVARIAN\\nMELANOMA\\nPROSTATEOVARIANOVARIAN\\nOVARIANOVARIAN\\nOVARIAN\\nPROSTATENSCLCNSCLCNSCLCLEUKEMIAK562B-reproK562A-reproLEUKEMIA\\nLEUKEMIALEUKEMIALEUKEMIALEUKEMIA\\nCOLONCOLON\\nCOLON\\nCOLONCOLONCOLON\\nCOLONMCF7A-repro\\nBREAST\\nMCF7D-reproBREASTNSCLC\\nNSCLCNSCLCMELANOMA\\nBREASTBREAST\\nMELANOMA\\nMELANOMA\\nMELANOMAMELANOMAMELANOMA\\nMELANOMA\\nFIGURE 14.12. Dendrogram from agglomerative hierarchical clustering with\\naverage linkage to the human tumor microarray data.\\nchical structure produced by the algorithm. Hierarchical m ethods impose\\nhierarchical structure whether or not such structure actua lly exists in the\\ndata.\\nThe extent to which the hierarchical structure produced by a dendro-\\ngram actually represents the data itself can be judged by the cophenetic\\ncorrelation coeﬃcient . This is the correlation between the N(N−1)/2 pair-\\nwise observation dissimilarities dii′input to the algorithm and their corre-\\nsponding cophenetic dissimilarities Cii′derived from the dendrogram. The\\ncophenetic dissimilarity Cii′between two observations ( i,i′) is the inter-\\ngroup dissimilarity at which observations iandi′are ﬁrst joined together\\nin the same cluster.\\nThe cophenetic dissimilarity is a very restrictive dissimi larity measure.\\nFirst,theCii′overtheobservationsmustcontainmanyties,sinceonly N−1\\nof the total N(N−1)/2 values can be distinct. Also these dissimilarities\\nobey the ultrametric inequality\\nCii′≤max{Cik,Ci′k} (14.40)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 540}),\n",
       " Document(page_content='14.3 Cluster Analysis 523\\nfor any three observations ( i,i′,k). As a geometric example, suppose the\\ndata were represented as points in a Euclidean coordinate sy stem. In order\\nfor the set of interpoint distances over the data to conform t o (14.40), the\\ntriangles formed by all triples of points must be isosceles t riangles with the\\nunequal length no longer than the length of the two equal side s (Jain and\\nDubes, 1988). Therefore it is unrealistic to expect general dissimilarities\\nover arbitrary data sets to closely resemble their correspo nding cophenetic\\ndissimilarities as calculated from a dendrogram, especial ly if there are not\\nmany tied values. Thus the dendrogram should be viewed mainl y as a de-\\nscriptionofthe clustering structureofthedataasimposedbytheparticular\\nalgorithm employed.\\nAgglomerative Clustering\\nAgglomerative clustering algorithms begin with every obse rvation repre-\\nsenting a singleton cluster. At each of the N−1 steps the closest two (least\\ndissimilar) clusters are merged into a single cluster, prod ucing one less clus-\\nter at the next higher level. Therefore, a measure of dissimi larity between\\ntwo clusters (groups of observations) must be deﬁned.\\nLetGandHrepresent two such groups. The dissimilarity d(G,H) be-\\ntweenGandHis computed from the set of pairwise observation dissim-\\nilaritiesdii′where one member of the pair iis inGand the other i′is\\ninH.Single linkage (SL) agglomerative clustering takes the intergroup\\ndissimilarity to be that of the closest (least dissimilar) p air\\ndSL(G,H) = min\\ni∈G\\ni′∈Hdii′. (14.41)\\nThis is also often called the nearest-neighbor technique. Complete linkage\\n(CL) agglomerative clustering ( furthest-neighbor technique) takes the in-\\ntergroup dissimilarity to be that of the furthest (most diss imilar) pair\\ndCL(G,H) = max\\ni∈G\\ni′∈Hdii′. (14.42)\\nGroup average (GA) clustering uses the average dissimilarity between the\\ngroups\\ndGA(G,H) =1\\nNGNH∑\\ni∈G∑\\ni′∈Hdii′ (14.43)\\nwhereNGandNHare the respective number of observations in each group.\\nAlthough there have been many other proposals for deﬁning in tergroup\\ndissimilarity in the context of agglomerative clustering, the above three are\\nthe ones most commonly used. Figure 14.13 shows examples of a ll three.\\nIfthedatadissimilarities {dii′}exhibit astrongclusteringtendency,with\\neach of the clusters being compact and well separated from ot hers, then all\\nthree methods produce similar results. Clusters are compac t if all of the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 541}),\n",
       " Document(page_content='524 14. Unsupervised Learning\\nAverage Linkage Complete Linkage Single Linkage\\nFIGURE 14.13. Dendrograms from agglomerative hierarchical clustering of h u-\\nman tumor microarray data.\\nobservationswithinthemarerelativelyclosetogether(sm alldissimilarities)\\nas compared with observations in diﬀerent clusters. To the e xtent this is\\nnot the case, results will diﬀer.\\nSingle linkage (14.41) only requires that a single dissimil aritydii′,i∈G\\nandi′∈H, be small for two groups GandHto be considered close\\ntogether, irrespective of the other observation dissimila rities between the\\ngroups. It will therefore have a tendency to combine, at rela tively low\\nthresholds, observations linked by a series of close interm ediate observa-\\ntions. This phenomenon, referred to as chaining, is often considered a de-\\nfect of the method. The clusters produced by single linkage c an violate the\\n“compactness” property that all observations within each c luster tend to\\nbe similar to one another, based on the supplied observation dissimilari-\\nties{dii′}. If we deﬁne the diameterDGof a group of observations as the\\nlargest dissimilarity among its members\\nDG= max\\ni∈G\\ni′∈Gdii′, (14.44)\\nthen single linkage can produce clusters with very large dia meters.\\nComplete linkage (14.42) represents the opposite extreme. Two groups\\nGandHare considered close only if all of the observations in their union\\nare relatively similar. It will tend to produce compact clus ters with small\\ndiameters (14.44). However, it can produce clusters that vi olate the “close-\\nness” property. That is, observations assigned to a cluster can be much', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 542}),\n",
       " Document(page_content='14.3 Cluster Analysis 525\\ncloser to members of other clusters than they are to some memb ers of their\\nown cluster.\\nGroup average clustering (14.43) represents a compromise b etween the\\ntwo extremes of single and complete linkage. It attempts to p roducerel-\\nativelycompact clusters that are relatively far apart. However, its results\\ndepend on the numerical scale on which the observation dissi milaritiesdii′\\nare measured. Applying a monotone strictly increasing tran sformation h(·)\\nto thedii′,hii′=h(dii′), can change the result produced by (14.43). In\\ncontrast, (14.41) and (14.42) depend only on the ordering of thedii′and\\nare thus invariant to such monotone transformations. This i nvariance is\\noften used as an argument in favor of single or complete linka ge over group\\naverage methods.\\nOne can argue that group average clustering has a statistica l consis-\\ntency property violated by single and complete linkage. Ass ume we have\\nattribute-value data XT= (X1,...,X p) and that each cluster kis a ran-\\ndom sample from some population joint density pk(x). The complete data\\nset is a random sample from a mixture of Ksuch densities. The group\\naverage dissimilarity dGA(G,H) (14.43) is an estimate of\\n∫ ∫\\nd(x,x′)pG(x)pH(x′)dxdx′, (14.45)\\nwhered(x,x′) is the dissimilarity between points xandx′in the space\\nof attribute values. As the sample size Napproaches inﬁnity dGA(G,H)\\n(14.43) approaches (14.45), which is a characteristic of th e relationship\\nbetween the two densities pG(x) andpH(x). For single linkage, dSL(G,H)\\n(14.41) approaches zero as N→∞independent of pG(x) andpH(x). For\\ncomplete linkage, dCL(G,H) (14.42) becomes inﬁnite as N→∞, again\\nindependent of the two densities. Thus, it is not clear what a spects of the\\npopulation distribution are being estimated by dSL(G,H) anddCL(G,H).\\nExample: Human Cancer Microarray Data (Continued)\\nThe left panel of Figure 14.13 shows the dendrogram resultin g from average\\nlinkageagglomerative clusteringofthesamples(columns) ofthemicroarray\\ndata.Themiddleandrightpanelsshowtheresultusingcompl eteandsingle\\nlinkage. Average and complete linkage gave similar results , while single\\nlinkage produced unbalanced groups with long thin clusters . We focus on\\nthe average linkage clustering.\\nLikeK-meansclustering,hierarchicalclusteringissuccessful atclustering\\nsimple cancers together. However it has other nice features . By cutting oﬀ\\nthe dendrogram at various heights, diﬀerent numbers of clus ters emerge,\\nand the sets of clusters are nested within one another. Secon dly, it gives\\nsome partial ordering information about the samples. In Fig ure 14.14, we\\nhave arranged the genes (rows) and samples (columns) of the e xpression\\nmatrix in orderings derived from hierarchical clustering.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 543}),\n",
       " Document(page_content='526 14. Unsupervised Learning\\nNotethatifweﬂiptheorientationofthebranchesofadendro gramatany\\nmerge, the resulting dendrogram is still consistent with th e series of hierar-\\nchical clustering operations. Hence to determine an orderi ng of the leaves,\\nwe must add a constraint. To produce the row ordering of Figur e 14.14,\\nwe have used the default rule in S-PLUS: at each merge, the sub tree with\\nthe tighter cluster is placed to the left (toward the bottom i n the rotated\\ndendrogram in the ﬁgure.) Individual genes are the tightest clusters possi-\\nble, and merges involving two individual genes place them in order by their\\nobservation number. The same rule was used for the columns. M any other\\nrules are possible—for example, ordering by a multidimensio nal scaling of\\nthe genes; see Section 14.8.\\nThe two-way rearrangement of Figure 14.14 produces an infor mative pic-\\nture of the genes and samples. This picture is more informati ve than the\\nrandomly ordered rows and columns of Figure 1.3 of Chapter 1. Further-\\nmore,thedendrogramsthemselvesareuseful,asbiologists can,forexample,\\ninterpret the gene clusters in terms of biological processe s.\\nDivisive Clustering\\nDivisive clustering algorithms begin with the entire data s et as a single\\ncluster, and recursively divide one of the existing cluster s into two daugh-\\nter clusters at each iteration in a top-down fashion. This ap proach has not\\nbeen studied nearly as extensively as agglomerative method s in the cluster-\\ning literature. It has been explored somewhat in the enginee ring literature\\n(Gersho and Gray, 1992) in the context of compression. In the clustering\\nsetting, a potential advantage of divisive over agglomerat ive methods can\\noccur when interest is focused on partitioning the data into a relatively\\nsmallnumber of clusters.\\nThe divisive paradigm can be employed by recursively applyi ng any of\\nthe combinatorial methods such as K-means (Section 14.3.6) or K-medoids\\n(Section 14.3.10), with K= 2, to perform the splits at each iteration. How-\\never,suchanapproachwoulddependonthestartingconﬁgura tionspeciﬁed\\nat each step. In addition, it would not necessarily produce a splitting se-\\nquence that possesses the monotonicity property required f or dendrogram\\nrepresentation.\\nA divisive algorithm that avoids these problems was propose d by Mac-\\nnaughton Smith et al. (1965). It begins by placing all observ ations in a\\nsingle cluster G. It then chooses that observation whose average dissimi-\\nlarity from all the other observations is largest. This obse rvation forms the\\nﬁrstmemberofasecondcluster H.Ateachsuccessivestepthatobservation\\ninGwhose average distance from those in H, minus that for the remaining\\nobservations in Gis largest, is transferred to H. This continues until the\\ncorresponding diﬀerence in averages becomes negative. Tha t is, there are\\nno longer any observations in Gthat are, on average, closer to those in\\nH. The result is a split of the original cluster into two daught er clusters,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 544}),\n",
       " Document(page_content='14.3 Cluster Analysis 527\\nFIGURE 14.14. DNA microarray data: average linkage hierarchical clusterin g\\nhas been applied independently to the rows (genes) and columns ( samples), de-\\ntermining the ordering of the rows and columns (see text). The colors range from\\nbright green (negative, under-expressed) to bright red (po sitive, over-expressed).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 545}),\n",
       " Document(page_content='528 14. Unsupervised Learning\\nthe observations transferred to H, and those remaining in G. These two\\nclusters represent the second level of the hierarchy. Each s uccessive level\\nis produced by applying this splitting procedure to one of th e clusters at\\nthe previous level. Kaufman and Rousseeuw (1990) suggest ch oosing the\\ncluster at each level with the largest diameter (14.44) for s plitting. An al-\\nternative would be to choose the one with the largest average dissimilarity\\namong its members\\n¯dG=1\\nNG2∑\\ni∈G∑\\ni′∈Gdii′.\\nThe recursive splitting continues until all clusters eithe r become singletons\\nor all members of each one have zero dissimilarity from one an other.\\n14.4 Self-Organizing Maps\\nThis method can be viewed as a constrained version of K-means clustering,\\nin which the prototypes are encouraged to lie in a one- or two- dimensional\\nmanifold in the feature space. The resulting manifold is als o referred to\\nas aconstrained topological map , since the original high-dimensional obser-\\nvations can be mapped down onto the two-dimensional coordin ate system.\\nThe original SOM algorithm was online—observations are proc essed one at\\na time—and later a batch version was proposed. The technique a lso bears\\na close relationship to principal curves and surfaces , which are discussed in\\nthe next section.\\nWe consider a SOM with a two-dimensional rectangular grid of Kproto-\\ntypesmj∈IRp(other choices, such as hexagonal grids, can also be used).\\nEach of the Kprototypes are parametrized with respect to an integer\\ncoordinate pair ℓj∈Q1×Q2. HereQ1={1,2,...,q 1}, similarlyQ2, and\\nK=q1·q2.Themjareinitialized,forexample,tolieinthetwo-dimensional\\nprincipal component plane of the data (next section). We can think of the\\nprototypes as “buttons,” “sewn” on the principal component plane in a\\nregular pattern. The SOM procedure tries to bend the plane so that the\\nbuttons approximate the data points as well as possible. Onc e the model is\\nﬁt, the observations can be mapped down onto the two-dimensi onal grid.\\nThe observations xiare processed one at a time. We ﬁnd the closest\\nprototypemjtoxiin Euclidean distance in IRp, and then for all neighbors\\nmkofmj, movemktowardxivia the update\\nmk←mk+α(xi−mk). (14.46)\\nThe “neighbors” of mjare deﬁned to be all mksuch that the distance\\nbetweenℓjandℓkis small. The simplest approach uses Euclidean distance,\\nand “small” is determined by a threshold r. This neighborhood always\\nincludes the closest prototype mjitself.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 546}),\n",
       " Document(page_content='14.4 Self-Organizing Maps 529\\nNotice that distance is deﬁned in the space Q1×Q2of integer topological\\ncoordinates of the prototypes, rather than in the feature sp ace IRp. The\\neﬀect of the update (14.46) is to move the prototypes closer t o the data,\\nbutalsotomaintainasmoothtwo-dimensionalspatialrelat ionshipbetween\\nthe prototypes.\\nThe performance of the SOM algorithm depends on the learning rate\\nαand the distance threshold r. Typically αis decreased from say 1 .0 to\\n0.0 over a few thousand iterations (one per observation). Simi larlyris\\ndecreasedlinearlyfromstartingvalue Rto1overafewthousanditerations.\\nWe illustrate a method for choosing Rin the example below.\\nWe have described the simplest version of the SOM. More sophi sticated\\nversions modify the update step according to distance:\\nmk←mk+αh(∥ℓj−ℓk∥)(xi−mk), (14.47)\\nwherethe neighborhood function hgivesmoreweighttoprototypes mkwith\\nindicesℓkcloser toℓjthan to those further away.\\nIfwetakethedistance rsmallenoughsothateachneighborhoodcontains\\nonly one point, then the spatial connection between prototy pes is lost. In\\nthat case one can show that the SOM algorithm is an online vers ion of\\nK-means clustering, and eventually stabilizes at one of the l ocal minima\\nfound byK-means. Since the SOM is a constrained version of K-means\\nclustering, it is important to check whether the constraint is reasonable\\nin any given problem. One can do this by computing the reconst ruction\\nerror∥x−mj∥2, summed over observations, for both methods. This will\\nnecessarily be smaller for K-means, but should not be much smaller if the\\nSOM is a reasonable approximation.\\nAs an illustrative example, we generated 90 data points in th ree dimen-\\nsions, near the surface of a half sphere of radius 1. The point s were in each\\nof three clusters—red, green, and blue—located near (0 ,1,0), (0,0,1) and\\n(1,0,0). The data are shown in Figure 14.15\\nBy design, the red cluster was much tighter than the green or b lue ones.\\n(Full details of the data generation are given in Exercise 14 .5.) A 5×5 grid\\nof prototypes was used, with initial grid size R= 2; this meant that about\\na third of the prototypes were initially in each neighborhoo d. We did a\\ntotal of 40 passes through the dataset of 90 observations, an d letrandα\\ndecrease linearly over the 3600 iterations.\\nIn Figure 14.16 the prototypes are indicated by circles, and the points\\nthat project to each prototype are plotted randomly within t he correspond-\\ning circle. The left panel shows the initial conﬁguration, w hile the right\\npanel shows the ﬁnal one. The algorithm has succeeded in sepa rating the\\nclusters; however, the separation of the red cluster indica tes that the man-\\nifold has folded back on itself (see Figure 14.17). Since the distances in the\\ntwo-dimensional display are not used, there is little indic ation in the SOM\\nprojection that the red cluster is tighter than the others.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 547}),\n",
       " Document(page_content='530 14. Unsupervised Learning\\n−1−0.500.511.5\\n−1−0.500.511.5−1−0.500.511.5\\nFIGURE 14.15. Simulated data in three classes, near the surface of a half–\\nsphere.\\n•••\\n••\\n•••\\n••\\n•••\\n•••••\\n••••••••\\n•••\\n•••\\n••\\n•\\n••\\n••••\\n••••\\n•• ••• ••\\n••\\n••• •\\n••••\\n•••••\\n••\\n••••\\n••• •\\n••\\n•\\n•••\\n••\\n•••\\n••\\n1 2 3 4 512345\\n••••\\n••\\n•\\n••\\n••••••\\n•\\n••\\n••••\\n•\\n•••••\\n••\\n• • • •••\\n••\\n•\\n••••\\n•••••\\n•\\n•••\\n•• •• ••\\n• •\\n•••\\n••••\\n•••\\n•••\\n••\\n••\\n•• •\\n•••\\n• • ••\\n•••\\n1 2 3 4 512345\\nFIGURE 14.16. Self-organizing map applied to half-sphere data example. Left\\npanel is the initial conﬁguration, right panel the ﬁnal one. The5×5grid of\\nprototypes are indicated by circles, and the points that proj ect to each prototype\\nare plotted randomly within the corresponding circle.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 548}),\n",
       " Document(page_content='14.4 Self-Organizing Maps 531\\nFIGURE 14.17. Wiremesh representation of the ﬁtted SOM model in IR3. The\\nlines represent the horizontal and vertical edges of the topo logical lattice. The\\ndouble lines indicate that the surface was folded diagonally bac k on itself in order\\nto model the red points. The cluster members have been jittere d to indicate their\\ncolor, and the purple points are the node centers.\\nFigure 14.18 shows the reconstruction error, equal to the to tal sum of\\nsquares of each data point around its prototype. For compari son we carried\\nout aK-means clustering with 25 centroids, and indicate its recon struction\\nerror by the horizontal line on the graph. We see that the SOM s igniﬁcantly\\ndecreases the error, nearly to the level of the K-means solution. This pro-\\nvides evidence that the two-dimensional constraint used by the SOM is\\nreasonable for this particular dataset.\\nIn the batch version of the SOM, we update each mjvia\\nmj=∑wkxk∑wk. (14.48)\\nThe sum is over points xkthat mapped (i.e., were closest to) neighbors mk\\nofmj. The weight function may be rectangular, that is, equal to 1 f or the\\nneighborsof mk,ormaydecreasesmoothlywithdistance ∥ℓk−ℓj∥asbefore.\\nIf the neighborhood size is chosen small enough so that it con sists only\\nofmk, with rectangular weights, this reduces to the K-means clustering\\nprocedure described earlier. It can also be thought of as a di screte version\\nof principal curves and surfaces, described in Section 14.5 .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 549}),\n",
       " Document(page_content='532 14. Unsupervised Learning\\nIterationReconstruction Error\\n0 500 1000 1500 2000 25000 10 20 30 40 50••\\n•\\n•••\\n••\\n••\\n••••\\n•••••••••••••••••••••••••••••••••••••\\nFIGURE 14.18. Half-sphere data: reconstruction error for the SOM as a func-\\ntion of iteration. Error for k-means clustering is indicated by the horizontal line.\\nExample: Document Organization and Retrieval\\nDocument retrieval has gained importance with the rapid dev elopment of\\nthe Internet and the Web, and SOMs have proved to be useful for organiz-\\ning and indexing large corpora. This example is taken from th e WEBSOM\\nhomepage http://websom.hut.fi/ (Kohonen et al., 2000). Figure 14.19 rep-\\nresents a SOM ﬁt to 12,088 newsgroup comp.ai.neural-nets articles. The\\nlabels are generated automatically by the WEBSOM software a nd provide\\na guide as to the typical content of a node.\\nIn applications such as this, the documents have to be prepro cessed in\\norder to create a feature vector. A term-document matrix is created, where\\neach row represents a single document. The entries in each ro w are the\\nrelative frequency of each of a predeﬁned set of terms. These terms could\\nbe a large set of dictionary entries (50,000 words), or an eve n larger set\\nof bigrams (word pairs), or subsets of these. These matrices are typically\\nvery sparse, and so often some preprocessing is done to reduc e the number\\nof features (columns). Sometimes the SVD (next section) is u sed to reduce\\nthe matrix; Kohonen et al. (2000) use a randomized variant th ereof. These\\nreduced vectors are then the input to the SOM.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 550}),\n",
       " Document(page_content='14.4 Self-Organizing Maps 533\\nFIGURE 14.19. Heatmap representation of the SOM model ﬁt to a corpus\\nof12,088newsgroup comp.ai.neural-nets contributions (courtesy WEBSOM\\nhomepage). The lighter areas indicate higher-density areas . Populated nodes are\\nautomatically labeled according to typical content.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 551}),\n",
       " Document(page_content='534 14. Unsupervised Learning\\n•••\\n•\\n••\\n••••\\n•••\\n••\\n• ••\\n••\\n•••\\n•\\n••\\n••••\\n•••\\n••\\n• ••\\n•v1v1v1v1v1v1v1v1\\nui1d1ui1d1ui1d1ui1d1ui1d1ui1d1ui1d1ui1d1\\nxixixixixixixixi\\nFIGURE 14.20. The ﬁrst linear principal component of a set of data. The line\\nminimizes the total squared distance from each point to its o rthogonal projection\\nonto the line.\\nIn this application the authors have developed a “zoom” feat ure, which\\nallows one to interact with the map in order to get more detail . The ﬁnal\\nlevel of zooming retrieves the actual news articles, which c an then be read.\\n14.5 Principal Components, Curves and Surfaces\\nPrincipal components are discussed in Sections 3.4.1, wher e they shed light\\non the shrinkage mechanism of ridge regression. Principal c omponents are\\na sequence of projections of the data, mutually uncorrelate d and ordered\\nin variance. In the next section we present principal compon ents as linear\\nmanifolds approximating a set of Npointsxi∈IRp. We then present\\nsome nonlinear generalizations in Section 14.5.2. Other re cent proposals\\nfor nonlinear approximating manifolds are discussed in Sec tion 14.9.\\n14.5.1 Principal Components\\nThe principal components of a set of data in IRpprovide a sequence of best\\nlinear approximations to that data, of all ranks q≤p.\\nDenote the observations by x1,x2,...,x N, and consider the rank- qlinear\\nmodel for representing them', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 552}),\n",
       " Document(page_content='14.5 Principal Components, Curves and Surfaces 535\\nf(λ) =µ+Vqλ, (14.49)\\nwhereµis a location vector in IRp,Vqis ap×qmatrix with qorthogonal\\nunit vectors as columns, and λis aqvector of parameters. This is the\\nparametric representation of an aﬃne hyperplane of rank q. Figures 14.20\\nand 14.21 illustrate for q= 1 andq= 2, respectively. Fitting such a model\\nto the data by least squares amounts to minimizing the reconstruction error\\nmin\\nµ,{λi},VqN∑\\ni=1∥xi−µ−Vqλi∥2. (14.50)\\nWe can partially optimize for µand theλi(Exercise 14.7) to obtain\\nˆµ= ¯x, (14.51)\\nˆλi=VT\\nq(xi−¯x). (14.52)\\nThis leaves us to ﬁnd the orthogonal matrix Vq:\\nmin\\nVqN∑\\ni=1||(xi−¯x)−VqVT\\nq(xi−¯x)||2. (14.53)\\nFor convenience we assume that ¯ x= 0 (otherwise we simply replace the\\nobservations by their centered versions ˜ xi=xi−¯x). Thep×pmatrix\\nHq=VqVT\\nqis aprojection matrix , and maps each point xionto its rank-\\nqreconstruction Hqxi, the orthogonal projection of xionto the subspace\\nspanned by the columns of Vq. The solution can be expressed as follows.\\nStack the (centered) observations into the rows of an N×pmatrixX. We\\nconstruct the singular value decomposition ofX:\\nX=UDVT. (14.54)\\nThis is a standard decomposition in numerical analysis, and many algo-\\nrithms exist for its computation (Golub and Van Loan, 1983, f or example).\\nHereUis anN×porthogonal matrix ( UTU=Ip) whose columns ujare\\ncalled the left singular vectors ;Vis ap×porthogonal matrix ( VTV=Ip)\\nwith columns vjcalled the right singular vectors , andDis ap×pdiagonal\\nmatrix, with diagonal elements d1≥d2≥···≥dp≥0 known as the sin-\\ngular values . For each rank q, the solution Vqto (14.53) consists of the ﬁrst\\nqcolumns of V. The columns of UDare called the principal components\\nofX(see Section 3.5.1). The Noptimalˆλiin (14.52) are given by the ﬁrst\\nqprincipal components (the Nrows of the N×qmatrixUqDq).\\nThe one-dimensional principal component line in IR2is illustrated in Fig-\\nure 14.20. For each data point xi, there is a closest point on the line, given\\nbyui1d1v1. Herev1is the direction of the line and ˆλi=ui1d1measures\\ndistance along the line from the origin. Similarly Figure 14 .21 shows the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 553}),\n",
       " Document(page_content='536 14. Unsupervised Learning\\nFirst principal componentSecond principal component\\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0•\\n•••\\n•••\\n•\\n••\\n•••\\n••••\\n•\\n•••\\n••••••\\n••\\n•••\\n••\\n•\\n••\\n•••\\n•\\n••••••\\n•••\\n••\\n••\\n•••\\n•\\n••••\\n•••\\n••\\n••\\n••\\n•\\n••\\n•••\\n•\\n•••\\n••••\\n•••\\n••\\nFIGURE 14.21. The best rank-two linear approximation to the half-sphere dat a.\\nThe right panel shows the projected points with coordinates given by U2D2, the\\nﬁrst two principal components of the data.\\ntwo-dimensional principal component surface ﬁt to the half -sphere data\\n(left panel). The right panel shows the projection of the dat a onto the\\nﬁrst two principal components. This projection was the basi s for the initial\\nconﬁguration for the SOM method shown earlier. The procedur e is quite\\nsuccessful at separating the clusters. Since the half-sphe re is nonlinear, a\\nnonlinear projection will do a better job, and this is the top ic of the next\\nsection.\\nPrincipal components have many other nice properties, for e xample, the\\nlinear combination Xv1has the highest variance among all linear com-\\nbinations of the features; Xv2has the highest variance among all linear\\ncombinations satisfying v2orthogonal to v1, and so on.\\nExample: Handwritten Digits\\nPrincipal components are a useful tool for dimension reduct ion and com-\\npression.Weillustratethisfeatureonthehandwrittendig itsdatadescribed\\nin Chapter 1. Figure 14.22 shows a sample of 130 handwritten 3 ’s, each a\\ndigitized 16×16 grayscale image, from a total of 658 such 3’s. We see\\nconsiderable variation in writing styles, character thick ness and orienta-\\ntion. We consider these images as points xiin IR256, and compute their\\nprincipal components via the SVD (14.54).\\nFigure 14.23 shows the ﬁrst two principal components of thes e data. For\\neach of these ﬁrst two principal components ui1d1andui2d2, we computed\\nthe 5%, 25%, 50%, 75% and 95% quantile points, and used them to deﬁne\\nthe rectangular grid superimposed on the plot. The circled p oints indicate', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 554}),\n",
       " Document(page_content='14.5 Principal Components, Curves and Surfaces 537\\nFIGURE 14.22. A sample of 130handwritten 3’s shows a variety of writing\\nstyles.\\nthose images close to the vertices of the grid, where the dist ance measure\\nfocuses mainly on these projected coordinates, but gives so me weight to the\\ncomponents in the orthogonal subspace. The right plot shows the images\\ncorresponding to these circled points. This allows us to vis ualize the nature\\nof the ﬁrst two principal components. We see that the v1(horizontal move-\\nment) mainly accounts for the lengthening of the lower tail o f the three,\\nwhilev2(vertical movement) accounts for character thickness. In t erms of\\nthe parametrized model (14.49), this two-component model h as the form\\nˆf(λ) = ¯x+λ1v1+λ2v2\\n= +λ1·+λ2·. (14.55)\\nHere we have displayed the ﬁrst two principal component dire ctions,v1\\nandv2, as images. Although there are a possible 256 principal comp onents,\\napproximately 50 account for 90% of the variation in the thre es, 12 ac-\\ncount for 63%. Figure 14.24 compares the singular values to t hose obtained\\nfor equivalent uncorrelated data, obtained by randomly scr ambling each\\ncolumn of X. The pixels in a digitized image are inherently correlated,\\nand since these are all the same digit the correlations are ev en stronger.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 555}),\n",
       " Document(page_content='538 14. Unsupervised Learning\\nFirst Principal ComponentSecond Principal Component\\n-6 -4 -2 0 2 4 6 8-5 0 5••\\n••\\n••\\n••\\n• ••••\\n••\\n•••\\n••\\n••••\\n••\\n•\\n•••\\n•\\n••\\n••••\\n•••\\n••\\n•\\n••\\n••\\n•\\n• •\\n••\\n•••\\n••\\n••••\\n•••\\n•••••\\n••\\n•\\n••\\n••\\n•\\n•••\\n•\\n••\\n••••\\n•\\n•\\n•\\n••••••\\n•\\n•••\\n•••\\n•\\n•••••\\n•\\n•••\\n•\\n••\\n••\\n••\\n••\\n•••\\n•••\\n•\\n•••\\n••\\n••••\\n•\\n•\\n••\\n••\\n••\\n••\\n•\\n•••\\n•\\n••\\n•\\n•• •\\n••\\n••\\n•••\\n••\\n•••\\n•\\n•••••\\n••\\n•\\n••\\n••\\n•\\n•••\\n•\\n••\\n•••\\n•••••\\n•\\n••\\n••\\n•\\n••\\n••\\n•\\n•• ••\\n••••\\n••\\n••\\n•••\\n••\\n•\\n•••\\n•••\\n•\\n•\\n•\\n•• •\\n•\\n•••\\n• •• ••\\n••••\\n•\\n•\\n••\\n•• •\\n•\\n•\\n••\\n•\\n••\\n•••\\n••\\n••\\n••\\n• •••\\n•\\n•\\n•••\\n••••\\n•••\\n••\\n••••\\n••\\n•\\n• ••\\n•••\\n•••\\n••\\n••••\\n••••\\n••\\n• ••\\n••\\n•••••\\n•\\n•\\n••••\\n•\\n••••\\n•\\n••\\n••\\n•\\n• •••\\n••\\n••••\\n•\\n•••\\n•\\n•\\n••\\n••••\\n••••\\n•••\\n•\\n•••\\n•\\n•••\\n••\\n•• •\\n•\\n•\\n••\\n•••\\n••\\n••\\n•\\n••\\n•\\n••\\n•••\\n•••\\n•\\n•••\\n•\\n•••\\n•••••\\n••••\\n•• •\\n•\\n•••\\n•\\n••\\n••\\n•••\\n•\\n••\\n•• ••\\n••\\n••\\n••••\\n••\\n•\\n•••\\n••\\n•\\n•\\n•••\\n•\\n•\\n••\\n••\\n••\\n••\\n••\\n••\\n•\\n••\\n•\\n•\\n••\\n•\\n•\\n••••\\n•\\n••\\n•••\\n•••\\n•••• ••\\n••\\n••••\\n••\\n•••\\n••••\\n•\\n••\\n•••\\n••\\n••\\n••\\n•\\n•••\\n••\\n••\\n••••\\n•\\n•\\n•••••\\n•••\\n••\\n••••\\n•••••\\n••\\n•••\\n••\\n••\\n••••\\n•\\n••\\n•• •••\\n•••\\n••\\n•\\n••••\\n••• ••\\n•\\n••\\n•••\\n•••\\n•••\\n•\\n•\\n•••\\n•••••\\n••\\n•\\n••\\n••\\n••\\n•\\n••••••\\n••\\n•\\n••\\n•\\n••\\nO O O OOO O OO\\nOOO\\nOO OO O O OOO O OOO\\nFIGURE 14.23. (Left panel:) the ﬁrst two principal components of the hand-\\nwritten threes. The circled points are the closest projected i mages to the vertices\\nof a grid, deﬁned by the marginal quantiles of the principal co mponents. (Right\\npanel:) The images corresponding to the circled points. These show the nature of\\nthe ﬁrst two principal components.\\nDimensionSingular Values\\n0 50 100 150 200 2500 20 40 60 80•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\\n•••\\n•\\n••\\n••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••Real Trace\\n•Randomized Trace\\nFIGURE 14.24. The256singular values for the digitized threes, compared to\\nthose for a randomized version of the data (each column of Xwas scrambled).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 556}),\n",
       " Document(page_content='14.5 Principal Components, Curves and Surfaces 539\\nA relatively small subset of the principal components serve as excellent\\nlower-dimensional features for representing the high-dim ensional data.\\nExample: Procrustes Transformations and Shape Averaging\\nFIGURE 14.25. (Left panel:) Two diﬀerent digitized handwritten Ss, each re p-\\nresented by 96 corresponding points in IR2. The green S has been deliberately\\nrotated and translated for visual eﬀect. (Right panel:) A Proc rustes transforma-\\ntion applies a translation and rotation to best match up the two set of points.\\nFigure 14.25 represents two sets of points, the orange and gr een, in the\\nsame plot. In this instance these points represent two digit ized versions\\nof a handwritten S, extracted from the signature of a subject “Suresh.”\\nFigure 14.26 shows the entire signatures from which these we re extracted\\n(third and fourth panels). The signatures are recorded dyna mically using\\ntouch-screen devices, familiar sights in modern supermark ets. There are\\nN= 96 points representing each S, which we denote by the N×2 matrices\\nX1andX2. There is a correspondence between the points—the ith rows\\nofX1andX2are meant to represent the same positions along the two S’s.\\nIn the language of morphometrics, these points represent landmarks on\\nthe two objects. How one ﬁnds such corresponding landmarks i s in general\\ndiﬃcult and subject speciﬁc. In this particular case we used dynamic time\\nwarping of the speed signal along each signature (Hastie et al., 1992 ), but\\nwill not go into details here.\\nIn the right panel we have applied a translation and rotation to the green\\npoints so as best to match the orange—a so-called Procrustes3transforma-\\ntion (Mardia et al., 1979, for example).\\nConsider the problem\\nmin\\nµ,R||X2−(X1R+1µT)||F, (14.56)\\n3Procrustes was an African bandit in Greek mythology, who stretched or sq uashed\\nhis visitors to ﬁt his iron bed (eventually killing them).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 557}),\n",
       " Document(page_content='540 14. Unsupervised Learning\\nwithX1andX2bothN×pmatrices of corresponding points, Ran or-\\nthonormal p×pmatrix4, andµap-vector of location coordinates. Here\\n||X||2\\nF= trace(XTX) is the squared Frobenius matrix norm.\\nLet ¯x1and ¯x2be the column mean vectors of the matrices, and ˜X1and\\n˜X2be the versions of these matrices with the means removed. Con sider\\nthe SVD ˜XT\\n1˜X2=UDVT. Then the solution to (14.56) is given by (Exer-\\ncise 14.8)\\nˆR=UVT\\nˆµ= ¯x2−ˆR¯x1,(14.57)\\nand the minimal distances is referred to as the Procrustes distance . From\\nthe form of the solution, we can center each matrix at its colu mn centroid,\\nand then ignore location completely. Hereafter we assume th is is the case.\\nTheProcrustes distance with scaling solves a slightly more general\\nproblem,\\nmin\\nβ,R||X2−βX1R||F, (14.58)\\nwhereβ >0 is a positive scalar. The solution for Ris as before, with\\nˆβ= trace(D)/||X1||2\\nF.\\nRelated to Procrustes distance is the Procrustes average of a collection\\nofLshapes, which solves the problem\\nmin\\n{Rℓ}L\\n1,ML∑\\nℓ=1||XℓRℓ−M||2\\nF; (14.59)\\nthat is, ﬁnd the shape Mclosest in average squared Procrustes distance to\\nall the shapes. This is solved by a simple alternating algori thm:\\n0. Initialize M=X1(for example).\\n1. Solve the LProcrustes rotation problems with Mﬁxed, yielding\\nX′\\nℓ←XˆRℓ.\\n2. LetM←1\\nL∑L\\nℓ=1X′\\nℓ.\\nSteps 1. and 2. are repeated until the criterion (14.59) conv erges.\\nFigure 14.26 shows a simple example with three shapes. Note t hat we can\\nonly expect a solution up to a rotation; alternatively, we ca n impose a\\nconstraint, such as that Mbe upper-triangular, to force uniqueness. One\\ncan easily incorporate scaling in the deﬁnition (14.59); se e Exercise 14.9.\\nMost generally we can deﬁne the aﬃne-invariant average of a set of\\nshapes via\\n4To simplify matters, we consider only orthogonal matrices which inclu de reﬂections\\naswellasrotations[the O(p)group];althoughreﬂectionsareunlikelyhere,thesemethods\\ncan be restricted further to allow only rotations [ SO(p) group].', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 558}),\n",
       " Document(page_content='14.5 Principal Components, Curves and Surfaces 541\\nFIGURE 14.26. The Procrustes average of three versions of the leading S in\\nSuresh’s signatures. The left panel shows the preshape average, with each of the\\nshapesX′\\nℓin preshape space superimposed. The right three panels map th e pre-\\nshapeMseparately to match each of the original S’s.\\nmin\\n{Aℓ}L\\n1,ML∑\\nℓ=1||XℓAℓ−M||2\\nF, (14.60)\\nwhere the Aℓare anyp×pnonsingular matrices. Here we require a stan-\\ndardization, such as MTM=I, to avoid a trivial solution. The solution is\\nattractive, and can be computed without iteration (Exercis e 14.10):\\n1. LetHℓ=Xℓ(XT\\nℓXℓ)−1XT\\nℓbe the rank- pprojection matrix deﬁned\\nbyXℓ.\\n2.Mis theN×p matrix formed from the plargest eigenvectors of ¯H=\\n1\\nL∑L\\nℓ=1Hℓ.\\n14.5.2 Principal Curves and Surfaces\\nPrincipalcurvesgeneralizetheprincipalcomponentline, providingasmooth\\none-dimensionalcurvedapproximationtoasetofdatapoint sinIRp.Aprin-\\ncipal surface is more general, providing a curved manifold a pproximation\\nof dimension 2 or more.\\nWe will ﬁrst deﬁne principal curves for random variables X∈IRp, and\\nthen move to the ﬁnite data case. Let f(λ) be a parameterized smooth\\ncurve in IRp. Hencef(λ) is a vector function with pcoordinates, each a\\nsmooth function of the single parameter λ. The parameter λcan be chosen,\\nfor example, to be arc-length along the curve from some ﬁxed o rigin. For\\neach data value x, letλf(x) deﬁne the closest point on the curve to x. Then\\nf(λ) is called a principal curve for the distribution of the rand om vector\\nXif\\nf(λ) = E(X|λf(X) =λ). (14.61)\\nThis saysf(λ)is theaverage ofall data points that project toit, that is, the\\npoints for which it is “responsible.” This is also known as a self-consistency\\nproperty. Although in practice, continuous multivariate d istributes have\\ninﬁnitely many principal curves (Duchamp and Stuetzle, 199 6), we are', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 559}),\n",
       " Document(page_content='542 14. Unsupervised Learning\\n....\\n•••\\n•••\\n••••\\n•• ••\\n•\\n••\\n•••\\n•\\n•••\\n•\\n•\\n••••\\n••••\\n•••\\n••••\\n••••\\n•\\n••\\n•••\\n•\\n•••\\n•\\n•\\n••••\\n•.....\\nf(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)]\\nFIGURE 14.27. The principal curve of a set of data. Each point on the curve\\nis the average of all data points that project there.\\ninterested mainly in the smooth ones. A principal curve is il lustrated in\\nFigure 14.27.\\nPrincipal points are an interesting related concept. Consider a set of k\\nprototypes and for each point xin the support of a distribution, identify\\nthe closest prototype, that is, the prototype that is respon sible for it. This\\ninduces a partition of the feature space into so-called Voro noi regions. The\\nset ofkpoints that minimize the expected distance from Xto its prototype\\nare called the principal points of the distribution. Each pr incipal point is\\nself-consistent, in that it equals the mean of Xin its Voronoi region. For\\nexample, with k= 1, the principal point of a circular normal distribution is\\nthe mean vector; with k= 2 they are a pair of points symmetrically placed\\non a ray through the mean vector. Principal points are the dis tributional\\nanalogs of centroids found by K-means clustering. Principal curves can be\\nviewed ask=∞principal points, but constrained to lie on a smooth curve,\\nin a similar way that a SOM constrains K-means cluster centers to fall on\\na smooth manifold.\\nToﬁndaprincipalcurve f(λ)ofadistribution,weconsideritscoordinate\\nfunctionsf(λ) = [f1(λ),f2(λ),...,f p(λ)] and letXT= (X1,X2,...,X p).\\nConsider the following alternating steps:\\n(a)ˆfj(λ)←E(Xj|λ(X) =λ);j= 1,2,...,p,\\n(b)ˆλf(x)←argminλ′||x−ˆf(λ′)||2.(14.62)\\nThe ﬁrst equation ﬁxes λand enforces the self-consistency requirement\\n(14.61). The second equation ﬁxes the curve and ﬁnds the clos est point on', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 560}),\n",
       " Document(page_content='14.5 Principal Components, Curves and Surfaces 543\\n-0.1 0.0 0.1 0.2-0.2 -0.1 0.0 0.1 0.2••••\\n•••\\n•\\n••\\n•••\\n•••••\\n•••\\n••••••\\n••\\n• ••\\n••\\n•••\\n•••\\n•\\n••••••\\n•••\\n••\\n••\\n•••\\n• •••\\n•\\n•••\\n•••\\n•\\n••\\n•\\n•••••\\n••••\\n••••\\n•••\\n••\\nλ1λ2\\nFIGURE 14.28. Principal surface ﬁt to half-sphere data. (Left panel:) ﬁtted\\ntwo-dimensional surface. (Right panel:) projections of dat a points onto the sur-\\nface, resulting in coordinates ˆλ1,ˆλ2.\\nthecurvetoeachdatapoint.Withﬁnitedata,theprincipalc urvealgorithm\\nstarts with the linear principal component, and iterates th e two steps in\\n(14.62) until convergence. A scatterplot smoother is used t o estimate the\\nconditional expectations in step (a) by smoothing each Xjas a function of\\nthe arc-length ˆλ(X), and the projection in (b) is done for each of the ob-\\nserved data points. Proving convergence in general is diﬃcu lt, but one can\\nshow that if a linear least squares ﬁt is used for the scatterp lot smoothing,\\nthen the procedure converges to the ﬁrst linear principal co mponent, and\\nis equivalent to the power method for ﬁnding the largest eigenvector of a\\nmatrix.\\nPrincipal surfaces have exactly the same form as principal c urves, but\\nare of higher dimension. The mostly commonly used is the two- dimensional\\nprincipal surface, with coordinate functions\\nf(λ1,λ2) = [f1(λ1,λ2),...,f p(λ1,λ2)].\\nThe estimates in step (a) above are obtained from two-dimens ional surface\\nsmoothers.Principalsurfacesofdimensiongreaterthantw oarerarelyused,\\nsince the visualization aspect is less attractive, as is smo othing in high\\ndimensions.\\nFigure 14.28 shows the result of a principal surface ﬁt to the half-sphere\\ndata. Plotted are the data points as a function of the estimat ed nonlinear\\ncoordinates ˆλ1(xi),ˆλ2(xi). The class separation is evident.\\nPrincipal surfaces are very similar to self-organizing map s. If we use a\\nkernel surface smoother to estimate each coordinate functi onfj(λ1,λ2),\\nthis has the same form as the batch version of SOMs (14.48). Th e SOM\\nweightswkare just the weights in the kernel. There is a diﬀerence, howe ver:', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 561}),\n",
       " Document(page_content='544 14. Unsupervised Learning\\nthe principal surface estimates a separate prototype f(λ1(xi),λ2(xi)) for\\neach data point xi, while the SOM shares a smaller number of prototypes\\nfor all data points. As a result, the SOM and principal surfac e will agree\\nonly as the number of SOM prototypes grows very large.\\nThere also is a conceptual diﬀerence between the two. Princi pal sur-\\nfaces provide a smooth parameterization of the entire manif old in terms\\nof its coordinate functions, while SOMs are discrete and pro duce only the\\nestimated prototypes for approximating the data. The smoot h parameter-\\nization in principal surfaces preserves distance locally: in Figure 14.28 it\\nreveals that the red cluster is tighter than the green or blue clusters. In\\nsimple examples the estimates coordinate functions themse lves can be in-\\nformative: see Exercise 14.13.\\n14.5.3 Spectral Clustering\\nTraditional clustering methods like K-means use a spherical or elliptical\\nmetric to group data points. Hence they will not work well whe n the clus-\\nters are non-convex, such as the concentric circles in the to p left panel of\\nFigure 14.29. Spectral clustering is a generalization of st andard clustering\\nmethods, and is designed for these situations. It has close c onnections with\\nthe local multidimensional-scaling techniques (Section 1 4.9) that generalize\\nMDS.\\nThe starting point is a N×Nmatrix of pairwise similarities sii′≥0 be-\\ntween all observation pairs. We represent the observations in an undirected\\nsimilarity graph G=⟨V, E⟩. TheNverticesvirepresent the observations,\\nand pairs of vertices are connected by an edge if their simila rity is positive\\n(or exceeds some threshold). The edges are weighted by the sii′. Clustering\\nis now rephrased as a graph-partition problem, where we iden tify connected\\ncomponents with clusters. We wish to partition the graph, su ch that edges\\nbetween diﬀerent groups have low weight, and within a group h ave high\\nweight. The idea in spectral clustering is to construct simi larity graphs that\\nrepresent the local neighborhood relationships between ob servations.\\nTomakethingsmoreconcrete,considerasetof Npointsxi∈IRp,andlet\\ndii′be the Euclidean distance between xiandxi′. We will use as similarity\\nmatrix the radial-kernel gram matrix; that is, sii′= exp(−d2\\nii′/c), where\\nc>0 is a scale parameter.\\nThere are many ways to deﬁne a similarity matrix and its assoc iated\\nsimilarity graph that reﬂect local behavior. The most popul ar is the mutual\\nK-nearest-neighbor graph . DeﬁneNKto be the symmetric set of nearby\\npairs of points; speciﬁcally a pair ( i,i′) is inNKif pointiis among the\\nK-nearest neighbors of i′, or vice-versa. Then we connect all symmetric\\nnearest neighbors, and give them edge weight wii′=sii′; otherwise the\\nedge weight is zero. Equivalently we set to zero all the pairw ise similarities\\nnot inNK, and draw the graph for this modiﬁed similarity matrix.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 562}),\n",
       " Document(page_content='14.5 Principal Components, Curves and Surfaces 545\\nAlternatively, a fully connected graph includes all pairwi se edges with\\nweightswii′=sii′, and the local behavior is controlled by the scale param-\\neterc.\\nThe matrix of edge weights W={wii′}from a similarity graph is called\\ntheadjacency matrix . Thedegreeof vertexiisgi=∑\\ni′wii′, the sum of\\nthe weights of the edges connected to it. Let Gbe a diagonal matrix with\\ndiagonal elements gi.\\nFinally, the graph Laplacian is deﬁned by\\nL=G−W (14.63)\\nThis is called the unnormalized graph Laplacian ; a number of normalized\\nversions have been proposed—these standardize the Laplacia n with respect\\nto the node degrees gi, for example, ˜L=I−G−1W.\\nSpectral clustering ﬁnds the meigenvectors ZN×mcorresponding to the\\nmsmallest eigenvalues of L(ignoring the trivial constant eigenvector).\\nUsing a standard method like K-means, we then cluster the rows of Zto\\nyield a clustering of the original data points.\\nAn example is presented in Figure 14.29. The top left panel sh ows 450\\nsimulated data points in three circular clusters indicated by the colors. K-\\nmeans clustering would clearly have diﬃculty identifying t he outer clusters.\\nWe applied spectral clustering using a 10-nearest neighbor similarity graph,\\nand display the eigenvector corresponding to the second and third smallest\\neigenvalue of the graph Laplacian in the lower left. The 15 sm allest eigen-\\nvalues are shown in the top right panel. The two eigenvectors shown have\\nidentiﬁed the three clusters, and a scatterplot of the rows o f the eigenvector\\nmatrixYin the bottom right clearly separates the clusters. A proced ure\\nsuch asK-means clustering applied to these transformed points woul d eas-\\nily identify the three groups.\\nWhy does spectral clustering work? For any vector fwe have\\nfTLf=N∑\\ni=1gif2\\ni−N∑\\ni=1N∑\\ni′=1fifi′wii′\\n=1\\n2N∑\\ni=1N∑\\ni′=1wii′(fi−fi′)2. (14.64)\\nFormula 14.64 suggests that a small value of fTLfwill be achieved if pairs\\nof points with large adjacencies have coordinates fiandfi′close together.\\nSince1TL1= 0 for any graph, the constant vector is a trivial eigenvecto r\\nwith eigenvalue zero. Not so obvious is the fact that if the gr aph is con-\\nnected5, it is the onlyzero eigenvector (Exercise 14.21). Generalizing this\\nargument,itiseasytoshowthatforagraphwith mconnectedcomponents,\\n5A graph is connected if any two nodes can be reached via a path of connected no des.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 563}),\n",
       " Document(page_content='546 14. Unsupervised Learning\\n−4 −2 0 2 4−4 −2 0 2 4\\nx1x2\\n0.0 0.1 0.2 0.3 0.4 0.5\\nNumberEigenvalue\\n1 3 5 10 15\\n0 100 200 300 400Eigenvectors\\nIndex2nd Smallest 3rd Smallest\\n−0.05  0.05 −0.05  0.05\\n−0.04 −0.02 0.00 0.02−0.06 −0.02 0.02 0.06\\nSecond Smallest EigenvectorThird Smallest EigenvectorSpectral Clustering\\nFIGURE 14.29. Toy example illustrating spectral clustering. Data in top left a re\\n450points falling in three concentric clusters of 150points each. The points are\\nuniformly distributed in angle, with radius 1,2.8and5in the three groups, and\\nGaussian noise with standard deviation 0.25added to each point. Using a k= 10\\nnearest-neighbor similarity graph, the eigenvector corres ponding to the second and\\nthird smallest eigenvalues of Lare shown in the bottom left; the smallest eigen-\\nvector is constant. The data points are colored in the same way as in the top left.\\nThe 15 smallest eigenvalues are shown in the top right panel. The coordinates of\\nthe 2nd and 3rd eigenvectors (the 450rows ofZ) are plotted in the bottom right\\npanel. Spectral clustering does standard (e.g., K-means) clustering of these points\\nand will easily recover the three original clusters.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 564}),\n",
       " Document(page_content='14.5 Principal Components, Curves and Surfaces 547\\nthe nodes can be reordered so that Lis block diagonal with a block for each\\nconnected component. Then Lhasmeigenvectors of eigenvalue zero, and\\nthe eigenspace of eigenvalue zero is spanned by the indicato r vectors of the\\nconnected components. In practice one has strong and weak co nnections,\\nso zero eigenvalues are approximated by small eigenvalues.\\nSpectralclusteringisaninterestingapproachforﬁndingn on-convexclus-\\nters. When a normalized graph Laplacian is used, there is ano ther way to\\nview this method. Deﬁning P=G−1W, we consider a random walk on\\nthe graph with transition probability matrix P. Then spectral clustering\\nyields groups of nodes such that the random walk seldom trans itions from\\none group to another.\\nThere are a number of issues that one must deal with in applyin g spec-\\ntralclusteringinpractice.Wemustchoosethetypeofsimil arity graph—eg.\\nfully connected or nearest neighbors, and associated param eters such as the\\nnumber of nearest of neighbors kor the scale parameter of the kernel c. We\\nmust also choose the number of eigenvectors to extract from Land ﬁnally,\\nas with all clustering methods, the number of clusters. In th e toy example\\nof Figure 14.29 we obtained good results for k∈[5,200], the value 200 cor-\\nresponding to a fully connected graph. With k<5 the results deteriorated.\\nLooking at the top-right panel of Figure 14.29, we see no stro ng separation\\nbetween the smallest three eigenvalues and the rest. Hence i t is not clear\\nhow many eigenvectors to select.\\n14.5.4 Kernel Principal Components\\nSpectral clustering is related to kernel principal components , a non-linear\\nversion of linear principal components. Standard linear pr incipal compo-\\nnents (PCA) are obtained from the eigenvectors of the covari ance matrix,\\nand give directions in which the data have maximal variance. Kernel PCA\\n(Sch¨ olkopfetal.,1999)expandthescopeofPCA,mimicking whatwewould\\nobtain if we were to expand the features by non-linear transf ormations, and\\nthen apply PCA in this transformed feature space.\\nWe show in Section 18.5.2 that the principal components vari ablesZof\\na data matrix Xcan be computed from the inner-product (gram) matrix\\nK=XXT. In detail, we compute the eigen-decomposition of the doubl e-\\ncentered version of the gram matrix\\n˜K= (I−M)K(I−M) =UD2UT, (14.65)\\nwithM=11T/N, and then Z=UD. Exercise 18.15 shows how to com-\\npute the projections of new observations in this space.\\nKernel PCA simply mimics this procedure, interpreting the k ernel ma-\\ntrixK={K(xi,xi′)}as an inner-product matrix of the implicit fea-\\ntures⟨φ(xi),φ(xi′)⟩and ﬁnding its eigenvectors. The elements of the mth\\ncomponent zm(mth column of Z) can be written (up to centering) as\\nzim=∑N\\nj=1αjmK(xi,xj), whereαjm=ujm/dm(Exercise 14.16).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 565}),\n",
       " Document(page_content='548 14. Unsupervised Learning\\nWe can gain more insight into kernel PCA by viewing the zmas sam-\\nple evaluations of principal component functionsgm∈HK, withHKthe\\nreproducing kernel Hilbert space generated by K(see Section 5.8.1). The\\nﬁrst principal component function g1solves\\nmax\\ng1∈HKVarTg1(X) subject to||g1||HK= 1 (14.66)\\nHere Var Trefers to the sample variance over training data T. The norm\\nconstraint||g1||HK= 1 controls the size and roughness of the function g1,\\nas dictated by the kernel K. As in the regression case it can be shown that\\nthe solution to (14.66) is ﬁnite dimensional with represent ationg1(x) =∑N\\nj=1cjK(x,xj). Exercise 14.17 shows that the solution is deﬁned by ˆ cj=\\nαj1, j= 1,...,Nabove. The second principal component function is de-\\nﬁned in a similar way, with the additional constraint that ⟨g1,g2⟩HK= 0,\\nand so on.6\\nSch¨ olkopf et al. (1999) demonstrate the use of kernel princ ipal compo-\\nnents as features for handwritten-digit classiﬁcation, an d show that they\\ncan improve the performance of a classiﬁer when these are use d instead of\\nlinear principal components.\\nNote that if we use the radial kernel\\nK(x,x′) = exp(−∥x−x′∥2/c), (14.67)\\nthen the kernel matrix Khas the same form as the similarity matrix Sin\\nspectral clustering. The matrix of edge weights Wis a localized version of\\nK, setting to zero all similarities for pairs of points that ar e not nearest\\nneighbors.\\nKernel PCA ﬁnds the eigenvectors corresponding to the large st eigenval-\\nues of˜K; this is equivalent to ﬁnding the eigenvectors correspondi ng to the\\nsmallest eigenvalues of\\nI−˜K. (14.68)\\nThis is almost the same as the Laplacian (14.63), the diﬀeren ces being the\\ncentering of˜Kand the fact that Ghas the degrees of the nodes along the\\ndiagonal.\\nFigure 14.30 examines the performance of kernel principal c omponents\\nin the toy example of Figure 14.29. In the upper left panel we u sed the ra-\\ndial kernel with c= 2, the same value that was used in spectral clustering.\\nThis does not separate the groups, but with c= 10 (upper right panel), the\\nﬁrst component separates the groups well. In the lower-left panel we ap-\\nplied kernel PCA using the nearest-neighbor radial kernel Wfrom spectral\\nclustering. In the lower right panel we use the kernel matrix itself as the\\n6This section beneﬁted from helpful discussions with Jonathan Taylor.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 566}),\n",
       " Document(page_content='14.5 Principal Components, Curves and Surfaces 549\\n−0.10 −0.06 −0.02 0.02−0.10 −0.05 0.00 0.05 0.10\\nFirst Largest EigenvectorSecond Largest EigenvectorRadial Kernel (c=2)\\n−0.06 −0.02 0.02 0.06−0.05 0.00 0.05\\nFirst Largest EigenvectorSecond Largest EigenvectorRadial Kernel (c=10)\\n0.00 0.05 0.10 0.15−0.2 −0.1 0.0 0.1 0.2\\nFirst Largest EigenvectorSecond Largest EigenvectorNN Radial Kernel (c=2)\\n−0.05 0.00 0.05 0.10 0.15−0.10 0.00 0.05 0.10 0.15\\nSecond Smallest EigenvectorThird Smallest EigenvectorRadial Kernel Laplacian (c=2)\\nFIGURE 14.30. Kernel principal components applied to the toy example of Fig-\\nure 14.29, using diﬀerent kernels. (Top left:) Radial kernel ( 14.67) with c= 2.\\n(Top right:) Radial kernel with c= 10. (Bottom left): Nearest neighbor radial ker-\\nnelWfrom spectral clustering. (Bottom right:) Spectral clusteri ng with Laplacian\\nconstructed from the radial kernel.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 567}),\n",
       " Document(page_content='550 14. Unsupervised Learning\\nsimilarity matrix for constructing the Laplacian (14.63) i n spectral cluster-\\ning. In neither case do the projections separate the two grou ps. Adjusting\\ncdid not help either.\\nIn this toy example, we see that kernel PCA is quite sensitive to the scale\\nand nature of the kernel. We also see that the nearest-neighb or truncation\\nof the kernel is important for the success of spectral cluste ring.\\n14.5.5 Sparse Principal Components\\nWeofteninterpretprincipalcomponentsbyexaminingthedi rectionvectors\\nvj, also known as loadings, to see which variables play a role. We did this\\nwith the image loadings in (14.55). Often this interpretati on is made easier\\nif the loadings are sparse. In this section we brieﬂy discuss some methods\\nfor deriving principal components with sparse loadings. Th ey are all based\\non lasso (L1) penalties.\\nWe start with an N×pdata matrix X, with centered columns. The\\nproposed methods focus on either the maximum-variance prop erty of prin-\\ncipal components, or the minimum reconstruction error. The SCoTLASS\\nprocedure of Joliﬀe et al. (2003) takes the ﬁrst approach, by solving\\nmaxvT(XTX)v,subject to∑p\\nj=1|vj|≤t,vTv= 1.(14.69)\\nThe absolute-value constraint encourages some of the loadi ngs to be zero\\nand hencevto be sparse. Further sparse principal components are found\\nin the same way, by forcing the kth component to be orthogonal to the\\nﬁrstk−1 components. Unfortunately this problem is not convex and t he\\ncomputations are diﬃcult.\\nZou et al. (2006) start instead with the regression/reconst ruction prop-\\nerty of PCA, similar to theapproach in Section 14.5.1. Let xibetheith row\\nofX. For a single component, their sparse principal component technique\\nsolves\\nmin\\nθ,vN∑\\ni=1||xi−θvTxi||2\\n2+λ||v||2\\n2+λ1||v||1 (14.70)\\nsubject to||θ||2= 1.\\nLet’s examine this formulation in more detail.\\n•If bothλandλ1are zero and N > p, it is easy to show that v=θ\\nand is the largest principal component direction.\\n•Whenp≫Nthe solution is not necessarily unique unless λ>0. For\\nanyλ>0 andλ1= 0 the solution for vis proportional to the largest\\nprincipal component direction.\\n•The second penalty on vencourages sparseness of the loadings.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 568}),\n",
       " Document(page_content='14.5 Principal Components, Curves and Surfaces 551\\nWalking Speed\\nVerbal Fluency\\nPrincipal Components Sparse Principal Components\\nFIGURE 14.31. Standard and sparse principal components from a study of\\nthe corpus callosum variation. The shape variations corresp onding to signiﬁcant\\nprincipal components (red curves) are overlaid on the mean CC shape (black\\ncurves).\\nFor multiple components, the sparse principal components p rocedures\\nminimizes\\nN∑\\ni=1||xi−ΘVTxi||2+λK∑\\nk=1||vk||2\\n2+K∑\\nk=1λ1k||vk||1, (14.71)\\nsubject to ΘTΘ=IK. HereVis ap×Kmatrix with columns vkandΘ\\nis alsop×K.\\nCriterion (14.71) is not jointly convex in VandΘ, but it is convex in\\neach parameter with the other parameter ﬁxed7. Minimization over Vwith\\nΘﬁxed is equivalent to Kelastic net problems (Section 18.4) and can be\\ndone eﬃciently. On the other hand, minimization over ΘwithVﬁxed is a\\nversion of the Procrustes problem (14.56), and is solved by a simple SVD\\ncalculation (Exercise 14.12). These steps are alternated u ntil convergence.\\nFigure 14.31 shows an example of sparse principal component s analysis\\nusing (14.71), taken from Sj¨ ostrand et al. (2007). Here the shape of the\\nmid-sagittal cross-section of the corpus callosum (CC) is r elated to various\\nclinical parameters in a study involving 569 elderly person s8. In this exam-\\n7Note that the usual principal component criterion, for example (14. 50), is not jointly\\nconvex in the parameters either. Nevertheless, the solution is well deﬁned and an eﬃcient\\nalgorithm is available.\\n8We thank Rasmus Larsen and Karl Sj¨ ostrand for suggesting this appli cation, and\\nsupplying us with the postscript ﬁgures reproduced here.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 569}),\n",
       " Document(page_content='552 14. Unsupervised Learning\\nFIGURE 14.32. An example of a mid-saggital brain slice, with the corpus col-\\nlosum annotated with landmarks.\\nple PCA is applied to shapedata, and is a popular tool in morphometrics.\\nFor such applications, a number of landmarks are identiﬁed a long the cir-\\ncumference of the shape; an example is given in Figure 14.32. These are\\naligned by Procrustes analysis to allow for rotations, and i n this case scal-\\ning as well (see Section 14.5.1). The features used for PCA ar e the sequence\\nof coordinate pairs for each landmark, unpacked into a singl e vector.\\nIn this analysis, both standard and sparse principal compon ents were\\ncomputed, and components that were signiﬁcantly associate d with various\\nclinical parameters were identiﬁed. In the ﬁgure, the shape variations cor-\\nresponding to signiﬁcant principal components (red curves ) are overlaid on\\nthe mean CC shape (black curves). Low walking speed relates t o CCs that\\nare thinner (displaying atrophy) in regions connecting the motor control\\nand cognitive centers of the brain. Low verbal ﬂuency relate s to CCs that\\nare thinner in regions connecting auditory/visual/cognit ive centers. The\\nsparse principal components procedure gives a more parsimo nious, and po-\\ntentially more informative picture of the important diﬀere nces.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 570}),\n",
       " Document(page_content='14.6 Non-negative Matrix Factorization 553\\n14.6 Non-negative Matrix Factorization\\nNon-negative matrix factorization (Lee and Seung, 1999) is a recent al-\\nternative approach to principal components analysis, in wh ich the data\\nand components are assumed to be non-negative. It is useful f or modeling\\nnon-negative data such as images.\\nTheN×pdata matrix Xis approximated by\\nX≈WH (14.72)\\nwhereWisN×randHisr×p,r≤max(N,p). We assume that\\nxij,wik,hkj≥0.\\nThe matrices WandHare found by maximizing\\nL(W,H) =N∑\\ni=1p∑\\nj=1[xijlog(WH)ij−(WH)ij]. (14.73)\\nThis is the log-likelihood from a model in which xijhas a Poisson dis-\\ntribution with mean ( WH)ij—quite reasonable for positive data.\\nThe following alternating algorithm (Lee and Seung, 2001) c onverges to\\na local maximum of L(W,H):\\nwik←wik∑p\\nj=1hkjxij/(WH)ij∑p\\nj=1hkj\\nhkj←hkj∑N\\ni=1wikxij/(WH)ij∑N\\ni=1wik(14.74)\\nThis algorithm can be derived as a minorization procedure fo r maximizing\\nL(W,H) (Exercise 14.23) and is also related to the iterative-prop ortional-\\nscaling algorithm for log-linear models (Exercise 14.24).\\nFigure 14.33 shows an example taken from Lee and Seung (1999)9, com-\\nparing non-negative matrix factorization (NMF), vector qu antization (VQ,\\nequivalentto k-meansclustering)andprincipalcomponentsanalysis(PCA ).\\nThe three learning methods were applied to a database of N= 2,429 fa-\\ncial images, each consisting of 19 ×19 pixels, resulting in a 2 ,429×381\\nmatrixX. As shown in the 7 ×7 array of montages (each a 19 ×19 image),\\neach method has learned a set of r= 49 basis images. Positive values are\\nillustrated with black pixels and negative values with red p ixels. A par-\\nticular instance of a face, shown at top right, is approximat ed by a linear\\nsuperposition of basis images. The coeﬃcients of the linear superposition\\nare shown next to each montage, in a 7 ×7 array10, and the resulting su-\\nperpositions are shown to the right of the equality sign. The authors point\\n9We thank Sebastian Seung for providing this image.\\n10These 7 ×7 arrangements allow for a compact display, and have no structural\\nsigniﬁcance.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 571}),\n",
       " Document(page_content='554 14. Unsupervised Learning\\nout that unlike VQ and PCA, NMF learns to represent faces with a set of\\nbasis images resembling parts of faces.\\nDonoho and Stodden (2004) point out a potentially serious pr oblem with\\nnon-negativematrixfactorization.Eveninsituationswhe reX=WHholds\\nexactly, the decomposition may not be unique. Figure 14.34 i llustrates the\\nproblem. The data points lie in p= 2 dimensions, and there is “open space”\\nbetween the data and the coordinate axes. We can choose the ba sis vectors\\nh1andh2anywhere in this open space, and represent each data point\\nexactly with a nonnegative linear combination of these vect ors. This non-\\nuniqueness means that the solution found by the above algori thm depends\\non the starting values, and it would seem to hamper the interp retability of\\nthe factorization. Despite this interpretational drawbac k, the non-negative\\nmatrix factorization and its applications has attracted a l ot of interest.\\n14.6.1 Archetypal Analysis\\nThis method, due to Cutler and Breiman (1994), approximates data points\\nby prototypes that are themselves linear combinations of da ta points. In\\nthis sense it has a similar ﬂavor to K-means clustering. However, rather\\nthan approximating each data point by a single nearby protot ype, archety-\\npal analysis approximates each data point by a convex combin ation of a\\ncollection of prototypes. The use of a convex combination fo rces the proto-\\ntypestolieontheconvexhullofthedatacloud.Inthissense ,theprototypes\\nare “pure,”, or “archetypal.”\\nAs in (14.72), the N×pdata matrix Xis modeled as\\nX≈WH (14.75)\\nwhereWisN×randHisr×p. We assume that wik≥0 and∑r\\nk=1wik=\\n1∀i. Hence the Ndata points (rows of X) inp-dimensional space are\\nrepresented by convex combinations of the rarchetypes (rows of H). We\\nalso assume that\\nH=BX (14.76)\\nwhereBisr×Nwithbki≥0 and∑N\\ni=1bki= 1∀k. Thus the archetypes\\nthemselves are convex combinations of the data points. Usin g both (14.75)\\nand (14.76) we minimize\\nJ(W,B) =||X−WH||2\\n=||X−WBX||2(14.77)\\nover the weights WandB. This function is minimized in an alternating\\nfashion, with each separate minimization involving a conve x optimization.\\nThe overall problem is not convex however, and so the algorit hm converges\\nto a local minimum of the criterion.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 572}),\n",
       " Document(page_content='14.6 Non-negative Matrix Factorization 555\\nVQ\\n×\\r =NMF\\n= ×\\r\\nPCA\\n= ×\\r\\nOriginal\\nFIGURE 14.33. Non-negative matrix factorization (NMF), vector quantiza tion\\n(VQ, equivalent to k-means clustering) and principal components analysis (PCA)\\napplied to a database of facial images. Details are given in the text. Unlike VQ\\nand PCA, NMF learns to represent faces with a set of basis image s resembling\\nparts of faces.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 573}),\n",
       " Document(page_content='556 14. Unsupervised Learning\\nh1\\nh2\\nFIGURE 14.34. Non-uniqueness of the non-negative matrix factorization.\\nThere are 11 data points in two dimensions. Any choice of the b asis vectors h1\\nandh2in the open space between the coordinate axes and data, gives an exact\\nreconstruction of the data.\\nFigure 14.35 shows an example with simulated data in two dime nsions.\\nThe top panel displays the results of archetypal analysis, w hile the bottom\\npanel shows the results from K-means clustering. In order to best recon-\\nstruct the data from convexcombinations of the prototypes, it pays to\\nlocate the prototypes on the convex hull of the data. This is s een in the top\\npanels of Figure 14.35 and is the case in general, as proven by Cutler and\\nBreiman (1994). K-means clustering, shown in the bottom panels, chooses\\nprototypes in the middle of the data cloud.\\nWe can think of K-means clustering as a special case of the archetypal\\nmodel, in which each row of Whas a single one and the rest of the entries\\nare zero.\\nNotice also that the archetypal model (14.75) has the same ge neral form\\nas the non-negative matrix factorization model (14.72). Ho wever, the two\\nmodels are applied in diﬀerent settings, and have somewhat d iﬀerent goals.\\nNon-negative matrix factorization aims to approximate the columns of the\\ndata matrix X, and the main output of interest are the columns of W\\nrepresenting the primary non-negative components in the da ta. Archetypal\\nanalysis focuses instead on the approximation of the rows of Xusing the\\nrowsofH,whichrepresentthearchetypaldatapoints.Non-negative matrix\\nfactorization also assumes that r≤p. Withr=p, we can get an exact\\nreconstruction simply choosing Wto be the data Xwith columns scaled\\nso that they sum to 1. In contrast, archetypal analysis requi resr≤N,\\nbut allows r > p. In Figure 14.35, for example, p= 2,N= 50 while\\nr= 2,4 or 8. The additional constraint (14.76) implies that the ar chetypal\\napproximation will not be perfect, even if r>p.\\nFigure 14.36 shows the results of archetypal analysis appli ed to the\\ndatabase of 3’s displayed in Figure 14.22. The three rows in F igure 14.36\\nare the resulting archetypes from three runs, specifying tw o, three and four', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 574}),\n",
       " Document(page_content='14.7IndependentComponentAnalysisandExploratoryProject ionPursuit 557\\n2 Prototypes 4 Prototypes 8 Prototypes\\nFIGURE 14.35. Archetypal analysis (top panels) and K-means clustering (bot-\\ntom panels) applied to 50data points drawn from a bivariate Gaussian distribu-\\ntion. The colored points show the positions of the prototypes in each case.\\narchetypes, respectively. As expected, the algorithm has p roduced extreme\\n3’s both in size and shape.\\n14.7 Independent Component Analysis and\\nExploratory Projection Pursuit\\nMultivariate data are often viewed as multiple indirect mea surements aris-\\ningfromanunderlyingsource,whichtypicallycannotbedir ectlymeasured.\\nExamples include the following:\\n•Educational and psychological tests use the answers to ques tionnaires\\nto measure the underlying intelligence and other mental abi lities of\\nsubjects.\\n•EEG brain scans measure the neuronal activity in various par ts of\\nthe brain indirectly via electromagnetic signals recorded at sensors\\nplaced at various positions on the head.\\n•The trading prices of stocks change constantly over time, an d reﬂect\\nvarious unmeasured factors such as market conﬁdence, exter nal in-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 575}),\n",
       " Document(page_content='558 14. Unsupervised Learning\\nFIGURE 14.36. Archetypal analysis applied to the database of digitized 3’s. The\\nrows in the ﬁgure show the resulting archetypes from three run s, specifying two,\\nthree and four archetypes, respectively.\\nﬂuences, and other driving forces that may be hard to identif y or\\nmeasure.\\nFactor analysis is a classical technique developed in the st atistical liter-\\nature that aims to identify these latent sources. Factor ana lysis models\\nare typically wed to Gaussian distributions, which has to so me extent hin-\\ndered their usefulness. More recently, independent compon ent analysis has\\nemerged as a strong competitor to factor analysis, and as we w ill see, relies\\non the non-Gaussian nature of the underlying sources for its success.\\n14.7.1 Latent Variables and Factor Analysis\\nThesingular-valuedecomposition X=UDVT(14.54) hasalatentvariable\\nrepresentation. Writing S=√\\nNUandAT=DVT/√\\nN, we have X=\\nSAT, and hence each of the columns of Xis a linear combination of the\\ncolumns of S. Now since Uis orthogonal, and assuming as before that the\\ncolumns of X(and hence U) each have mean zero, this implies that the\\ncolumns of Shave zero mean, are uncorrelated and have unit variance. In\\nterms of random variables, we can interpret the SVD, or the co rresponding\\nprincipal component analysis (PCA) as an estimate of a laten t variable\\nmodel', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 576}),\n",
       " Document(page_content='14.7IndependentComponentAnalysisandExploratoryProject ionPursuit 559\\nX1=a11S1+a12S2+···+a1pSp\\nX2=a21S1+a22S2+···+a2pSp\\n......\\nXp=ap1S1+ap2S2+···+appSp,(14.78)\\nor simplyX=AS. The correlated Xjare each represented as a linear\\nexpansion in the uncorrelated, unit variance variables Sℓ. This is not too\\nsatisfactory, though, because given any orthogonal p×pmatrixR, we can\\nwrite\\nX=AS\\n=ARTRS\\n=A∗S∗, (14.79)\\nand Cov(S∗) =RCov(S)RT=I. Hence there are many such decom-\\npositions, and it is therefore impossible to identify any pa rticular latent\\nvariables as unique underlying sources. The SVD decomposit ion does have\\nthe property that any rank q < ptruncated decomposition approximates\\nXin an optimal way.\\nTheclassical factor analysis model,developedprimarilybyresearchersin\\npsychometrics, alleviates these problems to some extent; s ee, for example,\\nMardia et al. (1979). With q<p, a factor analysis model has the form\\nX1=a11S1+···+a1qSq+ε1\\nX2=a21S1+···+a2qSq+ε2\\n......\\nXp=ap1S1+···+apqSq+εp,(14.80)\\norX=AS+ε. HereSis a vector of q <punderlying latent variables or\\nfactors,Ais ap×qmatrix of factor loadings, and theεjare uncorrelated\\nzero-mean disturbances. The idea is that the latent variabl esSℓare com-\\nmon sources of variation amongst the Xj, and account for their correlation\\nstructure, while the uncorrelated εjare unique to each Xjand pick up the\\nremaining unaccounted variation. Typically the Sℓand theεjare modeled\\nas Gaussian random variables, and the model is ﬁt by maximum l ikelihood.\\nThe parameters all reside in the covariance matrix\\nΣ=AAT+Dε, (14.81)\\nwhereDε= diag[Var( ε1),...,Var(εp)]. TheSℓbeing Gaussian and un-\\ncorrelated makes them statistically independent random va riables. Thus a\\nbattery of educational test scores would be thought to be dri ven by the\\nindependent underlying factors such as intelligence ,driveand so on. The\\ncolumns of Aare referred to as the factor loadings , and are used to name\\nand interpret the factors.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 577}),\n",
       " Document(page_content='560 14. Unsupervised Learning\\nUnfortunately the identiﬁability issue (14.79) remains, s inceAandART\\nare equivalent in (14.81) for any q×qorthogonal R. This leaves a certain\\nsubjectivity in the use of factor analysis, since the user ca n search for ro-\\ntated versions of the factors that are more easily interpret able. This aspect\\nhas left many analysts skeptical of factor analysis, and may account for its\\nlack of popularity in contemporary statistics. Although we will not go into\\ndetails here, the SVD plays a key role in the estimation of (14 .81). For ex-\\nample, if the Var( εj) are all assumed to be equal, the leading qcomponents\\nof the SVD identify the subspace determined by A.\\nBecause of the separate disturbances εjfor eachXj, factor analysis can\\nbe seen to be modeling the correlation structure of the Xjrather than the\\ncovariance structure. This can be easily seen by standardiz ing the covari-\\nance structure in (14.81) (Exercise 14.14). This is an impor tant distinction\\nbetween factor analysis and PCA, although not central to the discussion\\nhere. Exercise 14.15 discusses a simple example where the so lutions from\\nfactor analysis and PCA diﬀer dramatically because of this d istinction.\\n14.7.2 Independent Component Analysis\\nThe independent component analysis (ICA) model has exactly the same\\nform as (14.78), except the Sℓare assumed to be statistically indepen-\\ndentrather than uncorrelated. Intuitively, lack of correlatio n determines\\nthe second-degree cross-moments (covariances) of a multiv ariate distribu-\\ntion, while in general statistical independence determine s all of the cross-\\nmoments. These extra moment conditions allow us to identify the elements\\nofAuniquely. Since the multivariate Gaussian distribution is determined\\nby its second moments alone, it is the exception, and any Gaus sian inde-\\npendent components can be determined only up to a rotation, a s before.\\nHence identiﬁability problems in (14.78) and (14.80) can be avoided if we\\nassume that the Sℓare independent and non-Gaussian .\\nHere we will discuss the full p-component model as in (14.78), where the\\nSℓare independent with unit variance; ICA versions of the fact or analysis\\nmodel (14.80) exist as well. Our treatment is based on the sur vey article\\nby Hyv¨ arinen and Oja (2000).\\nWe wish to recover the mixing matrix AinX=AS. Without loss\\nof generality, we can assume that Xhas already been whitened to have\\nCov(X) =I; this is typically achieved via the SVD described above. Thi s\\nin turn implies that Ais orthogonal, since Salso has covariance I. So\\nsolving the ICA problem amounts to ﬁnding an orthogonal Asuch that\\nthe components of the vector random variable S=ATXare independent\\n(and non-Gaussian).\\nFigure 14.37 shows the power of ICA in separating two mixed si gnals.\\nThis is an example of the classical cocktail party problem , where diﬀer-\\nent microphones Xjpick up mixtures of diﬀerent independent sources Sℓ\\n(music, speech from diﬀerent speakers, etc.). ICA is able to perform blind', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 578}),\n",
       " Document(page_content='14.7IndependentComponentAnalysisandExploratoryProject ionPursuit 561\\nSource Signals Measured Signals\\nPCA Solution ICA Solution\\nFIGURE 14.37. Illustration of ICA vs. PCA on artiﬁcial time-series data. Th e\\nupper left panel shows the two source signals, measured at 1000uniformly spaced\\ntime points. The upper right panel shows the observed mixed s ignals. The lower\\ntwo panels show the principal components and independent com ponent solutions.\\nsource separation , by exploiting the independence and non-Gaussianity of\\nthe original sources.\\nMany of the popular approaches to ICA are based on entropy. Th e dif-\\nferential entropy Hof a random variable Ywith density g(y) is given by\\nH(Y) =−∫\\ng(y)logg(y)dy. (14.82)\\nA well-known result in information theory says that among al l random\\nvariables with equal variance, Gaussian variables have the maximum en-\\ntropy. Finally, the mutual information I(Y) between the components of the\\nrandom vector Yis a natural measure of dependence:\\nI(Y) =p∑\\nj=1H(Yj)−H(Y). (14.83)\\nThe quantity I(Y) is called the Kullback–Leibler distance between the\\ndensityg(y) ofYand its independence version∏p\\nj=1gj(yj), wheregj(yj)\\nis the marginal density of Yj. Now ifXhas covariance I, andY=ATX\\nwithAorthogonal, then it is easy to show that\\nI(Y) =p∑\\nj=1H(Yj)−H(X)−log|detA| (14.84)\\n=p∑\\nj=1H(Yj)−H(X). (14.85)\\nFinding an Ato minimize I(Y) =I(ATX) looks for the orthogonal trans-\\nformation that leads to the most independence between its co mponents. In', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 579}),\n",
       " Document(page_content='562 14. Unsupervised Learning\\n*\\n**\\n**\\n**\\n**\\n**\\n**\\n*\\n*\\n**\\n**\\n**\\n*\\n**\\n**\\n****\\n***\\n*\\n**\\n*\\n**\\n*\\n*\\n**\\n** ***\\n**\\n**\\n**\\n**\\n**\\n**\\n***\\n*\\n***\\n*****\\n**\\n**\\n*\\n**\\n**\\n*\\n**\\n*\\n***\\n*****\\n**\\n* **\\n*\\n***\\n* **\\n***\\n** **\\n**\\n****\\n*\\n******\\n***\\n*\\n**\\n*\\n* ***\\n***\\n**\\n**\\n**\\n**\\n**\\n***\\n*\\n***\\n** *\\n**\\n***\\n****\\n** *\\n**\\n*\\n**\\n**\\n*\\n**\\n**\\n*\\n* **\\n***\\n***\\n******\\n***\\n**\\n**\\n******\\n*\\n*****\\n***\\n**\\n*\\n*****\\n**\\n***\\n*\\n***\\n*\\n***\\n**\\n***\\n*\\n***\\n*\\n**\\n*\\n*\\n**\\n*\\n***\\n**\\n*\\n**\\n****\\n***\\n*\\n*\\n**\\n***\\n**\\n**\\n***\\n***\\n*\\n****\\n**\\n***\\n*\\n**\\n**\\n*\\n*\\n*\\n****\\n**\\n***\\n**\\n**\\n***\\n*\\n***\\n****\\n**\\n**\\n** *\\n***\\n**\\n* ***\\n***\\n***\\n*\\n**\\n**\\n****\\n**\\n*\\n*\\n***\\n*\\n***\\n**\\n**\\n**\\n***\\n* **\\n**\\n**\\n*\\n***\\n*\\n*\\n***\\n*\\n***\\n**\\n****\\n*\\n***\\n*\\n**\\n*\\n**\\n****\\n**\\n*\\n***\\n*****\\n**\\n**\\n** **\\n**\\n***\\n*** **\\n***\\n***\\n***\\n***\\n**\\n**\\n**\\n*\\n** **\\n*\\n**\\n**\\n**\\n**\\n*\\n**\\n*\\n***\\n***\\n***\\n**Source S\\n**\\n*\\n**\\n**\\n****\\n**\\n*\\n*****\\n****\\n*\\n**\\n***\\n***\\n**\\n***\\n*\\n**\\n*\\n**\\n****\\n*\\n**\\n*\\n***\\n*\\n***\\n**\\n***\\n*\\n***\\n***\\n****\\n****\\n*\\n**\\n*\\n**\\n*****\\n*\\n*\\n****\\n***\\n*\\n*\\n***\\n**\\n****\\n**\\n***\\n***\\n*\\n*\\n***\\n*\\n**\\n*\\n**\\n**\\n***\\n***\\n***\\n****\\n****\\n**\\n***\\n**\\n**\\n**\\n***\\n***\\n****\\n***\\n**\\n**\\n* **\\n*\\n****\\n***\\n***\\n*\\n***\\n*\\n**\\n***\\n***\\n**\\n**\\n**\\n****\\n**\\n*\\n****\\n*\\n*\\n**\\n**\\n**\\n***\\n*\\n***\\n**\\n**\\n****\\n*\\n*\\n***\\n****\\n*\\n***\\n****\\n*\\n***\\n**\\n**\\n*\\n**\\n*\\n***\\n**\\n**\\n*****\\n*\\n***\\n*\\n****\\n******\\n*\\n**\\n**\\n**\\n*\\n*\\n**\\n****\\n***\\n**\\n****\\n***\\n**\\n**\\n****\\n*\\n***\\n***\\n***\\n**\\n*\\n***\\n***\\n***\\n*\\n***\\n****\\n*\\n***\\n**\\n******\\n*\\n*****\\n**\\n*\\n*\\n**\\n*\\n*\\n**\\n*\\n***\\n**\\n***\\n***\\n*\\n**\\n**\\n**\\n***\\n*\\n******\\n****\\n*\\n**\\n***\\n****\\n*\\n**\\n**\\n***\\n* *\\n**\\n***\\n****\\n******\\n**\\n***\\n*\\n**\\n**\\n***\\n****\\n*\\n**\\n**\\n**\\n**\\n***\\n****\\n*\\n**\\n*\\n**\\n**Data X\\n*\\n***\\n***\\n*\\n*\\n***\\n***\\n**\\n**\\n***\\n**\\n*** ***\\n*\\n**\\n* **\\n***\\n*\\n***\\n**\\n***\\n****\\n**\\n**\\n***\\n***\\n**\\n***\\n*\\n****\\n****\\n*\\n** *\\n***\\n**\\n**\\n*****\\n*\\n***\\n***\\n***\\n***\\n***\\n**\\n**\\n*\\n*\\n*****\\n*\\n*\\n****\\n**\\n*\\n*\\n**\\n*\\n**\\n****\\n***\\n***\\n*\\n***\\n*\\n**\\n**\\n****\\n**\\n**\\n**\\n**\\n*** **\\n*\\n***\\n**\\n***\\n**\\n***\\n*\\n**\\n*\\n**\\n* *\\n*\\n**\\n**\\n**\\n**\\n**\\n***\\n***\\n****\\n****\\n*\\n***\\n***\\n**\\n**\\n*\\n**\\n*\\n***\\n**\\n**\\n*\\n*\\n***\\n*\\n** *\\n***\\n***\\n**\\n**\\n*\\n***\\n**\\n**\\n*\\n**\\n**\\n* *\\n**\\n** *\\n***\\n**\\n**\\n*\\n***\\n**\\n*\\n****\\n**\\n**\\n*\\n*\\n**\\n**\\n**\\n*******\\n** *\\n**\\n***\\n***\\n***\\n**\\n**\\n**\\n*\\n* **\\n**\\n***\\n*\\n***\\n*\\n*******\\n**\\n**\\n**\\n**\\n**\\n*\\n*\\n** *\\n*\\n*****\\n**\\n**\\n*\\n**\\n***\\n**\\n***\\n**\\n**\\n** ***\\n****\\n*\\n****\\n*\\n*\\n***\\n**\\n*\\n****\\n*\\n***\\n**\\n***\\n****\\n*\\n*\\n*\\n***\\n**\\n*****\\n***\\n*****\\n*\\n***\\n**\\n*\\n**\\n*\\n*****\\n**\\n*\\n*****\\n**\\n*\\n*\\n***\\n***\\n***\\n***\\n***\\n**PCA Solution\\n*\\n**\\n**\\n**\\n**\\n**\\n* *\\n*\\n*\\n**\\n**\\n**\\n*\\n**\\n**\\n****\\n***\\n*\\n**\\n*\\n**\\n*\\n*\\n**\\n*****\\n**\\n**\\n**\\n**\\n**\\n**\\n***\\n*\\n***\\n*** **\\n**\\n**\\n*\\n**\\n**\\n*\\n**\\n*\\n***\\n**** *\\n**\\n*\\n**\\n*\\n***\\n* **\\n***\\n** **\\n**\\n****\\n*\\n** ****\\n***\\n*\\n**\\n*\\n** **\\n***\\n**\\n**\\n*\\n*\\n**\\n**\\n**\\n*\\n*\\n***\\n** *\\n**\\n***\\n****\\n***\\n**\\n*\\n**\\n**\\n*\\n**\\n**\\n*\\n* **\\n***\\n***\\n*** ***\\n***\\n**\\n**\\n******\\n*\\n** ***\\n***\\n***\\n*****\\n**\\n***\\n*\\n***\\n*\\n***\\n**\\n***\\n*\\n***\\n*\\n**\\n*\\n*\\n**\\n*\\n***\\n**\\n*\\n**\\n* ***\\n***\\n*\\n*\\n**\\n***\\n**\\n**\\n***\\n***\\n*\\n****\\n**\\n***\\n*\\n**\\n**\\n*\\n*\\n*\\n****\\n**\\n***\\n**\\n**\\n***\\n*\\n***\\n****\\n**\\n**\\n** *\\n***\\n* *\\n****\\n***\\n***\\n*\\n**\\n**\\n*** *\\n**\\n*\\n*\\n***\\n*\\n***\\n**\\n**\\n**\\n***\\n* **\\n**\\n**\\n*\\n***\\n*\\n* ***\\n*\\n*****\\n****\\n*\\n***\\n*\\n**\\n*\\n**\\n****\\n**\\n*\\n***\\n*\\n****\\n**\\n**\\n****\\n**\\n***\\n* ** **\\n***\\n***\\n** *\\n*\\n**\\n**\\n**\\n**\\n*\\n****\\n*\\n**\\n**\\n**\\n**\\n*\\n**\\n*\\n***\\n***\\n***\\n**ICA Solution\\nFIGURE 14.38. Mixtures of independent uniform random variables. The upper\\nleft panel shows 500realizations from the two independent uniform sources, the\\nupper right panel their mixed versions. The lower two panels sh ow the PCA and\\nICA solutions, respectively.\\nlight of (14.84) this is equivalent to minimizing the sum of t he entropies of\\nthe separate components of Y, which in turn amounts to maximizing their\\ndepartures from Gaussianity.\\nFor convenience, rather than using the entropy H(Yj), Hyv¨ arinen and\\nOja (2000) use the negentropy measureJ(Yj) deﬁned by\\nJ(Yj) =H(Zj)−H(Yj), (14.86)\\nwhereZjis a Gaussian random variable with the same variance as Yj. Ne-\\ngentropy is non-negative, and measures the departure of Yjfrom Gaussian-\\nity. They propose simple approximations to negentropy whic h can be com-\\nputed and optimized on data. The ICA solutions shown in Figur es 14.37–\\n14.39 use the approximation\\nJ(Yj)≈[EG(Yj)−EG(Zj)]2, (14.87)\\nwhereG(u) =1\\nalogcosh(au) for 1≤a≤2. When applied to a sample', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 580}),\n",
       " Document(page_content='puted and optimized on data. The ICA solutions shown in Figur es 14.37–\\n14.39 use the approximation\\nJ(Yj)≈[EG(Yj)−EG(Zj)]2, (14.87)\\nwhereG(u) =1\\nalogcosh(au) for 1≤a≤2. When applied to a sample\\nofxi, the expectations are replaced by data averages. This is one of the\\noptions in the FastICA software provided by these authors. More classical\\n(andlessrobust)measuresarebasedonfourthmoments,andh encelookfor\\ndepartures from the Gaussian via kurtosis. See Hyv¨ arinen a nd Oja (2000)\\nfor more details. In Section 14.7.4 we describe their approx imate Newton\\nalgorithm for ﬁnding the optimal directions.\\nIn summary then, ICA applied to multivariate data looks for a sequence\\nof orthogonal projections such that the projected data look as far from', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 580}),\n",
       " Document(page_content='14.7IndependentComponentAnalysisandExploratoryProject ionPursuit 563\\nComponent\\n 1oooooo o\\nooo\\noo\\noo\\noo\\no\\noooooo\\noo o\\noooo ooo\\nooooooo\\noo\\no\\noo\\noooooo\\noo\\nooo ooooo\\nooooooooo\\nooooooooooo\\noooo\\noo o\\noo\\noooooooooo\\noo\\no\\noooooooo\\nooo\\noooooooooo\\noooooo\\noooooo\\noooooo\\noo\\nooo\\nooo ooo\\nooooo\\no\\noo\\nooooooooooo\\noooo\\nooooo\\noooooooo\\nooo\\noo\\nooooo\\noo\\nooo\\nooooo\\nooooooo\\nooo\\noooo\\noo\\noooo\\noooooo\\noo\\nooo\\noooo\\noooooooooooooo\\noooooooooooo\\noooooo\\nooo\\nooo\\noo\\noo\\nooooo\\nooooooo\\nooo\\noooo\\nooooo\\noo\\noooo\\nooo ooo\\nooooo\\noooooooo\\nooooooooooo\\nooo\\nooo\\noooo\\noo\\noo\\nooooo\\nooooo\\nooo\\noo\\noo\\noo\\nooooooooo\\nooooooo\\nooooo\\noo\\noooo ooooo\\noo\\no\\noooo\\nooo\\noo\\noooooooo\\nooo\\noo\\nooo\\nooo\\noooo\\nooo\\noooo\\nooo\\noooooooooo\\nooooo\\nooo\\nooooo\\no\\noo\\nooo\\noo\\no\\noooooooo\\nooo\\noooooo oooooooo\\nooooooo\\nooooo oo\\noo\\noooo\\noooo\\nooooo o\\noooooooo\\nooooo\\noo\\nooooooo\\nooo\\noooo\\noooo\\no\\nooo\\noo ooo o\\nooo\\nooooo\\nooo\\nooooo\\noooooooo\\noo\\no\\nooo\\noo ooooooooo\\nooooooo\\noo\\noo\\noo\\nooo oo\\no\\nooooo\\noo\\noooo\\noooooooo\\nooo\\noo\\noo\\noo\\no\\noooooo\\nooo\\nooooooo\\nooooooo\\noo\\no\\noo\\noooooo\\noo\\nooo ooooo\\nooooooooo\\nooooooooooo\\noooo\\noo o\\noo\\noooooooooo\\noo\\no\\noooooooo\\nooo\\noooooooooo\\noooooo\\noooooo\\noooooo\\noo\\nooo\\nooo ooo\\nooooo\\no\\noo\\nooooooooooo\\noooo\\noooo o\\noooooooo\\nooo\\noo\\nooooo\\noo\\nooo\\nooooo\\nooooooo\\nooo\\noooo\\noo\\noooo\\noooooo\\noo\\nooo\\noooo\\noooooo oooooooo\\noooooooooooo\\noooooo\\nooo\\nooo\\noo\\noo\\nooooo\\nooooooo\\nooo\\noooo\\nooooo\\noo\\noooo\\noooooo\\nooooo\\noooooooo\\nooooooooooo\\nooo\\nooo\\noooo\\noo\\noo\\nooooo\\nooooo\\nooo\\noo\\noo\\noo\\nooooooooo\\nooooooo\\nooooo\\noo\\nooooooooo\\noo\\no\\noooo\\nooo\\noo\\noooooooo\\nooo\\noo\\nooo\\nooo\\noooo\\nooo\\noooo\\noo o\\noooooooooo\\nooooo\\nooo\\nooooo\\no\\noo\\nooo\\noo\\no\\noooooooo\\nooo\\noooo o o oooooooo\\nooooooo\\nooooo oo\\noo\\noooo\\noooo\\nooooo o\\noooooooo\\nooooo\\noo\\nooooooo\\nooo\\noooo\\noooo\\no\\nooo\\noo oooo\\no oo\\nooooo\\nooo\\nooooo\\noooooooo\\noo\\no\\nooo\\noo ooooooooo\\nooooooo\\noo\\noo\\noo\\nooooo\\no\\nooooo\\noo\\noooo\\nooooooo o\\nooo\\noo\\noo\\noo\\no\\noooooo\\noo o\\nooooooo\\nooooooo\\noo\\no\\noo\\noooooo\\noo\\noooooooo\\nooooooooo\\nooooooooooo\\noooo\\noo o\\noo\\nooooo o oooo\\noo\\no\\noooooooo\\nooo\\noooo oooooo\\noooooo\\noooooo\\noooooo\\noo\\nooo\\nooo ooo\\nooooo\\no\\noo\\noooooooo ooo\\noooo\\nooooo\\noooooooo\\nooo\\noo\\nooooo\\noo\\nooo\\nooooo\\nooooooo\\nooo\\noooo\\noo\\noooo\\noooooo\\noo\\nooo\\noooo\\noooooo oooooooo\\noooooooooooo\\noooooo\\nooo\\nooo\\noo\\noo\\nooooo\\nooooooo\\nooo\\noooo\\nooooo\\noo\\noooo\\nooo ooo\\nooooo\\noooooooo\\nooo oooooooo\\nooo\\nooo\\noooo\\noo\\noo\\nooooo\\nooooo\\nooo\\noo\\noo\\noo\\nooooooooo\\nooooooo\\nooooo\\noo\\noooo ooooo\\noo\\no\\noooo\\nooo\\noo\\noooooooo\\nooo\\noo\\nooo\\nooo\\noooo\\nooo\\noooo\\nooo\\noooooooooo\\nooooo\\nooo\\nooooo\\no\\noo\\nooo\\noo\\no\\noooooooo\\nooo\\nooooo o oooooooo\\nooooooo\\nooooo oo\\noo\\noooo\\noooo\\noooooo\\noooooooo\\nooooo\\noo\\nooooooo\\nooo\\noooo\\no ooo\\no\\nooo\\noo ooo o\\nooo\\nooooo\\nooo\\nooooo\\noooooooo\\noo\\no\\nooo\\noo ooooooooo\\nooooooo\\noo\\noo\\noo\\nooooo\\no\\nooooo\\noo\\noooo\\nooooooo o\\nooo\\noo\\noo\\noo\\no\\noooooo\\noo o\\noooo ooo\\nooooooo\\noo\\no\\noo\\noooooo\\noo\\nooo ooooo\\nooooooooo\\nooooooooooo\\noooo\\noo o\\noo\\noooooooooo\\noo\\no\\noooooooo\\nooo\\noooo oooooo\\noooo oo\\noooooo\\nooo ooo\\noo\\nooo\\nooo ooo\\nooooo\\no\\noo\\nooooooooooo\\noooo\\nooooo\\noooooooo\\nooo\\noo\\nooooo\\noo\\nooo\\nooooo\\nooooooo\\nooo\\noooo\\noo\\noooo\\noooooo\\noo\\nooo\\noooo\\nooo ooo oooooooo\\noooooooooooo\\noooooo\\nooo\\nooo\\noo\\noo\\nooooo\\nooooooo\\nooo\\noooo\\nooooo\\noo\\noooo\\nooo ooo\\nooooo\\noooooooo\\nooooooooooo\\nooo\\nooo\\noooo\\noo\\noo\\nooooo\\nooooo\\nooo\\noo\\noo\\noo\\nooooooooo\\nooooooo\\nooooo\\noo\\noooo ooooo\\noo\\no\\noooo\\nooo\\noo\\noooooooo\\nooo\\noo\\nooo\\nooo\\no ooo\\noo o\\noooo\\nooo\\noooooooooo\\nooooo\\nooo\\nooooo\\no\\noo\\nooo\\noo\\no\\noooooooo\\nooo\\noooooooooooooo\\nooooooo\\nooooo oo\\noo\\noooo\\no ooo\\noooooo\\noooooooo\\nooooo\\noo\\nooooooo\\nooo\\noooo\\noooo\\no\\nooo\\noo oooo\\nooo\\nooooo\\nooo\\nooooo\\noooooooo\\noo\\no\\noo o\\noo ooooooooo\\nooooooo\\noo\\noo\\noo\\nooooo\\no\\nooooo\\noo\\noooo\\no\\noo\\noo\\noo\\noo\\nooooo\\noo\\nooooo\\noooo\\nooo\\noooo\\noo\\noooo\\nooo\\nooo\\noo\\noo\\nooooo\\nooo\\noo\\noooo\\nooo\\nooooo\\noo\\no\\noo\\noooooo\\no\\noo\\noo oo\\no\\noo\\noooooo\\no\\nooo\\noooo\\nooooo\\no\\nooo\\no\\noo\\noo\\noo\\noo\\nooo\\noooo\\nooo\\noo\\nooooo\\no\\noooo\\noo\\nooo\\nooo\\no\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooooo oooo\\noo\\nooo\\nooo\\nooo\\nooo\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\no\\noo oo\\noooo\\noooo\\nooo\\noo\\no\\nooo\\nooo\\no\\no\\no\\nooo\\no\\nooo\\nooooo\\noooo\\noo\\noooo o\\noo\\noo\\noooooo\\noo\\noo\\noo\\nooooo\\noooo\\noooo\\nooo\\noo\\noooo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noooo\\noooo\\nooooo\\noo\\noooooo\\no\\nooooo\\nooooo\\nooooo\\noooo\\noo\\noooo\\noooo\\no\\no\\noooo oooooo\\nooo\\no\\noooo\\nooo\\noo\\noo o\\noooo\\nooo\\noo\\noo\\no\\noo\\no\\noo\\nooo\\nooo\\noooo\\noooo\\nooooo\\noooo\\nooo\\no\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 581}),\n",
       " Document(page_content='oo\\noooo\\noooo\\nooooo\\noo\\noooooo\\no\\nooooo\\nooooo\\nooooo\\noooo\\noo\\noooo\\noooo\\no\\no\\noooo oooooo\\nooo\\no\\noooo\\nooo\\noo\\noo o\\noooo\\nooo\\noo\\noo\\no\\noo\\no\\noo\\nooo\\nooo\\noooo\\noooo\\nooooo\\noooo\\nooo\\no\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noooo\\noo\\no\\nooo\\noo\\no\\noooo\\noo\\noo\\noo\\noo\\noo\\noo\\noo\\noooo\\no\\noooo\\noooo\\nooo\\nooo\\nooo\\noooooo\\noooooo\\noo\\nooo\\noooo\\no\\nooooooo\\noo\\noo\\no\\nooo\\noo\\noooooo\\no\\no\\nooooo\\nooo\\noo\\noooo\\nooooo\\noo\\nooooooo\\noooo\\nooo\\noo ooo\\nooo\\nooo\\noooo\\nooooo\\nooo\\nooo\\nooo\\nooo\\noo\\nooo\\nooooo\\noo\\no\\noo\\noo\\nooo\\noooooo\\noo\\no\\noo\\nooo\\nComponent\\n 2\\noo oo\\nooooo\\nooo\\noooo\\nooooooo\\noo\\noooo\\noooo\\noooo\\noooo\\noooo ooo ooooooo\\nooooooooooo\\nooo\\noo\\noooo\\noo\\noooo\\noo oooo o\\noo\\noooooooooo\\nooo\\no\\noo\\nooooooooooooooo\\no o\\noo o\\nooooo\\noooo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\no\\no oooooooooooo\\noooo\\no\\nooo\\noo\\nooooo oo\\noo\\no\\nooo\\noooo\\nooo\\noo\\noo\\no\\noooo\\noooooo\\nooooo\\noooo\\noooooo\\nooooo\\nooo\\noooo\\nooooo\\nooooo\\noooo\\no oooo\\nooo\\nooooooo\\noo\\no\\noooo\\nooooo\\noo\\noo\\nooo\\noo\\noooo\\noo\\noo\\nooo ooooo\\noooooo\\noo\\nooooo\\nooooo\\nooo\\noo\\nooo\\nooooo\\noooooo\\nooooo\\nooooo\\noooooo\\noooo\\noooooooo\\no\\noo oooooo\\no\\nooooooooo\\noooooooo\\noooo\\nooo\\noo\\noo\\noooo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\noo\\noooooo\\noo\\nooooo\\nooooo\\noooo oooo\\noo\\noo\\nooooo\\noo\\noooooooo\\noooo\\noo\\noooo\\no\\noo\\no oooooo\\noooooooo\\nooooooooooo\\noo\\noooo\\no\\nooooo\\nooo\\no\\noooooo\\noooooo\\nooo\\nooo\\noo oooooo ooo\\nooo\\no\\nooo\\nooo\\noooooo\\noo\\noooo\\nooo\\noo oooo\\nooo\\noooooo\\noooo\\nooooo\\noooooo\\no oo\\nooooo\\noooo\\no\\noooo\\noo\\noo\\nooooo\\noo\\nooooo oooo\\no\\no\\noo\\noooo oo\\nooooo\\nooo\\noooo\\nooooooo\\noo\\noooo\\noooo\\noooo\\noooo\\noooo ooo ooooooo\\nooooooooooo\\nooo\\noo\\noooo\\noo\\noooo\\noo ooooo\\noo\\nooooo ooooo\\nooo\\no\\noo\\noo ooooooooooooo\\no o\\noo o\\nooooo\\noooo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\no\\no oooooooooooo\\noooo\\no\\nooo\\noo\\nooooo oo\\noo\\no\\nooo\\noooo\\nooo\\noo\\noo\\no\\noooo\\noooooo\\nooooo\\noooo\\noooooo\\nooooo\\nooo\\noooo\\nooooo\\nooo oo\\noooo\\nooooo\\nooo\\nooooooo\\noo\\no\\noooo\\nooooo\\noo\\noo\\nooo\\noo\\noooo\\noo\\noo\\nooo ooooo\\noooooo\\noo\\nooooo\\nooooo\\nooo\\noo\\nooo\\nooooo\\noo o ooo\\nooo oo\\nooo oo\\noooooo\\noooo\\noooooooo\\no\\noooooooo\\no\\nooooooooo\\noooooooo\\noooo\\nooo\\noo\\noo\\noooo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\noo\\noooooo\\noo\\nooooo\\no oooo\\noooooooo\\noo\\noo\\nooooo\\noo\\noooooooo\\noooo\\noo\\noooo\\no\\noo\\no oooooo\\noooooooo\\nooooooooooo\\noo\\noooo\\no\\nooooo\\nooo\\no\\noooooo\\noooooo\\nooo\\nooo\\noooooooo ooo\\nooo\\no\\nooo\\nooo\\noooooo\\noo\\noooo\\nooo\\noooooo\\nooo\\noooooo\\noooo\\nooooo\\noooooo\\no oo\\nooooo\\noooo\\no\\noooo\\noo\\noo\\nooooo\\noo\\nooooooooo\\no\\no\\noo\\noooo oo\\nooooo\\nooo\\noooo\\nooooooo\\no o\\noooo\\noooo\\noooo\\noooo\\noooo ooo ooooooo\\nooooooooooo\\nooo\\noo\\noooo\\noo\\noooo\\noo oo ooo\\noo\\noooooooooo\\nooo\\no\\noo\\noo ooooooooooooo\\no o\\noo o\\nooooo\\noooo\\nooo\\noo oooo\\no\\noo\\noo\\nooo\\no\\no oooo oooooooo\\noooo\\no\\nooo\\noo\\nooooo oo\\noo\\no\\nooo\\noooo\\nooo\\noo\\noo\\no\\noooo\\noooooo\\nooooo\\noooo\\noooooo\\nooooo\\nooo\\noooo\\nooooo\\nooooo\\noooo\\nooooo\\nooo\\nooooooo\\noo\\no\\noooo\\nooooo\\noo\\noo\\nooo\\noo\\noooo\\noo\\noo\\nooo ooooo\\noooooo\\noo\\nooooo\\nooooo\\nooo\\noo\\nooo\\nooooo\\noooooo\\nooo oo\\nooo oo\\noooooo\\noooo\\noooooooo\\no\\noooooooo\\no\\nooooooooo\\noooooooo\\noooo\\nooo\\noo\\noo\\noooo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\noo\\noooooo\\noo\\nooooo\\noo oo o\\noooooooo\\noo\\noo\\nooooo\\noo\\noooooooo\\noooo\\noo\\noooo\\no\\noo\\no oooooo\\noooooooo\\nooooooooooo\\noo\\noooo\\no\\nooooo\\nooo\\no\\nooo ooo\\noooooo\\nooo\\nooo\\noooooooo ooo\\nooo\\no\\nooo\\nooo\\noooooo\\noo\\noooo\\nooo\\noooooo\\nooo\\noooooo\\noooo\\nooooo\\noooooo\\no oo\\nooooo\\noooo\\no\\noooo\\noo\\noo\\nooooo\\noo\\nooooooooo\\no\\no\\noo\\noo\\no\\noooooo\\nooo\\nooo\\no\\no oo\\nooo\\no\\nooo oooo\\noo\\noo o\\nooooo\\noo\\noo\\no\\noooooo\\no\\nooo\\nooo\\nooo\\noooo\\nooo\\nooo\\nooooo\\noo\\nooo\\noo\\nooooooo\\no o\\noo\\no\\nooo\\noo\\noo\\noo\\no\\nooo\\nooooo\\noo\\noo\\nooo\\no\\nooo\\nooo o\\no oooo\\noo\\no\\noo\\noo\\no\\nooo\\no\\nooo\\noo\\noooo\\nooo ooo\\nooo\\no\\noo\\noo\\nooo\\nooo o\\noo\\noo\\no\\nooo\\no\\noo\\nooooo o\\nooo\\noo\\nooo\\no\\noo\\nooo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooooo\\no\\no\\noooo\\nooo\\noooo\\noo\\nooo\\noo\\no\\nooooo\\nooo\\noo\\no\\no\\noo\\noo\\no\\noo\\noo\\noooooooo\\nooooo\\nooooo\\no\\noooo\\noo\\nooooo\\noo\\nooooo\\noooo\\nooo\\noo\\nooo\\no\\noo\\no\\noooo\\nooo\\noo\\noo\\noooo\\noo\\nooo\\no\\nooo\\noo\\noooooo\\noo\\noooo\\noo\\nooo\\noooo\\nooooo\\noooo\\no\\noooo\\nooooo\\noo\\no\\noooo\\noo\\noo\\no\\no\\noooo\\noo\\nooo\\no\\noo\\noo\\no\\noooooo\\no\\no\\nooooo\\nooo\\noooo\\no\\no\\nooo\\nooo\\noooooooooo\\noo\\nooo\\noo\\noooooo\\nooo\\noooo\\noo\\nooo\\nooo\\noooooo\\noo\\no\\noooooo\\no\\noo\\no\\nooo\\no\\nooo\\no\\noo oo\\noo\\nooo\\nooo\\no\\noo\\nooo\\nooooo\\noo\\nooo\\no\\noooo\\no\\noooo\\noo\\no\\noo\\nooo\\nooo\\no\\noooo o\\noo\\noo\\nooo\\noo\\nooo\\nooo\\nooooo\\no\\nooooooo\\noooo\\nooo\\nooooo\\noo\\noo\\nooooooo\\noo o\\noo\\no\\nooo\\noo o\\noo\\no\\noo\\noooooo\\noo\\no\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\no\\noooo\\noooooo\\nooo\\nooo\\no\\nooo\\nooo\\no\\nooooooo\\noo\\noo o\\nooooo\\noo\\noo\\no\\nooooo oo\\nooo\\nooo\\nooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 581}),\n",
       " Document(page_content='ooo\\nooooo\\no\\nooooooo\\noooo\\nooo\\nooooo\\noo\\noo\\nooooooo\\noo o\\noo\\no\\nooo\\noo o\\noo\\no\\noo\\noooooo\\noo\\no\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\no\\noooo\\noooooo\\nooo\\nooo\\no\\nooo\\nooo\\no\\nooooooo\\noo\\noo o\\nooooo\\noo\\noo\\no\\nooooo oo\\nooo\\nooo\\nooo\\noooo\\nooo\\nooo\\nooooo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\no\\nooo\\noo\\noo\\noo\\no\\nooo\\noo ooo\\noo\\noo\\nooo\\no\\nooo\\nooo o\\no oooo\\noo\\no\\noo\\noo\\no\\nooo\\no\\nooo\\noo\\noooo\\noooooo\\nooo\\no\\noo\\noo\\nooo\\noooo\\noo\\noo\\no\\nooo\\no\\noo\\nooooo o\\nooo\\noo\\nooo\\no\\noo\\nooo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooooo\\no\\no\\noooo\\nooo\\noooo\\noo\\nooo\\noo\\no\\nooooo\\nooo\\noo\\no\\no\\noo\\noo\\no\\noo\\noo\\noooooo oo\\nooooo\\nooooo\\no\\noooo\\noo\\nooooo\\noo\\nooooo\\noooo\\nooo\\noo\\nooo\\no\\noo\\no\\noooo\\nooo\\noo\\noo\\noooo\\noo\\nooo\\no\\nooo\\noo\\noooooo\\noo\\noooo\\noo\\nooo\\noooo\\nooooo\\noooo\\no\\no ooo\\nooooo\\noo\\no\\noooo\\noo\\noo\\noo\\noooo\\noo\\nooo\\no\\noo\\noo\\no\\noooooo\\no\\no\\nooo oo\\nooo\\noooo\\no\\no\\nooo\\nooo\\noooooooooo\\noo\\nooo\\noo\\noooooo\\nooo\\noooo\\noo\\nooo\\nooo\\noooooo\\noo\\no\\noooooo\\no\\noo\\no\\nooo\\no\\nooo\\no\\noooo\\noo\\nooo\\nooo\\no\\noo\\nooo\\nooooo\\noo\\nooo\\no\\noooo\\no\\noooo\\noo\\no\\noo\\nooo\\nooo\\no\\nooooo\\noo\\noo\\nooo\\no o\\nooo\\nooo\\nooooo\\no\\nooooooo\\noooo\\nooo\\nooooo\\noo\\noo\\nooooooo\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\noo\\noooooo\\noo\\no\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\no\\noooComponent\\n 3oo\\no oooooooo\\noooo\\noo\\noo\\noo\\nooo\\nooo\\noo\\nooooooo\\nooooo\\noooo\\nooooooo\\nooo\\nooooo\\nooo\\nooooo\\no ooooooo\\nooo\\no\\noo\\no o\\nooo\\noo\\nooooo\\no ooo\\nooo ooo\\nooo\\noooooooo\\noo\\nooo\\nooooo\\noooooooooo\\noo\\noooooooo\\noo\\nooooo o ooooooooo\\nooooo\\nooooo\\nooo\\nooo\\noo\\nooooooo\\nooooooo\\no\\no\\noo\\noooo\\noo\\noooo\\noo\\noo\\nooooooo\\nooo\\noooooo\\nooo\\nooo\\nooo\\noooooo\\nooo oo\\noooo\\noo\\nooooo\\no\\noo\\noooooooo\\noo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooooooooo\\noo\\noooo\\noooo\\nooo\\noooo\\nooo\\noo\\noooooo\\no\\noooo\\noooo\\noooooo o\\nooo\\noooooo\\nooo\\noooo\\no\\noo\\noo\\noo\\noooooooooo\\no\\noooo\\nooo\\nooooo\\noo oo\\noo\\nooo\\nooo\\noo\\nooooooooo\\nooo\\nooo\\noooooo\\nooo\\noooooooo\\noooooo\\no\\noooo\\noooooooooooo\\noo\\noo\\nooooo ooo o\\noo\\nooo\\noooooooo\\noooooooo\\noooo\\noo\\noooooooo\\noo\\noo\\noooooo\\noooooooo\\noooo\\nooo\\noooooooo\\noooooo\\no\\noo\\no ooo\\noooo\\no\\nooooo\\noooo\\noooo\\nooooo\\no ooooooo\\noo\\nooo ooo\\nooooooo\\nooooooo\\nooooo\\noooo\\nooooooooo\\noooo oo\\noooo\\noooo\\noooooooo\\nooo o\\nooo\\no\\nooo\\noo\\nooooooo\\no oooooooo\\noooo\\noo\\noo\\noo\\nooo\\nooo\\noo\\nooooooo\\nooooo\\noooo\\nooooooo\\nooo\\nooooo\\nooo\\nooooo\\noooooooo\\nooo\\no\\noo\\no o\\nooo\\noo\\nooooo\\noooo\\nooo ooo\\nooo\\noooooooo\\noo\\nooo\\nooooo\\noooooooooo\\noo\\noooooooo\\noo\\nooooooooooo o ooo\\nooooo\\nooo oo\\nooo\\nooo\\noo\\nooooo oo\\nooooooo\\no\\no\\noo\\noooo\\noo\\noooo\\noo\\noo\\nooooooo\\nooo\\noooooo\\nooo\\nooo\\nooo\\noooooo\\nooooo\\noooo\\noo\\nooooo\\no\\noo\\noooooooo\\noo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooooooooo\\noo\\noooo\\noooo\\nooo\\noooo\\nooo\\noo\\noooooo\\no\\noooo\\noooo\\nooooooo\\nooo\\noooooo\\nooo\\noooo\\no\\noo\\noo\\noo\\noooooooooo\\no\\noooo\\nooo\\nooooo\\noooo\\noo\\nooo\\nooo\\noo\\nooooooooo\\nooo\\nooo\\noooooo\\nooo\\noooooooo\\noooooo\\no\\noooo\\nooooo o oooooo\\noo\\noo\\nooooo ooo o\\noo\\nooo\\noooooooo\\noooooooo\\noooo\\noo\\noooooooo\\noo\\noo\\noooooo\\noooooooo\\noooo\\nooo\\noooooooo\\noooooo\\no\\noo\\no ooo\\noooo\\no\\nooooo\\noooo\\noooo\\nooooo\\no ooooooo\\noo\\nooo ooo\\nooooooo\\nooooooo\\nooooo\\noooo\\noooo oo ooo\\noooo oo\\noooo\\noooo\\noooooooo\\noo o o\\nooo\\no\\nooo\\noo\\nooooo\\noooo\\noooooooo\\noo\\noo\\noo\\no\\noooo\\noo\\noo\\no\\noooo\\noo\\nooo\\no\\noooo\\noo\\noo\\nooo\\noo\\noo\\noooo\\no\\nooo\\noo\\no\\noo\\nooo\\noooo\\noooo\\no\\no\\nooo\\no\\noo\\nooo\\noo\\no\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\noooo\\noooooo\\noooo\\noo\\noo o\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\noo\\nooo\\nooooo\\no\\noo\\noooo\\nooo\\noo\\no\\nooo\\nooooooooooo\\nooo\\no\\nooo\\noooo\\no\\nooo\\nooo\\no\\nooo\\no\\noo\\nooo\\nooo\\noooo\\nooo\\noo o\\nooooo\\no\\noo\\nooo\\nooo\\nooo\\nooo\\noooooo\\noo\\no\\nooo\\no\\nooooooooooo\\nooo\\no\\noo\\no\\noooo\\no\\nooooooo\\noo\\noo\\noo\\nooooo\\noo\\nooooo\\no\\noooo\\no\\noo\\noo\\nooo\\no\\nooo oo\\noo\\nooo\\noo\\no\\noooo\\noo\\noo\\nooo\\no\\noo\\nooooo\\noooo\\no\\noo\\no\\noooooooo\\noo\\noooooo\\nooo\\nooooooo\\noooo\\no\\noo\\no\\nooo\\no\\noooo\\noooo\\noo\\noo\\noo\\no\\no\\noooo\\noo\\no\\noooo\\no oooooo\\no\\noooo\\noo\\noo\\noooo\\nooo\\nooo\\noo\\noooo\\nooo\\no\\nooo\\noooo\\noo\\nooo\\nooo\\no\\noooo\\noo\\no\\noo\\no\\noooo\\nooo\\noooo\\nooo\\nooooooooo\\no\\nooooo\\no\\nooo\\no\\noo\\nooo\\noooo\\nooo\\noooo\\no\\nooooo\\no\\nooo\\nooo\\nooo\\noo\\noooooo\\nooo\\noo\\noo o\\nooooooooo\\noo\\noooo\\nooo\\noo\\nooo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\nooooo\\noo\\noooooo\\noooooo\\noo\\no\\noooo\\nooo\\nooo\\nooo\\noo\\no\\nooo\\noo\\noo\\no oooo\\noooooooo\\noo\\noo\\noo\\no\\noooo\\noo\\noo\\no\\noooo\\noo\\nooo\\no\\noooo\\noo\\noo\\nooo\\noo\\noo\\noooo\\no\\nooo\\noo\\no\\noo\\nooo\\noooo\\noooo\\no\\no\\nooo\\no\\noo\\nooo\\noo\\no\\nooooo\\noo\\nooo\\nooo\\noo\\noo\\nooooo\\noooo oo\\noooo\\noo\\noo o\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\noo\\nooo\\no\\noooo\\no\\noo\\noooo\\nooo\\noo\\no\\no oo\\noooo ooooo oo\\nooo\\no\\nooo\\noooo\\no\\nooo\\nooo\\no\\nooo\\no\\noo\\nooo\\nooo\\noooo\\nooo\\nooo\\nooooo\\no\\noo\\nooo\\nooo\\nooo\\nooo\\noooooo\\noo\\no\\nooo\\no\\nooooooooooo\\nooo\\no\\noo\\no', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 581}),\n",
       " Document(page_content='oo\\noo\\nooo\\noo\\nooo\\noo\\noo\\noo\\nooo\\no\\noooo\\no\\noo\\noooo\\nooo\\noo\\no\\no oo\\noooo ooooo oo\\nooo\\no\\nooo\\noooo\\no\\nooo\\nooo\\no\\nooo\\no\\noo\\nooo\\nooo\\noooo\\nooo\\nooo\\nooooo\\no\\noo\\nooo\\nooo\\nooo\\nooo\\noooooo\\noo\\no\\nooo\\no\\nooooooooooo\\nooo\\no\\noo\\no\\noooo\\no\\noooo\\nooo\\noo\\noo\\noo\\nooooo\\noo\\nooooo\\no\\noooo\\no\\noo\\noo\\nooo\\no\\nooooo\\noo\\nooo\\noo\\no\\noooo\\noo\\noo\\nooo\\no\\noo\\nooooo\\noooo\\no\\noo\\no\\noooooooo\\noo\\noooooo\\nooo\\nooooooo\\noooo\\no\\noo\\no\\nooo\\no\\noooo\\nooo o\\noo\\noo\\noo\\no\\no\\noooo\\noo\\no\\noooo\\nooooooo\\no\\noooo\\noo\\noo\\noooo\\nooo\\nooo\\noo\\noooo\\nooo\\no\\nooo\\noooo\\noo\\nooo\\nooo\\no\\noooo\\noo\\no\\noo\\no\\noooo\\nooo\\noooo\\nooo\\nooooooooo\\no\\nooooo\\no\\nooo\\no\\noo\\nooo\\noooo\\nooo\\noooo\\no\\nooooo\\no\\nooo\\nooo\\nooo\\noo\\noooooo\\nooo\\noo\\nooo\\nooooooooo\\noo\\noooo\\nooo\\noo\\nooo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\nooooo\\noo\\nooo ooo\\noooooo\\noo\\no\\noooo\\nooo\\nooo\\nooo\\noo\\no\\nooo\\noo\\noo\\nooooo\\noooooooo\\noo\\noo\\noo\\no\\noooo\\noo\\noo\\no\\noooo\\noo\\nooo\\no\\noooo\\noo\\noo\\nooo\\noo\\noo\\noooo\\no\\nooo\\noo\\no\\noo\\nooo\\noooo\\noooo\\no\\no\\noo o\\no\\noo\\nooo\\noo\\no\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\noooo\\noooooo\\no ooo\\noo\\noo o\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\noo\\nooo\\nooooo\\no\\noo\\noooo\\nooo\\noo\\no\\nooo\\nooooooooooo\\nooo\\no\\nooo\\noooo\\no\\nooo\\nooo\\no\\nooo\\no\\noo\\nooo\\nooo\\noooo\\nooo\\nooo\\nooooo\\no\\noo\\nooo\\nooo\\nooo\\nooo\\noooooo\\noo\\no\\nooo\\no\\noooo ooooooo\\nooo\\no\\noo\\no\\noooo\\no\\nooooooo\\noo\\noo\\noo\\nooooo\\noo\\nooooo\\no\\noooo\\no\\noo\\noo\\nooo\\no\\nooooo\\noo\\nooo\\noo\\no\\noooo\\noo\\noo\\nooo\\no\\noo\\nooooo\\noooo\\no\\noo\\no\\noooooooo\\noo\\noooooo\\nooo\\nooooooo\\noooo\\no\\noo\\no\\nooo\\no\\noooo\\noooo\\noo\\noo\\noo\\no\\no\\noooo\\noo\\no\\noooo\\no oooooo\\no\\noooo\\noo\\noo\\noooo\\nooo\\nooo\\noo\\noooo\\nooo\\no\\nooo\\noooo\\noo\\nooo\\nooo\\no\\noooo\\noo\\no\\noo\\no\\noooo\\nooo\\noooo\\nooo\\nooooooooo\\no\\nooooo\\no\\nooo\\no\\noo\\nooo\\noooo\\nooo\\noooo\\no\\nooooo\\no\\nooo\\nooo\\nooo\\noo\\noo oooo\\nooo\\noo\\noo o\\nooooooooo\\noo\\noooo\\nooo\\noo\\nooo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\nooooo\\noo\\nooo ooo\\noooooo\\noo\\no\\noooo\\nooo\\nooo\\nooo\\noo\\no\\nooo\\noo\\noo\\noComponent\\n 4ooo\\nooo\\nooo\\no\\noooo oooooo\\noooooo\\noooooo\\noo\\noo\\nooo\\noo\\noo ooo\\no\\nooo\\noo\\nooooo\\no\\nooo\\no\\noooo\\no\\no\\noooooo ooo\\nooooooo\\no oooo\\nooooo\\no\\noooooo\\noo\\noooooooo\\no\\nooo\\noo\\noooo\\nooo\\noo\\nooo\\noooooo\\no\\noooo\\nooo\\noo\\noo\\no\\noo\\noo\\noooooo o\\nooo\\nooooo\\noo\\no ooo\\no\\noo\\nooooooo\\nooo\\nooo\\no\\nooo\\nooo\\noo\\noooo\\nooooooo oo\\nooooooo\\no\\nooooo\\noo\\noooo\\noooo\\noo\\no\\nooooooo\\noo\\nooooooooooo\\no\\noooooo\\noo\\noooo\\nooo o\\nooooooo\\nooo\\nooo\\no\\noooo\\noooo\\nooo\\noooooooooooo\\nooooooooooo\\noooo\\nooooo\\nooo\\no\\no\\no\\nooooo\\noooooooooo\\noooooo\\noo o\\no\\noooo\\noooooo\\no\\noooooooooo\\noo\\noooooooooo\\noo ooooo\\nooo oo\\noo\\no\\noooo\\nooo\\nooo oo\\nooooooo\\no\\nooo\\noo\\no\\no\\nooooo\\noo\\noo\\nooo\\no\\noo\\noooo\\nooo\\nooo\\noo\\nooo\\noo\\noo\\noo\\noo\\nooo\\nooooooo\\noo\\noo\\noooooooo\\nooo\\noo\\noo\\noo\\noooooooo\\nooo\\nooooooo\\nooo\\nooooooooooo\\noooo\\noooo\\no\\noooooooo\\noo\\noooooo\\no\\noo\\nooooooo\\nooo\\nooooooooo\\nooo\\noo\\no\\noooooo\\noooo\\noooooo\\noo\\noo\\noo oo o\\no\\no\\noo oooooo\\nooo\\no\\noo\\noo\\no\\noooo\\no\\noooo\\nooooo\\noo\\no\\noo\\noo\\noo\\no\\no\\noo\\noooo\\no\\no\\noo\\noo\\nooo\\nooo\\nooo\\no ooo\\nooooo\\no\\no\\noo\\noo\\no\\noo\\noo\\no\\nooooo\\nooooo\\noo\\noo\\nooo\\noo\\no\\nooo\\noooo\\noooo\\noooo\\noooo\\no\\noo\\no\\no\\noooooo\\nooooooooo\\nooo\\noo\\noo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\noo\\no\\noooo\\nooo\\noo\\noo\\no\\noo\\nooooo\\noo\\nooo\\noo\\nooo\\no\\noooooooooo\\noo\\no\\noooo\\noooo\\noo\\no\\nooo\\noo\\noo\\noooooo\\nooo\\noo\\nooo\\noooo\\nooo\\noo\\nooooooo\\noo\\nooo\\no\\noo\\nooooo\\no\\nooooo\\no\\noooo\\noo\\nooo\\noooo\\noooo\\noo\\no\\no\\noo\\nooo\\nooooo\\noo\\noo\\no\\no\\nooo\\nooo\\noo\\nooooooooo\\no\\noooo\\no\\noo\\nooo\\noo\\noooo\\noo oo\\noo\\noo\\noo\\noo\\noo\\noooo\\nooo\\nooooo\\noo\\noo\\nooo\\noo\\noooooo\\noo\\no\\nooooo\\noo\\noooooooooo\\noooooo\\noo\\noooo\\no\\nooo\\noo\\no\\noooooo\\nooo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\nooo\\no\\nooo\\nooo\\nooooooo\\no\\nooooo\\noo\\no\\noo\\noo\\nooo\\noo\\noo\\nooo\\no\\noooo\\no\\nooo\\noooooo\\no\\no\\nooooooo\\no\\noooo\\noo\\noooo\\nooo\\nooo\\no\\nooooo\\noooo\\noooooo oooooo\\noo\\noooo\\noo\\nooooo\\nooo\\noo\\nooo\\noo\\noooo\\nooo\\noo\\no\\noooo\\nooooo\\no\\noo\\no\\noooo\\noo\\no\\no\\nooo\\noo\\no\\nooo\\noo\\noooo\\noo\\no\\noo\\noo\\noo\\noooooo\\noo\\noo\\noo\\nooooo\\nooo\\noooo\\noo\\no\\noo\\no\\nooooo\\nooooo\\no\\noo\\noooo\\no\\no\\noo\\noo\\nooo\\nooo\\nooo\\noooo\\nooooo\\no\\no\\noo\\noo\\no\\noo\\noo\\no\\nooooo\\nooooo\\noo\\noo\\nooo\\noo\\no\\nooo\\noooo\\noooo\\noooo\\noooo\\no\\noo\\no\\no\\noooooo\\nooooooooo\\nooooo\\noo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\noo\\no\\noooo\\nooo\\noo\\noo\\no\\noo\\nooooo\\noo\\nooo\\noo\\nooo\\no\\no\\nooooooooo\\noo\\no\\noooo\\noooo\\noo\\no\\nooo\\noo\\noo\\noooooo\\nooo\\noo\\nooo\\noooo\\nooo\\noo\\nooooooo\\noo\\nooo\\no\\noo\\nooooo\\no\\nooooo\\no\\noooo\\noo\\nooo\\noooo\\noooo\\noo\\no\\no\\noo\\nooo\\nooooo\\noo\\noo\\no\\no\\nooo\\nooo\\noo\\nooooooooo\\no\\noooo\\no\\noo\\nooo\\noo\\noooo\\noooo\\noo\\noo\\noo\\noo\\noo\\no ooo\\nooo\\nooooo\\noo\\noo\\nooo\\noo\\noooooo\\noo\\no\\nooooo\\noo\\noooooooo oo\\noooooo\\noo\\noooo\\no\\nooo\\noo\\no\\noooooo\\nooo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\nooo\\no\\nooo\\nooo\\nooooooo\\no\\nooooo', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 581}),\n",
       " Document(page_content='oo\\noo\\noo\\noo\\noo\\no ooo\\nooo\\nooooo\\noo\\noo\\nooo\\noo\\noooooo\\noo\\no\\nooooo\\noo\\noooooooo oo\\noooooo\\noo\\noooo\\no\\nooo\\noo\\no\\noooooo\\nooo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\nooo\\no\\nooo\\nooo\\nooooooo\\no\\nooooo\\noo\\no\\noo\\noo\\nooo\\noo\\noo\\nooo\\no\\noooo\\no\\nooo\\nooo ooo\\no\\no\\nooooooo\\no\\noooo\\noo\\noooo\\nooo\\nooo\\no\\nooooo\\noooo\\noooooooooooo\\noo\\noooo\\noo\\nooooo\\nooo\\noo\\nooo\\noo\\nooo o\\nooo\\noo\\no\\noooo\\nooooo\\no\\noo\\no\\noooo\\noo\\no\\no\\nooo\\noo\\no\\nooo\\noo\\noooo\\noo\\no\\noo\\noo\\noo\\noooooo\\noo\\noo\\noo\\nooooo\\no oo\\noooo\\noo\\no\\noo\\no\\nooooo\\nooooo\\no\\noo\\noooo\\no\\no\\noo\\noo\\nooo\\nooo\\nooo\\noooo\\nooooo\\no\\no\\noo\\noo\\no\\noo\\noo\\no\\nooooo\\nooooo\\noo\\noo\\nooo\\noo\\no\\nooo\\noooo\\noooo\\no ooo\\noooo\\no\\noo\\no\\no\\noooooo\\nooooooo oo\\nooo\\noo\\noo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\noo\\no\\noooo\\nooo\\noo\\noo\\no\\noo\\nooooo\\noo\\nooo\\noo\\nooo\\no\\noooooooooo\\noo\\no\\noooo\\noooo\\noo\\no\\nooo\\noo\\noo\\noooooo\\nooo\\noo\\nooo\\noooo\\nooo\\noo\\noooo ooo\\noo\\nooo\\no\\noo\\nooooo\\no\\nooooo\\no\\noooo\\noo\\nooo\\noooo\\noooo\\noo\\no\\no\\noo\\nooo\\nooooo\\noo\\noo\\no\\no\\nooo\\nooo\\noo\\nooooooooo\\no\\noooo\\no\\noo\\nooo\\noo\\noooo\\noooo\\noo\\noo\\noo\\noo\\noo\\noooo\\nooo\\nooooo\\noo\\noo\\nooo\\noo\\noooooo\\noo\\no\\nooooo\\noo\\nooooo ooooo\\noooooo\\noo\\noooo\\no\\nooo\\noo\\no\\noooooo\\nooo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\nooo\\no\\nooo\\nooo\\nooooooo\\no\\nooooo\\noo\\no\\noo\\noo\\nooo\\noo\\noo\\nooo\\no\\noooo\\no\\nooo\\noooooo\\no\\no\\nooooooo\\no\\noooo\\noo\\noooo\\nooo\\nooo\\no\\nooooo\\noooo\\noooooo oooo oo\\noo\\noooo\\noo\\nooooo\\nooo\\noo\\nooo\\noo\\noooo\\nooo\\noo\\no\\noooo\\nooooo\\no\\noo\\no\\noooo\\noo\\no\\no\\nooo\\noo\\no\\nooo\\noo\\noooo\\noo\\no\\noo\\noo\\noo\\noooooo\\noo\\noo\\noo\\nooooo\\nooo\\noooo\\noo\\no\\noo\\no\\nooooo\\nooooo\\no\\noo\\noooo\\no\\no\\noo\\noo\\nooo\\nooo\\nooo\\noooo\\nooooo\\no\\no\\noo\\noo\\no\\noo\\noo\\no\\nooooo\\nooooo\\noo\\noo\\nooo\\noo\\no\\nooo\\noooo\\noooo\\noooo\\noooo\\no\\noo\\no\\no\\noooooo\\nooooooooo\\nooo\\noo\\noo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\noo\\no\\noooo\\nooo\\noo\\noo\\no\\noo\\nooooo\\noo\\nooo\\noo\\nooo\\no\\no\\nooooooooo\\noo\\no\\noooo\\noooo\\noo\\no\\nooo\\noo\\noo\\noooooo\\nooo\\noo\\nooo\\noooo\\nooo\\noo\\noooo ooo\\noo\\nooo\\no\\noo\\nooooo\\no\\nooooo\\no\\noooo\\noo\\nooo\\noooo\\noooo\\noo\\no\\no\\noo\\nooo\\nooooo\\noo\\noo\\no\\no\\nooo\\nooo\\noo\\nooooooooo\\no\\noooo\\no\\noo\\nooo\\noo\\noooo\\noooo\\noo\\noo\\noo\\noo\\noo\\noooo\\nooo\\nooooo\\noo\\noo\\nooo\\noo\\noooooo\\noo\\no\\nooooo\\noo\\noooooooo oo\\noooooo\\noo\\noooo\\no\\nooo\\noo\\no\\noooooo\\nooo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\nooo\\no\\nooo\\nooo\\nooooo oo\\no\\nooooo\\noo\\no\\noo\\noo\\nooo\\noo\\noo\\nooo\\no\\noooo\\no\\nooo\\noooooo\\no\\no\\nooooooo\\no\\noooo\\noooooo\\nooo\\nooo\\no\\nooooo\\noooo\\noooo\\noo oooooo\\noo\\noooo\\noo\\nooooo\\nooo\\noo\\nooo\\noo\\noooo\\nooo\\noo\\no\\noooo\\nooooo\\no\\noo\\no\\noo\\noo\\noo\\no\\no\\nooo\\noo\\no\\nooo\\noo\\noooo\\noo\\no\\noo\\noo\\noo\\noooooo\\noooo\\noo\\nooooo\\nooo\\noooo\\noo\\no\\noo\\no\\nooooo\\noooo Component\\n 5PCA ComponentsICA Components\\nFIGURE 14.39. A comparison of the ﬁrst ﬁve ICA components computed using\\nFastICA (above diagonal) with the ﬁrst ﬁve PCA components(below diago nal).\\nEach component is standardized to have unit variance.\\nGaussian as possible. With pre-whitened data, this amounts to looking for\\ncomponents that are as independent as possible.\\nICA starts from essentially a factor analysis solution, and looks for rota-\\ntions that lead to independent components. From this point o f view, ICA is\\njust another factor rotation method, along with the traditi onal “varimax”\\nand “quartimax” methods used in psychometrics.\\nExample: Handwritten Digits\\nWe revisit the handwritten threes analyzed by PCA in Section 14.5.1. Fig-\\nure 14.39 compares the ﬁrst ﬁve (standardized) principal co mponents with\\nthe ﬁrst ﬁve ICA components, all shown in the same standardiz ed units.\\nNote that each plot is a two-dimensional projection from a 25 6-dimensional', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 581}),\n",
       " Document(page_content='564 14. Unsupervised Learning\\nMean ICA 1 ICA 2 ICA 3 ICA 4 ICA 5\\nFIGURE 14.40. The highlighted digits from Figure 14.39. By comparing with\\nthe mean digits, we see the nature of the ICA component.\\nspace. While the PCA components all appear to have joint Gaus sian distri-\\nbutions, the ICA components have long-tailed distribution s. This is not too\\nsurprising, since PCA focuses on variance, while ICA speciﬁ cally looks for\\nnon-Gaussian distributions. All the components have been s tandardized,\\nso we do not see the decreasing variances of the principal com ponents.\\nFor each ICA component we have highlighted two of the extreme digits,\\nas well as a pair of central digits and displayed them in Figur e 14.40.\\nThis illustrates the nature of each of the components. For ex ample, ICA\\ncomponent ﬁve picks up the long sweeping tailed threes.\\nExample: EEG Time Courses\\nICA has become an important tool in the study of brain dynamic s—the\\nexample we present here uses ICA to untangle the components o f signals\\nin multi-channel electroencephalographic (EEG) data (Ont on and Makeig,\\n2006).\\nSubjects wear a cap embedded with a lattice of 100 EEG electro des,\\nwhichrecordbrainactivityatdiﬀerentlocationsonthesca lp.Figure14.4111\\n(top panel) shows 15 seconds of output from a subset of nine of these elec-\\ntrodes from a subject performing a standard “two-back” lear ning task over\\na 30 minute period. The subject is presented with a letter (B, H, J, C, F, or\\nK) at roughly 1500-ms intervals, and responds by pressing on e of two but-\\ntons to indicate whether the letter presented is the same or d iﬀerent from\\nthat presented two steps back. Depending on the answer, the s ubject earns\\nor loses points, and occasionally earns bonus or loses penal ty points. The\\ntime-course data show spatial correlation in the EEG signal s—the signals\\nof nearby sensors look very similar.\\nThe key assumption here is that signals recorded at each scal p electrode\\nare a mixture of independent potentials arising from diﬀere nt cortical ac-\\n11Reprinted from Progress in Brain Research , Vol. 159, Julie Onton and Scott Makeig,\\n“Information based modeling of event-related brain dynamics,” Page 1 06 , Copyright\\n(2006), with permission from Elsevier. We thank Julie Onton and Scot t Makeig for\\nsupplying an electronic version of the image.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 582}),\n",
       " Document(page_content='14.7IndependentComponentAnalysisandExploratoryProject ionPursuit 565\\ntivities, as well as non-cortical artifact domains; see the reference for a\\ndetailed overview of ICA in this domain.\\nThe lower part of Figure 14.41 shows a selection of ICA compon ents.\\nThe colored images represent the estimated unmixing coeﬃci ent vectors ˆ aj\\nas heatmap images superimposed on the scalp, indicating the location of\\nactivity. The corresponding time-courses show the activit y of the learned\\nICA components.\\nFor example, the subject blinked after each performance fee dback signal\\n(colored vertical lines), which accounts for the location a nd artifact signal\\nin IC1 and IC3. IC12 is an artifact associated with the cardia c pulse. IC4\\nandIC7accountforfrontaltheta-bandactivities,andappe arafterastretch\\nof correct performance. See Onton and Makeig (2006) for a mor e detailed\\ndiscussion of this example, and the use of ICA in EEG modeling .\\n14.7.3 Exploratory Projection Pursuit\\nFriedman and Tukey (1974) proposed exploratory projection pursuit, a\\ngraphicalexplorationtechniqueforvisualizinghigh-dim ensionaldata.Their\\nview was that most low (one- or two-dimensional) projection s of high-\\ndimensional data look Gaussian. Interesting structure, su ch as clusters or\\nlong tails, would be revealed by non-Gaussian projections. They proposed\\na number of projection indices for optimization, each focusing on a diﬀer-\\nent departure from Gaussianity. Since their initial propos al, a variety of\\nimprovements have been suggested (Huber, 1985; Friedman, 1 987), and a\\nvariety of indices, including entropy, are implemented in t he interactive\\ngraphics package Xgobi (Swayne et al., 1991, now called GGob i). These\\nprojection indices are exactly of the same form as J(Yj) above, where\\nYj=aT\\njX, a normalized linear combination of the components of X. In\\nfact, some of the approximations and substitutions for cros s-entropy coin-\\ncide with indices proposed for projection pursuit. Typical ly with projection\\npursuit, the directions ajare not constrained to be orthogonal. Friedman\\n(1987) transforms the data to look Gaussian in the chosen pro jection, and\\nthen searches for subsequent directions. Despite their diﬀ erent origins, ICA\\nand exploratory projection pursuit are quite similar, at le ast in the repre-\\nsentation described here.\\n14.7.4 A Direct Approach to ICA\\nIndependent components have by deﬁnition a joint product de nsity\\nfS(s) =p∏\\nj=1fj(sj), (14.88)\\nso here we present an approach that estimates this density di rectly us-\\ning generalized additive models (Section 9.1). Full detail s can be found in', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 583}),\n",
       " Document(page_content='566 14. Unsupervised Learning\\nFIGURE 14.41. Fifteen seconds of EEG data (of 1917seconds) at nine (of\\n100) scalp channels (top panel), as well as nine ICA components (lowe r panel).\\nWhile nearby electrodes record nearly identical mixtures of br ain and non-brain\\nactivity, ICA components are temporally distinct. The colore d scalps represent the\\nICA unmixing coeﬃcients ˆajas a heatmap, showing brain or scalp location of the\\nsource.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 584}),\n",
       " Document(page_content='14.7IndependentComponentAnalysisandExploratoryProject ionPursuit 567\\nHastie and Tibshirani (2003), and the method is implemented in the R\\npackageProDenICA , available from CRAN.\\nIn the spirit of representing departures from Gaussianity, we represent\\neachfjas\\nfj(sj) =φ(sj)egj(sj), (14.89)\\natiltedGaussian density. Here φis the standard Gaussian density, and\\ngjsatisﬁes the normalization conditions required of a densit y. Assuming\\nas before that Xis pre-whitened, the log-likelihood for the observed data\\nX=ASis\\nℓ(A,{gj}p\\n1;X) =N∑\\ni=1p∑\\nj=1[\\nlogφj(aT\\njxi)+gj(aT\\njxi)\\n], (14.90)\\nwhich we wish to maximize subject to the constraints that Ais orthogonal\\nand that the gjresult in densities in (14.89). Without imposing any furthe r\\nrestrictions on gj, the model (14.90) is over-parametrized, so we instead\\nmaximize a regularized version\\np∑\\nj=1[\\n1\\nNN∑\\ni=1[\\nlogφ(aT\\njxi)+gj(aT\\njxi)]\\n−∫\\nφ(t)egj(t)dt−λj∫\\n{g′′′\\nj(t)}2(t)dt]\\n.\\n(14.91)\\nWe have subtracted two penalty terms (for each j) in (14.91), inspired by\\nSilverman (1986, Section 5.4.4):\\n•The ﬁrst enforces the density constraint∫\\nφ(t)eˆgj(t)dt= 1 on any\\nsolution ˆgj.\\n•The second is a roughness penalty, which guarantees that the solution\\nˆgjis a quartic-spline with knots at the observed values of sij=aT\\njxi.\\nIt can further be shown that the solution densities ˆfj=φeˆgjeach have\\nmean zero and variance one (Exercise 14.18). As we increase λj, these\\nsolutions approach the standard Gaussian φ.\\nAlgorithm 14.3 Product Density ICA Algorithm: ProDenICA\\n1. Initialize A(random Gaussian matrix followed by orthogonalization).\\n2. Alternate until convergence of A:\\n(a) Given A, optimize (14.91) w.r.t. gj(separately for each j).\\n(b) Givengj, j= 1,...,p, perform one step of a ﬁxed point algo-\\nrithm towards ﬁnding the optimal A.\\nWe ﬁt the functions gjand directions ajby optimizing (14.91) in an\\nalternating fashion, as described in Algorithm 14.3.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 585}),\n",
       " Document(page_content='568 14. Unsupervised Learning\\nStep 2(a) amounts to a semi-parametric density estimation, which can\\nbe solved using a novel application of generalized additive models. For\\nconvenience we extract one of the pseparate problems,\\n1\\nNN∑\\ni=1[logφ(si)+g(si)]−∫\\nφ(t)eg(t)dt−λ∫\\n{g′′′(t)}2(t)dt.(14.92)\\nAlthough the second integral in (14.92) leads to a smoothing spline, the\\nﬁrst integral is problematic, and requires an approximatio n. We construct\\na ﬁne grid of Lvaluess∗\\nℓin increments ∆ covering the observed values si,\\nand count the number of siin the resulting bins:\\ny∗\\nℓ=#si∈(s∗\\nℓ−∆/2,s∗\\nℓ+∆/2)\\nN. (14.93)\\nTypically we pick Lto be 1000, which is more than adequate. We can then\\napproximate (14.92) by\\nL∑\\nℓ=1{\\ny∗\\ni[log(φ(s∗\\nℓ))+g(s∗\\nℓ)]−∆φ(s∗\\nℓ)eg(s∗\\nℓ)}\\n−λ∫\\ng′′′2(s)ds.(14.94)\\nThis last expression can be seen to be proportional to a penal ized Poisson\\nlog-likelihood with response y∗\\nℓ/∆ and penalty parameter λ/∆, and mean\\nµ(s) =φ(s)eg(s). This is a generalized additive spline model (Hastie and\\nTibshirani, 1990; Efron and Tibshirani, 1996), with an oﬀsetterm logφ(s),\\nand can be ﬁt using a Newton algorithm in O(L) operations. Although\\na quartic spline is called for, we ﬁnd in practice that a cubic spline is\\nadequate. We have ptuning parameters λjto set; in practice we make\\nthem all the same, and specify the amount of smoothing via the eﬀective\\ndegrees-of-freedom df( λ). Our software uses 5df as a default value.\\nStep 2(b) in Algorithm 14.3 requires optimizing (14.91) wit h respect to\\nA, holding the ˆ gjﬁxed. Only the ﬁrst terms in the sum involve A, and\\nsinceAis orthogonal, the collection of terms involving φdo not depend on\\nA(Exercise 14.19). Hence we need to maximize\\nC(A) =1\\nNp∑\\nj=1N∑\\ni=1ˆgj(aT\\njxi) (14.95)\\n=p∑\\nj=1Cj(aj)\\nC(A) is a log-likelihood ratio between the ﬁtted density and a Ga ussian,\\nand can be seen as an estimate of negentropy (14.86), with eac h ˆgja con-\\ntrastfunctionasin(14.87).Theﬁxedpointupdateinstep2( b)isamodiﬁed\\nNewton step (Exercise 14.20)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 586}),\n",
       " Document(page_content='14.7IndependentComponentAnalysisandExploratoryProject ionPursuit 569\\n1. For each jupdate\\naj←E{\\nXˆg′\\nj(aT\\njX)−E[ˆg′′\\nj(aT\\njX)]aj}\\n,(14.96)\\nwhereErepresentsexpectationw.r.tthesample xi.Since ˆgjisaﬁtted\\nquartic (or cubic) spline, the ﬁrst and second derivatives a re readily\\navailable.\\n2. Orthogonalize Ausing the symmetric square-root transformation\\n(AAT)−1\\n2A. IfA=UDVTis the SVD of A, it is easy to show that\\nthis leads to the update A←UVT.\\nOurProDenICA algorithm works as well as FastICA on the artiﬁcial time\\nseries data of Figure 14.37, the mixture of uniforms data of F igure 14.38,\\nand the digit data in Figure 14.39.\\nExample: Simulations\\na b c\\nd e f\\ng h i\\nj k l\\nm n o\\np q r\\nDistributionAmari Distance from True A\\na b c d e f g h i j k l m n o p q r0.01 0.02 0.05 0.10 0.20 0.50FastICA\\nKernelICA\\nProdDenICA\\nFIGURE 14.42. The left panel shows 18distributions used for comparisons.\\nThese include the “t”, uniform, exponential, mixtures of expo nentials, symmetric\\nand asymmetric Gaussian mixtures. The right panel shows (on t he log scale)\\nthe average Amari metric for each method and each distributi on, based on 30\\nsimulations in IR2for each distribution.\\nFigure 14.42 shows the results of a simulation comparing ProDenICA to\\nFastICA, and another semi-parametric competitor KernelICA (Bach and\\nJordan, 2002). The left panel shows the 18 distributions use d as a basis\\nof comparison. For each distribution, we generated a pair of independent\\ncomponents( N= 1024),andarandommixingmatrixinIR2withcondition\\nnumberbetween1and2.WeusedourRimplementationsof FastICA,using\\nthe negentropy criterion (14.87), and ProDenICA . ForKernelICA we used', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 587}),\n",
       " Document(page_content='570 14. Unsupervised Learning\\nthe authors MATLAB code.12Since the search criteria are nonconvex, we\\nused ﬁve random starts for each method. Each of the algorithm s delivers\\nan orthogonal mixing matrix A(the data were pre-whitened ), which is\\navailable for comparison with the generating orthogonaliz ed mixing matrix\\nA0. We used the Amari metric (Bach and Jordan, 2002) as a measure of\\nthe closeness of the two frames:\\nd(A0,A) =1\\n2pp∑\\ni=1(∑p\\nj=1|rij|\\nmaxj|rij|−1)\\n+1\\n2pp∑\\nj=1(∑p\\ni=1|rij|\\nmaxi|rij|−1)\\n,(14.97)\\nwhererij= (AoA−1)ij. The right panel in Figure 14.42 compares the\\naverages (on the log scale) of the Amari metric between the tr uth and the\\nestimated mixing matrices. ProDenICA is competitive with FastICA and\\nKernelICA inallsituations,anddominatesmostofthemixturesimulat ions.\\n14.8 Multidimensional Scaling\\nBoth self-organizing maps and principal curves and surface s map data\\npoints in IRpto a lower-dimensional manifold. Multidimensional scalin g\\n(MDS) has a similar goal, but approaches the problem in a some what dif-\\nferent way.\\nWe start with observations x1,x2,...,x N∈IRp, and letdijbe the dis-\\ntance between observations iandj. Often we choose Euclidean distance\\ndij=||xi−xj||, but other distances may be used. Further, in some ap-\\nplications we may not even have available the data points xi, but only\\nhave some dissimilarity measuredij(see Section 14.3.10). For example, in\\na wine tasting experiment, dijmight be a measure of how diﬀerent a sub-\\nject judged wines iandj, and the subject provides such a measure for all\\npairs of wines i,j. MDS requires only the dissimilarities dij, in contrast to\\nthe SOM and principal curves and surfaces which need the data pointsxi.\\nMultidimensional scaling seeks values z1,z2,...,z N∈IRkto minimize\\nthe so-called stress function13\\nSM(z1,z2,...,z N) =∑\\ni̸=i′(dii′−||zi−zi′||)2. (14.98)\\nThis is known as least squares orKruskal–Shephard scaling. The idea is\\nto ﬁnd a lower-dimensional representation of the data that p reserves the\\npairwise distances as well as possible. Notice that the appr oximation is\\n12Francis Bach kindly supplied this code, and helped us set up the simulatio ns.\\n13Some authors deﬁne stress as the square-root of SM; since it does not aﬀect the\\noptimization, we leave it squared to make comparisons with other crit eria simpler.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 588}),\n",
       " Document(page_content='14.8 Multidimensional Scaling 571\\nin terms of the distances rather than squared distances (whi ch results in\\nslightly messier algebra). A gradient descent algorithm is used to minimize\\nSM.\\nA variation on least squares scaling is the so-called Sammon mapping\\nwhich minimizes\\nSSm(z1,z2,...,z N) =∑\\ni̸=i′(dii′−||zi−zi′||)2\\ndii′.(14.99)\\nHere more emphasis is put on preserving smaller pairwise dis tances.\\nInclassical scaling , we instead start with similarities sii′: often we use\\nthe centered inner product sii′=⟨xi−¯x,xi′−¯x⟩. The problem then is to\\nminimize\\nSC(z1,z2,...,z N) =∑\\ni,i′(sii′−⟨zi−¯z,zi′−¯z⟩)2(14.100)\\noverz1,z2,...,z N∈IRk. This is attractive because there is an explicit\\nsolution in terms of eigenvectors: see Exercise 14.11. If we have distances\\nratherthaninner-products,wecanconvertthemtocentered inner-products\\nif the distances are Euclidean ;14see (18.31) on page 671 in Chapter 18.\\nIf the similarities are in fact centered inner-products, cl assical scaling is\\nexactly equivalent to principal components, an inherently linear dimension-\\nreduction technique. Classical scaling is not equivalent t o least squares\\nscaling; the loss functions are diﬀerent, and the mapping ca n be nonlinear.\\nLeast squares and classical scaling are referred to as metricscaling meth-\\nods, in the sense that the actual dissimilarities or similar ities are approx-\\nimated.Shephard–Kruskal nonmetric scaling eﬀectively uses only ranks.\\nNonmetric scaling seeks to minimize the stress function\\nSNM(z1,z2,...,z N) =∑\\ni̸=i′[\\n||zi−zi′||−θ(dii′)]2\\n∑\\ni̸=i′||zi−zi′||2(14.101)\\nover theziand an arbitrary increasing function θ. Withθﬁxed, we min-\\nimize over ziby gradient descent. With the ziﬁxed, the method of iso-\\ntonic regression is used to ﬁnd the best monotonic approxima tionθ(dii′)\\nto||zi−zi′||. These steps are iterated until the solutions stabilize.\\nLike the self-organizing map and principal surfaces, multi dimensional\\nscaling represents high-dimensional data in a low-dimensi onal coordinate\\nsystem. Principal surfaces and SOMs go a step further, and ap proximate\\nthe original data by a low-dimensional manifold, parametri zed in the low\\ndimensional coordinate system. In a principal surface and S OM, points\\n14AnN×Ndistance matrix is Euclidean if the entries represent pairwise Euclidean\\ndistances between Npoints in some dimensional space.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 589}),\n",
       " Document(page_content='572 14. Unsupervised Learning\\nFirst MDS CoordinateSecond MDS Coordinate\\n-1.0 -0.5 0.0 0.5 1.0-1.0 -0.5 0.0 0.5 1.0•\\n•••\\n•••\\n•\\n••\\n•••\\n••••\\n•\\n•••\\n••••••\\n••\\n•••\\n••\\n•\\n••\\n•••\\n•\\n••••••\\n•••\\n••\\n••\\n•••\\n•\\n••••\\n•••\\n••\\n••\\n••\\n•\\n••\\n•••\\n•\\n•••\\n••••\\n•••\\n••\\nFIGURE 14.43. First two coordinates for half-sphere data, from classical mu lti-\\ndimensional scaling.\\nclose together in the original feature space should map clos e together on\\nthe manifold, but points far apart in feature space might als o map close\\ntogether. This is less likely in multidimensional scaling s ince it explicitly\\ntries to preserve all pairwise distances.\\nFigure 14.43 shows the ﬁrst two MDS coordinates from classic al scaling\\nfor the half-sphere example. There is clear separation of th e clusters, and\\nthe tighter nature of the red cluster is apparent.\\n14.9 Nonlinear Dimension Reduction and Local\\nMultidimensional Scaling\\nSeveral methods have been recently proposed for nonlinear d imension re-\\nduction, similar in spirit to principal surfaces. The idea i s that the data lie\\nclose to an intrinsically low-dimensional nonlinear manif old embedded in a\\nhigh-dimensional space. These methods can be thought of as “ ﬂattening”\\nthe manifold, and hence reducing the data to a set of low-dime nsional co-\\nordinates that represent their relative positions in the ma nifold. They are\\nuseful for problems where signal-to-noise ratio is very hig h (e.g., physical\\nsystems), and are probably not as useful for observational d ata with lower\\nsignal-to-noise ratios.\\nThe basic goal is illustrated in the left panel of Figure 14.4 4. The data\\nlie near a parabola with substantial curvature. Classical M DS does not pre-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 590}),\n",
       " Document(page_content='14.9 Nonlinear Dimension Reduction and Local Multidimensi onal Scaling 573\\n−5 0 5−15 −10 −5 0Classical MDS\\n−5 0 5−15 −10 −5 0Local MDS\\nx1 x1\\nx2x2\\nFIGURE 14.44. The orange points show data lying on a parabola, while the blue\\npoints shows multidimensional scaling representations in on e dimension. Classical\\nmultidimensional scaling (left panel) does not preserve the ord ering of the points\\nalong the curve, because it judges points on opposite ends of t he curve to be close\\ntogether. In contrast, local multidimensional scaling (right panel) does a good job\\nof preserving the ordering of the points along the curve.\\nserve the ordering of the points along the curve, because it j udges points\\non opposite ends of the curve to be close together. The right p anel shows\\nthe results of local multi-dimensional scaling , one of the three methods for\\nnon-linear multi-dimensional scaling that we discuss belo w. These meth-\\nods use only the coordinates of the points in pdimensions, and have no\\nother information about the manifold. Local MDS has done a go od job of\\npreserving the ordering of the points along the curve.\\nWe now brieﬂy describe three new approaches to nonlinear dim ension\\nreduction and manifold mapping.\\nIsometric feature mapping (ISOMAP) (Tenenbaum et al., 2000) con-\\nstructs a graph to approximate the geodesic distance betwee n points along\\nthe manifold. Speciﬁcally, for each data point we ﬁnd its nei ghbors—points\\nwithin some small Euclidean distance of that point. We const ruct a graph\\nwith an edge between any two neighboring points. The geodesi c distance\\nbetween any two points is then approximated by the shortest p ath be-\\ntween points on the graph. Finally, classical scaling is app lied to the graph\\ndistances, to produce a low-dimensional mapping.\\nLocal linear embedding (Roweis and Saul, 2000) takes a very diﬀerent ap-\\nproach, trying to preserve the local aﬃne structure of the hi gh-dimensional\\ndata. Each data point is approximated by a linear combinatio n of neigh-\\nboring points. Then a lower dimensional representation is c onstructed that', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 591}),\n",
       " Document(page_content='574 14. Unsupervised Learning\\nbest preserves these local approximations. The details are interesting, so\\nwe give them here.\\n1. For each data point xiinpdimensions, we ﬁnd its K-nearest neigh-\\nborsN(i) in Euclidean distance.\\n2. We approximate each point by an aﬃne mixture of the points i n its\\nneighborhood:\\nmin\\nWik||xi−∑\\nk∈N(i)wikxk||2(14.102)\\nover weights wiksatisfyingwik= 0, k /∈N(i),∑N\\nk=1wik= 1.wik\\nis the contribution of point kto the reconstruction of point i. Note\\nthat for a hope of a unique solution, we must have K <p.\\n3. Finally, we ﬁnd points yiin a space of dimension d<pto minimize\\nN∑\\ni=1||yi−N∑\\nk=1wikyk||2(14.103)\\nwithwikﬁxed.\\nIn step 3, we minimize\\ntr[(Y−WY)T(Y−WY)] = tr[YT(I−W)T(I−W)Y] (14.104)\\nwhereWisN×N;YisN×d, for some small d < p. The solutions ˆY\\nare the trailing eigenvectors of M= (I−W)T(I−W). Since1is a trivial\\neigenvector with eigenvalue 0, we discard it and keep the nex td. This has\\nthe side eﬀect that 1TY= 0, and hence the embedding coordinates are\\nmean centered.\\nLocal MDS (Chen and Buja, 2008) takes the simplest and arguably the\\nmost direct approach. We deﬁne Nto be the symmetric set of nearby pairs\\nof points; speciﬁcally a pair ( i,i′) is inNif pointiis among the K-nearest\\nneighbors of i′, or vice-versa. Then we construct the stress function\\nSL(z1,z2,...,z N) =∑\\n(i,i′)∈N(dii′−||zi−zi′||)2\\n+∑\\n(i,i′)/∈Nw·(D−||zi−zi′||)2.(14.105)\\nHereDis some large constant and wis a weight. The idea is that points\\nthat are not neighbors are considered to be very far apart; su ch pairs are\\ngiven a small weight wso that they don’t dominate the overall stress func-\\ntion. To simplify the expression, we take w∼1/D, and letD→ ∞.\\nExpanding (14.105), this gives', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 592}),\n",
       " Document(page_content='14.9 Nonlinear Dimension Reduction and Local Multidimensi onal Scaling 575\\nFIGURE 14.45. Images of faces mapped into the embedding space described by\\nthe ﬁrst two coordinates of LLE. Next to the circled points, re presentative faces\\nare shown in diﬀerent parts of the space. The images at the bot tom of the plot\\ncorrespond to points along the top right path (linked by solid lin e), and illustrate\\none particular mode of variability in pose and expression.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 593}),\n",
       " Document(page_content='576 14. Unsupervised Learning\\nSL(z1,z2,...,z N) =∑\\n(i,i′)∈N(dii′−||zi−zi′||)2−τ∑\\n(i,i′)/∈N||zi−zi′||,\\n(14.106)\\nwhereτ= 2wD. The ﬁrst term in (14.106) tries to preserve local structure\\nin the data, while the second term encourages the representa tionszi,zi′\\nfor pairs (i,i′) that are non-neighbors to be farther apart. Local MDS\\nminimizesthestressfunction(14.106)over zi,forﬁxedvaluesofthenumber\\nof neighbors Kand the tuning parameter τ.\\nTherightpanelofFigure14.44showstheresultoflocalMDS, usingk= 2\\nneighbors and τ= 0.01. We used coordinate descent with multiple starting\\nvalues to ﬁnd a good minimum of the (nonconvex) stress functi on (14.106).\\nThe ordering of the points along the curve has been largely pr eserved,\\nFigure 14.45 shows a more interesting application of one of t hese meth-\\nods (LLE)15. The data consist of 1965 photographs, digitized as 20 ×28\\ngrayscale images. The result of the ﬁrst two-coordinates of LLE are shown\\nand reveal some variability in pose and expression. Similar pictures were\\nproduced by local MDS.\\nIn experiments reported in Chen and Buja (2008), local MDS sh ows su-\\nperior performance, as compared to ISOMAP and LLE. They also demon-\\nstrate the usefulness of local MDS for graph layout. There ar e also close\\nconnections between the methods discussed here, spectral c lustering (Sec-\\ntion 14.5.3) and kernel PCA (Section 14.5.4).\\n14.10 The Google PageRank Algorithm\\nIn this section we give a brief description of the original PageRank algo-\\nrithm used by the Google search engine, an interesting recen t application\\nof unsupervised learning methods.\\nWe suppose that we have Nweb pages and wish to rank them in terms\\nof importance. For example, the Npages might all contain a string match\\nto “statistical learning” and we might wish to rank the pages in terms of\\ntheir likely relevance to a websurfer.\\nThePageRank algorithm considers a webpage to be important if many\\nother webpages point to it. However the linking webpages tha t point to a\\ngiven page are not treated equally: the algorithm also takes into account\\nboth the importance ( PageRank ) of the linking pages and the number of\\noutgoing links that they have. Linking pages with higher PageRank are\\ngiven more weight, while pages with more outgoing links are g iven less\\nweight. These ideas lead to a recursive deﬁnition for PageRank , detailed\\nnext.\\n15Sam Roweis and Lawrence Saul kindly provided this ﬁgure.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 594}),\n",
       " Document(page_content='14.10 The Google PageRank Algorithm 577\\nLetLij= 1 if page jpoints to page i, and zero otherwise. Let cj=∑N\\ni=1Lijequal the number of pages pointed to by page j(number of out-\\nlinks). Then the Google PageRanks piare deﬁned by the recursive rela-\\ntionship\\npi= (1−d)+dN∑\\nj=1(Lij\\ncj)\\npj (14.107)\\nwheredis a positive constant (apparently set to 0.85).\\nThe idea is that the importance of page iis the sum of the importances of\\npages that point to that page. The sums are weighted by 1 /cj, that is, each\\npage distributes a total vote of 1 to other pages. The constan tdensures\\nthat each page gets a PageRank of at least 1−d. In matrix notation\\np= (1−d)e+d·LD−1\\ncp (14.108)\\nwhereeis a vector of Nones and Dc= diag(c) is a diagonal matrix with\\ndiagonal elements cj. Introducing the normalization eTp=N(i.e., the\\naveragePageRank is 1), we can write (14.108) as\\np=[\\n(1−d)eeT/N+dLD−1\\nc]\\np\\n=Ap (14.109)\\nwhere the matrix Ais the expression in square braces.\\nExploiting a connection with Markov chains (see below), it c an be shown\\nthat the matrix Ahas a real eigenvalue equal to one, and one is its largest\\neigenvalue. This means that we can ﬁnd ˆpby the power method: starting\\nwith some p=p0we iterate\\npk←Apk−1;pk←Npk\\neTpk. (14.110)\\nThe ﬁxed points ˆpare the desired PageRanks .\\nIn the original paper of Page et al. (1998), the authors consi deredPageR-\\nankas a model of user behavior, where a random web surfer clicks o n links\\nat random, without regard to content. The surfer does a rando m walk on\\nthe web, choosing among available outgoing links at random. The factor\\n1−dis the probability that he does not click on a link, but jumps i nstead\\nto a random webpage.\\nSome descriptions of PageRank have (1−d)/Nas the ﬁrst term in def-\\ninition (14.107), which would better coincide with the rand om surfer in-\\nterpretation. Then the page rank solution (divided by N) is the stationary\\ndistributionofanirreducible,aperiodicMarkovchainove rtheNwebpages.\\nDeﬁnition (14.107) also corresponds to an irreducible, ape riodic Markov\\nchain, with diﬀerent transition probabilities than those f rom the (1−d)/N\\nversion. Viewing PageRank as a Markov chain makes clear why the matrix\\nAhas a maximal real eigenvalue of 1. Since Ahas positive entries with', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 595}),\n",
       " Document(page_content='578 14. Unsupervised Learning\\nPage 2\\nPage 3Page 4Page 1\\nFIGURE 14.46. PageRank algorithm: example of a small network\\neach column summing to one, Markov chain theory tells us that it has a\\nunique eigenvector with eigenvalue one, corresponding to t he stationary\\ndistribution of the chain (Bremaud, 1999).\\nA small network is shown for illustration in Figure 14.46. Th e link matrix\\nis\\nL=\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ed0 0 1 0\\n1 0 0 0\\n1 1 0 1\\n0 0 0 0\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8(14.111)\\nand the number of outlinks is c= (2,1,1,1).\\nThePageRank solution is ˆp= (1.49,0.78,1.58,0.15). Notice that page 4\\nhas no incoming links, and hence gets the minimum PageRank of 0.15.\\nBibliographic Notes\\nThere are many books on clustering, including Hartigan (197 5), Gordon\\n(1999) and Kaufman and Rousseeuw (1990). K-means clustering goes back\\natleasttoLloyd(1957),Forgy(1965),Jancey(1966)andMac Queen(1967).\\nApplications in engineering, especially in image compress ion via vector\\nquantization, can be found in Gersho and Gray (1992). The k-medoid pro-\\ncedure is described in Kaufman and Rousseeuw (1990). Associ ation rules\\nare outlined in Agrawal et al. (1995). The self-organizing m ap was proposed\\nby Kohonen (1989) and Kohonen (1990); Kohonen et al. (2000) g ive a more\\nrecent account. Principal components analysis and multidi mensional scal-\\ning are described in standard books on multivariate analysi s, for example,\\nMardia et al. (1979). Buja et al. (2008) have implemented a po werful en-\\nvironment called Ggvis for multidimensional scaling, and t he user manual', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 596}),\n",
       " Document(page_content='Exercises 579\\ncontains a lucid overview of the subject. Figures 14.17, 14. 21 (left panel)\\nand 14.28 (left panel) were produced in Xgobi, a multidimens ional data\\nvisualization package by the same authors. GGobi is a more re cent im-\\nplementation (Cook and Swayne, 2007). Goodall (1991) gives a technical\\noverview of Procrustes methods in statistics, and Ramsay an d Silverman\\n(1997)discusstheshaperegistrationproblem.Principalc urvesandsurfaces\\nwere proposed in Hastie (1984) and Hastie and Stuetzle (1989 ). The idea of\\nprincipal points was formulated in Flury (1990), Tarpey and Flury (1996)\\ngive an exposition of the general concept of self-consisten cy. An excellent\\ntutorial on spectral clustering can be found in von Luxburg ( 2007); this was\\nthe main source for Section 14.5.3. Luxborg credits Donath a nd Hoﬀman\\n(1973) and Fiedler (1973) with the earliest work on the subje ct. A history\\nof spectral clustering my be found in Spielman and Teng (1996 ). Indepen-\\ndent component analysis was proposed by Comon (1994), with s ubsequent\\ndevelopments by Bell and Sejnowski (1995); our treatment in Section 14.7\\nis based on Hyv¨ arinen and Oja (2000). Projection pursuit wa s proposed by\\nFriedman and Tukey (1974), and is discussed in detail in Hube r (1985). A\\ndynamic projection pursuit algorithm is implemented in GGo bi.\\nExercises\\nEx. 14.1 Weights for clustering . Show that weighted Euclidean distance\\nd(w)\\ne(xi,xi′) =∑p\\nl=1wl(xil−xi′l)2\\n∑p\\nl=1wl\\nsatisﬁes\\nd(w)\\ne(xi,xi′) =de(zi,zi′) =p∑\\nl=1(zil−zi′l)2, (14.112)\\nwhere\\nzil=xil·(wl∑p\\nl=1wl)1/2\\n. (14.113)\\nThus weighted Euclidean distance based on xis equivalent to unweighted\\nEuclidean distance based on z.\\nEx. 14.2 Consider a mixture model density in p-dimensional feature space,\\ng(x) =K∑\\nk=1πkgk(x), (14.114)\\nwheregk=N(µk,L·σ2) andπk≥0∀kwith∑\\nkπk= 1. Here{µk,πk},k=\\n1,...,Kandσ2are unknown parameters.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 597}),\n",
       " Document(page_content='580 14. Unsupervised Learning\\nSuppose we have data x1,x2,...,x N∼g(x) and we wish to ﬁt the mix-\\nture model.\\n1. Write down the log-likelihood of the data\\n2. Derive an EM algorithm for computing the maximum likeliho od es-\\ntimates (see Section 8.1).\\n3. Show that if σhas a known value in the mixture model and we take\\nσ→0, then in a sense this EM algorithm coincides with K-means\\nclustering.\\nEx. 14.3 In Section 14.2.6 we discuss the use of CART or PRIM for con-\\nstructing generalized association rules. Show that a probl em occurs with ei-\\ntherofthesemethodswhenwegeneratetherandomdatafromth eproduct-\\nmarginal distribution; i.e., by randomly permuting the val ues for each of\\nthe variables. Propose ways to overcome this problem.\\nEx. 14.4 Cluster the demographic data of Table 14.1 using a classiﬁca tion\\ntree. Speciﬁcally, generate a reference sample of the same s ize of the train-\\ning set, by randomly permuting the values within each featur e. Build a\\nclassiﬁcation tree to the training sample (class 1) and the r eference sample\\n(class 0) and describe the terminal nodes having highest est imated class 1\\nprobability. Compare the results to the PRIM results near Ta ble 14.1 and\\nalso to the results of K-means clustering applied to the same data.\\nEx. 14.5 Generate data with three features, with 30 data points in eac h of\\nthree classes as follows:\\nθ1=U(−π/8,π/8)\\nφ1=U(0,2π)\\nx1= sin(θ1)cos(φ1)+W11\\ny1= sin(θ1)sin(φ1)+W12\\nz1= cos(θ1)+W13\\nθ2=U(π/2−π/4,π/2+π/4)\\nφ2=U(−π/4,π/4)\\nx2= sin(θ2)cos(φ2)+W21\\ny2= sin(θ2)sin(φ2)+W22\\nz2= cos(θ2)+W23\\nθ3=U(π/2−π/4,π/2+π/4)\\nφ3=U(π/2−π/4,π/2+π/4)\\nx3= sin(θ3)cos(φ3)+W31\\ny3= sin(θ3)sin(φ3)+W32\\nz3= cos(θ3)+W33\\nHereU(a,b) indicates a uniform variate on the range [ a,b] andWjkare\\nindependent normal variates with standard deviation 0 .6. Hence the data', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 598}),\n",
       " Document(page_content='Exercises 581\\nlie near the surface of a sphere in three clusters centered at (1,0,0), (0,1,0)\\nand (0,0,1).\\nWrite a program to ﬁt a SOM to these data, using the learning ra tes\\ngiven in the text. Carry out a K-means clustering of the same data, and\\ncompare the results to those in the text.\\nEx. 14.6 Write programs to implement K-means clustering and a self-\\norganizing map (SOM), with the prototype lying on a two-dime nsional\\ngrid. Apply them to the columns of the human tumor microarray data, us-\\ningK= 2,5,10,20 centroids for both. Demonstrate that as the size of the\\nSOM neighborhood is taken to be smaller and smaller, the SOM s olution\\nbecomes more similar to the K-means solution.\\nEx. 14.7 Derive (14.51) and (14.52) in Section 14.5.1. Show that ˆ µis not\\nunique, and characterize the family of equivalent solution s.\\nEx. 14.8 Derive the solution (14.57) to the Procrustes problem (14.5 6).\\nDerive also the solution to the Procrustes problem with scal ing (14.58).\\nEx. 14.9 Write an algorithm to solve\\nmin\\n{βℓ,Rℓ}L\\n1,ML∑\\nℓ=1||XℓRℓ−M||2\\nF. (14.115)\\nApply it to the three S’s, and compare the results to those sho wn in Fig-\\nure 14.26.\\nEx.14.10 Derivethesolutiontotheaﬃne-invariantaverageproblem( 14.60).\\nApply it to the three S’s, and compare the results to those com puted in\\nExercise 14.9.\\nEx. 14.11 Classical multidimensional scaling. LetSbe the centered in-\\nner product matrix with elements ⟨xi−¯x,xj−¯x⟩. Letλ1> λ2>···>\\nλkbe theklargest eigenvalues of S, with associated eigenvectors Ek=\\n(e1,e2,...,ek). LetDkbe a diagonal matrix with diagonal entries√λ1,√λ2,...,√λk. Show that the solutions zito the classical scaling problem\\n(14.100) are the rowsofEkDk.\\nEx. 14.12 Consider the sparse PCA criterion (14.71).\\n1. Show that with Θﬁxed, solving for Vamounts to Kseparate elastic-\\nnet regression problems, with responses the Kelements of ΘTxi.\\n2. Show that with Vﬁxed, solving for Θamounts to a reduced-rank\\nversion of the Procrustes problem, which reduces to\\nmax\\nΘtrace(ΘTM) subject to ΘTΘ=IK, (14.116)\\nwhereMandΘare bothp×KwithK≤p. IfM=UDQTis the\\nSVD ofM, show that the optimal Θ=UQT.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 599}),\n",
       " Document(page_content='582 14. Unsupervised Learning\\nEx. 14.13 Generate 200 data points with three features, lying close to a\\nhelix. In detail, deﬁne X1= cos(s)+0.1·Z1,X2= sin(s)+0.1·Z2,X3=\\ns+0.1·Z3wherestakes on 200 equally spaced values between 0 and 2 π,\\nandZ1,Z2,Z3are independent and have standard Gaussian distributions.\\n(a) Fit a principal curve to the data and plot the estimated co ordinate\\nfunctions. Compare them to the underlying functions cos( s),sin(s)\\nands.\\n(b) Fit a self-organizing map to the same data, and see if you c an discover\\nthe helical shape of the original point cloud.\\nEx. 14.14 Pre- and post-multiply equation (14.81) by a diagonal matri x\\ncontaining the inverse variances of the Xj. Hence obtain an equivalent\\ndecomposition for the correlation matrix, in the sense that a simple scaling\\nis applied to the matrix A.\\nEx. 14.15 Generate 200 observations of three variates X1,X2,X3according\\nto\\nX1∼Z1\\nX2=X1+0.001·Z2\\nX3= 10·Z3 (14.117)\\nwhereZ1,Z2,Z3are independent standard normal variates. Compute the\\nleading principal component and factor analysis direction s. Hence show\\nthat the leading principal component aligns itself in the ma ximal variance\\ndirectionX3, while the leading factor essentially ignores the uncorrel ated\\ncomponent X3, and picks up the correlated component X2+X1(Geoﬀrey\\nHinton, personal communication).\\nEx. 14.16 Consider the kernel principal component procedure outline d in\\nSection 14.5.4. Argue that the number Mof principal components is equal\\nto the rank of K, which is the number of non-zero elements in D. Show\\nthat themth component zm(mth column of Z) can be written (up to\\ncentering) as zim=∑N\\nj=1αjmK(xi,xj), whereαjm=ujm/dm. Show that\\nthe mapping of a new observation x0to themth component is given by\\nz0m=∑N\\nj=1αjmK(x0,xj).\\nEx. 14.17 Show that with g1(x) =∑N\\nj=1cjK(x,xj), the solution to (14.66)\\nis given by ˆ cj=uj1/d1, whereu1is the ﬁrst column of Uin (14.65), and\\nd1the ﬁrst diagonal element of D. Show that the second and subsequent\\nprincipal component functions are deﬁned in a similar manne r (hint: see\\nSection 5.8.1.)\\nEx. 14.18 Consider the regularized log-likelihood for the density es timation\\nproblem arising in ICA,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 600}),\n",
       " Document(page_content='Exercises 583\\n1\\nNN∑\\ni=1[logφ(si)+g(si)]−∫\\nφ(t)eg(t)dt−λ∫\\n{g′′′(t)}2(t)dt.(14.118)\\nThe solution ˆ gis a quartic smoothing spline, and can be written as ˆ g(s) =\\nˆq(s) + ˆq⊥(s), whereqis a quadratic function (in the null space of the\\npenalty). Let q(s) =θ0+θ1s+θ2s2. By examining the stationarity condi-\\ntions for ˆθk, k= 1,2,3, show that the solution ˆf=φeˆgis a density, and\\nhas mean zero and variance one. If we used a second-derivativ e penalty∫\\n{g′′(t)}2(t)dtinstead, what simple modiﬁcation could we make to the\\nproblem to maintain the three moment conditions?\\nEx. 14.19 IfAisp×porthogonal, show that the ﬁrst term in (14.91) on\\npage 567\\np∑\\nj=1N∑\\ni=1logφ(aT\\njxi),\\nwithajthejth column of A, does not depend on A.\\nEx. 14.20 Fixed point algorithm for ICA (Hyv¨ arinen et al., 2001). Consider\\nmaximizing C(a) =E{g(aTX)}with respect to a, with||a||= 1 and\\nCov(X) =I. Use a Lagrange multiplier to enforce the norm constraint,\\nand write down the ﬁrst two derivatives of the modiﬁed criter ion. Use the\\napproximation\\nE{XXTg′′(aTX)}≈E{XXT}E{g′′(aTX)}\\nto show that the Newton update can be written as the ﬁxed-poin t update\\n(14.96).\\nEx. 14.21 Consider an undirected graph with non-negative edge weight s\\nwii′and graph Laplacian L. Suppose there are mconnected components\\nA1,A2,...,A minthegraph.Showthat thereare meigenvectors of Lcorre-\\nsponding to eigenvalue zero, and the indicator vectors of th ese components\\nIA1,IA2,...,I Amspan the zero eigenspace.\\nEx. 14.22\\n(a) Show that deﬁnition (14.108) implies that the sum of the PageRanks\\npiisN, the number of web pages.\\n(b) Write a program to compute the PageRank solutions by the power\\nmethod using formulation (14.107). Apply it to the network o f Fig-\\nure 14.47.\\nEx. 14.23 Algorithm for non-negative matrix factorization (Wu and Lange,\\n2007). A function g(x,y) to said to minorize a functionf(x) if', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 601}),\n",
       " Document(page_content='584 14. Unsupervised Learning\\nPage 2Page 1\\nPage 3\\nPage 4\\nPage 5Page 6\\nFIGURE 14.47. Example of a small network.\\ng(x,y)≤f(x), g(x,x) =f(x) (14.119)\\nfor allx,yin the domain. This is useful for maximizing f(x) since it is easy\\nto show that f(x) is nondecreasing under the update\\nxs+1= argmaxxg(x,xs) (14.120)\\nThere are analogous deﬁnitions for majorization , for minimizing a function\\nf(x). The resulting algorithms are known as MMalgorithms, for “minorize-\\nmaximize” or “majorize-minimize” (Lange, 2004). It also ca n be shown\\nthat the EM algorithm (8.5) is an example of an MM algorithm: s ee Sec-\\ntion 8.5.3 and Exercise 8.2 for details.\\n(a) Consider maximization of the function L(W,H) in (14.73), written\\nhere without the matrix notation\\nL(W,H) =N∑\\ni=1p∑\\nj=1[\\nxijlog(r∑\\nk=1wikhkj)\\n−r∑\\nk=1wikhkj]\\n.\\nUsing the concavity of log( x), show that for any set of rvaluesyk≥0\\nand 0≤ck≤1 with∑r\\nk=1ck= 1,\\nlog(r∑\\nk=1yk)\\n≥r∑\\nk=1cklog(yk/ck)\\nHence\\nlog(r∑\\nk=1wikhkj)\\n≥r∑\\nk=1as\\nikj\\nbs\\nijlog(\\nbs\\nij\\nas\\nikjwikhkj)\\n,\\nwhere\\nas\\nikj=ws\\nikhs\\nkjandbs\\nij=r∑\\nk=1ws\\nikhs\\nkj,\\nandsindicates the current iteration.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 602}),\n",
       " Document(page_content='Exercises 585\\n(b) Hence show that, ignoring constants, the function\\ng(W,H|Ws,Hs) =N∑\\ni=1p∑\\nj=1r∑\\nk=1xijas\\nikj\\nbs\\nij(\\nlogwik+loghkj)\\n−N∑\\ni=1p∑\\nj=1r∑\\nk=1wikhkj\\nminorizesL(W,H).\\n(c) Set the partial derivatives of g(W,H|Ws,Hs) to zero and hence\\nderive the updating steps (14.74).\\nEx. 14.24 Consider the non-negative matrix factorization (14.72) in the\\nrank one case ( r= 1).\\n(a) Show that the updates (14.74) reduce to\\nwi←wi∑p\\nj=1xij∑p\\nj=1wihj\\nhj←hj∑N\\ni=1xij∑N\\ni=1wihj(14.121)\\nwherewi=wi1,hj=h1j. This is an example of the iterative pro-\\nportional scaling procedure, applied to the independence model for a\\ntwo-way contingency table (Fienberg, 1977, for example).\\n(b) Show that the ﬁnal iterates have the explicit form\\nwi=c·∑p\\nj=1xij∑N\\ni=1∑p\\nj=1xij, hk=1\\nc·∑N\\ni=1xik∑N\\ni=1∑p\\nj=1xij(14.122)\\nfor any constant c >0. These are equivalent to the usual row and\\ncolumn estimates for a two-way independence model.\\nEx. 14.25 Fit a non-negative matrix factorization model to the collec tion\\nof two’s in the digits database. Use 25 basis elements, and co mpare with a\\n24- component (plus mean) PCA model. In both cases display th eWand\\nHmatrices as in Figure 14.33.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 603}),\n",
       " Document(page_content='586 14. Unsupervised Learning', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 604}),\n",
       " Document(page_content='This is page 587\\nPrinter: Opaque this\\n15\\nRandom Forests\\n15.1 Introduction\\nBagging or bootstrap aggregation (section 8.7) is a technique for reducing\\nthe variance of an estimated prediction function. Bagging s eems to work\\nespecially well for high-variance, low-bias procedures, s uch as trees. For\\nregression, we simply ﬁt the same regression tree many times to bootstrap-\\nsampled versions of the training data, and average the resul t. For classiﬁ-\\ncation, a committee of trees each cast a vote for the predicted class.\\nBoosting in Chapter 10 was initially proposed as a committee method as\\nwell, although unlike bagging, the committee of weak learners evolves over\\ntime, and the members cast a weighted vote. Boosting appears to dominate\\nbagging on most problems, and became the preferred choice.\\nRandom forests (Breiman, 2001) is a substantial modiﬁcation of bagging\\nthat builds a large collection of de-correlated trees, and then averages them.\\nOn many problems the performance of random forests is very si milar to\\nboosting,andtheyaresimplertotrainandtune.Asaconsequ ence,random\\nforests are popular, and are implemented in a variety of pack ages.\\n15.2 Deﬁnition of Random Forests\\nThe essential idea in bagging (Section 8.7) is to average man y noisy but\\napproximately unbiased models, and hence reduce the varian ce. Trees are\\nideal candidates for bagging, since they can capture comple x interaction', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 605}),\n",
       " Document(page_content='588 15. Random Forests\\nAlgorithm 15.1 Random Forest for Regression or Classiﬁcation.\\n1. Forb= 1 toB:\\n(a) Draw a bootstrap sample Z∗of sizeNfrom the training data.\\n(b) Grow a random-forest tree Tbto the bootstrapped data, by re-\\ncursively repeating the following steps for each terminal n ode of\\nthe tree, until the minimum node size nminis reached.\\ni. Selectmvariables at random from the pvariables.\\nii. Pick the best variable/split-point among the m.\\niii. Split the node into two daughter nodes.\\n2. Output the ensemble of trees {Tb}B\\n1.\\nTo make a prediction at a new point x:\\nRegression: ˆfB\\nrf(x) =1\\nB∑B\\nb=1Tb(x).\\nClassiﬁcation: LetˆCb(x) be the class prediction of the bth random-forest\\ntree. Then ˆCB\\nrf(x) =majority vote{ˆCb(x)}B\\n1.\\nstructures in the data, and if grown suﬃciently deep, have re latively low\\nbias. Since trees are notoriously noisy, they beneﬁt greatl y from the averag-\\ning.Moreover,sinceeachtreegeneratedinbaggingisident icallydistributed\\n(i.d.), the expectation of an average of Bsuch trees is the same as the ex-\\npectation of any one of them. This means the bias of bagged tre es is the\\nsame as that of the individual (bootstrap) trees, and the onl y hope of im-\\nprovement is through variance reduction. This is in contras t to boosting,\\nwhere the trees are grown in an adaptive way to remove bias, an d hence\\nare not i.d.\\nAn average of Bi.i.d. random variables, each with variance σ2, has vari-\\nance1\\nBσ2. If the variables are simply i.d. (identically distributed , but not\\nnecessarily independent) with positive pairwise correlat ionρ, the variance\\nof the average is (Exercise 15.1)\\nρσ2+1−ρ\\nBσ2. (15.1)\\nAsBincreases, the second term disappears, but the ﬁrst remains , and\\nhence the size of the correlation of pairs of bagged trees lim its the beneﬁts\\nof averaging. The idea in random forests (Algorithm 15.1) is to improve\\nthe variance reduction of bagging by reducing the correlati on between the\\ntrees, without increasing the variance too much. This is ach ieved in the\\ntree-growing process through random selection of the input variables.\\nSpeciﬁcally, when growing a tree on a bootstrapped dataset:', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 606}),\n",
       " Document(page_content='15.2 Deﬁnition of Random Forests 589\\nBefore each split, select m≤pof the input variables at random\\nas candidates for splitting.\\nTypically values for mare√por even as low as 1.\\nAfterBsuchtrees{T(x;Θb)}B\\n1aregrown,therandomforest(regression)\\npredictor is\\nˆfB\\nrf(x) =1\\nBB∑\\nb=1T(x;Θb). (15.2)\\nAsinSection10.9(page356),Θ bcharacterizesthe bthrandomforesttreein\\nterms of split variables, cutpoints at each node, and termin al-node values.\\nIntuitively, reducing mwill reduce the correlation between any pair of trees\\nin the ensemble, and hence by (15.1) reduce the variance of th e average.\\n0 500 1000 1500 2000 25000.040 0.045 0.050 0.055 0.060 0.065 0.070Spam Data\\nNumber of TreesTest ErrorBagging\\nRandom Forest\\nGradient Boosting (5 Node)\\nFIGURE 15.1. Bagging, random forest, and gradient boosting, applied to th e\\nspam data. For boosting, 5-node trees were used, and the number of trees were\\nchosen by 10-fold cross-validation ( 2500trees). Each “step” in the ﬁgure corre-\\nsponds to a change in a single misclassiﬁcation (in a test set of 1536).\\nNot all estimators can be improved by shaking up the data like this.\\nIt seems that highly nonlinear estimators, such as trees, be neﬁt the most.\\nFor bootstrapped trees, ρis typically small (0 .05 or lower is typical; see\\nFigure 15.9), while σ2is not much larger than the variance for the original\\ntree. On the other hand, bagging does not change linearestimates, such\\nas the sample mean (hence its variance either); the pairwise correlation\\nbetween bootstrapped means is about 50% (Exercise 15.4).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 607}),\n",
       " Document(page_content='590 15. Random Forests\\nRandom forests are popular. Leo Breiman’s1collaborator Adele Cutler\\nmaintains a random forest website2where the software is freely available,\\nwithmorethan3000downloadsreportedby2002.Thereisa randomForest\\npackage in R, maintained by Andy Liaw, available from the CRANwebsite.\\nThe authors make grand claims about the success of random for ests:\\n“most accurate,” “most interpretable,” and the like. In our experience ran-\\ndom forests do remarkably well, with very little tuning requ ired. A ran-\\ndom forest classiﬁer achieves 4 .88% misclassiﬁcation error on the spamtest\\ndata, which compares well with all other methods, and is not s igniﬁcantly\\nworse than gradient boosting at 4 .5%. Bagging achieves 5 .4% which is\\nsigniﬁcantly worse than either (using the McNemar test outl ined in Ex-\\nercise 10.6), so it appears on this example the additional ra ndomization\\nhelps.\\nRF−1 RF−3 Bagging GBM−1 GBM−60.00 0.05 0.10 0.15Nested SpheresTest Misclassification Error\\nBayes Error\\nFIGURE 15.2. The results of 50simulations from the “nested spheres” model in\\nIR10. The Bayes decision boundary is the surface of a sphere (addi tive). “RF-3”\\nrefers to a random forest with m= 3, and “GBM-6” a gradient boosted model\\nwith interaction order six; similarly for “RF-1” and “GBM-1.” T he training sets\\nwere of size 2000, and the test sets 10,000.\\nFigure 15.1 shows the test-error progression on 2500 trees f or the three\\nmethods. In this case there is some evidence that gradient bo osting has\\nstarted to overﬁt, although 10-fold cross-validation chos e all 2500 trees.\\n1Sadly, Leo Breiman died in July, 2005.\\n2http://www.math.usu.edu/ ∼adele/forests/', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 608}),\n",
       " Document(page_content='15.2 Deﬁnition of Random Forests 591\\n0 200 400 600 800 10000.32 0.34 0.36 0.38 0.40 0.42 0.44California Housing Data\\nNumber of TreesTest Average Absolute ErrorRF m=2\\nRF m=6\\nGBM depth=4\\nGBM depth=6\\nFIGURE 15.3. Random forests compared to gradient boosting on the Californ ia\\nhousing data. The curves represent mean absolute error on the test data as a\\nfunction of the number of trees in the models. Two random fores ts are shown, with\\nm= 2andm= 6. The two gradient boosted models use a shrinkage parameter\\nν= 0.05in (10.41), and have interaction depths of 4and6. The boosted models\\noutperform random forests.\\nFigure 15.2 shows the results of a simulation3comparing random forests\\nto gradient boosting on the nested spheres problem [Equation (10.2) in\\nChapter 10]. Boosting easily outperforms random forests he re. Notice that\\nsmallermis better here, although part of the reason could be that the t rue\\ndecision boundary is additive.\\nFigure 15.3 compares random forests to boosting (with shrin kage) in a\\nregression problem, using the California housing data (Sec tion 10.14.1).\\nTwo strong features that emerge are\\n•Randomforestsstabilizeatabout200trees,whileat1000tr eesboost-\\ning continues to improve. Boosting is slowed down by the shri nkage,\\nas well as the fact that the trees are much smaller.\\n•Boosting outperforms random forests here. At 1000 terms, th e weaker\\nboosting model (GBM depth 4) has a smaller error than the stro nger\\n3Details: The random forests were ﬁt using the R package randomForest 4.5-11 ,\\nwith 500 trees. The gradient boosting models were ﬁt using R package gbm 1.5, with\\nshrinkage parameter set to 0.05, and 2000 trees.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 609}),\n",
       " Document(page_content='592 15. Random Forests\\n0 500 1000 1500 2000 25000.045 0.055 0.065 0.075\\nNumber of TreesMisclassification ErrorOOB Error\\nTest Error\\nFIGURE 15.4. ooberror computed on the spamtraining data, compared to the\\ntest error computed on the test set.\\nrandom forest (RF m= 6); a Wilcoxon test on the mean diﬀerences\\nin absolute errors has a p-value of 0 .007. For larger mthe random\\nforests performed no better.\\n15.3 Details of Random Forests\\nWe have glossed over the distinction between random forests for classiﬁca-\\ntionversusregression.Whenusedforclassiﬁcation,arand omforestobtains\\na class vote from each tree, and then classiﬁes using majorit y vote (see Sec-\\ntion 8.7 on bagging for a similar discussion). When used for r egression, the\\npredictions from each tree at a target point xare simply averaged, as in\\n(15.2). In addition, the inventors make the following recom mendations:\\n•For classiﬁcation, the default value for mis⌊√p⌋and the minimum\\nnode size is one.\\n•For regression, the default value for mis⌊p/3⌋and the minimum\\nnode size is ﬁve.\\nInpracticethebestvaluesfortheseparameterswilldepend ontheproblem,\\nand they should be treated as tuning parameters. In Figure 15 .3m= 6\\nperforms much better than the default value ⌊8/3⌋= 2.\\n15.3.1 Out of Bag Samples\\nAn important feature of random forests is its use of out-of-bag (oob) sam-\\nples:', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 610}),\n",
       " Document(page_content='15.3 Details of Random Forests 593\\nFor each observation zi= (xi,yi), construct its random forest\\npredictor by averaging onlythose trees corresponding to boot-\\nstrap samples in which zidid notappear.\\nAnooberror estimate is almost identical to that obtained by N-fold cross-\\nvalidation;seeExercise15.2.Henceunlikemanyothernonl inearestimators,\\nrandom forests can be ﬁt in one sequence, with cross-validat ion being per-\\nformed along the way. Once the ooberror stabilizes, the training can be\\nterminated.\\nFigure 15.4 shows the oobmisclassiﬁcation error for the spamdata, com-\\npared to the test error. Although 2500 trees are averaged her e, it appears\\nfrom the plot that about 200 would be suﬃcient.\\n15.3.2 Variable Importance\\nVariable importance plots can be constructed for random for ests in exactly\\nthe same way as they were for gradient-boosted models (Secti on 10.13).\\nAt each split in each tree, the improvement in the split-crit erion is the\\nimportancemeasureattributedtothesplittingvariable,a ndisaccumulated\\nover all the trees in the forest separately for each variable . The left plot\\nof Figure 15.5 shows the variable importances computed in th is way for\\nthespamdata; compare with the corresponding Figure 10.6 on page 354 for\\ngradient boosting. Boosting ignores some variables comple tely, while the\\nrandom forest does not. The candidate split-variable selec tion increases\\nthe chance that any single variable gets included in a random forest, while\\nno such selection occurs with boosting.\\nRandomforestsalsousethe oobsamplestoconstructadiﬀerent variable-\\nimportance measure, apparently to measure the prediction strength of e ach\\nvariable. When the bth tree is grown, the oobsamples are passed down\\nthe tree, and the prediction accuracy is recorded. Then the v alues for the\\njth variable are randomly permuted in the oobsamples, and the accuracy\\nis again computed. The decrease in accuracy as a result of thi s permuting\\nis averaged over all trees, and is used as a measure of the impo rtance of\\nvariablejin the random forest. These are expressed as a percent of the\\nmaximum in the right plot in Figure 15.5. Although the rankin gs of the\\ntwo methods are similar, the importances in the right plot ar e more uni-\\nform over the variables. The randomization eﬀectively void s the eﬀect of\\na variable, much like setting a coeﬃcient to zero in a linear m odel (Exer-\\ncise 15.7). This does not measure the eﬀect on prediction wer e this variable\\nnot available, because if the model was reﬁtted without the v ariable, other\\nvariables could be used as surrogates.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 611}),\n",
       " Document(page_content='594 15. Random Forests\\n!$removefreeCAPAVEyourCAPMAXhpCAPTOTmoneyouryougeorge000eduhplbusiness1999internet(willallemailrereceiveovermail;650meetinglabsorderaddresspmpeoplemake#creditfontdatatechnology85[labtelnetreportoriginalprojectconferencedirect415857addresses3dcspartstableGini\\n0 20 40 60 80 100\\nVariable Importance!remove$CAPAVEhpfreeCAPMAXedugeorgeCAPTOTyourour1999reyouhplbusiness000meetingmoney(willinternet650pmreceiveoveremail;fontmailtechnologyorderalllabs[85addressoriginallabtelnetpeopleprojectdatacreditconference857#415makecsreportdirectaddresses3dpartstableRandomization\\n0 20 40 60 80 100\\nVariable Importance\\nFIGURE 15.5. Variable importance plots for a classiﬁcation random forest\\ngrown on the spamdata. The left plot bases the importance on the Gini split-\\nting index, as in gradient boosting. The rankings compare we ll with the rankings\\nproduced by gradient boosting (Figure 10.6 on page 354). The right plot uses oob\\nrandomization to compute variable importances, and tends to spread the impor-\\ntances more uniformly.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 612}),\n",
       " Document(page_content='15.3 Details of Random Forests 595\\nProximity Plot\\n12\\n34\\n5\\n6Random Forest Classifier\\n123456\\nDimension 1Dimension 2\\nX1X2\\nFIGURE 15.6. (Left): Proximity plot for a random forest classiﬁer grown to\\nthe mixture data. (Right): Decision boundary and training d ata for random forest\\non mixture data. Six points have been identiﬁed in each plot.\\n15.3.3 Proximity Plots\\nOne of the advertised outputs of a random forest is a proximity plot . Fig-\\nure15.6showsaproximityplotforthemixturedatadeﬁnedin Section2.3.3\\nin Chapter 2. In growing a random forest, an N×Nproximity matrix is\\naccumulated for the training data. For every tree, any pair o foobobser-\\nvations sharing a terminal node has their proximity increas ed by one. This\\nproximity matrix is then represented in two dimensions usin g multidimen-\\nsional scaling (Section 14.8). The idea is that even though t he data may be\\nhigh-dimensional, involving mixed variables, etc., the pr oximity plot gives\\nan indication of which observations are eﬀectively close to gether in the eyes\\nof the random forest classiﬁer.\\nProximity plots for random forests often look very similar, irrespective of\\nthe data, which casts doubt on their utility. They tend to hav e a star shape,\\none arm per class, which is more pronounced the better the cla ssiﬁcation\\nperformance.\\nSince the mixture data are two-dimensional, we can map point s from the\\nproximityplottotheoriginalcoordinates,andgetabetter understandingof\\nwhat they represent. It seems that points in pure regions cla ss-wise map to\\nthe extremities of the star, while points nearer the decisio n boundaries map\\nnearer the center. This is not surprising when we consider th e construction\\nof the proximity matrices. Neighboring points in pure regio ns will often\\nend up sharing a bucket, since when a terminal node is pure, it is no longer', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 613}),\n",
       " Document(page_content='596 15. Random Forests\\nsplit by a random forest tree-growing algorithm. On the othe r hand, pairs\\nof points that are close but belong to diﬀerent classes will s ometimes share\\na terminal node, but not always.\\n15.3.4 Random Forests and Overﬁtting\\nWhen the number of variables is large, but the fraction of rel evant variables\\nsmall, random forests are likely to perform poorly with smal lm. At each\\nsplit the chance can be small that the relevant variables wil l be selected.\\nFigure 15.7 shows the results of a simulation that supports t his claim. De-\\ntails are given in the ﬁgure caption and Exercise 15.3. At the top of each\\npair we see the hyper-geometric probability that a relevant variable will be\\nselectedatanysplitbyarandomforesttree(inthissimulat ion,therelevant\\nvariables are all equal in stature). As this probability get s small, the gap\\nbetween boosting and random forests increases. When the num ber of rele-\\nvant variables increases, the performance of random forest s is surprisingly\\nrobust to an increase in the number of noise variables. For ex ample, with 6\\nrelevant and 100 noise variables, the probability of a relev ant variable being\\nselected at any split is 0.46, assuming m=√\\n(6+100)≈10. According to\\nFigure15.7,thisdoesnothurttheperformanceofrandomfor estscompared\\nwith boosting. This robustness is largely due to the relativ e insensitivity of\\nmisclassiﬁcation cost to the bias and variance of the probab ility estimates\\nin each tree. We consider random forests for regression in th e next section.\\nAnother claim is that random forests “cannot overﬁt” the dat a. It is\\ncertainly true that increasing Bdoes not cause the random forest sequence\\nto overﬁt; like bagging, the random forest estimate (15.2) a pproximates the\\nexpectation\\nˆfrf(x) = EΘT(x;Θ) = lim\\nB→∞ˆf(x)B\\nrf (15.3)\\nwith an average over Brealizations of Θ. The distribution of Θ here is con-\\nditional on the training data. However, this limit can overﬁt the data ; the\\naverage of fully grown trees can result in too rich a model, an d incur unnec-\\nessary variance. Segal (2004) demonstrates small gains in p erformance by\\ncontrolling the depths of the individual trees grown in rand om forests. Our\\nexperience is that using full-grown trees seldom costs much , and results in\\none less tuning parameter.\\nFigure15.8showsthemodesteﬀectofdepthcontrolinasimpl eregression\\nexample. Classiﬁers are less sensitive to variance, and thi s eﬀect of over-\\nﬁtting is seldom seen with random-forest classiﬁcation.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 614}),\n",
       " Document(page_content='15.4 Analysis of Random Forests 597Test Misclassification Error\\n0.10 0.15 0.20 0.25 0.30Bayes Error\\n(2, 5) (2, 25) (2, 50) (2, 100) (2, 150)\\nNumber of (Relevant, Noise) Variables0.52 0.34 0.25 0.19 0.15\\nRandom Forest\\nGradient Boosting\\nFIGURE 15.7. A comparison of random forests and gradient boosting on prob -\\nlems with increasing numbers of noise variables. In each case t he true decision\\nboundary depends on two variables, and an increasing number o f noise variables\\nare included. Random forests uses its default value m=√p. At the top of each\\npair is the probability that one of the relevant variables is cho sen at any split.\\nThe results are based on 50simulations for each pair, with a training sample of\\n300, and a test sample of 500. See Exercise 15.3.\\n15.4 Analysis of Random Forests\\nIn this section we analyze the mechanisms at play with the add itional\\nrandomization employed by random forests. For this discuss ion we focus\\non regression and squared error loss, since this gets at the m ain points,\\nand bias and variance are more complex with 0–1 loss (see Sect ion 7.3.1).\\nFurthermore, even in the case of a classiﬁcation problem, we can consider\\ntherandom-forestaverageasanestimateoftheclassposter iorprobabilities,\\nfor which bias and variance are appropriate descriptors.\\n15.4.1 Variance and the De-Correlation Eﬀect\\nThe limiting form ( B→∞) of the random forest regression estimator is\\nˆfrf(x) = EΘ|ZT(x;Θ(Z)), (15.4)\\nwhere we have made explicit the dependence on the training da taZ. Here\\nwe consider estimation at a single target point x. From (15.1) we see that', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 615}),\n",
       " Document(page_content='598 15. Random Forests\\n50 30 20 10 51.00 1.05 1.10\\nMinimum Node SizeMean Squared Test ErrorShallow Deep\\nFIGURE 15.8. The eﬀect of tree size on the error in random forest regres-\\nsion. In this example, the true surface was additive in two of t he12variables,\\nplus additive unit-variance Gaussian noise. Tree depth is con trolled here by the\\nminimum node size; the smaller the minimum node size, the deep er the trees.\\nVarˆfrf(x) =ρ(x)σ2(x). (15.5)\\nHere\\n•ρ(x) is thesampling correlation between any pair of trees used in the\\naveraging:\\nρ(x) = corr[T(x;Θ1(Z)),T(x;Θ2(Z))], (15.6)\\nwhere Θ 1(Z) and Θ 2(Z) are a randomly drawn pair of random forest\\ntrees grown to the randomly sampled Z;\\n•σ2(x) is the sampling variance of any single randomly drawn tree,\\nσ2(x) = VarT(x;Θ(Z)). (15.7)\\nIt is easy to confuse ρ(x) with the average correlation between ﬁtted trees\\nin agivenrandom-forest ensemble; that is, think of the ﬁtted trees as N-\\nvectors, and compute the average pairwise correlation betw een these vec-\\ntors, conditioned on the data. This is notthe case; this conditional corre-\\nlation is not directly relevant in the averaging process, an d the dependence\\nonxinρ(x) warns us of the distinction. Rather, ρ(x) is the theoretical\\ncorrelation between a pair of random-forest trees evaluate d atx, induced\\nby repeatedly making training sample draws Zfrom the population, and\\nthen drawing a pair of random forest trees. In statistical ja rgon, this is the\\ncorrelation induced by the sampling distribution ofZand Θ.\\nMore precisely, the variability averaged over in the calcul ations in (15.6)\\nand (15.7) is both', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 616}),\n",
       " Document(page_content='15.4 Analysis of Random Forests 599\\n•conditional on Z: due to the bootstrap sampling and feature sampling\\nat each split, and\\n•a result of the sampling variability of Zitself.\\nIn fact, the conditional covariance of a pair of tree ﬁts at xis zero, because\\nthe bootstrap and feature sampling is i.i.d; see Exercise 15 .5.\\n1 4 7 13 19 25 31 37 43 490.00 0.02 0.04 0.06 0.08\\nNumber of Randomly Selected Splitting Variables mCorrelation between Trees\\nFIGURE 15.9. Correlations between pairs of trees drawn by a random-forest\\nregression algorithm, as a function of m. The boxplots represent the correlations\\nat600randomly chosen prediction points x.\\nThe following demonstrations are based on a simulation mode l\\nY=1√\\n5050∑\\nj=1Xj+ε, (15.8)\\nwith all the Xjandεiid Gaussian. We use 500 training sets of size 100, and\\na single set of test locations of size 600. Since regression t rees are nonlinear\\ninZ, the patterns we see below will diﬀer somewhat depending on t he\\nstructure of the model.\\nFigure 15.9 shows how the correlation (15.6) between pairs o f trees de-\\ncreases asmdecreases: pairs of tree predictions at xfor diﬀerent training\\nsetsZare likely to be less similar if they do not use the same splitt ing\\nvariables.\\nIn the left panel of Figure 15.10 we consider the variances of single tree\\npredictors, Var T(x;Θ(Z)) (averaged over 600 prediction points xdrawn\\nrandomlyfromoursimulationmodel).Thisisthetotalvaria nce,andcanbe', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 617}),\n",
       " Document(page_content='600 15. Random Forests\\ndecomposed into two parts using standard conditional varia nce arguments\\n(see Exercise 15.5):\\nVarΘ,ZT(x;Θ(Z)) = Var ZEΘ|ZT(x;Θ(Z)) + E ZVarΘ|ZT(x;Θ(Z))\\nTotal Variance = Var Zˆfrf(x) + within-ZVariance\\n(15.9)\\nThe second term is the within- Zvariance—a result of the randomization,\\nwhich increases as mdecreases. The ﬁrst term is in fact the sampling vari-\\nance of the random forest ensemble (shown in the right panel) , which de-\\ncreases asmdecreases. The variance of the individual trees does not cha nge\\nappreciably over much of the range of m, hence in light of (15.5), the vari-\\nance of the ensemble is dramatically lower than this tree var iance.\\n0 10 20 30 40 501.80 1.85 1.90 1.95Single Tree\\nmVariance\\nWithin Z\\nTotal\\n0 10 20 30 40 500.65 0.70 0.75 0.80 0.85Random Forest Ensemble\\nmMean Squared Error and Squared Bias\\nVariance\\nMean Squared Error\\nSquared Bias\\nVariance\\n0.0 0.05 0.10 0.15 0.20\\nFIGURE 15.10. Simulation results. The left panel shows the average variance o f\\na single random forest tree, as a function of m. “Within Z” refers to the average\\nwithin-sample contribution to the variance, resulting from t he bootstrap sampling\\nand split-variable sampling (15.9). “Total” includes the samplin g variability of\\nZ. The horizontal line is the average variance of a single fully gr own tree (with-\\nout bootstrap sampling). The right panel shows the average me an-squared error,\\nsquared bias and variance of the ensemble, as a function of m. Note that the\\nvariance axis is on the right (same scale, diﬀerent level). The h orizontal line is\\nthe average squared-bias of a fully grown tree.\\n15.4.2 Bias\\nAs in bagging, the bias of a random forest is the same as the bia s of any\\nof the individual sampled trees T(x;Θ(Z)):', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 618}),\n",
       " Document(page_content='15.4 Analysis of Random Forests 601\\nBias(x) =µ(x)−EZˆfrf(x)\\n=µ(x)−EZEΘ|ZT(x;Θ(Z)). (15.10)\\nThis is also typically greater (in absolute terms) than the b ias of an un-\\npruned tree grown to Z, since the randomization and reduced sample space\\nimposerestrictions.Hencetheimprovementsinprediction obtainedbybag-\\nging or random forests are solely a result of variance reduction .\\nAny discussion of bias depends on the unknown true function. Fig-\\nure 15.10 (right panel) shows the squared bias for our additi ve model simu-\\nlation (estimated from the 500 realizations). Although for diﬀerent models\\nthe shape and rate of the bias curves may diﬀer, the general tr end is that\\nasmdecreases, the bias increases. Shown in the ﬁgure is the mean -squared\\nerror, and we see a classical bias-variance trade-oﬀ in the c hoice ofm. For\\nallmthe squared bias of the random forest is greater than that for a single\\ntree (horizontal line).\\nThese patterns suggest a similarity with ridge regression ( Section 3.4.1).\\nRidge regression is useful (in linear models) when one has a l arge number\\nof variables with similarly sized coeﬃcients; ridge shrink s their coeﬃcients\\ntoward zero, and those of strongly correlated variables tow ard each other.\\nAlthough the size of the training sample might not permit all the variables\\nto be in the model, this regularization via ridge stabilizes the model and al-\\nlows all the variables to have their say (albeit diminished) . Random forests\\nwith small mperform a similar averaging. Each of the relevant variables\\nget their turn to be the primary split, and the ensemble avera ging reduces\\nthe contribution of any individual variable. Since this sim ulation exam-\\nple (15.8) is based on a linear model in all the variables, rid ge regression\\nachieves a lower mean-squared error (about 0 .45 with df(λopt)≈29).\\n15.4.3 Adaptive Nearest Neighbors\\nThe random forest classiﬁer has much in common with the k-nearest neigh-\\nbor classiﬁer (Section 13.3); in fact a weighted version the reof. Since each\\ntree is grown to maximal size, for a particular Θ∗,T(x;Θ∗(Z)) is the re-\\nsponse value for one of the training samples4. The tree-growing algorithm\\nﬁnds an “optimal” path to that observation, choosing the mos t informative\\npredictors from those at its disposal. The averaging proces s assigns weights\\nto these training responses, which ultimately vote for the p rediction. Hence\\nvia the random-forest voting mechanism, those observation scloseto the\\ntarget point get assigned weights—an equivalent kernel—whic h combine to\\nform the classiﬁcation decision.\\nFigure 15.11 demonstrates the similarity between the decis ion boundary\\nof 3-nearest neighbors and random forests on the mixture dat a.\\n4We gloss over the fact that pure nodes are not split further, and hence t here can be\\nmore than one observation in a terminal node', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 619}),\n",
       " Document(page_content='602 15. Random Forests\\nRandom Forest Classifier\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\nooooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo oooo\\noo\\noo\\noooo\\no\\noo\\noo\\nooooo\\nooo\\no\\no\\nooo\\noooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.000\\nTest Error:       0.238\\nBayes Error:    0.2103−Nearest Neighbors\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\nooooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo oooo\\noo\\noo\\noooo\\no\\noo\\noo\\nooooo\\nooo\\no\\no\\nooo\\noooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.130\\nTest Error:       0.242\\nBayes Error:    0.210\\nFIGURE 15.11. Random forests versus 3-NN on the mixture data. The axis-ori -\\nented nature of the individual trees in a random forest lead to decision regions\\nwith an axis-oriented ﬂavor.\\nBibliographic Notes\\nRandom forests as described here were introduced by Breiman (2001), al-\\nthough many of the ideas had cropped up earlier in the literat ure in dif-\\nferent forms. Notably Ho (1995) introduced the term “random forest,” and\\nused a consensus of trees grown in random subspaces of the fea tures. The\\nidea of using stochastic perturbation and averaging to avoi d overﬁtting was\\nintroduced by Kleinberg (1990), and later in Kleinberg (199 6). Amit and\\nGeman (1997) used randomized trees grown on image features f or image\\nclassiﬁcation problems. Breiman (1996a) introduced baggi ng, a precursor\\nto his version of random forests. Dietterich (2000b) also pr oposed an im-\\nprovement on bagging using additional randomization. His a pproach was\\nto rank the top 20 candidate splits at each node, and then sele ct from the\\nlist at random. He showed through simulations and real examp les that this\\nadditional randomization improved over the performance of bagging. Fried-\\nman and Hall (2007) showed that sub-sampling (without repla cement) is\\nan eﬀective alternative to bagging. They showed that growin g and aver-\\naging trees on samples of size N/2 is approximately equivalent (in terms\\nbias/variance considerations) to bagging, while using sma ller fractions of\\nNreduces the variance even further (through decorrelation) .\\nThere are several free software implementations of random f orests. In\\nthis chapter we used the randomForest package in R, maintained by Andy\\nLiaw, available from the CRANwebsite. This allows both split-variable se-\\nlection, as well as sub-sampling. Adele Cutler maintains a r andom forest\\nwebsitehttp://www.math.usu.edu/ ∼adele/forests/ where (as of Au-\\ngust 2008) the software written by Leo Breiman and Adele Cutl er is freely', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 620}),\n",
       " Document(page_content='Exercises 603\\navailable. Their code, and the name “random forests”, is exc lusively li-\\ncensed to Salford Systems for commercial release. The Wekamachine learn-\\ning archive http://www.cs.waikato.ac.nz/ml/weka/ at Waikato Univer-\\nsity, New Zealand, oﬀers a free javaimplementation of random forests.\\nExercises\\nEx. 15.1 Derive the variance formula (15.1). This appears to fail if ρis\\nnegative; diagnose the problem in this case.\\nEx. 15.2 Show that as the number of bootstrap samples Bgets large, the\\nooberror estimate for a random forest approaches its N-fold CV error\\nestimate, and that in the limit, the identity is exact.\\nEx. 15.3 Consider the simulation model used in Figure 15.7 (Mease and\\nWyner, 2008). Binary observations are generated with proba bilities\\nPr(Y= 1|X) =q+(1−2q)·I\\uf8ee\\n\\uf8f0J∑\\nj=1Xj>J/2\\uf8f9\\n\\uf8fb, (15.11)\\nwhereX∼U[0,1]p, 0≤q≤1\\n2, andJ≤pis some predeﬁned (even)\\nnumber. Describe this probability surface, and give the Bay es error rate.\\nEx. 15.4 Supposexi, i= 1,...,Nare iid (µ,σ2). Let ¯x∗\\n1and ¯x∗\\n2be two\\nbootstrap realizations of the sample mean. Show that the sam pling cor-\\nrelation corr(¯ x∗\\n1,¯x∗\\n2) =n\\n2n−1≈50%. Along the way, derive var(¯ x∗\\n1) and\\nthe variance of the bagged mean ¯ xbag. Here ¯xis alinearstatistic; bagging\\nproduces no reduction in variance for linear statistics.\\nEx. 15.5 Show that the sampling correlation between a pair of random-\\nforest trees at a point xis given by\\nρ(x) =VarZ[EΘ|ZT(x;Θ(Z))]\\nVarZ[EΘ|ZT(x;Θ(Z))]+E ZVarΘ|Z[T(x;Θ(Z)].(15.12)\\nThe term in the numerator is Var Z[ˆfrf(x)], and the second term in the\\ndenominator is the expected conditional variance due to the randomization\\nin random forests.\\nEx. 15.6 Fit a series of random-forest classiﬁers to the spamdata, to explore\\nthe sensitivity to the parameter m. Plot both the ooberror as well as the\\ntest error against a suitably chosen range of values for m.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 621}),\n",
       " Document(page_content='604 15. Random Forests\\nEx. 15.7 Suppose we ﬁt a linear regression model to Nobservations with\\nresponseyiand predictors xi1,...,x ip. Assume that all variables are stan-\\ndardized to have mean zero and standard deviation one. Let RSSbe the\\nmean-squaredresidualonthetrainingdata,and ˆβtheestimatedcoeﬃcient.\\nDenote byRSS∗\\njthe mean-squared residual on the training data using the\\nsameˆβ, but with the Nvalues for the jth variable randomly permuted\\nbefore the predictions are calculated. Show that\\nEP[RSS∗\\nj−RSS] = 2ˆβ2\\nj, (15.13)\\nwhereE Pdenotesexpectationwithrespecttothepermutationdistri bution.\\nArgue that this is approximately true when the evaluations a re done using\\nan independent test set.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 622}),\n",
       " Document(page_content='This is page 605\\nPrinter: Opaque this\\n16\\nEnsemble Learning\\n16.1 Introduction\\nThe idea of ensemble learning is to build a prediction model b y combining\\nthe strengths of a collection of simpler base models. We have already seen\\na number of examples that fall into this category.\\nBagging in Section 8.7 and random forests in Chapter 15 are en semble\\nmethods for classiﬁcation, where a committee of trees each cast a vote for\\nthe predicted class. Boosting in Chapter 10 was initially pr oposed as a\\ncommittee method as well, although unlike random forests, t he committee\\nofweak learners evolves over time, and the members cast a weighted vote.\\nStacking (Section 8.8) is a novel approach to combining the s trengths of\\na number of ﬁtted models. In fact one could characterize any d ictionary\\nmethod, such as regression splines, as an ensemble method, w ith the basis\\nfunctions serving the role of weak learners.\\nBayesian methods for nonparametric regression can also be v iewed as\\nensemble methods: a large number of candidate models are ave raged with\\nrespect to the posterior distribution of their parameter se ttings (e.g. (Neal\\nand Zhang, 2006)).\\nEnsemble learning can be broken down into two tasks: develop ing a pop-\\nulation of base learners from the training data, and then com bining them\\nto form the composite predictor. In this chapter we discuss b oosting tech-\\nnology that goes a step further; it builds an ensemble model b y conducting\\na regularized and supervised search in a high-dimensional s pace of weak\\nlearners.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 623}),\n",
       " Document(page_content='606 16. Ensemble Learning\\nAn early example of a learning ensemble is a method designed f or multi-\\nclassclassiﬁcationusing error-correcting output codes (DietterichandBakiri,\\n1995, ECOC). Consider the 10-class digit classiﬁcation pro blem, and the\\ncoding matrix Cgiven in Table 16.1.\\nTABLE 16.1. Part of a 15-bit error-correcting coding matrix Cfor the10-class\\ndigit classiﬁcation problem. Each column deﬁnes a two-class clas siﬁcation prob-\\nlem.\\nDigit C1C2C3C4C5C6···C15\\n01 1 0 0 0 0 ···1\\n10 0 1 1 1 1 ···0\\n21 0 0 1 0 0 ···1\\n.....................···...\\n81 1 0 1 0 1 ···1\\n90 1 1 1 0 0 ···0\\nNote that the ℓth column of the coding matrix Cℓdeﬁnes a two-class\\nvariable that merges all the original classes into two group s. The method\\nworks as follows:\\n1. Learn a separate classiﬁer for each of the L= 15 two class problems\\ndeﬁned by the columns of the coding matrix.\\n2. At a test point x, let ˆpℓ(x) be the predicted probability of a one for\\ntheℓth response.\\n3. Deﬁneδk(x) =∑L\\nℓ=1|Ckℓ−ˆpℓ(x)|, the discriminant function for the\\nkth class, where Ckℓis the entry for row kand column ℓin Table 16.1.\\nEach row of Cis a binary code for representing that class. The rows have\\nmore bits than is necessary, and the idea is that the redundan t “error-\\ncorrecting” bits allow for some inaccuracies, and can impro ve performance.\\nIn fact, the full code matrix Cabove has a minimum Hamming distance1\\nof 7 between any pair of rows. Note that even the indicator res ponse coding\\n(Section 4.2) is redundant, since 10 classes require only ⌈log210 = 4 bits for\\ntheiruniquerepresentation.DietterichandBakiri(1995) showedimpressive\\nimprovements in performance for a variety of multiclass pro blems when\\nclassiﬁcation trees were used as the base classiﬁer.\\nJames and Hastie (1998) analyzed the ECOC approach, and show ed\\nthat random code assignment worked as well as the optimally c onstructed\\nerror-correctingcodes.Theyalsoarguedthatthemainbene ﬁtofthecoding\\nwas in variance reduction (as in bagging and random forests) , because the\\ndiﬀerent coded problems resulted in diﬀerent trees, and the decoding step\\n(3) above has a similar eﬀect as averaging.\\n1The Hamming distance between two vectors is the number of mismatches betw een\\ncorresponding entries.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 624}),\n",
       " Document(page_content='16.2 Boosting and Regularization Paths 607\\n16.2 Boosting and Regularization Paths\\nIn Section 10.12.2 of the ﬁrst edition of this book, we sugges ted an analogy\\nbetween the sequence of models produced by a gradient boosti ng algorithm\\nand regularized model ﬁtting in high-dimensional feature s paces. This was\\nprimarily motivated by observing the close connection betw een a boosted\\nversion of linear regression and the lasso (Section 3.4.2). These connec-\\ntions have been pursued by us and others, and here we present o ur current\\nthinking in this area. We start with the original motivation , which ﬁts more\\nnaturally in this chapter on ensemble learning.\\n16.2.1 Penalized Regression\\nIntuition for the success of the shrinkage strategy (10.41) of gradient boost-\\ning (page 364 in Chapter 10) can be obtained by drawing analog ies with\\npenalized linear regression with a large basis expansion. C onsider the dic-\\ntionary of all possible J-terminal node regressiontrees T={Tk}that could\\nbe realized on the training data as basis functions in IRp. The linear model\\nis\\nf(x) =K∑\\nk=1αkTk(x), (16.1)\\nwhereK= card(T). Suppose the coeﬃcients are to be estimated by least\\nsquares. Since the number of such trees is likely to be much la rger than\\neven the largest training data sets, some form of regulariza tion is required.\\nLet ˆα(λ) solve\\nmin\\nα\\uf8f1\\n\\uf8f2\\n\\uf8f3N∑\\ni=1(\\nyi−K∑\\nk=1αkTk(xi))2\\n+λ·J(α)\\uf8fc\\n\\uf8fd\\n\\uf8fe, (16.2)\\nJ(α) is a function of the coeﬃcients that generally penalizes la rger values.\\nExamples are\\nJ(α) =K∑\\nk=1|αk|2ridge regression , (16.3)\\nJ(α) =K∑\\nk=1|αk|lasso, (16.4)\\n(16.5)\\nboth covered in Section 3.4. As discussed there, the solutio n to the lasso\\nproblem with moderate to large λtends to be sparse; many of the ˆ αk(λ) =\\n0. That is, only a small fraction of all possible trees enter t he model (16.1).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 625}),\n",
       " Document(page_content='608 16. Ensemble Learning\\nAlgorithm 16.1 Forward Stagewise Linear Regression.\\n1. Initialize ˇ αk= 0, k= 1,...,K. Setε >0 to some small constant,\\nandMlarge.\\n2. Form= 1 toM:\\n(a) (β∗,k∗) = argmin β,k∑N\\ni=1(\\nyi−∑K\\nl=1ˇαlTl(xi)−βTk(xi))2\\n.\\n(b) ˇαk∗←ˇαk∗+ε·sign(β∗).\\n3. OutputfM(x) =∑K\\nk=1ˇαkTk(x).\\nThis seems reasonable since it is likely that only a small fra ction of all pos-\\nsible trees will be relevant in approximating any particula r target function.\\nHowever, the relevant subset will be diﬀerent for diﬀerent t argets. Those\\ncoeﬃcients that are not set to zero are shrunk by the lasso in t hat their\\nabsolute values are smaller than their corresponding least squares values2:\\n|ˆαk(λ)|<|ˆαk(0)|. Asλincreases, the coeﬃcients all shrink, each one\\nultimately becoming zero.\\nOwing to the very large number of basis functions Tk, directly solving\\n(16.2) with the lasso penalty (16.4) is not possible. Howeve r, a feasible\\nforward stagewise strategy exists that closely approximat es the eﬀect of\\nthe lasso, and is very similar to boosting and the forward sta gewise Algo-\\nrithm 10.2. Algorithm 16.1 gives the details. Although phra sed in terms\\nof tree basis functions Tk, the algorithm can be used with any set of ba-\\nsis functions. Initially all coeﬃcients are zero in line 1; t his corresponds\\ntoλ=∞in (16.2). At each successive step, the tree Tk∗is selected that\\nbest ﬁts the current residuals in line 2(a). Its correspondi ng coeﬃcient ˇ αk∗\\nis then incremented or decremented by an inﬁnitesimal amoun t in 2(b),\\nwhile all other coeﬃcients ˇ αk, k̸=k∗are left unchanged. In principle, this\\nprocess could be iterated until either all the residuals are zero, orβ∗= 0.\\nThe latter case can occur if K <N, and at that point the coeﬃcient values\\nrepresent a least squares solution. This corresponds to λ= 0 in (16.2).\\nAfter applying Algorithm 16.1 with M <∞iterations, many of the coef-\\nﬁcientswillbezero,namely,thosethathaveyettobeincrem ented.Theoth-\\ners will tend to have absolute values smaller than their corr esponding least\\nsquares solution values, |ˇαk(M)|<|ˆαk(0)|. Therefore this M-iteration\\nsolution qualitatively resembles the lasso, with Minversely related to λ.\\nFigure 16.1 shows an example, using the prostate data studie d in Chap-\\nter 3. Here, instead of using trees Tk(X) as basis functions, we use the origi-\\n2IfK > N, there is in general no unique “least squares value,” since inﬁnitely man y\\nsolutionswillexistthatﬁtthedataperfectly.Wecanpicktheminim umL1-normsolution\\namongst these, which is the unique lasso solution.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 626}),\n",
       " Document(page_content='16.2 Boosting and Regularization Paths 609−0.2 0.0 0.2 0.4 0.6lcavol\\nlweight\\nagelbphsvi\\nlcpgleasonpgg45\\n0.0 0.5 1.0 1.5 2.0\\n−0.2 0.0 0.2 0.4 0.6lcavol\\nlweight\\nagelbphsvi\\nlcpgleasonpgg45\\n0 50 100 150 200\\nt=∑\\nk|αk|\\nCoeﬃcientsCoeﬃcientsLasso Forward Stagewise\\nIteration\\nFIGURE 16.1. Proﬁles of estimated coeﬃcients from linear regression, for t he\\nprostate data studied in Chapter 3. The left panel shows the re sults from the lasso,\\nfor diﬀerent values of the bound parameter t=∑\\nk|αk|. The right panel shows\\nthe results of the stagewise linear regression Algorithm 16.1, usingM= 220\\nconsecutive steps of size ε=.01.\\nnalvariables Xkthemselves;thatis,amultiple linearregressionmodel.Th e\\nleft panel displays the proﬁles of estimated coeﬃcients fro m the lasso, for\\ndiﬀerent values of the bound parameter t=∑\\nk|αk|. The right panel shows\\nthe results of the stagewise Algorithm 16.1, with M= 250 and ε= 0.01.\\n[The left and right panels of Figure 16.1 are the same as Figur e 3.10 and\\nthe left panel of Figure 3.19, respectively.] The similarit y between the two\\ngraphs is striking.\\nInsomesituationstheresemblanceismorethanqualitative .Forexample,\\nif all of the basis functions Tkare mutually uncorrelated, then as ε↓0,M↑\\nsuch thatMǫ→t, Algorithm 16.1 yields exactly the same solution as the\\nlasso for bound parameter t=∑\\nk|αk|(and likewise for all solutions along\\nthe path). Of course, tree-based regressors are not uncorre lated. However,\\nthe solution sets are also identical if the coeﬃcients ˆ αk(λ) are all monotone\\nfunctions of λ. This is often the case when the correlation between the\\nvariables is low. When the ˆ αk(λ) are not monotone in λ, then the solution\\nsets are not identical. The solution sets for Algorithm 16.1 tend to change\\nlessrapidlywithchangingvaluesoftheregularizationpar ameterthanthose\\nof the lasso.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 627}),\n",
       " Document(page_content='610 16. Ensemble Learning\\nEfron et al. (2004) make the connections more precise, by cha racterizing\\nthe exact solution paths in the ε-limiting case. They show that the coeﬃ-\\ncient paths are piece-wise linear functions, both for the la sso and forward\\nstagewise. This facilitates eﬃcient algorithms which allo w the entire paths\\nto be computed with the same cost as a single least-squares ﬁt . Thisleast\\nangle regression algorithm is described in more detail in Section 3.8.1.\\nHastie et al. (2007) show that this inﬁnitesimal forward sta gewise algo-\\nrithm (FS 0) ﬁts a monotone version of the lasso, which optimally reduce s\\nat each step the loss function for a given increase in the arc length of the\\ncoeﬃcient path (see Sections 16.2.3 and 3.8.1). The arc-len gth for theǫ>0\\ncase isMǫ, and hence proportional to the number of steps.\\nTree boosting (Algorithm 10.3) with shrinkage (10.41) clos ely resembles\\nAlgorithm 16.1, with the learning rate parameter νcorresponding to ε. For\\nsquared error loss, the only diﬀerence is that the optimal tr ee to be selected\\nat each iteration Tk∗is approximated by the standard top-down greedy\\ntree-induction algorithm. For other loss functions, such a s the exponential\\nloss of AdaBoost and the binomial deviance, Rosset et al. (20 04a) show\\nsimilar results to what we see here. Thus, one can view tree bo osting with\\nshrinkage as a form of monotone ill-posed regression on all p ossible (J-\\nterminal node) trees, with the lasso penalty (16.4) as a regu larizer. We\\nreturn to this topic in Section 16.2.3.\\nThe choice of no shrinkage [ ν= 1 in equation (10.41)] is analogous to\\nforward-stepwise regression, and its more aggressive cous in best-subset se-\\nlection,whichpenalizesthe numberofnonzerocoeﬃcients J(α) =∑\\nk|αk|0.\\nWith a small fraction of dominant variables, best subset app roaches often\\nwork well. But with a moderate fraction of strong variables, it is well known\\nthatsubsetselectioncanbeexcessivelygreedy(Copas,198 3),oftenyielding\\npoor results when compared to less aggressive strategies su ch as the lasso\\nor ridge regression. The dramatic improvements often seen w hen shrinkage\\nis used with boosting are yet another conﬁrmation of this app roach.\\n16.2.2 The “Bet on Sparsity” Principle\\nAs shown in the previous section, boosting’s forward stagew ise strategy\\nwith shrinkage approximately minimizes the same loss funct ion with a\\nlasso-style L1penalty. The model is built up slowly, searching through\\n“model space” and adding shrunken basis functions derived f rom impor-\\ntant predictors. In contrast, the L2penalty is computationally much easier\\nto deal with, as shown in Section 12.3.7. With the basis funct ions andL2\\npenalty chosen to match a particular positive-deﬁnite kern el, one can solve\\nthe corresponding optimization problem without explicitl y searching over\\nindividual basis functions.\\nHowever, the sometimes superior performance of boosting ov er proce-\\ndures such as the support vector machine may be largely due to the im-\\nplicit use of the L1versusL2penalty. The shrinkage resulting from the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 628}),\n",
       " Document(page_content='16.2 Boosting and Regularization Paths 611\\nL1penalty is better suited to sparsesituations, where there are few basis\\nfunctions with nonzero coeﬃcients (among all possible choi ces).\\nWe can strengthen this argument through a simple example, ta ken from\\nFriedman et al. (2004). Suppose we have 10 ,000 data points and our model\\nis a linear combination of a million trees. If the true popula tion coeﬃcients\\nof these trees arose from a Gaussian distribution, then we kn ow that in a\\nBayesian sense the best predictor is ridge regression (Exer cise 3.6). That is,\\nwe should use an L2rather than an L1penalty when ﬁtting the coeﬃcients.\\nOn the other hand, if there are only a small number (e.g., 1000 ) coeﬃcients\\nthat are nonzero, the lasso ( L1penalty) will work better. We think of this\\nas asparsescenario, while the ﬁrst case (Gaussian coeﬃcients) is dense.\\nNote however that in the dense scenario, although the L2penalty is best,\\nneither method does very well since there is too little data f rom which to\\nestimate such a large number of nonzero coeﬃcients. This is t he curse of\\ndimensionality taking its toll. In a sparse setting, we can p otentially do\\nwell with the L1penalty, since the number of nonzero coeﬃcients is small.\\nTheL2penalty fails again.\\nIn other words, use of the L1penalty follows what we call the “bet on\\nsparsity” principle for high-dimensional problems:\\nUse a procedure that does well in sparse problems, since no pr o-\\ncedure does well in dense problems.\\nThese comments need some qualiﬁcation:\\n•Foranygivenapplication,thedegreeofsparseness/densen essdepends\\non the unknown true target function, and the chosen dictiona ryT.\\n•The notion of sparse versus dense is relative to the size of th e train-\\ning data set and/or the noise-to-signal ratio (NSR). Larger training\\nsets allow us to estimate coeﬃcients with smaller standard e rrors.\\nLikewise in situations with small NSR, we can identify more n onzero\\ncoeﬃcients with a given sample size than in situations where the NSR\\nis larger.\\n•Thesizeofthedictionaryplaysaroleaswell.Increasingth esizeofthe\\ndictionary may lead to a sparser representation for our func tion, but\\nthe search problem becomes more diﬃcult leading to higher va riance.\\nFigure 16.2 illustrates these points in the context of linea r models us-\\ning simulation. We compare ridge regression and lasso, both for classiﬁ-\\ncation and regression problems. Each run has 50 observation s with 300\\nindependent Gaussian predictors. In the top row all 300 coeﬃ cients are\\nnonzero, generated from a Gaussian distribution. In the mid dle row, only\\n10 are nonzero and generated from a Gaussian, and the last row has 30\\nnon zero Gaussian coeﬃcients. For regression, standard Gau ssian noise is', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 629}),\n",
       " Document(page_content='612 16. Ensemble Learning\\n0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Gaussian0.0 0.2 0.4 0.6 0.8 1.0\\n0.1 0.2 0.3 0.4 0.5Ridge/Gaussian\\n0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Subset 100.0 0.2 0.4 0.6 0.8 1.0\\n0.1 0.2 0.3 0.4 0.5Ridge/Subset 10\\n0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Subset 300.0 0.2 0.4 0.6 0.8 1.0\\n0.1 0.2 0.3 0.4 0.5Ridge/Subset 30Percentage Squared Prediction Error Explained\\nNoise−to−Signal RatioRegression\\n0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Gaussian0.0 0.2 0.4 0.6 0.8 1.0\\n0.1 0.2 0.3 0.4 0.5Ridge/Gaussian\\n0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Subset 100.0 0.2 0.4 0.6 0.8 1.0\\n0.1 0.2 0.3 0.4 0.5Ridge/Subset 10\\n0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Subset 300.0 0.2 0.4 0.6 0.8 1.0\\n0.1 0.2 0.3 0.4 0.5Ridge/Subset 30Percentage Misclassification Error Explained\\nNoise−to−Signal RatioClassification\\nFIGURE 16.2. Simulations that show the superiority of the L1(lasso) penalty\\noverL2(ridge) in regression and classiﬁcation. Each run has 50observations\\nwith300independent Gaussian predictors. In the top row all 300coeﬃcients are\\nnonzero, generated from a Gaussian distribution. In the midd le row, only 10are\\nnonzero, and the last row has 30nonzero. Gaussian errors are added to the linear\\npredictorη(X)for the regression problems, and binary responses generated via the\\ninverse-logit transform for the classiﬁcation problems. Scali ng ofη(X)resulted in\\nthe noise-to-signal ratios shown. Lasso is used in the left su b-columns, ridge in the\\nright. We report the optimal percentage of error explained on test data (relative\\nto the error of a constant model), displayed as boxplots over 20realizations for\\neach combination. In the only situation where ridge beats lass o (top row), neither\\ndo well.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 630}),\n",
       " Document(page_content='16.2 Boosting and Regularization Paths 613\\nadded to the linear predictor η(X) =XTβto produce a continuous re-\\nsponse. For classiﬁcation the linear predictor is transfor med via the inverse-\\nlogit to a probability, and a binary response is generated. F ive diﬀer-\\nent noise-to-signal ratios are presented, obtained by scal ingη(X) prior\\nto generating the response. In both cases this is deﬁned to be NSR=\\nVar(Y|η(X))/Var(η(X)). Both the ridge regression and lasso coeﬃcient\\npaths were ﬁt using a series of 50 values of λcorresponding to a range of\\ndffrom 1 to 50 (see Chapter 3 for details). The models were evalu ated on\\na large test set (inﬁnite for Gaussian, 5000 for binary), and in each case the\\nvalue forλwas chosen to minimize the test-set error. We report percent age\\nvariance explained for the regression problems, and percen tage misclassiﬁ-\\ncation error explained for the classiﬁcation problems (rel ative to a baseline\\nerror of 0.5). There are 20 simulation runs for each scenario.\\nNote that for the classiﬁcation problems, we are using squar ed-error loss\\nto ﬁt the binary response. Note also that we do not using the tr aining\\ndata to select λ, but rather are reporting the best possible behavior for\\neach method in the diﬀerent scenarios. The L2penalty performs poorly\\neverywhere. The Lasso performs reasonably well in the only t wo situations\\nwhere it can (sparse coeﬃcients). As expected the performan ce gets worse\\nas the NSR increases (less so for classiﬁcation), and as the m odel becomes\\ndenser. The diﬀerences are less marked for classiﬁcation th an for regression.\\nThese empirical results are supported by a large body of theo retical\\nresults (Donoho and Johnstone, 1994; Donoho and Elad, 2003; Donoho,\\n2006b;CandesandTao,2007)thatsupportthesuperiorityof L1estimation\\nin sparse settings.\\n16.2.3 Regularization Paths, Over-ﬁtting and Margins\\nIt has often been observed that boosting “does not overﬁt,” o r more as-\\ntutely is “slow to overﬁt.” Part of the explanation for this p henomenon was\\nmade earlier for random forests — misclassiﬁcation error is less sensitive to\\nvariance than is mean-squared error, and classiﬁcation is t he major focus\\nin the boosting community. In this section we show that the re gulariza-\\ntion paths of boosted models are “well behaved,” and that for certain loss\\nfunctions they have an appealing limiting form.\\nFigure 16.3 shows the coeﬃcient paths for lasso and inﬁnites imal forward\\nstagewise (FS 0) in a simulated regression setting. The data consists of a\\ndictionary of 1000 Gaussian variables, strongly correlate d (ρ= 0.95) within\\nblocks of 20, but uncorrelated between blocks. The generati ng model has\\nnonzero coeﬃcients for 50 variables, one drawn from each blo ck, and the\\ncoeﬃcient values are drawn from a standard Gaussian. Finall y, Gaussian\\nnoise is added, with a noise-to-signal ratio of 0 .72 (Exercise 16.1.) The\\nFS0algorithm is a limiting form of algorithm 16.1, where the ste p sizeε\\nis shrunk to zero (Section 3.8.1). The grouping of the variab les is intended\\nto mimic the correlations of nearby trees, and with the forwa rd-stagewise', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 631}),\n",
       " Document(page_content='614 16. Ensemble Learning\\n0.0 0.2 0.4 0.6 0.8 1.0−20 −10 0 10 20 30Standardized CoefficientsLASSO\\n0.0 0.2 0.4 0.6 0.8 1.0−20 −10 0 10 20 30Standardized CoefficientsForward Stagewise\\n|α(m)|/|α(∞)| |α(m)|/|α(∞)|\\nFIGURE 16.3. Comparison of lasso and inﬁnitesimal forward stagewise path s\\non simulated regression data. The number of samples is 60and the number of\\nvariables is 1000. The forward-stagewise paths ﬂuctuate less than those of lass o\\nin the ﬁnal stages of the algorithms.\\nalgorithm,thissetupisintendedasanidealizedversionof gradientboosting\\nwith shrinkage. For both these algorithms, the coeﬃcient pa ths can be\\ncomputed exactly, since they are piecewise linear (see the L ARS algorithm\\nin Section 3.8.1).\\nHere the coeﬃcient proﬁles are similar only in the early stag es of the\\npaths. For the later stages, the forward stagewise paths ten d to be mono-\\ntone and smoother, while those for the lasso ﬂuctuate widely . This is due\\nto the strong correlations among subsets of the variables —la sso suﬀers\\nsomewhat from the multi-collinearity problem (Exercise 3. 28).\\nThe performance of the two models is rather similar (Figure 1 6.4), and\\ntheyachieveaboutthesameminimum.Inthelaterstagesforw ardstagewise\\ntakes longer to overﬁt, a likely consequence of the smoother paths.\\nHastie et al. (2007) show that FS 0solves amonotone version of the lasso\\nproblem for squared error loss. Let Ta=T ∪{−T} be the augmented\\ndictionary obtained by including a negative copy of every ba sis element\\ninT. We consider models f(x) =∑\\nTk∈TaαkTk(x) with non-negative co-\\neﬃcientsαk≥0. In this expanded space, the lasso coeﬃcient paths are\\npositive, while those of FS 0are monotone nondecreasing.\\nThe monotone lasso path is characterized by a diﬀerential eq uation\\n∂α\\n∂ℓ=ρml(α(ℓ)), (16.6)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 632}),\n",
       " Document(page_content='16.2 Boosting and Regularization Paths 615\\no\\noo\\no\\no\\noo\\noo\\noo\\noooo\\nooo\\nooooooooooooooooooooooooooooooo ooooooo ooooooooooooooooooooooooooooooooooooooooo ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo oo ooo\\noo\\no\\no o\\noo\\nooo\\nooo oo\\no ooooo\\noooooooo ooooo ooo ooooooooooo oooooo o ooo oooo oo ooooo oo oo ooooooo oo oo ooooo oo oo oo oo ooo oo oo oo ooooooo oo ooo oooooo oo ooo oo oooooooo oooo ooo ooooooo ooo ooo ooooo oooo ooo oooooo ooooooo oooooooooooooo ooooo ooooooooooo ooooooooo oo oooo oooooo oo ooooooooooo ooo oo ooo oo ooo oooooooooo ooooo ooooooo ooooooooo oooo oooooo oooo ooo oooooo oooo oo oo oooo ooooo ooooo ooooooo ooooo o o oooooooooooooooooo oo ooo oo oo oo oooo o oo o o o o o oo o o oooo oo o oo o o oo oo o ooo o o o ooo o o o o o o o o o o o o oo o o o o o o o o o o o o o o o o o oo o o o o o o o o o o o o o o o o o o\\n0 10 20 30 40 50 60 7025 30 35 40 45 50 55Lasso\\nForward Stagewise\\n|α(m)|Mean Squared Error\\nFIGURE 16.4. Mean squared error for lasso and inﬁnitesimal forward stagew ise\\non the simulated data. Despite the diﬀerence in the coeﬃcient paths, the two\\nmodels perform similarly over the critical part of the regulariz ation path. In the\\nright tail, lasso appears to overﬁt more rapidly.\\nwith initial condition α(0) = 0, where ℓis theL1arc-length of the path\\nα(ℓ) (Exercise 16.2). The monotone lasso move direction (veloc ity vector)\\nρml(α(ℓ)) decreases the loss at the optimal quadratic rate per unit i ncrease\\nin theL1arc-length of the path. Since ρml\\nk(α(ℓ))≥0∀k,ℓ, the solution\\npaths are monotone.\\nThe lasso can similarly be characterized as the solution to a diﬀerential\\nequation as in (16.6), except that the move directions decre ase the loss\\noptimally per unit increase in the L1norm of the path. As a consequence,\\nthey are not necessarily positive, and hence the lasso paths need not be\\nmonotone.\\nIn this augmented dictionary, restricting the coeﬃcients t o be positive is\\nnatural, since it avoids an obvious ambiguity. It also ties i n more naturally\\nwith tree boosting—we always ﬁnd trees positively correlate d with the\\ncurrent residual.\\nThere have been suggestions that boosting performs well (fo r two-class\\nclassiﬁcation) because it exhibits maximal-margin proper ties, much like the\\nsupport-vector machines of Chapters 4.5.2 and 12. Schapire et al. (1998)\\ndeﬁne the normalized L1margin of a ﬁtted model f(x) =∑\\nkαkTk(x) as\\nm(f) = min\\niyif(xi)∑K\\nk=1|αk|. (16.7)\\nHere the minimum is taken over the training sample, and yi∈{−1,+1}.\\nUnlike the L2margin (4.40) of support vector machines, the L1margin\\nm(f) measures the distance to the closest training point in L∞units (max-\\nimum coordinate distance).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 633}),\n",
       " Document(page_content='616 16. Ensemble Learning\\n−0.3 −0.2 −0.1 0.0 0.1\\nNumber of TreesMargin\\n0 2K 4K 6K 8K 10K\\n0.25 0.26 0.27 0.28\\nNumber of TreesTest Error\\n0 2K 4K 6K 8K 10K\\nFIGURE 16.5. The left panel shows the L1marginm(f)for the Adaboost clas-\\nsiﬁer on the mixture data, as a function of the number of 4-node trees. The model\\nwas ﬁt using the R package gbm, with a shrinkage factor of 0.02. After10,000\\ntrees,m(f)has settled down. Note that when the margin crosses zero, the t raining\\nerror becomes zero. The right panel shows the test error, whi ch is minimized at\\n240trees. In this case, Adaboost overﬁts dramatically if run to c onvergence.\\nSchapire et al. (1998) prove that with separable data, Adabo ost in-\\ncreasesm(f) with each iteration, converging to a margin-symmetric so-\\nlution. R¨ atsch and Warmuth (2002) prove the asymptotic con vergence of\\nAdaboost with shrinkage to a L1-margin-maximizing solution. Rosset et\\nal. (2004a) consider regularized models of the form (16.2) f or general loss\\nfunctions. Theyshow that as λ↓0,for particular loss functions the solution\\nconverges to a margin-maximizing conﬁguration. In particu lar they show\\nthis to be the case for the exponential loss of Adaboost, as we ll as binomial\\ndeviance.\\nCollecting together the results of this section, we reach th e following\\nsummary for boosted classiﬁers:\\nThe sequence of boosted classiﬁers form an L1-regularized mono-\\ntone path to a margin-maximizing solution.\\nOfcoursethemargin-maximizingendofthepathcanbeaveryp oor,overﬁt\\nsolution, as it is in the example in Figure 16.5. Early stoppi ng amounts\\nto picking a point along the path, and should be done with the a id of a\\nvalidation dataset.\\n16.3 Learning Ensembles\\nTheinsightslearnedfromtheprevioussectionscanbeharne ssedtoproduce\\na more eﬀective and eﬃcient ensemble model. Again we conside r functions', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 634}),\n",
       " Document(page_content='16.3 Learning Ensembles 617\\nof the form\\nf(x) =α0+∑\\nTk∈TαkTk(x), (16.8)\\nwhereTis a dictionary of basis functions, typically trees. For gra dient\\nboosting and random forests, |T|is very large, and it is quite typical for the\\nﬁnal model to involve many thousands of trees. In the previou s section we\\narguethatgradientboostingwithshrinkageﬁtsan L1regularizedmonotone\\npath in this space of trees.\\nFriedman and Popescu (2003) propose a hybrid approach which breaks\\nthis process down into two stages:\\n•A ﬁnite dictionary TL={T1(x),T2(x),...,T M(x)}of basis functions\\nis induced from the training data;\\n•A family of functions fλ(x) is built by ﬁtting a lasso path in this\\ndictionary:\\nα(λ) = argmin\\nαN∑\\ni=1L[yi,α0+M∑\\nm=1αmTm(xi)]+λM∑\\nm=1|αm|.(16.9)\\nIn its simplest form this model could be seen as a way of post-p rocessing\\nboosting or random forests, taking for TLthe collection of trees produced\\nby the gradient boosting or random forest algorithms. By ﬁtt ing the lasso\\npathtothesetrees,wewouldtypically useamuchreducedset ,whichwould\\nsave in computations and storage for future predictions. In the next section\\nwedescribemodiﬁcationsofthisprescriptionthatreducet hecorrelationsin\\nthe ensembleTL, and improve the performance of the lasso post processor.\\nAs an initial illustration, we apply this procedure to a rand om forest\\nensemble grown on the spam data.\\nFigure 16.6 shows that a lasso post-processing oﬀers modest improve-\\nment over the random forest (blue curve), and reduces the for est to about\\n40 trees, rather than the original 1000. The post-processed performance\\nmatches that of gradient boosting. The orange curves repres ent a modiﬁed\\nversion of random forests, designed to reduce the correlati ons between trees\\neven more. Here a random sub-sample (without replacement) o f 5% of the\\ntraining sample is used to grow each tree, and the trees are re stricted to be\\nshallow (about six terminal nodes). The post-processing oﬀ ers more dra-\\nmatic improvements here, and the training costs are reduced by a factor\\nof about 100. However, the performance of the post-processe d model falls\\nsomewhat short of the blue curves.\\n16.3.1 Learning a Good Ensemble\\nNot all ensembles TLwill perform well with post-processing. In terms of\\nbasis functions, we want a collection that covers the space w ell in places', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 635}),\n",
       " Document(page_content='618 16. Ensemble Learning\\n0 100 200 300 400 5000.04 0.05 0.06 0.07 0.08 0.09Spam Data\\nNumber of TreesTest ErrorRandom Forest\\nRandom Forest (5%, 6)\\nGradient Boost (5 node)\\nFIGURE 16.6. Application of the lasso post-processing (16.9) to the spam da ta.\\nThe horizontal blue line is the test error of a random forest ﬁt t o the spam data,\\nusing1000trees grown to maximum depth (with m= 7; see Algorithm 15.1).\\nThe jagged blue curve is the test error after post-processing the ﬁrst 500trees\\nusing the lasso, as a function of the number of trees with nonze ro coeﬃcients.\\nThe orange curve/line use a modiﬁed form of random forest, whe re a random\\ndraw of 5% of the data are used to grow each tree, and the trees are force d to\\nbe shallow (typically six terminal nodes). Here the post-proc essing oﬀers much\\ngreater improvement over the random forest that generated t he ensemble.\\nwhere they are needed, and are suﬃciently diﬀerent from each other for\\nthe post-processor to be eﬀective.\\nFriedman and Popescu (2003) gain insights from numerical qu adrature\\nand importance sampling. They view the unknown function as a n integral\\nf(x) =∫\\nβ(γ)b(x;γ)dγ, (16.10)\\nwhereγ∈Γ indexes the basis functions b(x;γ). For example, if the basis\\nfunctions are trees, then γindexes the splitting variables, the split-points\\nand the values in the terminal nodes. Numerical quadrature a mounts to\\nﬁnding a set of Mevaluation points γm∈Γ and corresponding weights\\nαmso thatfM(x) =α0+∑M\\nm=1αmb(x;γm) approximates f(x) well over\\nthe domain of x. Importance sampling amounts to sampling γat random,\\nbut giving more weight to relevant regions of the space Γ. Fri edman and\\nPopescu (2003) suggest a measure of (lack of) relevance that uses the loss\\nfunction (16.9):', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 636}),\n",
       " Document(page_content='16.3 Learning Ensembles 619\\nQ(γ) = min\\nc0,c1N∑\\ni=1L(yi,c0+c1b(xi;γ)), (16.11)\\nevaluated on the training data.\\nIf a single basis function were to be selected (e.g., a tree), it would be\\nthe global minimizer γ∗= argmin γ∈ΓQ(γ). Introducing randomness in the\\nselection of γwould necessarily produce less optimal values with Q(γ)≥\\nQ(γ∗). They propose a natural measure of the characteristic widthσof the\\nsampling scheme S,\\nσ= ES[Q(γ)−Q(γ∗)]. (16.12)\\n•σtoo narrow suggests too many of the b(x;γm) look alike, and similar\\ntob(x;γ∗);\\n•σtoo wide implies a large spread in the b(x;γm), but possibly con-\\nsisting of many irrelevant cases.\\nFriedman and Popescu (2003) use sub-sampling as a mechanism for intro-\\nducing randomness, leading to their ensemble-generation a lgorithm 16.2.\\nAlgorithm 16.2 ISLE Ensemble Generation.\\n1.f0(x) = argmin c∑N\\ni=1L(yi,c)\\n2. Form= 1 toMdo\\n(a)γm= argmin γ∑\\ni∈Sm(η)L(yi,fm−1(xi)+b(xi;γ))\\n(b)fm(x) =fm−1(x)+νb(x;γm)\\n3.TISLE={b(x;γ1),b(x;γ2),...,b(x;γM)}.\\nSm(η) refers to a subsample of N·η(η∈(0,1]) of the training obser-\\nvations, typically withoutreplacement. Their simulations suggest picking\\nη≤1\\n2, and for large Npickingη∼1/√\\nN. Reducing ηincreases the\\nrandomness, and hence the width σ. The parameter ν∈[0,1] introduces\\nmemory into the randomization process; the larger ν, the more the pro-\\ncedure avoids b(x;γ) similar to those found before. A number of familiar\\nrandomization schemes are special cases of Algorithm 16.2:\\nBagging hasη= 1, but samples with replacement, and has ν= 0. Fried-\\nman and Hall (2007) argue that sampling without replacement with\\nη= 1/2 is equivalent to sampling with replacement with η= 1, and\\nthe former is much more eﬃcient.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 637}),\n",
       " Document(page_content='620 16. Ensemble Learning\\nRandom forest sampling is similar, with more randomness introduced by\\nthe selection of the splitting variable. Reducing η <1/2 in algo-\\nrithm 16.2 has a similar eﬀect to reducing min random forests, but\\ndoes not suﬀer from the potential biases discussed in Sectio n 15.4.2.\\nGradient boosting with shrinkage (10.41) uses η= 1, but typically does\\nnot produce suﬃcient width σ.\\nStochastic gradient boosting (Friedman, 1999) follows the recipe exactly.\\nThe authors recommend values ν= 0.1 andη≤1\\n2, and call their combined\\nprocedure (ensemble generation and post processing) Importance sampled\\nlearning ensemble (ISLE).\\nFigure 16.7 shows the performance of an ISLE on the spam data. It does\\n0 500 1000 1500 2000 25000.040 0.045 0.050 0.055 0.060Spam Data\\nNumber of TreesTest ErrorGradient Boosting (5 Node)\\nLasso Post−processed\\nFIGURE 16.7. Importance sampling learning ensemble (ISLE) ﬁt to the spam\\ndata. Here we used η= 1/2,ν= 0.05, and trees with ﬁve terminal nodes. The\\nlasso post-processed ensemble does not improve the predictio n error in this case,\\nbut it reduces the number of trees by a factor of ﬁve.\\nnot improve the predictive performance, but is able to produ ce a more\\nparsimonious model. Note that in practice the post-process ing includes\\nthe selection of the regularization parameter λin (16.9), which would be', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 638}),\n",
       " Document(page_content='16.3 Learning Ensembles 621\\nchosen by cross-validation. Here we simply demonstrate the eﬀects of post-\\nprocessing by showing the entire path on the test data.\\nFigure 16.8 shows various ISLEs on a regression example. The generating\\n0 500 1000 1500 2000 25001.0 1.5 2.0 2.5 3.0 3.5\\nNumber of TreesMean Squared ErrorGBM (1, 0.01)\\nGBM (0.1, 0.01)\\nISLE  GB\\nISLE RF\\nRandom Forest\\nFIGURE 16.8. Demonstration of ensemble methods on a regression simulation\\nexample. The notation GBM (0.1, 0.01) refers to a gradient boos ted model, with\\nparameters (η,ν). We report mean-squared error from the true (known) functio n.\\nNote that the sub-sampled GBM model (green) outperforms the fu ll GBM model\\n(orange). The lasso post-processed version achieves similar error. The random\\nforest is outperformed by its post-processed version, but b oth fall short of the\\nother models.\\nfunction is\\nf(X) = 10·5∏\\nj=1e−2X2\\nj+35∑\\nj=6Xj, (16.13)\\nwhereX∼U[0,1]100(the last 65 elements are noise variables). The re-\\nsponseY=f(X)+εwhereε∼N(0,σ2); we chose σ= 1.3 resulting in a\\nsignal-to-noise ratio of approximately 2. We used a trainin g sample of size\\n1000, and estimated the mean squared error E( ˆf(X)−f(X))2by averaging\\nover a test set of 500 samples. The sub-sampled GBM curve (lig ht blue)\\nis an instance of stochastic gradient boosting (Friedman, 1999) discussed in\\nSection 10.12, and it outperforms gradient boosting on this example.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 639}),\n",
       " Document(page_content='622 16. Ensemble Learning\\n16.3.2 Rule Ensembles\\nHere we describe a modiﬁcation of the tree-ensemble method t hat focuses\\non individual rules (Friedman and Popescu, 2003). We encoun tered rules\\nin Section 9.3 in the discussion of the PRIM method. The idea i s to enlarge\\nan ensemble of trees by constructing a set of rules from each o f the trees\\nin the collection.\\n1 2\\n30\\n4\\n5 6X1<2.1 X1≥2.1\\nX3∈ {M,L} X3∈ {S}\\nX7<4.5 X7≥4.5\\nFIGURE 16.9. A typical tree in an ensemble, from which rules can be derived.\\nFigure 16.9 depicts a small tree, with numbered nodes. The fo llowing\\nrules can be derived from this tree:\\nR1(X) =I(X1<2.1)\\nR2(X) =I(X1≥2.1)\\nR3(X) =I(X1≥2.1)·I(X3∈ {S})\\nR4(X) =I(X1≥2.1)·I(X3∈ {M,L})\\nR5(X) =I(X1≥2.1)·I(X3∈ {S})·I(X7<4.5)\\nR6(X) =I(X1≥2.1)·I(X3∈ {S})·I(X7≥4.5)(16.14)\\nA linear expansion in rules 1, 4, 5 and 6 is equivalent to the tr ee itself\\n(Exercise 16.3); hence (16.14) is an over-complete basis for the tree.\\nFor each tree Tmin an ensembleT, we can construct its mini-ensemble\\nof rulesTm\\nRULE, and then combine them all to form a larger ensemble\\nTRULE=M⋃\\nm=1Tm\\nRULE. (16.15)\\nThis is then treated like any other ensemble, and post-proce ssed via the\\nlasso or similar regularized procedure.\\nThere are several advantages to this approach of deriving ru les from the\\nmore complex trees:\\n•The space of models is enlarged, and can lead to improved perf or-\\nmance.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 640}),\n",
       " Document(page_content='16.3 Learning Ensembles 623\\nRules Rules + Linear0.9 1.0 1.1 1.2 1.3Mean Squared Error\\nFIGURE 16.10. Mean squared error for rule ensembles, using 20realizations\\nof the simulation example (16.13).\\n•Rules are easier to interpret than trees, so there is the pote ntial for\\na simpliﬁed model.\\n•It is often natural to augment TRULEby including each variable Xj\\nseparately as well, thus allowing the ensemble to model line ar func-\\ntions well.\\nFriedmanandPopescu(2008)demonstratethepowerofthispr ocedureona\\nnumber of illustrative examples, including the simulation example (16.13).\\nFigure 16.10 shows boxplots of the mean-squared error from t he true model\\nfor twenty realizations from this model. The models were all ﬁt using the\\nRulefitsoftware, available on the ESL homepage3, which runs in an auto-\\nmatic mode.\\nOn the same training set as used in Figure 16.8, the rule based model\\nachieved a mean-squared error of 1.06. Although slightly wo rse than the\\nbest achieved in that ﬁgure, the results are not comparable b ecause cross-\\nvalidation was used here to select the ﬁnal model.\\nBibliographic Notes\\nAs noted in the introduction, many of the new methods inmachi ne learning\\nhave been dubbed “ensemble” methods. These include neural n etworks\\nboosting, bagging and random forests; Dietterich (2000a) g ives a survey of\\ntree-based ensemble methods. Neural networks (Chapter 11) are perhaps\\nmoredeservingofthename,sincetheysimultaneouslylearn theparameters\\n3ESL homepage: www-stat.stanford.edu/ElemStatLearn', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 641}),\n",
       " Document(page_content='624 16. Ensemble Learning\\nof the hidden units (basis functions), along with how to comb ine them.\\nBishop (2006) discusses neural networks in some detail, alo ng with the\\nBayesian perspective (MacKay, 1992; Neal, 1996). Support v ector machines\\n(Chapter 12) can also be regarded as an ensemble method; they perform\\nL2regularized model ﬁtting in high-dimensional feature spac es. Boosting\\nand lasso exploit sparsity through L1regularization to overcome the high-\\ndimensionality, while SVMs rely on the “kernel trick” chara cteristic of L2\\nregularization.\\nC5.0 (Quinlan, 2004) is a commercial tree and rule generatio n package,\\nwith some goals in common with Rulefit.\\nThere is a vast and varied literature often referred to as “co mbining clas-\\nsiﬁers” which abounds in ad-hoc schemes for mixing methods o f diﬀerent\\ntypes to achieve better performance. For a principled appro ach, see Kittler\\net al. (1998).\\nExercises\\nEx. 16.1 Describe exactly how to generate the block correlated data u sed\\nin the simulation in Section 16.2.3.\\nEx. 16.2 Letα(t)∈IRpbe a piecewise-diﬀerentiable and continuous coef-\\nﬁcient proﬁle, with α(0) = 0. The L1arc-length of αfrom time 0 to tis\\ndeﬁned by\\nΛ(t) =∫t\\n0|˙α(t)|1dt. (16.16)\\nShow that Λ( t)≥|α(t)|1, with equality iﬀ α(t) is monotone.\\nEx. 16.3 Show that ﬁtting a linear regression model using rules 1, 4, 5 and\\n6 in equation (16.14) gives the same ﬁt as the regression tree corresponding\\nto this tree. Show the same is true for classiﬁcation, if a log istic regression\\nmodel is ﬁt.\\nEx. 16.4 Program and run the simulation study described in Figure 16. 2.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 642}),\n",
       " Document(page_content='This is page 625\\nPrinter: Opaque this\\n17\\nUndirected Graphical Models\\n17.1 Introduction\\nA graph consists of a set of vertices (nodes), along with a set of edges join-\\ning some pairs of the vertices. In graphical models, each ver tex represents\\na random variable, and the graph gives a visual way of underst anding the\\njoint distribution of the entire set of random variables. Th ey can be use-\\nful for either unsupervised or supervised learning. In an undirected graph ,\\nthe edges have no directional arrows. We restrict our discus sion to undi-\\nrected graphical models, also known as Markov random ﬁelds orMarkov\\nnetworks . In these graphs, the absence of an edge between two vertices has\\na special meaning: the corresponding random variables are c onditionally\\nindependent, given the other variables.\\nFigure 17.1 shows an example of a graphical model for a ﬂow-cy tometry\\ndataset with p= 11 proteins measured on N= 7466 cells, from Sachs\\net al. (2005). Each vertex in the graph corresponds to the rea l-valued ex-\\npression level of a protein. The network structure was estim ated assuming\\na multivariate Gaussian distribution, using the graphical lasso procedure\\ndiscussed later in this chapter.\\nSparsegraphshavearelativelysmallnumberofedges,andar econvenient\\nfor interpretation. They are useful in a variety of domains, including ge-\\nnomics and proteomics, where they provide rough models of ce ll pathways.\\nMuch work has been done in deﬁning and understanding the stru cture of\\ngraphical models; see the Bibliographic Notes for referenc es.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 643}),\n",
       " Document(page_content='626 17. Undirected Graphical Models\\nRaf\\nMek\\nPlcg\\nPIP2\\nPIP3\\nErk AktPKAPKCP38Jnk\\nFIGURE 17.1. Example of a sparse undirected graph, estimated from a ﬂow\\ncytometry dataset, with p= 11proteins measured on N= 7466cells. The net-\\nwork structure was estimated using the graphical lasso proce dure discussed in this\\nchapter.\\nAs we will see, the edges in a graph are parametrized by values orpo-\\ntentialsthat encode the strength of the conditional dependence betw een\\nthe random variables at the corresponding vertices. The mai n challenges in\\nworking with graphical models are model selection (choosin g the structure\\nof the graph), estimation of the edge parameters from data, a nd compu-\\ntation of marginal vertex probabilities and expectations, from their joint\\ndistribution. The last two tasks are sometimes called learningandinference\\nin the computer science literature.\\nWe do not attempt a comprehensive treatment of this interest ing area.\\nInstead, we introduce some basic concepts, and then discuss a few sim-\\nple methods for estimation of the parameters and structure o f undirected\\ngraphical models; methods that relate to the techniques alr eady discussed\\nin this book. The estimation approaches that we present for c ontinuous\\nand discrete-valued vertices are diﬀerent, so we treat them separately. Sec-\\ntions 17.3.1 and 17.3.2 may be of particular interest, as the y describe new,\\nregression-based procedures for estimating graphical mod els.\\nThere is a large and active literature on directed graphical models or\\nBayesian networks ; these are graphical models in which the edges have\\ndirectional arrows (but no directed cycles). Directed grap hical models rep-\\nresent probability distributions that can be factored into products of condi-\\ntional distributions, and have the potential for causal int erpretations. We\\nrefer the reader to Wasserman (2004) for a brief overview of b oth undi-\\nrected and directed graphs; the next section follows closel y his Chapter 18.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 644}),\n",
       " Document(page_content='17.2 Markov Graphs and Their Properties 627\\nX\\nXXX\\nYYYY\\nZ\\nZZ\\nZ\\nWWW\\n(a) (b)\\n(c) (d)\\nFIGURE 17.2. Examples of undirected graphical models or Markov networks.\\nEach node or vertex represents a random variable, and the lack o f an edge between\\ntwo nodes indicates conditional independence. For example, in graph (a), Xand\\nZare conditionally independent, given Y. In graph (b), Zis independent of each\\nofX,Y, andW.\\nA longer list of useful references is given in the Bibliograp hic Notes on\\npage 645.\\n17.2 Markov Graphs and Their Properties\\nIn this section we discuss the basic properties of graphs as m odels for the\\njoint distribution of a set of random variables. We defer dis cussion of (a)\\nparametrization and estimation of the edge parameters from data, and (b)\\nestimation of the topology of a graph, to later sections.\\nFigure17.2showsfourexamplesofundirectedgraphs.Agrap hGconsists\\nof a pair (V,E), whereVis a set of vertices and Ethe set of edges (deﬁned\\nby pairs of vertices). Two vertices XandYare called adjacent if there\\nis a edge joining them; this is denoted by X∼Y. ApathX1,X2,...,X n\\nis a set of vertices that are joined, that is Xi−1∼Xifori= 2,...,n. A\\ncomplete graph is a graph with every pair of vertices joined by an edge.\\nAsubgraphU∈Vis a subset of vertices together with their edges. For\\nexample, (X,Y,Z) in Figure 17.2(a) form a path but not a complete graph.\\nSuppose that we have a graph Gwhose vertex set Vrepresents a set of\\nrandom variables having joint distribution P. In a Markov graph G, the\\nabsence of an edge implies that the corresponding random var iables are\\nconditionally independent given the variables at the other vertices. This is\\nexpressed with the following notation:', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 645}),\n",
       " Document(page_content='628 17. Undirected Graphical Models\\nNo edge joining XandY⇐⇒X⊥Y|rest (17.1)\\nwhere “rest” refers to all of the other vertices in the graph. For example\\nin Figure 17.2(a) X⊥Z|Y. These are known as the pairwise Markov\\nindependencies ofG.\\nIfA,BandCare subgraphs, then Cis said to separateAandBif every\\npath between AandBintersects a node in C. For example, Yseparates\\nXandZin Figures 17.2(a) and (d), and ZseparatesYandWin (d). In\\nFigure 17.2(b) Zis not connected to X,Y,W so we say that the two sets\\nare separated by the empty set. In Figure 17.2(c), C={X,Z}separates\\nYandW.\\nSeparators have the nice property that they break the graph i nto con-\\nditionally independent pieces. Speciﬁcally, in a Markov gr aphGwith sub-\\ngraphsA,BandC,\\nifCseparatesAandBthenA⊥B|C. (17.2)\\nThese are known as the global Markov properties ofG. It turns out that the\\npairwise and global Markov properties of a graph are equival ent (for graphs\\nwith positive distributions). That is, the set of graphs wit h associated prob-\\nability distributions that satisfy the pairwise Markov ind ependencies and\\nglobal Markov assumptions are the same. This result is usefu l for inferring\\nglobal independence relations from simple pairwise proper ties. For example\\nin Figure 17.2(d) X⊥Z|{Y,W}since it is a Markov graph and there is no\\nlink joining XandZ. ButYalso separates XfromZandWand hence by\\nthe global Markov assumption we conclude that X⊥Z|YandX⊥W|Y.\\nSimilarly we have Y⊥W|Z.\\nThe global Markov property allows us to decompose graphs int o smaller\\nmore manageable pieces and thus leads to essential simpliﬁc ations in com-\\nputation and interpretation. For this purpose we separate t he graph into\\ncliques. A cliqueis a complete subgraph— a set of vertices that are all\\nadjacent to one another; it is called maximal if it is a clique and no other\\nvertices can be added to it and still yield a clique. The maxim al cliques for\\nthe graphs of Figure 17.2 are\\n(a){X,Y},{Y,Z},\\n(b){X,Y,W},{Z},\\n(c){X,Y},{Y,Z},{Z,W},{X,W}, and\\n(d){X,Y},{Y,Z},{Z,W}.\\nAlthough the following applies to both continuous and discr ete distri-\\nbutions, much of the development has been for the latter. A pr obability\\ndensity function fover a Markov graph Gcan be can represented as', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 646}),\n",
       " Document(page_content='17.2 Markov Graphs and Their Properties 629\\nf(x) =1\\nZ∏\\nC∈CψC(xC) (17.3)\\nwhereCis the set of maximal cliques, and the positive functions ψC(·) are\\ncalledclique potentials . These are not in general density functions1, but\\nrather are aﬃnities that capture the dependence in XCby scoring certain\\ninstancesxChigher than others. The quantity\\nZ=∑\\nx∈X∏\\nC∈CψC(xC) (17.4)\\nis the normalizing constant, also known as the partition function. Alterna-\\ntively, the representation (17.3) implies a graph with inde pendence prop-\\nerties deﬁned by the cliques in the product. This result hold s for Markov\\nnetworksGwith positive distributions, and is known as the Hammersley-\\nCliﬀordtheorem (Hammersley and Cliﬀord, 1971; Cliﬀord, 1990).\\nMany of the methods for estimation and computation on graphs ﬁrst de-\\ncompose the graph into its maximal cliques. Relevant quanti ties are com-\\nputed in the individual cliques and then accumulated across the entire\\ngraph. A prominent example is the join tree orjunction tree algorithm for\\ncomputing marginal and low order probabilities from the joi nt distribution\\non a graph. Details can be found in Pearl (1986), Lauritzen an d Spiegel-\\nhalter (1988), Pearl (1988), Shenoy and Shafer (1988), Jens en et al. (1990),\\nor Koller and Friedman (2007).\\nXY\\nZ\\nFIGURE 17.3. A complete graph does not uniquely specify the higher-order\\ndependence structure in the joint distribution of the varia bles.\\nA graphical model does not always uniquely specify the highe r-order\\ndependence structure of a joint probability distribution. Consider the com-\\nplete three-node graph in Figure 17.3. It could represent th e dependence\\nstructure of either of the following distributions:\\nf(2)(x,y,z) =1\\nZψ(x,y)ψ(x,z)ψ(y,z);\\nf(3)(x,y,z) =1\\nZψ(x,y,z).(17.5)\\nThe ﬁrst speciﬁes only second order dependence (and can be re presented\\nwith fewer parameters). Graphical models for discrete data are a special\\n1If the cliques are separated, then the potentials can be densities, but thi s is in general\\nnot the case.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 647}),\n",
       " Document(page_content='630 17. Undirected Graphical Models\\ncase ofloglinear models for multiway contingency tables (Bishop et al.,\\n1975, e.g.); in that language f(2)is referred to as the “no second-order\\ninteraction” model.\\nFor the remainder of this chapter we focus on pairwise Markov graphs\\n(Koller and Friedman, 2007). Here there is a potential funct ion for each\\nedge (pair of variables as in f(2)above), and at most second–order interac-\\ntions are represented. These are more parsimonious in terms of parameters,\\neasier to work with, and give the minimal complexity implied by the graph\\nstructure. The models for both continuous and discrete data are functions\\nof only the pairwise marginal distributions of the variable s represented in\\nthe edge set.\\n17.3 Undirected Graphical Models for Continuous\\nVariables\\nHere we consider Markov networks where all the variables are continuous.\\nThe Gaussian distribution is almost always used for such gra phical models,\\nbecauseofitsconvenientanalyticalproperties.Weassume thattheobserva-\\ntionshaveamultivariateGaussiandistributionwithmean µandcovariance\\nmatrixΣ. Since the Gaussian distribution represents at most second -order\\nrelationships,itautomatically encodesapairwiseMarkov graph.Thegraph\\nin Figure 17.1 is an example of a Gaussian graphical model.\\nThe Gaussian distribution has the property that all conditi onal distri-\\nbutions are also Gaussian. The inverse covariance matrix Σ−1contains\\ninformation about the partial covariances between the variables; that is,\\nthe covariances between pairs iandj, conditioned on all other variables.\\nInparticular,ifthe ijthcomponentof Θ=Σ−1iszero,thenvariables iand\\njare conditionally independent, given the other variables ( Exercise 17.3).\\nIt is instructive to examine the conditional distribution o f one variable\\nversus the rest, where the role of Θis explicit. Suppose we partition X=\\n(Z,Y) whereZ= (X1,...,X p−1) consists of the ﬁrst p−1 variables and\\nY=Xpis the last. Then we have the conditional distribution of YgiveZ\\n(Mardia et al., 1979, e.g.)\\nY|Z=z∼N(\\nµY+(z−µZ)TΣ−1\\nZZσZY, σYY−σT\\nZYΣ−1\\nZZσZY)\\n,(17.6)\\nwhere we have partitioned Σas\\nΣ=(ΣZZσZY\\nσT\\nZYσYY)\\n. (17.7)\\nThe conditional mean in (17.6) has exactly the same form as th e pop-\\nulation multiple linear regression of YonZ, with regression coeﬃcient\\nβ=Σ−1\\nZZσZY[see (2.16) on page 19]. If we partition Θin the same way,\\nsinceΣΘ=Istandard formulas for partitioned inverses give', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 648}),\n",
       " Document(page_content='17.3 Undirected Graphical Models for Continuous Variables 6 31\\nθZY=−θYY·Σ−1\\nZZσZY, (17.8)\\nwhere 1/θYY=σYY−σT\\nZYΣ−1\\nZZσZY>0. Hence\\nβ=Σ−1\\nZZσZY\\n=−θZY/θYY.(17.9)\\nWe have learned two things here:\\n•The dependence of YonZin (17.6) is in the mean term alone. Here\\nwe see explicitly that zero elements in βand henceθZYmean that\\nthe corresponding elements of Zare conditionally independent of Y,\\ngiven the rest.\\n•Wecanlearnaboutthis dependencestructurethroughmultip le linear\\nregression.\\nThusΘcaptures all the second-order information (both structura l and\\nquantitative) needed to describe the conditional distribu tion of each node\\ngiven the rest, and is the so-called “natural” parameter for the Gaussian\\ngraphical model2.\\nAnother(diﬀerent)kindofgraphicalmodelisthe covariance graph orrel-\\nevance network ,inwhichverticesareconnectedbybidirectionaledgesift he\\ncovariance (rather than the partial covariance) between th e corresponding\\nvariables is nonzero. These are popular in genomics, see esp ecially Butte\\net al. (2000). The negative log-likelihood from these model s is not convex,\\nmaking the computations more challenging (Chaudhuri et al. , 2007).\\n17.3.1 Estimation of the Parameters when the Graph\\nStructure is Known\\nGiven some realizations of X, we would like to estimate the parameters\\nof an undirected graph that approximates their joint distri bution. Suppose\\nﬁrst that the graph is complete (fully connected). We assume that we have\\nNmultivariate normal realizations xi, i= 1,...,Nwith population mean\\nµand covariance Σ. Let\\nS=1\\nNN∑\\ni=1(xi−¯x)(xi−¯x)T(17.10)\\nbetheempiricalcovariancematrix,with ¯ xthesamplemeanvector.Ignoring\\nconstants, the log-likelihood of the data can be written as\\n2The distribution arising from a Gaussian graphical model is a Wishar t distribution.\\nThis is a member of the exponential family, with canonical or “natur al” parameter\\nΘ=Σ−1. Indeed, the partially maximized log-likelihood (17.11) is (up t o constants)\\nthe Wishart log-likelihood.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 649}),\n",
       " Document(page_content='632 17. Undirected Graphical Models\\nℓ(Θ) = logdet Θ−trace(SΘ). (17.11)\\nIn (17.11) we have partially maximized with respect to the me an parameter\\nµ. The quantity−ℓ(Θ) is a convex function of Θ. It is easy to show that\\nthe maximum likelihood estimate of Σis simply S.\\nNow to make the graph more useful (especially in high-dimens ional set-\\ntings) let’s assume that some of the edges are missing; for ex ample, the\\nedge between PIP3andErkis one of several missing in Figure 17.1. As we\\nhave seen, for the Gaussian distribution this implies that t he correspond-\\ning entries of Θ=Σ−1are zero. Hence we now would like to maximize\\n(17.11) under the constraints that some pre-deﬁned subset o f the parame-\\nters are zero. This is an equality-constrained convex optim ization problem,\\nand a number of methods have been proposed for solving it, in p articular\\nthe iterative proportional ﬁtting procedure (Speed and Kii veri, 1986). This\\nand other methods are summarized for example in Whittaker (1 990) and\\nLauritzen (1996). These methods exploit the simpliﬁcation s that arise from\\ndecomposing the graph into its maximal cliques, as describe d in the previ-\\nous section. Here we outline a simple alternate approach, th at exploits the\\nsparsity in a diﬀerent way. The fruits of this approach will b ecome apparent\\nlater when we discuss the problem of estimation of the graph s tructure.\\nThe idea is based on linear regression, as inspired by (17.6) and (17.9).\\nIn particular, suppose that we want to estimate the edge para metersθijfor\\nthe vertices that are joined to a given vertex i, restricting those that are not\\njoined to be zero. Then it would seem that the linear regressi on of the node\\nivalues on the other relevant vertices might provide a reason able estimate.\\nBut this ignores the dependence structure among the predict ors in this\\nregression. It turns out that if instead we use our current (m odel-based)\\nestimate of the cross-product matrix of the predictors when we perform\\nour regressions, this gives the correct solutions and solve s the constrained\\nmaximum-likelihood problem exactly. We now give details.\\nTo constrain the log-likelihood (17.11), we add Lagrange co nstants for\\nall missing edges\\nℓC(Θ) = logdet Θ−trace(SΘ)−∑\\n(j,k)̸∈Eγjkθjk. (17.12)\\nThe gradient equation for maximizing (17.12) can be written as\\nΘ−1−S−Γ=0, (17.13)\\nusing the fact that the derivative of logdet ΘequalsΘ−1(Boyd and Van-\\ndenberghe, 2004, for example, page 641). Γis a matrix of Lagrange param-\\neters with nonzero values for all pairs with edges absent.\\nWe will show how we can use regression to solve for Θand its inverse\\nW=Θ−1one row and column at a time. For simplicity let’s focus on the\\nlast row and column. Then the upper right block of equation (1 7.13) can\\nbe written as', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 650}),\n",
       " Document(page_content='17.3 Undirected Graphical Models for Continuous Variables 6 33\\nw12−s12−γ12= 0. (17.14)\\nHere we have partitioned the matrices into two parts as in (17 .7): part 1\\nbeing the ﬁrst p−1 rows and columns, and part 2 the pth row and column.\\nWithWand its inverse Θpartitioned in a similar fashion, we have\\n(W11w12\\nwT\\n12w22)(Θ11θ12\\nθT\\n12θ22)\\n=(I0\\n0T1)\\n. (17.15)\\nThis implies\\nw12=−W11θ12/θ22 (17.16)\\n=W11β (17.17)\\nwhereβ=−θ12/θ22as in (17.9). Now substituting (17.17) into (17.14)\\ngives\\nW11β−s12−γ12= 0. (17.18)\\nThese can be interpreted as the p−1 estimating equations for the con-\\nstrained regression of Xpon the other predictors, except that the observed\\nmean cross-products matrix S11is replaced by W11, the current estimated\\ncovariance matrix from the model.\\nNow we can solve (17.18) by simple subset regression. Suppos e there are\\np−qnonzero elements in γ12—i.e.,p−qedges constrained to be zero. These\\np−qrows carry no information and can be removed. Furthermore we can\\nreduceβtoβ∗by removing its p−qzero elements, yielding the reduced\\nq×qsystem of equations\\nW∗\\n11β∗−s∗\\n12= 0, (17.19)\\nwith solution ˆβ∗=W∗\\n11−1s∗\\n12. This is padded with p−qzeros to give ˆβ.\\nAlthough it appears from (17.16) that we only recover the ele mentsθ12\\nup to a scale factor 1 /θ22, it is easy to show that\\n1\\nθ22=w22−wT\\n12β (17.20)\\n(using partitioned inverse formulas). Also w22=s22, since the diagonal of\\nΓin (17.13) is zero.\\nThis leads to the simple iterative procedure given in Algori thm 17.1 for\\nestimating both ˆWand its inverse ˆΘ, subject to the constraints of the\\nmissing edges.\\nNote that this algorithm makes conceptual sense. The graph e stimation\\nproblem is not pseparate regression problems, but rather pcoupled prob-\\nlems. The use of the common Win step (b), in place of the observed\\ncross-products matrix, couples the problems together in th e appropriate\\nfashion. Surprisingly, we were not able to ﬁnd this procedur e in the lit-\\nerature. However it is related to the covariance selection p rocedures of', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 651}),\n",
       " Document(page_content='634 17. Undirected Graphical Models\\nAlgorithm 17.1 A Modiﬁed Regression Algorithm for Estimation of an\\nUndirected Gaussian Graphical Model with Known Structure.\\n1. Initialize W=S.\\n2. Repeat for j= 1,2,...,p,1,...until convergence:\\n(a) Partition the matrix Winto part 1: all but the jth row and\\ncolumn, and part 2: the jth row and column.\\n(b) Solve W∗\\n11β∗−s∗\\n12= 0 for the unconstrained edge parameters\\nβ∗, using the reduced system of equations as in (17.19). Obtain\\nˆβby padding ˆβ∗with zeros in the appropriate positions.\\n(c) Update w12=W11ˆβ\\n3. In the ﬁnal cycle (for each j) solve for ˆθ12=−ˆβ·ˆθ22, with 1/ˆθ22=\\ns22−wT\\n12ˆβ.\\nX1X2 X3\\nX4S=\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ed10 1 5 4\\n1 10 2 6\\n5 2 10 3\\n4 6 3 10\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8\\nFIGURE 17.4. A simple graph for illustration, along with the empirical covar i-\\nance matrix.\\nDempster (1972), and is similar in ﬂavor to the iterative con ditional ﬁtting\\nprocedure for covariance graphs, proposed by Chaudhuri et a l. (2007).\\nHere is a little example, borrowed from Whittaker (1990). Su ppose that\\nour model is as depicted in Figure 17.4, along with its empiri cal covariance\\nmatrixS. We apply algorithm (17.1) to this problem; for example, in t he\\nmodiﬁed regression for variable 1 in step (b), variable 3 is l eft out. The\\nprocedure quickly converged to the solutions:\\nˆΣ=\\uf8eb\\n\\uf8ec\\uf8ed10.00 1.001.314.00\\n1.00 10.00 2.000.87\\n1.312.00 10.00 3.00\\n4.000.873.00 10.00\\uf8f6\\n\\uf8f7\\uf8f8,ˆΣ−1=\\uf8eb\\n\\uf8ec\\uf8ed0.12−0.010.00−0.05\\n−0.01 0.11−0.020.00\\n0.00−0.02 0.11−0.03\\n−0.050.00−0.03 0.13\\uf8f6\\n\\uf8f7\\uf8f8.\\nNote the zeroes in ˆΣ−1, corresponding to the missing edges (1,3) and (2,4).\\nNote also that the corresponding elements in ˆΣare the only elements dif-\\nferent from S. The estimation of ˆΣis an example of what is sometimes\\ncalled the positive deﬁnite “completion” of S.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 652}),\n",
       " Document(page_content='17.3 Undirected Graphical Models for Continuous Variables 6 35\\n17.3.2 Estimation of the Graph Structure\\nIn most cases we do not know which edges to omit from our graph, and\\nso would like to try to discover this from the data itself. In r ecent years a\\nnumber of authors have proposed the use of L1(lasso) regularization for\\nthis purpose.\\nMeinshausen and B¨ uhlmann (2006) take a simple approach to t he prob-\\nlem: rather than trying to fully estimate ΣorΘ=Σ−1, they only estimate\\nwhich components of θijare nonzero. To do this, they ﬁt a lasso regression\\nusing each variable as the response and the others as predict ors. The com-\\nponentθijis then estimated to be nonzero if either the estimated coeﬃc ient\\nof variable ionjis nonzero, orthe estimated coeﬃcient of variable jon\\niis nonzero (alternatively they use an andrule). They show that asymp-\\ntotically this procedure consistently estimates the set of nonzero elements\\nofΘ.\\nWecantakeamoresystematicapproachwiththelassopenalty ,following\\nthedevelopmentoftheprevioussection.Considermaximizi ngthepenalized\\nlog-likelihood\\nlogdetΘ−trace(SΘ)−λ||Θ||1, (17.21)\\nwhere||Θ||1is theL1norm—the sum of the absolute values of the elements\\nofΣ−1, and we have ignored constants. The negative of this penaliz ed\\nlikelihood is a convex function of Θ.\\nIt turns out that one can adapt the lasso to give the exact maxi mizer of\\nthe penalized log-likelihood. In particular, we simply rep lace the modiﬁed\\nregression step (b) in Algorithm 17.1 by a modiﬁed lasso step . Here are the\\ndetails.\\nThe analog of the gradient equation (17.13) is now\\nΘ−1−S−λ·Sign(Θ) =0. (17.22)\\nHere we use sub-gradient notation, with Sign( θjk) = sign(θjk) ifθjk̸= 0,\\nelse Sign(θjk)∈[−1,1] ifθjk= 0. Continuing the development in the\\nprevious section, we reach the analog of (17.18)\\nW11β−s12+λ·Sign(β) = 0 (17.23)\\n(recall that βandθ12haveoppositesigns). Wewill nowseethat thissystem\\nis exactly equivalent to the estimating equations for a lass o regression.\\nConsider the usual regression setup with outcome variables yand pre-\\ndictor matrix Z. There the lasso minimizes\\n1\\n2(y−Zβ)T(y−Zβ)+λ·||β||1 (17.24)\\n[see (3.52) on page 68; here we have added a factor1\\n2for convenience]. The\\ngradient of this expression is', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 653}),\n",
       " Document(page_content='636 17. Undirected Graphical Models\\nAlgorithm 17.2 Graphical Lasso.\\n1. Initialize W=S+λI. The diagonal of Wremains unchanged in\\nwhat follows.\\n2. Repeat for j= 1,2,...p,1,2,...p,... until convergence:\\n(a) Partition the matrix Winto part 1: all but the jth row and\\ncolumn, and part 2: the jth row and column.\\n(b) Solve the estimating equations W11β−s12+λ·Sign(β) = 0\\nusing the cyclical coordinate-descent algorithm (17.26) f or the\\nmodiﬁed lasso.\\n(c) Update w12=W11ˆβ\\n3. In the ﬁnal cycle (for each j) solve for ˆθ12=−ˆβ·ˆθ22, with 1/ˆθ22=\\nw22−wT\\n12ˆβ.\\nZTZβ−ZTy+λ·Sign(β) = 0 (17.25)\\nSo up to a factor 1 /N,ZTyis the analog of s12, and we replace ZTZby\\nW11, the estimated cross-product matrix from our current model .\\nThe resulting procedure is called the graphical lasso , proposed by Fried-\\nman et al. (2008b) building on the work of Banerjee et al. (200 8). It is\\nsummarized in Algorithm 17.2.\\nFriedman et al. (2008b) use the pathwise coordinate descent method\\n(Section 3.8.6) to solve the modiﬁed lasso problem at each st age. Here are\\nthe details of pathwise coordinate descent for the graphica l lasso algorithm.\\nLettingV=W11, the update has the form\\nˆβj←S(\\ns12j−∑\\nk̸=jVkjˆβk,λ)\\n/Vjj (17.26)\\nforj= 1,2,...,p−1,1,2,...,p−1,..., whereSis the soft-threshold\\noperator:\\nS(x,t) = sign(x)(|x|−t)+. (17.27)\\nThe procedure cycles through the predictors until converge nce.\\nIt is easy to show that the diagonal elements wjjof the solution matrix\\nWare simplysjj+λ, and these are ﬁxed in step 1 of Algorithm 17.23.\\nThe graphical lasso algorithm is extremely fast, and can sol ve a moder-\\nately sparse problem with 1000 nodes in less than a minute. It is easy to\\nmodify the algorithm to have edge-speciﬁc penalty paramete rsλjk; since\\n3An alternative formulation of the problem (17.21) can be posed, wh ere we don’t\\npenalize the diagonal of Θ. Then the diagonal elements wjjof the solution matrix are\\nsjj, and the rest of the algorithm is unchanged.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 654}),\n",
       " Document(page_content='17.3 Undirected Graphical Models for Continuous Variables 6 37\\nλjk=∞will force ˆθjkto be zero, this algorithm subsumes Algorithm 17.1.\\nBy casting the sparse inverse-covariance problem as a serie s of regressions,\\none can also quickly compute and examine the solution paths a s a function\\nof the penalty parameter λ. More details can be found in Friedman et al.\\n(2008b).\\nRaf\\nMek\\nPlcg\\nPIP2\\nPIP3\\nErk AktPKAPKCP38JnkRaf\\nMek\\nPlcg\\nPIP2\\nPIP3\\nErk AktPKAPKCP38Jnk\\nRaf\\nMek\\nPlcg\\nPIP2\\nPIP3\\nErk AktPKAPKCP38JnkRaf\\nMek\\nPlcg\\nPIP2\\nPIP3\\nErk AktPKAPKCP38Jnkλ= 0 λ= 7λ= 27 λ= 36\\nFIGURE 17.5. Four diﬀerent graphical-lasso solutions for the ﬂow-cytometr y\\ndata.\\nFigure 17.1 shows the result of applying the graphical lasso to the ﬂow-\\ncytometry dataset. Here the lasso penalty parameter λwas set at 14. In\\npractice it is informative to examine the diﬀerent sets of gr aphs that are\\nobtained as λis varied. Figure 17.5 shows four diﬀerent solutions. The\\ngraph becomes more sparse as the penalty parameter is increa sed.\\nFinallynotethatthevaluesatsomeofthenodesinagraphica lmodelcan\\nbe unobserved; that is, missing or hidden. If only some value s are missing\\nat a node, the EM algorithm can be used to impute the missing va lues', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 655}),\n",
       " Document(page_content='638 17. Undirected Graphical Models\\n(Exercise 17.9). However, sometimes the entire node is hidd en orlatent.\\nIn the Gaussian model, if a node has all missing values, due to linearity\\none can simply average over the missing nodes to yield anothe r Gaussian\\nmodel over the observed nodes. Hence the inclusion of hidden nodes does\\nnot enrich the resulting model for the observed nodes; in fac t, it imposes\\nadditionalstructureonitscovariancematrix.Howeverint hediscretemodel\\n(described next) the inherent nonlinearities make hidden u nits a powerful\\nway of expanding the model.\\n17.4 Undirected Graphical Models for Discrete\\nVariables\\nUndirected Markov networks with all discrete variables are popular, and\\nin particular pairwise Markov networks with binary variabl es being the\\nmost common. They are sometimes called Ising models in the statistical\\nmechanics literature, and Boltzmann machines in the machine learning lit-\\nerature, where the vertices are referred to as “nodes” or “un its” and are\\nbinary-valued.\\nIn addition, the values at each node can be observed (“visibl e”) or un-\\nobserved (“hidden”). The nodes are often organized in layer s, similar to a\\nneural network. Boltzmann machines are useful both for unsu pervised and\\nsupervised learning, especially for structured input data such as images,\\nbut have been hampered by computational diﬃculties. Figure 17.6 shows\\na restricted Boltzmann machine (discussed later), in which some variables\\nare hidden, and only some pairs of nodes are connected. We ﬁrs t consider\\nthe simpler case in which all pnodes are visible with edge pairs ( j,k) enu-\\nmerated in E.\\nDenoting the binary valued variable at node jbyXj, the Ising model\\nfor their joint probabilities is given by\\np(X,Θ) = exp[∑\\n(j,k)∈EθjkXjXk−Φ(Θ)]\\nforX∈X,(17.28)\\nwithX={0,1}p. As with the Gaussian model of the previous section,\\nonly pairwise interactions are modeled. The Ising model was developed in\\nstatistical mechanics, and is now used more generally to mod el the joint\\neﬀects of pairwise interactions. Φ( Θ) is the log of the partition function,\\nand is deﬁned by\\nΦ(Θ) = log∑\\nx∈X[\\nexp(∑\\n(j,k)∈Eθjkxjxk)]\\n. (17.29)\\nThe partition function ensures that the probabilities add t o one over the\\nsample space. The terms θjkXjXkrepresent a particular parametrization', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 656}),\n",
       " Document(page_content='17.4 Undirected Graphical Models for Discrete Variables 639\\nof the (log) potential functions (17.5), and for technical r easons requires\\naconstant nodeX0≡1 to be included (Exercise 17.10), with “edges” to\\nall the other nodes. In the statistics literature, this mode l is equivalent\\nto a ﬁrst-order-interaction Poisson log-linear model for m ultiway tables of\\ncounts (Bishop et al., 1975; McCullagh and Nelder, 1989; Agr esti, 2002).\\nThe Ising model implies a logistic form for each node conditi onal on the\\nothers (exercise 17.11):\\nPr(Xj= 1|X−j=x−j) =1\\n1+exp(−θj0−∑\\n(j,k)∈Eθjkxk),(17.30)\\nwhereX−jdenotes all of the nodes except j. Hence the parameter θjk\\nmeasures the dependence of XjonXk, conditional on the other nodes.\\n17.4.1 Estimation of the Parameters when the Graph\\nStructure is Known\\nGiven some data from this model, how can we estimate the param eters?\\nSupposewehaveobservations xi= (xi1,xi2,...,x ip)∈{0,1}p, i= 1,...,N.\\nThe log-likelihood is\\nℓ(Θ) =N∑\\ni=1logPrΘ(Xi=xi)\\n=N∑\\ni=1\\uf8ee\\n\\uf8f0∑\\n(j,k)∈Eθjkxijxik−Φ(Θ)\\uf8f9\\n\\uf8fb (17.31)\\nThe gradient of the log-likelihood is\\n∂ℓ(Θ)\\n∂θjk=N∑\\ni=1xijxik−N∂Φ(Θ)\\n∂θjk(17.32)\\nand\\n∂Φ(Θ)\\n∂θjk=∑\\nx∈Xxjxk·p(x,Θ)\\n= EΘ(XjXk) (17.33)\\nSetting the gradient to zero gives\\nˆE(XjXk)−EΘ(XjXk) = 0 (17.34)\\nwhere we have deﬁned', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 657}),\n",
       " Document(page_content='640 17. Undirected Graphical Models\\nˆE(XjXk) =1\\nNN∑\\ni=1xijxik, (17.35)\\ntheexpectationtakenwithrespecttotheempiricaldistrib utionofthedata.\\nLooking at (17.34), we see that the maximum likelihood estim ates simply\\nmatch the estimated inner products between the nodes to thei r observed\\ninner products. This is a standard form for the score (gradie nt) equation\\nfor exponential family models, in which suﬃcient statistic s are set equal to\\ntheir expectations under the model.\\nTo ﬁnd the maximum likelihood estimates, we can use gradient search\\nor Newton methods. However the computation of E Θ(XjXk) involves enu-\\nmeration of p(X,Θ) over 2p−2of the|X|= 2ppossible values of X, and is\\nnot generally feasible for large p(e.g., larger than about 30). For smaller\\np, a number of standard statistical approaches are available :\\nPoisson log-linear modeling , where we treat the problem as a large regres-\\nsion problem (Exercise 17.12). The response vector yis the vector of\\n2pcounts in each of the cells of the multiway tabulation of the d ata4.\\nThe predictor matrix Zhas 2prows and up to 1+ p+p2columns that\\ncharacterize each of the cells, although this number depend s on the\\nsparsity of the graph. The computational cost is essentiall y that of a\\nregression problem of this size, which is O(p42p) and is manageable\\nforp<20. The Newton updates aretypically computed byiterativel y\\nreweighted least squares, and the number of steps is usually in the\\nsingle digits. See Agresti (2002) and McCullagh and Nelder ( 1989) for\\ndetails. Standard software (such as the Rpackageglm) can be used\\nto ﬁt this model.\\nGradient descent requires at most O(p22p−2) computations to compute\\nthe gradient, but may require many more gradient steps than t he\\nsecond–order Newton methods. Nevertheless, it can handle s lightly\\nlarger problems with p≤30. These computations can be reduced\\nby exploiting the special clique structure in sparse graphs , using the\\njunction-tree algorithm. Details are not given here.\\nIterative proportional ﬁtting (IPF) performs cyclical coordinate descent on\\nthe gradient equations (17.34). At each step a parameter is u pdated\\nso that its gradient equation is exactly zero. This is done in a cyclical\\nfashion until all the gradients are zero. One complete cycle costs the\\nsameasagradientevaluation,butmaybemoreeﬃcient.Jirou ´ sekand\\nPˇ reuˇ cil (1995) implement an eﬃcient version of IPF, using junction\\ntrees.\\n4Each of the cell counts is treated as an independent Poisson variable. We g et the\\nmultinomial model corresponding to (17.28) by conditioning on th e total count N(which\\nis also Poisson under this framework).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 658}),\n",
       " Document(page_content='17.4 Undirected Graphical Models for Discrete Variables 641\\nWhenpis large (>30) other approaches have been used to approximate\\nthe gradient.\\n•The mean ﬁeld approximation (Peterson and Anderson, 1987) e sti-\\nmates E Θ(XjXk) by E Θ(Xj)EΘ(Xj), and replaces the input vari-\\nables by their means, leading to a set of nonlinear equations for the\\nparameters θjk.\\n•To obtain near-exact solutions, Gibbs sampling (Section 8. 6) is used\\nto approximate E Θ(XjXk) by successively sampling from the esti-\\nmated model probabilities Pr Θ(Xj|X−j) (see e.g. Ripley (1996)).\\nWe have not discussed decomposable models , for which the maximum\\nlikelihood estimates can be found in closed form without any iteration\\nwhatsoever. These models arise, for example, in trees: special graphs with\\ntree-structured topology. When computational tractabili ty is a concern,\\ntrees represent a useful class of models and they sidestep th e computational\\nconcerns raised in this section. For details, see for exampl e Chapter 12 of\\nWhittaker (1990).\\n17.4.2 Hidden Nodes\\nWe can increase the complexity of a discrete Markov network b y including\\nlatent or hidden nodes. Suppose that a subset of the variable sXHare\\nunobserved or “hidden”, and the remainder XVare observed or “visible.”\\nThen the log-likelihood of the observed data is\\nℓ(Θ) =N∑\\ni=1log[Pr Θ(XV=xiV)]\\n=N∑\\ni=1[\\nlog∑\\nxH∈XHexp∑\\n(j,k)∈E(θjkxijxik−Φ(Θ))]\\n.(17.36)\\nThe sum over xHmeans that we are summing over all possible {0,1}values\\nfor the hidden units. The gradient works out to be\\ndℓ(Θ)\\ndθjk=ˆEVEΘ(XjXk|XV)−EΘ(XjXk) (17.37)\\nThe ﬁrst term is an empirical average of XjXkif both are visible; if one\\nor both are hidden, they are ﬁrst imputed given the visible da ta, and then\\naveraged over the hidden variables. The second term is the un conditional\\nexpectation of XjXk.\\nThe inner expectation in the ﬁrst term can be evaluated using basic rules\\nof conditional expectation and properties of Bernoulli ran dom variables. In\\ndetail, for observation i', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 659}),\n",
       " Document(page_content='642 17. Undirected Graphical Models\\nEΘ(XjXk|XV=xiV) ={xijxik ifj,k∈V\\nxijPrΘ(Xk= 1|XV=xiV) ifj∈V,k∈H\\nPrΘ(Xj= 1,Xk= 1|XV=xiV) ifj,k∈H.\\n(17.38)\\nNow two separate runs of Gibbs sampling are required; the ﬁrs t to estimate\\nEΘ(XjXk) by sampling from the model as above, and the second to esti-\\nmate E Θ(XjXk|XV=xiV). In this latter run, the visible units are ﬁxed\\n(“clamped”) at their observed values and only the hidden var iables are\\nsampled. Gibbs sampling must be done for each observation in the training\\nset, at each stage of the gradient search. As a result this pro cedure can be\\nvery slow, even for moderate-sized models. In Section 17.4. 4 we consider\\nfurther model restrictions to make these computations mana geable.\\n17.4.3 Estimation of the Graph Structure\\nThe use of a lasso penalty with binary pairwise Markov networ ks has been\\nsuggested by Lee et al. (2007) and Wainwright et al. (2007). T he ﬁrst au-\\nthors investigate a conjugate gradient procedure for exact maximization of\\na penalized log-likelihood. The bottleneck is the computat ion of E Θ(XjXk)\\nin the gradient; exact computation via the junction tree alg orithm is man-\\nageable for sparse graphs but becomes unwieldy for dense gra phs.\\nThe second authors propose an approximate solution, analog ous to the\\nMeinshausen and B¨ uhlmann (2006) approach for the Gaussian graphical\\nmodel. They ﬁt an L1-penalized logistic regression model to each node as\\na function of the other nodes, and then symmetrize the edge pa rameter\\nestimates in some fashion. For example if ˜θjkis the estimate of the j-k\\nedge parameter from the logistic model for outcome node j, the “min”\\nsymmetrization sets ˆθjkto either ˜θjkor˜θkj, whichever is smallest in abso-\\nlute value. The “max” criterion is deﬁned similarly. They sh ow that under\\ncertain conditions either approximation estimates the non zero edges cor-\\nrectly as the sample size goes to inﬁnity. Hoeﬂing and Tibshi rani (2008)\\nextend the graphical lasso to discrete Markov networks, obt aining a pro-\\ncedure which is somewhat faster than conjugate gradients, b ut still must\\ndeal with computation of E Θ(XjXk). They also compare the exact and\\napproximate solutions in an extensive simulation study and ﬁnd the “min”\\nor “max” approximations are only slightly less accurate tha n the exact pro-\\ncedure, both for estimating the nonzero edges and for estima ting the actual\\nvalues of the edge parameters, and are muchfaster. Furthermore, they can\\nhandle denser graphs because they never need to compute the q uantities\\nEΘ(XjXk).\\nFinally, we point out a key diﬀerence between the Gaussian an d binary\\nmodels.IntheGaussiancase,both Σanditsinversewilloftenbeofinterest,\\nand the graphical lasso procedure delivers estimates for bo th of these quan-\\ntities. However, the approximation of Meinshausen and B¨ uh lmann (2006)\\nfor Gaussian graphical models, analogous to the Wainwright et al. (2007)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 660}),\n",
       " Document(page_content='17.4 Undirected Graphical Models for Discrete Variables 643\\nXjXk\\nXℓ\\nVisibleV1 VisibleV2HiddenH\\nθjk\\nFIGURE 17.6. A restricted Boltzmann machine (RBM) in which there are no\\nconnections between nodes in the same layer. The visible units are subdivided to\\nallow the RBM to model the joint density of feature V1and their labels V2.\\napproximation for the binary case, only yields an estimate o fΣ−1. In con-\\ntrast, in the Markov model for binary data, Θis the object of interest, and\\nits inverse is not of interest. The approximate method of Wai nwright et al.\\n(2007) estimates Θeﬃciently and hence is an attractive solution for the\\nbinary problem.\\n17.4.4 Restricted Boltzmann Machines\\nIn this section we consider a particular architecture for gr aphical models\\ninspired by neural networks, where the units are organized i n layers. A\\nrestricted Boltzmann machine (RBM) consists of one layer of visible units\\nand one layer of hidden units with no connections within each layer. It is\\nmuch simpler to compute the conditional expectations (as in (17.37) and\\n(17.38)) if the connections between hidden units are remove d5. Figure 17.6\\nshows an example; the visible layer is divided into input var iablesV1and\\noutput variables V2, and there is a hidden layer H. We denote such a\\nnetwork by\\nV1↔H↔V 2. (17.39)\\nFor example,V1could be the binary pixels of an image of a handwritten\\ndigit, andV2could have 10 units, one for each of the observed class labels\\n0-9.\\nThe restricted form of this model simpliﬁes the Gibbs sampli ng for es-\\ntimating the expectations in (17.37), since the variables i n each layer are\\nindependent of one another, given the variables in the other layers. Hence\\nthey can be sampled together, using the conditional probabi lities given by\\nexpression (17.30).\\nThe resulting model is less general than a Boltzmann machine , but is still\\nuseful; for example it can learn to extract interesting feat ures from images.\\n5We thank Geoﬀrey Hinton forassistance in the preparation of the mater ial on RBMs.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 661}),\n",
       " Document(page_content='644 17. Undirected Graphical Models\\nBy alternately sampling the variables in each layer of the RB M shown\\nin Figure 17.6, it is possible to generate samples from the jo int density\\nmodel. If theV1part of the visible layer is clamped at a particular feature\\nvector during the alternating sampling, it is possible to sa mple from the\\ndistribution over labels given V1. Alternatively classiﬁcation of test items\\ncan also be achieved by comparing the unnormalized joint den sities of each\\nlabel category with the observed features. We do not need to c ompute the\\npartition function as it is the same for all of these combinat ions.\\nAs noted the restricted Boltzmann machine has the same gener ic form\\nas a single hidden layer neural network (Section 11.3). The e dges in the\\nlatter model are directed, the hidden units are usually real -valued, and the\\nﬁtting criterion is diﬀerent. The neural network minimizes the error (cross-\\nentropy) between the targets and their model predictions, c onditional on\\nthe input features. In contrast, the restricted Boltzmann m achine maxi-\\nmizes the log-likelihood for the joint distribution of all v isible units—that\\nis, the features and targets. It can extract information fro m the input fea-\\ntures that is useful for predicting the labels, but, unlike s upervised learning\\nmethods, it may also use some of its hidden units to model stru cture in the\\nfeature vectors that is not immediately relevant for predic ting the labels.\\nThese features may turn out to be useful, however, when combi ned with\\nfeatures derived from other hidden layers.\\nUnfortunately, Gibbs sampling in a restricted Boltzmann ma chine can\\nbe very slow, as it can take a long time to reach stationarity. As the net-\\nwork weights get larger, the chain mixes more slowly and we ne ed to run\\nmore steps to get the unconditional estimates. Hinton (2002 ) noticed em-\\npirically that learning still works well if we estimate the s econd expectation\\nin (17.37) by starting the Markov chain at the data and only ru nning for a\\nfew steps (instead of to convergence). He calls this contrastive divergence :\\nwe sampleHgivenV1,V2, thenV1,V2givenHand ﬁnallyHgivenV1,V2\\nagain. The idea is that when the parameters are far from the so lution, it\\nmay be wasteful to iterate the Gibbs sampler to stationarity , as just a single\\niteration will reveal a good direction for moving the estima tes.\\nWe now give an example to illustrate the use of an RBM. Using co n-\\ntrastivedivergence,itispossibletotrainanRBMtorecogn izehand-written\\ndigits from the MNIST dataset (LeCun et al., 1998). With 2000 hidden\\nunits, 784 visible units for representing binary pixel inte nsities and one\\n10-way multinomial visible unit for representing labels, t he RBM achieves\\nan error rate of 1.9% on the test set. This is a little higher th an the 1.4%\\nachieved by a support vector machine and comparable to the er ror rate\\nachieved by a neural network trained with backpropagation. The error rate\\nof the RBM, however, can be reduced to 1.25% by replacing the 7 84 pixel\\nintensities by 500 features that are produced from the image s without using\\nany label information. First, an RBM with 784 visible units a nd 500 hidden\\nunits is trained, using contrastive divergence, to model th e set of images.\\nThen the hidden states of the ﬁrst RBM are used as data for trai ning a', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 662}),\n",
       " Document(page_content='Exercises 645\\nFIGURE 17.7. Example of a restricted Boltzmann machine for handwritten\\ndigit classiﬁcation. The network is depicted in the schemati c on the left. Displayed\\non the right are some diﬃcult test images that the model classiﬁ es correctly.\\nsecond RBM that has 500 visible units and 500 hidden units. Fi nally, the\\nhidden states of the second RBM are used as the features for tr aining an\\nRBM with 2000 hidden units as a joint density model. The detai ls and\\njustiﬁcation for learning features in this greedy, layer-b y-layer way are de-\\nscribed in Hinton et al. (2006). Figure 17.7 gives a represen tation of the\\ncomposite model that is learned in this way and also shows som e examples\\nof the types of distortion that it can cope with.\\nBibliographic Notes\\nMuch work has been done in deﬁning and understanding the stru cture of\\ngraphical models. Comprehensive treatments of graphical m odels can be\\nfound in Whittaker (1990), Lauritzen (1996), Cox and Wermut h (1996),\\nEdwards (2000), Pearl (2000), Anderson (2003), Jordan (200 4), and Koller\\nand Friedman (2007). Wasserman (2004) gives a brief introdu ction, and\\nChapter 8 of Bishop (2006) gives a more detailed overview. Bo ltzmann\\nmachineswereproposedinAckleyetal.(1985).Ripley(1996 )hasadetailed\\nchapter on topics in graphical models that relate to machine learning. We\\nfound this particularly useful for its discussion of Boltzm ann machines.\\nExercises\\nEx. 17.1 For the Markov graph of Figure 17.8, list all of the implied co ndi-\\ntional independence relations and ﬁnd the maximal cliques.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 663}),\n",
       " Document(page_content='646 17. Undirected Graphical Models\\nX1\\nX2X3X4\\nX5\\nX6\\nFIGURE 17.8.\\nEx. 17.2 Consider random variables X1,X2,X3,X4. In each of the following\\ncases draw a graph that has the given independence relations :\\n(a)X1⊥X3|X2andX2⊥X4|X3.\\n(b)X1⊥X4|X2,X3andX2⊥X4|X1,X3.\\n(c)X1⊥X4|X2,X3,X1⊥X3|X2,X4andX3⊥X4|X1,X2.\\nEx. 17.3 LetΣbe the covariance matrix of a set of pvariablesX. Consider\\nthe partial covariance matrix Σa.b=Σaa−ΣabΣ−1\\nbbΣbabetween the two\\nsubsets of variables Xa= (X1,X2) consisting of the ﬁrst two, and Xb\\nthe rest. This is the covariance matrix between these two var iables, after\\nlinear adjustment for all the rest. In the Gaussian distribu tion, this is the\\ncovariance matrix of the conditional distribution of Xa|Xb. The partial\\ncorrelation coeﬃcient ρjk|restbetween the pair Xaconditional on the rest\\nXb, is simply computed from this partial covariance. Deﬁne Θ=Σ−1.\\n1. Show that Σa.b=Θ−1\\naa.\\n2. Show that if any oﬀ-diagonal element of Θis zero, then the partial\\ncorrelation coeﬃcient between the corresponding variable s is zero.\\n3. Showthatifwetreat Θasifitwereacovariancematrix,andcompute\\nthe corresponding “correlation” matrix\\nR= diag(Θ)−1/2·Θ·diag(Θ)−1/2, (17.40)\\nthenrjk=−ρjk|rest\\nEx. 17.4 Denote by\\nf(X1|X2,X3,...,X p)\\nthe conditional density of X1givenX2,...,X p. If\\nf(X1|X2,X3,...,X p) =f(X1|X3,...,X p),\\nshow thatX1⊥X2|X3,...,X p.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 664}),\n",
       " Document(page_content='Exercises 647\\nEx. 17.5 Consider the setup in Section 17.3.1 with no missing edges. S how\\nthat\\nS11β−s12= 0\\nare the estimating equations for the multiple regression co eﬃcients of the\\nlast variable on the rest.\\nEx. 17.6 Recovery of ˆΘ=ˆΣ−1from Algorithm 17.1 . Use expression (17.16)\\nto derive the standard partitioned inverse expressions\\nθ12=−W−1\\n11w12θ22 (17.41)\\nθ22= 1/(w22−wT\\n12W−1\\n11w12). (17.42)\\nSinceˆβ=W−1\\n11w12, show that ˆθ22= 1/(w22−wT\\n12ˆβ) andˆθ12=−ˆβˆθ22.\\nThusˆθ12is a simply rescaling of ˆβby−ˆθ22.\\nEx.17.7Writeaprogramtoimplementthemodiﬁedregressionprocedu rein\\nAlgorithm 17.1 for ﬁtting the Gaussian graphical model with pre-speciﬁed\\nedges missing. Test it on the ﬂow cytometry data from the book website,\\nusing the graph of Figure 17.1.\\nEx. 17.8\\n(a)Writeaprogramtoﬁtthelassousingthecoordinatedesce ntprocedure\\n(17.26). Compare its results to those from the larsprogram or some\\nother convex optimizer, to check that it is working correctl y.\\n(b) Using the program from (a), write code to implement the gr aphical\\nlasso (Algorithm 17.2). Apply it to the ﬂow cytometry data fr om the\\nbook website. Vary the regularization parameter and examin e the\\nresulting networks.\\nEx. 17.9 Suppose that we have a Gaussian graphical model in which some\\nor all of the data at some vertices are missing.\\n(a) Consider the EM algorithm for a dataset of Ni.i.d. multivariate ob-\\nservationsxi∈IRpwith mean µand covariance matrix Σ. For each\\nsamplei, letoiandmiindex the predictors that are observed and\\nmissing, respectively. Show that in the E step, the observat ions are\\nimputed from the current estimates of µandΣ:\\nˆxi,mi= E(xi,mi|xi,oi,θ) = ˆµmi+ˆΣmi,oiˆΣ−1\\noi,oi(xi,oi−ˆµoi)\\n(17.43)\\nwhile in the M step, µandΣare re-estimated from the empirical\\nmean and (modiﬁed) covariance of the imputed data:\\nˆµj=N∑\\ni=1ˆxij/N', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 665}),\n",
       " Document(page_content='648 17. Undirected Graphical Models\\nˆΣjj′=N∑\\ni=1[(ˆxij−ˆµj)(ˆxij′−ˆµj′)+ci,jj′]/N(17.44)\\nwhereci,jj′=ˆΣjj′ifj,j′∈miand zerootherwise. Explain the reason\\nfor the correction term ci,jj′(Little and Rubin, 2002).\\n(b) Implement the EM algorithm for the Gaussian graphical mo del using\\nthe modiﬁed regression procedure from Exercise 17.7 for the M-step.\\n(c) For the ﬂow cytometry data on the book website, set the dat a for the\\nlastprotein Jnkintheﬁrst1000observationstomissing,ﬁtthemodel\\nof Figure 17.1, and compare the predicted values to the actua l values\\nforJnk. Compare the results to those obtained from a regression of\\nJnkon the other vertices with edges to Jnkin Figure 17.1, using only\\nthe non-missing data.\\nEx. 17.10 Using a simple binary graphical model with just two variable s,\\nshow why it is essential to include a constant node X0≡1 in the model.\\nEx. 17.11 Show that the Ising model (17.28) for the joint probabilitie s in\\na discrete graphical model implies that the conditional dis tributions have\\nthe logistic form (17.30).\\nEx. 17.12 Consider a Poisson regression problem with pbinary variables\\nxij, j= 1,...,pand response variable yiwhich measures the number of\\nobservations with predictor xi∈{0,1}p. The design is balanced, in that all\\nn= 2ppossible combinations are measured. We assume a log-linear model\\nfor the Poisson mean in each cell\\nlogµ(X) =θ00+∑\\n(j,k)∈Exijxikθjk, (17.45)\\nusingthesamenotationasinSection17.4.1(includingthec onstantvariable\\nxi0= 1∀i). We assume the response is distributed as\\nPr(Y=y|X=x) =e−µ(x)µ(x)y\\ny!. (17.46)\\nWrite down the conditional log-likelihood for the observed responsesyi,\\nand compute the gradient.\\n(a) Show that the gradient equation for θ00computes the partition func-\\ntion (17.29).\\n(b) Show that the gradient equations for the remainder of the parameters\\nare equivalent to the gradient (17.34).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 666}),\n",
       " Document(page_content='This is page 649\\nPrinter: Opaque this\\n18\\nHigh-Dimensional Problems: p≫N\\n18.1 When pis Much Bigger than N\\nIn this chapter we discuss prediction problems in which the n umber of\\nfeaturespis much larger than the number of observations N, often written\\np≫N. Such problems have become of increasing importance, espec ially in\\ngenomics and other areas of computational biology. We will s ee that high\\nvariance and overﬁtting are a major concern in this setting. As a result,\\nsimple, highly regularized approaches often become the met hods of choice.\\nThe ﬁrst part of the chapter focuses on prediction in both the classiﬁcation\\nand regression settings, while the second part discusses th e more basic\\nproblem of feature selection and assessment.\\nTo get us started, Figure 18.1 summarizes a small simulation study that\\ndemonstrates the “less ﬁtting is better” principle that app lies whenp≫N.\\nFor each of N= 100 samples, we generated pstandard Gaussian features\\nXwith pairwise correlation 0 .2. The outcome Ywas generated according\\nto a linear model\\nY=p∑\\nj=1Xjβj+σε (18.1)\\nwhereεwas generated from a standard Gaussian distribution. For ea ch\\ndataset, the set of coeﬃcients βjwere also generated from a standard Gaus-\\nsian distribution. We investigated three cases: p= 20,100,and 1000. The\\nstandard deviation σwas chosen in each case so that the signal-to-noise\\nratio Var[E( Y|X)]/σ2equaled 2. As a result, the number of signiﬁcant uni-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 667}),\n",
       " Document(page_content='650 18. High-Dimensional Problems: p≫N1.0 1.5 2.0 2.5 3.0Relative error\\n20 9 220 featuresTest Error\\n1.0 1.5 2.0 2.5 3.0\\n99 35 7100 features\\n1.0 1.5 2.0 2.5 3.0\\n99 87 431000 features\\nEffective Degrees of Freedom\\nFIGURE 18.1. Test-error results for simulation experiments. Shown are box -\\nplots of the relative test errors over 100simulations, for three diﬀerent values\\nofp, the number of features. The relative error is the test error d ivided by the\\nBayes error, σ2. From left to right, results are shown for ridge regression wit h\\nthree diﬀerent values of the regularization parameter λ:0.001,100and1000. The\\n(average) eﬀective degrees of freedom in the ﬁt is indicated below each plot.\\nvariate regression coeﬃcients1was 9, 33 and 331, respectively, averaged\\nover the 100 simulation runs. The p= 1000 case is designed to mimic the\\nkind of data that we might see in a high-dimensional genomic o r proteomic\\ndataset, for example.\\nWe ﬁt a ridge regression to the data, with three diﬀerent valu es for the\\nregularization parameter λ: 0.001, 100, and 1000. When λ= 0.001, this\\nis nearly the same as least squares regression, with a little regularization\\njust to ensure that the problem is non-singular when p > N. Figure 18.1\\nshows boxplots oftherelative testerrorachieved bythediﬀ erentestimators\\nin each scenario. The corresponding average degrees of free dom used in\\neach ridge-regression ﬁt is indicated (computed using form ula (3.50) on\\npage 682). The degrees of freedom is a more interpretable parameter t han\\nλ. We see that ridge regression with λ= 0.001 (20 df) wins when p= 20;\\nλ= 100 (35 df) wins when p= 100, and λ= 1000 (43 df) wins when\\np= 1000.\\nHere is an explanation for these results. When p= 20, we ﬁt all the way\\nand we can identify as many of the signiﬁcant coeﬃcients as po ssible with\\n1We call a regression coeﬃcient signiﬁcant if |ˆβj/ˆsej| ≥2, where ˆβjis the estimated\\n(univariate) coeﬃcient and ˆsejis its estimated standard error.\\n2For a ﬁxed value of the regularization parameter λ, the degrees of freedom depends\\non the observed predictor values in each simulation. Hence we compute the average\\ndegrees of freedom over simulations.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 668}),\n",
       " Document(page_content='18.2 Nearest Shrunken Centroids 651\\nlow bias. When p= 100, we can identify some non-zero coeﬃcients using\\nmoderate shrinkage. Finally, when p= 1000, even though there are many\\nnonzero coeﬃcients, we don’t have a hope for ﬁnding them and w e need\\nto shrink all the way down. As evidence of this, let tj=ˆβj/ˆsej, whereˆβj\\nis the ridge regression estimate and ˆsejits estimated standard error. Then\\nusing the optimal ridge parameter in each of the three cases, the median\\nvalue of|tj|was 2.0, 0.6 and 0.2, and the average number of |tj|values\\nexceeding 2 was equal to 9.8, 1.2 and 0.0.\\nRidge regression with λ= 0.001 successfully exploits the correlation in\\nthe features when p<N, but cannot do so when p≫N. In the latter case\\nthere is not enough information in the relatively small numb er of samples\\nto eﬃciently estimate the high-dimensional covariance mat rix. In that case,\\nmore regularization leads to superior prediction performa nce.\\nThus it is not surprising that the analysis of high-dimensio nal data re-\\nquires either modiﬁcation of procedures designed for the N >pscenario, or\\nentirely new procedures. In this chapter we discuss example s of both kinds\\nofapproachesforhighdimensionalclassiﬁcationandregre ssion;thesemeth-\\nods tend to regularize quite heavily, using scientiﬁc conte xtual knowledge\\nto suggest the appropriate form for this regularization. Th e chapter ends\\nwith a discussion of feature selection and multiple testing .\\n18.2 Diagonal Linear Discriminant Analysis and\\nNearest Shrunken Centroids\\nGene expression arrays are an important new technology in bi ology, and\\nare discussed in Chapters 1 and 14. The data in our next exampl e form\\na matrix of 2308 genes (columns) and 63 samples (rows), from a set of\\nmicroarray experiments. Each expression value is a log-rat io log(R/G).R\\nis the amount of gene-speciﬁc RNA in the target sample that hy bridizes\\nto a particular (gene-speciﬁc) spot on the microarray, and Gis the corre-\\nsponding amount of RNA from a reference sample. The samples a rose from\\nsmall, round blue-cell tumors (SRBCT) found in children, an d are classiﬁed\\ninto four major types: BL (Burkitt lymphoma), EWS (Ewing’s s arcoma),\\nNB (neuroblastoma), and RMS (rhabdomyosarcoma). There is a n addi-\\ntional test data set of 20 observations. We will not go into th e scientiﬁc\\nbackground here.\\nSincep≫N, we cannot ﬁt a full linear discriminant analysis (LDA) to\\nthe data; some sort of regularization is needed. The method w e describe\\nhere is similar to the methods of Section 4.3.1, but with impo rtant modiﬁ-\\ncations that achieve feature selection. The simplest form o f regularization\\nassumes that the features are independent within each class , that is, the\\nwithin-class covariance matrix is diagonal. Despite the fa ct that features\\nwill rarely be independent within a class, when p≫Nwe don’t have', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 669}),\n",
       " Document(page_content='652 18. High-Dimensional Problems: p≫N\\nenough data to estimate their dependencies. The assumption of indepen-\\ndence greatly reduces the number of parameters in the model a nd often\\nresults in an eﬀective and interpretable classiﬁer.\\nThus we consider the diagonal-covariance LDA rule for classifying the\\nclasses. The discriminant score [see (4.12) on page 110] for class kis\\nδk(x∗) =−p∑\\nj=1(x∗\\nj−¯xkj)2\\ns2\\nj+2logπk. (18.2)\\nHerex∗= (x∗\\n1,x∗\\n2,...,x∗\\np)Tis a vector of expression values for a test ob-\\nservation,sjis the pooled within-class standard deviation of the jth gene,\\nand ¯xkj=∑\\ni∈Ckxij/Nkis the mean of the Nkvalues for gene jin class\\nk, withCkbeing the index set for class k. We call ˜xk= (¯xk1,¯xk2,...¯xkp)T\\nthecentroid of classk. The ﬁrst part of (18.2) is simply the (negative)\\nstandardized squared distance of x∗to thekth centroid. The second part\\nis a correction based on the class prior probability πk, where∑K\\nk=1πk= 1.\\nThe classiﬁcation rule is then\\nC(x∗) =ℓifδℓ(x∗) = max kδk(x∗). (18.3)\\nWe see that the diagonal LDA classiﬁer is equivalent to a near est centroid\\nclassiﬁer after appropriate standardization. It is also a s pecial case of the\\nnaive-Bayes classiﬁer, as described in Section 6.6.3. It as sumes that the\\nfeatures in each class have independent Gaussian distribut ions with the\\nsame variance.\\nThe diagonal LDA classiﬁer is often eﬀective in high dimensi onal set-\\ntings. It is also called the “independence rule” in Bickel an d Levina (2004),\\nwho demonstrate theoretically that it will often outperfor m standard lin-\\near discriminant analysis in high-dimensional problems. H ere the diagonal\\nLDA classiﬁer yielded ﬁve misclassiﬁcation errors for the 2 0 test samples.\\nOne drawback of the diagonal LDA classiﬁer is that it uses all of the fea-\\ntures (genes), and hence is not convenient for interpretati on. With further\\nregularization we can do better—both in terms of test error an d inter-\\npretability.\\nWe would like to regularize in a way that automatically drops out fea-\\ntures that are not contributing to the class predictions. We can do this\\nby shrinking the classwise mean toward the overall mean, for each feature\\nseparately. The result is a regularized version of the neare st centroid clas-\\nsiﬁer, or equivalently a regularized version of the diagona l-covariance form\\nof LDA. We call the procedure nearest shrunken centroids (NSC).\\nThe shrinkage procedure is deﬁned as follows. Let\\ndkj=¯xkj−¯xj\\nmk(sj+s0), (18.4)\\nwhere ¯xjis the overall mean for gene j,m2\\nk= 1/Nk−1/Nands0is a\\nsmall positive constant, typically chosen to be the median o f thesjvalues.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 670}),\n",
       " Document(page_content='18.2 Nearest Shrunken Centroids 653\\n(0,0)∆\\nFIGURE 18.2. Soft thresholding function sign(x)(|x|−∆)+is shown in orange,\\nalong with the 45◦line in red.\\nThis constant guards against large dkjvalues that arise from expression\\nvalues near zero. With constant within-class variance σ2, the variance of\\nthe contrast ¯ xkj−¯xjin the numerator is m2\\nkσ2, and hence the form of the\\nstandardization in the denominator. We shrink the dkjtoward zero using\\nsoft thresholding\\nd′\\nkj= sign(dkj)(|dkj|−∆)+; (18.5)\\nsee Figure 18.2. Here ∆ is a parameter to be determined; we use d 10-fold\\ncross-validation in the example (see the top panel of Figure 18.4). Each dkj\\nis reduced by an amount ∆ in absolute value, and is set to zero i f its value\\nis less than zero. The soft-thresholding function is shown i n Figure 18.2;\\nthe same thresholding is applied to wavelet coeﬃcients in Se ction 5.9. An\\nalternative is to use hard thresholding\\nd′\\nkj=dkj·I(|dkj|≥∆); (18.6)\\nwe prefer soft-thresholding, as it is a smoother operation a nd typically\\nworks better. The shrunken versions of ¯ xkjare then obtained by reversing\\nthe transformation in (18.4):\\n¯x′\\nkj= ¯xj+mk(sj+s0)d′\\nkj. (18.7)\\nWe then use the shrunken centroids ¯ x′\\nkjin place of the original ¯ xkjin the\\ndiscriminant score (18.2). The estimator (18.7) can also be viewed as a\\nlasso-style estimator for the class means (Exercise 18.2).\\nNotice that only the genes that have a nonzero d′\\nkjfor at least one of the\\nclasses play a role in the classiﬁcation rule, and hence the v ast majority\\nof genes can often be discarded. In this example, all but 43 ge nes were\\ndiscarded, leaving a small interpretable set of genes that c haracterize each\\nclass. Figure 18.3 represents the genes in a heatmap.\\nFigure 18.4 (top panel) demonstrates the eﬀectiveness of th e shrinkage.\\nWith no shrinkage we make 5/20 errors on the test data, and sev eral errors', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 671}),\n",
       " Document(page_content='654 18. High-Dimensional Problems: p≫N\\non the training and CV data. The shrunken centroids achieve z ero test er-\\nrorsforafairlybroadbandofvaluesfor∆.Thebottompanelo fFigure18.4\\nshows the four centroids for the SRBCT data (gray), relative to the overall\\ncentroid. The blue bars are shrunken versions of these centr oids, obtained\\nby soft-thresholding the gray bars, using ∆ = 4 .3. The discriminant scores\\n(18.2) can be used to construct class probability estimates :\\nˆpk(x∗) =e1\\n2δk(x∗)\\n∑K\\nℓ=1e1\\n2δℓ(x∗). (18.8)\\nThese can be used to rate the classiﬁcations, or to decide not to classify a\\nparticular sample at all.\\nNote that other forms of feature selection can be used in this setting,\\nincluding hard thresholding. Fan and Fan (2008) show theore tically the\\nimportance of carrying out some kind of feature selection wi th diagonal\\nlinear discriminant analysis in high-dimensional problem s.\\n18.3 Linear Classiﬁers with Quadratic\\nRegularization\\nRamaswamy et al. (2001) present a more diﬃcult microarray cl assiﬁcation\\nproblem, involving a training set of 144 patients with 14 diﬀ erent types of\\ncancer, and a test set of 54 patients. Gene expression measur ements were\\navailable for 16 ,063 genes.\\nTable 18.1 shows the prediction results from eight diﬀerent classiﬁcation\\nmethods. The data from each patient was ﬁrst standardized to have mean\\n0 and variance 1; this seems to improve prediction accuracy o verall this\\nexample, suggesting that the “shape” of each gene-expressi on proﬁle is\\nimportant, rather than the absolute expression levels. In e ach case, the\\nBL EWS NB RMS\\nFIGURE 18.3. Heat-map of the chosen 43 genes. Within each of the horizonta l\\npartitions, we have ordered the genes by hierarchical cluste ring, and similarly\\nfor the samples within each vertical partition. Yellow repres ents over- and blue\\nunder-expression.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 672}),\n",
       " Document(page_content='18.3 Linear Classiﬁers with Quadratic Regularization 655\\n0 2 4 6Misclassification Error2308 2059 1223 598 284 159 81 43 23 15 10 5 1Number of Genes0.0 0.2 0.4 0.6 0.8Training\\n10−fold CV\\nTest\\nAmount of Shrinkage ∆\\n−1.0 −0.5 0.0 0.5 1.0BL0 500 1000 1500 2000\\n−1.0 −0.5 0.0 0.5 1.0EWS\\n−1.0 −0.5 0.0 0.5 1.0NB\\n−1.0 −0.5 0.0 0.5 1.0RMSGene\\nCentroids: Average Expression Centered at Overall Centroid\\nFIGURE 18.4. (Top): Error curves for the SRBCT data. Shown are the train-\\ning, 10-fold cross-validation, and test misclassiﬁcation err ors as the threshold\\nparameter ∆is varied. The value ∆ = 4.34is chosen by CV, resulting in a sub-\\nset of43selected genes. (Bottom): Four centroids proﬁles dkjfor the SRBCT\\ndata (gray), relative to the overall centroid. Each centroid has2308components,\\nand we see considerable noise. The blue bars are shrunken versi onsd′\\nkjof these\\ncentroids, obtained by soft-thresholding the gray bars, usi ng∆ = 4.3.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 673}),\n",
       " Document(page_content='656 18. High-Dimensional Problems: p≫N\\nTABLE 18.1. Prediction results for microarray data with 14 cancer classes .\\nMethod 1 is described in Section 18.2. Methods 2, 3 and 6 are di scussed in Sec-\\ntion 18.3, while 4, 7 and 8 are discussed in Section 18.4. Metho d 5 is described in\\nSection 13.3. The elastic-net penalized multinomial does the b est on the test data,\\nbut the standard error of each test-error estimate is about 3 , so such comparisons\\nare inconclusive.\\nMethods CV errors (SE) Test errors Number of\\nOut of 144 Out of 54 Genes Used\\n1. Nearest shrunken centroids 35 (5.0) 17 6,520\\n2.L2-penalized discriminant 25 (4.1) 12 16,063\\nanalysis\\n3. Support vector classiﬁer 26 (4.2) 14 16,063\\n4. Lasso regression (one vs all) 30.7 (1.8) 12.5 1,429\\n5.k-nearest neighbors 41 (4.6) 26 16,063\\n6.L2-penalized multinomial 26 (4.2) 15 16,063\\n7.L1-penalized multinomial 17 (2.8) 13 269\\n8. Elastic-net penalized 22 (3.7) 11.8 384\\nmultinomial\\nregularization parameter has been chosen to minimize the cr oss-validation\\nerror, and the test error at that value of the parameter is sho wn. When\\nmore than one value of the regularization parameter yields t he minimal\\ncross-validation error, the average test error at these val ues is reported.\\nRDA(regularizeddiscriminantanalysis),regularizedmul tinomiallogistic\\nregression, and the support vector machine are more complex methods that\\ntry to exploit multivariate information in the data. We desc ribe each in\\nturn, as well as a variety of regularization methods, includ ing bothL1and\\nL2and some in between.\\n18.3.1 Regularized Discriminant Analysis\\nRegularized discriminant analysis (RDA) is described in Se ction 4.3.1. Lin-\\near discriminant analysis involves the inversion of a p×pwithin-covariance\\nmatrix. When p≫N, this matrix can be huge, has rank at most N < p,\\nand hence is singular. RDA overcomes the singularity issues by regulariz-\\ning the within-covariance estimate ˆΣ. Here we use a version of RDA that\\nshrinksˆΣtowards its diagonal:\\nˆΣ(γ) =γˆΣ+(1−γ)diag(ˆΣ),withγ∈[0,1]. (18.9)\\nNote thatγ= 0 corresponds to diagonal LDA, which is the “no shrinkage”\\nversion of nearest shrunken centroids. The form of shrinkag e in (18.9) is', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 674}),\n",
       " Document(page_content='18.3 Linear Classiﬁers with Quadratic Regularization 657\\nmuchlikeridgeregression(Section3.4.1),whichshrinkst hetotalcovariance\\nmatrix of the features towards a diagonal (scalar) matrix. I n fact, viewing\\nlinear discriminant analysis as linear regression with opt imal scoring of the\\ncategorical response (see (12.57) in Section 12.6), the equ ivalence becomes\\nmore precise.\\nThecomputationalburdenofinvertingthislarge p×pmatrixisovercome\\nusing the methods discussed in Section 18.3.5. The value of γwas chosen\\nby cross-validation in line 2 of Table 18.1; all values of γ∈(0.002,0.550)\\ngave the same CV and test error. Further development of RDA, i ncluding\\nshrinkage of the centroids in addition to the covariance mat rix, can be\\nfound in Guo et al. (2006).\\n18.3.2 Logistic Regression with Quadratic Regularization\\nLogistic regression (Section 4.4) can be modiﬁed in a simila r way, to deal\\nwith thep≫Ncase. With Kclasses, we use a symmetric version of the\\nmulticlass logistic model (4.17) on page 119:\\nPr(G=k|X=x) =exp(βk0+xTβk)∑K\\nℓ=1exp(βℓ0+xTβℓ). (18.10)\\nThis hasKcoeﬃcient vectors of log-odds parameters β1,β2,...,β K. We\\nregularize the ﬁtting by maximizing the penalized log-like lihood\\nmax\\n{β0k,βk}K\\n1[N∑\\ni=1logPr(gi|xi)−λ\\n2K∑\\nk=1||βk||2\\n2]\\n. (18.11)\\nThis regularization automatically resolves the redundanc y in the paramet-\\nrization, and forces∑K\\nk=1ˆβkj= 0, j= 1,...,p(Exercise 18.3). Note that\\nthe constant terms βk0are not regularized (and so one should be set to\\nzero). The resulting optimization problem is convex, and ca n be solved by\\na Newton algorithm or other numerical techniques. Details a re given in Zhu\\nand Hastie (2004). Friedman et al. (2010) provide software f or computing\\nthe regularization path for the two- and multiclass logisti c regression mod-\\nels. Table 18.1, line 6 reports the results for the multiclas s logistic regres-\\nsion model, referred to there as “multinomial”. It can be sho wn (Rosset\\net al., 2004a) that for separable data, as λ→0, the regularized (two-\\nclass) logistic regression estimate (renormalized) conve rges to the maximal\\nmargin classiﬁer (Section 12.2). This gives an attractive a lternative to the\\nsupport-vector machine, discussed next, especially in the multiclass case.\\n18.3.3 The Support Vector Classiﬁer\\nThe support vector classiﬁer is described for the two-class case in Sec-\\ntion 12.2. When p > N, it is especially attractive because in general the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 675}),\n",
       " Document(page_content='658 18. High-Dimensional Problems: p≫N\\nclasses are perfectly separable by a hyperplane unless ther e are identical\\nfeature vectors in diﬀerent classes. Without any regulariz ation the support\\nvector classiﬁer ﬁnds the separating hyperplane with the la rgest margin;\\nthat is, the hyperplane yielding the biggest gap between the classes in\\nthe training data. Somewhat surprisingly, when p≫Nthe unregularized\\nsupport vector classiﬁer often works about as well as the bes t regularized\\nversion. Overﬁtting often does not seem to be a problem, part ly because of\\nthe insensitivity of misclassiﬁcation loss.\\nThere are many diﬀerent methods for generalizing the two-cl ass support-\\nvector classiﬁer to K >2 classes. In the “one versus one” ( ovo) approach,\\nwe compute all(K\\n2)\\npairwise classiﬁers. For each test point, the predicted\\nclass is the one that wins the most pairwise contests. In the “ one versus all”\\n(ova) approach, each class is compared to all of the others in Ktwo-class\\ncomparisons. To classify a test point, we compute the conﬁde nces (signed\\ndistancefromthehyperplane)foreachofthe Kclassiﬁers.Thewinneristhe\\nclass with the highest conﬁdence. Finally, Vapnik (1998) an d Weston and\\nWatkins (1999) suggested (somewhat complex) multiclass cr iteria which\\ngeneralize the two-class criterion (12.7).\\nTibshirani and Hastie (2007) propose the margin tree classiﬁer, in which\\nsupport-vector classiﬁers are used in a binary tree, much as in CART\\n(Chapter 9). The classes are organized in a hierarchical man ner, which can\\nbe useful for classifying patients into diﬀerent cancer typ es, for example.\\nLine 3 of Table 18.1 shows the results for the support vector c lassiﬁer\\nusing the ovamethod; Ramaswamy et al. (2001) reported (and we con-\\nﬁrmed) that this approach worked best for this problem. The e rrors are\\nvery similar to those in line 6, as we might expect from the com ments\\nat the end of the previous section. The error rates are insens itive to the\\nchoice ofC[the regularization parameter in (12.8) on page 420], for va lues\\nofC >0.001. Sincep > N, the support vector hyperplane can perfectly\\nseparate the training data by setting C=∞.\\n18.3.4 Feature Selection\\nFeature selection is an important scientiﬁc requirement fo r a classiﬁer when\\npislarge.Neitherdiscriminantanalysis,logisticregress ion,northesupport-\\nvector classiﬁer perform feature selection automatically , because all use\\nquadratic regularization. All features have nonzero weigh ts in both models.\\nAd-hoc methods for feature selection have been proposed, fo r example,\\nremoving genes with small coeﬃcients, and reﬁtting the clas siﬁer. This is\\ndoneinabackwardstepwisemanner,startingwiththesmalle stweightsand\\nmoving on to larger weights. This is known as recursive feature elimination\\n(Guyon et al., 2002). It was not successful in this example; R amaswamy\\net al. (2001) report, for example, that the accuracy of the su pport-vector\\nclassiﬁer starts to degrade as the number of genes is reduced from the full', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 676}),\n",
       " Document(page_content='18.3 Linear Classiﬁers with Quadratic Regularization 659\\nset of 16,063. This is rather remarkable, as the number of training sam ples\\nis only 144. We do not have an explanation for this behavior.\\nAll three methods discussed in this section (RDA, LR and SVM) can\\nbe modiﬁed to ﬁt nonlinear decision boundaries using kernel s. Usually the\\nmotivation for such an approach is to increase the model comp lexity. With\\np≫Nthe models are already suﬃciently complex and overﬁtting is always\\nadanger.Yetdespitethehighdimensionality,radialkerne ls(Section12.3.3)\\nsometimes deliver superior results in these high dimension al problems. The\\nradial kernel tends to dampen inner products between points far away from\\neach other, which in turn leads to robustness to outliers. Th is occurs often\\nin high dimensions, and may explain the positive results. We tried a radial\\nkernel with the SVM in Table 18.1, but in this case the perform ance was\\ninferior.\\n18.3.5 Computational Shortcuts When p≫N\\nThecomputationaltechniquesdiscussedinthissectionapp lytoanymethod\\nthat ﬁts a linear model with quadratic regularization on the coeﬃcients.\\nThat includes all the methods discussed in this section, and many more.\\nWhenp > N, the computations can be carried out in an N-dimensional\\nspace, rather than p, via the singular value decomposition introduced in\\nSection 14.5. Here is the geometric intuition: just like two points in three-\\ndimensional space always lie on a line, Npoints inp-dimensional space lie\\nin an (N−1)-dimensional aﬃne subspace.\\nGiven theN×pdata matrix X, let\\nX=UDVT(18.12)\\n=RVT(18.13)\\nbe the singular-value decomposition (SVD) of X; that is, Visp×Nwith\\northonormal columns, UisN×Northogonal, and Da diagonal matrix\\nwith elements d1≥d2≥dN≥0. The matrix RisN×N, with rows rT\\ni.\\nAs a simple example, let’s ﬁrst consider the estimates from a ridge re-\\ngression:\\nˆβ= (XTX+λI)−1XTy. (18.14)\\nReplacing XbyRVTand after some further manipulations, this can be\\nshown to equal\\nˆβ=V(RTR+λI)−1RTy (18.15)\\n(Exercise 18.4). Thus ˆβ=Vˆθ, where ˆθis the ridge-regression estimate\\nusing theNobservations ( ri,yi),i= 1,2,...,N. In other words, we can\\nsimply reduce the data matrix from XtoR, and work with the rows of\\nR. This trick reduces the computational cost from O(p3) toO(pN2) when\\np>N.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 677}),\n",
       " Document(page_content='660 18. High-Dimensional Problems: p≫N\\nThese results can be generalized to allmodels that are linear in the\\nparameters and have quadratic penalties. Consider any supe rvised learning\\nproblem where we use a linear function f(X) =β0+XTβto model a\\nparameter in the conditional distribution of Y|X. We ﬁt the parameters β\\nby minimizing some loss function∑N\\ni=1L(yi,f(xi)) over the data with a\\nquadratic penalty on β. Logistic regression is a useful example to have in\\nmind. Then we have the following simple theorem:\\nLetf∗(ri) =θ0+rT\\niθwithrideﬁned in (18.13), and consider the pair of\\noptimization problems:\\n(ˆβ0,ˆβ) = arg min\\nβ0,β∈IRpN∑\\ni=1L(yi,β0+xT\\niβ)+λβTβ; (18.16)\\n(ˆθ0,ˆθ) = arg min\\nθ0,θ∈IRNN∑\\ni=1L(yi,θ0+rT\\niθ)+λθTθ.(18.17)\\nThen the ˆβ0=ˆθ0, andˆβ=Vˆθ.\\nThe theorem says that we can simply replace the pvectorsxiby the\\nN-vectorsri, and perform our penalized ﬁt as before, but with far fewer\\npredictors. The N-vector solution ˆθis then transformed back to the p-\\nvector solution via a simple matrix multiplication. This re sult is part of\\nthe statistics folklore, and deserves to be known more widel y—see Hastie\\nand Tibshirani (2004) for further details.\\nGeometrically, we are rotating the features to a coordinate system in\\nwhich all but the ﬁrst Ncoordinates are zero. Such rotations are allowed\\nsince the quadratic penalty is invariant under rotations, a nd linear models\\nare equivariant.\\nThis result can be applied to many of the learning methods dis cussed\\nin this chapter, such as regularized (multiclass) logistic regression, linear\\ndiscriminant analysis (Exercise 18.6), and support vector machines. It also\\napplies to neural networks with quadratic regularization ( Section 11.5.2).\\nNote, however, that it does not apply to methods such as the la sso, which\\nuses nonquadratic ( L1) penalties on the coeﬃcients.\\nTypically we use cross-validation to select the parameter λ. It can be\\nseen (Exercise 18.12) that we only need to construct Ronce, on the original\\ndata, and use it as the data for each of the CV folds.\\nThe support vector “kernel trick” of Section 12.3.7 exploit s the same re-\\nduction used in this section, in a slightly diﬀerent context . Suppose we have\\nat our disposal the N×Ngram (inner-product) matrix K=XXT. From\\n(18.12) we have K=UD2UT, and soKcaptures the same information as\\nR. Exercise 18.13 shows how we can exploit the ideas in this sec tion to ﬁt\\na ridged logistic regression with Kusing its SVD.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 678}),\n",
       " Document(page_content='18.4 Linear Classiﬁers with L1Regularization 661\\n18.4 Linear Classiﬁers with L1Regularization\\nThe methods of Section 18.3 use an L2penalty to regularize their pa-\\nrameters, just as in ridge regression. All of the estimated c oeﬃcients are\\nnonzero, and hence no feature selection is performed. In thi s section we dis-\\ncuss methods that use L1penalties instead, and hence provide automatic\\nfeature selection.\\nRecall the lasso of Section 3.4.2,\\nmin\\nβ1\\n2N∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2\\n+λp∑\\nj=1|βj|,(18.18)\\nwhich we have written in the Lagrange form (3.52). As discuss ed there, the\\nuse of theL1penalty causes a subset of the solution coeﬃcients ˆβjto be\\nexactly zero, for a suﬃciently large value of the tuning para meterλ.\\nIn Section 3.8.1 we discussed the LARS algorithm, an eﬃcient procedure\\nfor computing the lasso solution for all λ. Whenp>N(as in this chapter),\\nasλapproaches zero, the lasso ﬁts the training data exactly. In fact, by\\nconvex duality one can show that when p > Nthe number of non-zero\\ncoeﬃcients is at most Nfor all values of λ(Rosset and Zhu, 2007, for\\nexample). Thus the lasso provides a (severe) form of feature selection.\\nLasso regression can be applied to a two-class classiﬁcatio n problem by\\ncodingtheoutcome ±1,andapplyingacutoﬀ(usually0)tothepredictions.\\nFor more than two classes, there are many possible approache s, including\\ntheovaandovomethods discussed in Section 18.3.3. We tried the ova-\\napproach on the cancer data in Section 18.3. The results are s hown in\\nline (4) of Table 18.1. Its performance is among the best.\\nA more natural approach for classiﬁcation problems is to use the lasso\\npenalty to regularize logistic regression. Several implem entations have been\\nproposedintheliterature,includingpathalgorithmssimi lartoLARS(Park\\nand Hastie, 2007). Because the paths are piecewise smooth bu t nonlinear,\\nexact methods are slower than the LARS algorithm, and are les s feasible\\nwhenpis large.\\nFriedman et al. (2010) provide very fast algorithms for ﬁtti ngL1-pen-\\nalized logistic and multinomial regression models. They us e the symmetric\\nmultinomial logistic regression model as in (18.10) in Sect ion 18.3.2, and\\nmaximize the penalized log-likelihood\\nmax\\n{β0k,βk∈IRp}K\\n1\\uf8ee\\n\\uf8f0N∑\\ni=1logPr(gi|xi)−λK∑\\nk=1p∑\\nj=1|βkj|\\uf8f9\\n\\uf8fb; (18.19)\\ncompare with (18.11). Their algorithm computes the exact so lution at a\\npre-chosen sequence of values for λby cyclical coordinate descent (Sec-\\ntion 3.8.6), and exploits the fact that solutions are sparse whenp≫N,', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 679}),\n",
       " Document(page_content='662 18. High-Dimensional Problems: p≫N\\nas well as the fact that solutions for neighboring values of λtend to be\\nvery similar. This method was used in line (7) of Table 18.1, w ith the over-\\nall tuning parameter λchosen by cross-validation. The performance was\\nsimilar to that of the best methods, except here the automati c feature se-\\nlection chose 269 genes altogether. A similar approach is us ed in Genkin\\net al. (2007); although they present their model from a Bayes ian point of\\nview, they in fact compute the posterior mode, which solves t he penalized\\nmaximum-likelihood problem.\\n−8 −7 −6 −5 −4 −3 −2 −10.0 0.5 1.0 1.5 2.0\\n−8 −7 −6 −5 −4 −3 −2 −10.0 0.5 1.0 1.5 2.0CoeﬃcientsCoeﬃcients\\nlog(λ) log(λ)Lasso Elastic Net\\nFIGURE 18.5. Regularized logistic regression paths for the leukemia data. T he\\nleft panel is the lasso path, the right panel the elastic-net pat h withα= 0.8. At\\nthe ends of the path (extreme left), there are 19nonzero coeﬃcients for the lasso,\\nand39for the elastic net. The averaging eﬀect of the elastic net resu lts in more\\nnon-zero coeﬃcients than the lasso, but with smaller magnitud es.\\nIn genomic applications, there are often strong correlatio ns among the\\nvariables; genes tend to operate in molecular pathways. The lasso penalty\\nis somewhat indiﬀerent to the choice among a set of strong but corre-\\nlated variables (Exercise 3.28). The ridge penalty, on the o ther hand, tends\\nto shrink the coeﬃcients of correlated variables toward eac h other (Exer-\\ncise 3.29 on page 99). The elastic net penalty (Zou and Hastie, 2005) is a\\ncompromise, and has the form\\np∑\\nj=1(\\nα|βj|+(1−α)β2\\nj)\\n. (18.20)\\nThesecondtermencourageshighlycorrelatedfeaturestobe averaged,while\\nthe ﬁrst term encourages a sparse solution in the coeﬃcients of these aver-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 680}),\n",
       " Document(page_content='18.4 Linear Classiﬁers with L1Regularization 663\\naged features. The elastic net penalty can be used with any li near model,\\nin particular for regression or classiﬁcation.\\nHence the multinomial problem above with elastic-net penal ty becomes\\nmax\\n{β0k,βk∈IRp}K\\n1\\uf8ee\\n\\uf8f0N∑\\ni=1logPr(gi|xi)−λK∑\\nk=1p∑\\nj=1(\\nα|βkj|+(1−α)β2\\nkj)\\uf8f9\\n\\uf8fb.\\n(18.21)\\nThe parameter αdetermines the mix of the penalties, and is often pre-\\nchosen on qualitative grounds. The elastic net can yield mor e thatNnon-\\nzero coeﬃcients when p > N, a potential advantage over the lasso. Line\\n(8) in Table 18.1 uses this model, with αandλchosen by cross-validation.\\nWe used a sequence of 20 values of αbetween 0.05 and 1.0, and a 100\\nvalues ofλuniform on the log scale covering the entire range. Values of\\nα∈[0.75,0.80] gave the minimum CV error, with values of λ<0.001 for all\\ntied solutions. Although it has the lowest test error among a ll methods, the\\nmargin is small and not signiﬁcant. Interestingly, when CV i s performed\\nseparately for each value of α, a minimum test error of 8 .8 is achieved at\\nα= 0.10, but this is not the value chosen in the two-dimensional CV .\\n−8 −7 −6 −5 −4 −3 −2 −10.0 0.1 0.2 0.3 0.4Misclassification ErrorTraining\\nTest\\n10−fold CV\\n−8 −7 −6 −5 −4 −3 −2 −10 5 10 15 20 25 30Deviance\\nlog(λ) log(λ)\\nFIGURE 18.6. Training, test, and 10-fold cross validation curves for lasso lo gis-\\ntic regression on the leukemia data. The left panel shows miscla ssiﬁcation errors,\\nthe right panel shows deviance.\\nFigure 18.5 shows the lasso and elastic-net coeﬃcient paths on the two-\\nclass leukemia data (Golub et al., 1999). There are 7129 gene -expression\\nmeasurements on 38 samples, 27 of them in class ALL (acute lym phocytic\\nleukemia), and 11 in class AML (acute myelogenous leukemia) . There is\\nalso a test set with 34 samples (20, 14). Since the data are lin early separa-\\nble, the solution is undeﬁned at λ= 0 (Exercise 18.11), and degrades for\\nvery small values of λ. Hence the paths have been truncated as the ﬁtted\\nprobabilities approach 0 and 1. There are 19 non-zero coeﬃci ents in the\\nleft plot, and 39 in the right. Figure 18.6 (left panel) shows the misclas-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 681}),\n",
       " Document(page_content='664 18. High-Dimensional Problems: p≫N\\nsiﬁcation errors for the lasso logistic regression on the tr aining and test\\ndata, as well as for 10-fold cross-validation on the trainin g data. The right\\npanel uses binomial deviance to measure errors, and is much s moother. The\\nsmall sample sizes lead to considerable sampling variance i n these curves,\\neven though individual curves are relatively smooth (see, f or example, Fig-\\nure 7.1 on page 220). Both of these plots suggest that the limi ting solution\\nλ↓0 is adequate, leading to 3/34 misclassiﬁcations in the test set. The\\ncorresponding ﬁgures for the elastic net are qualitatively similar and are\\nnot shown.\\nForp≫N, the limiting coeﬃcients diverge for all regularized logis tic\\nregression models, so in practical software implementatio ns a minimum\\nvalue forλ>0 is either explicitly or implicitly set. However, renormal ized\\nversions of the coeﬃcients converge, and these limiting sol utions can be\\nthought of as interesting alternatives to the linear optima l separating hy-\\nperplane (SVM). With α= 0 the limiting solution coincides with the SVM\\n(see end of Section 18.3.2), but all the 7129 genes are select ed. Withα= 1,\\nthe limiting solution coincides with an L1separating hyperplane (Rosset\\net al., 2004a), and includes at most 38 genes. As αdecreases from 1, the\\nelastic-net solutions include more genes in the separating hyperplane.\\n18.4.1 Application of Lasso to Protein Mass Spectroscopy\\nProtein mass spectrometry has become a popular technology f or analyzing\\nthe proteins in blood, and can be used to diagnose a disease or understand\\nthe processes underlying it.\\nFor each blood serum sample i, we observe the intensity xijfor many\\ntime of ﬂight valuestj. This intensity is related to the number of particles\\nobserved to take approximately tjtime to pass from the emitter to the\\ndetector during a cycle of operation of the machine. The time of ﬂight has\\na known relationship to the mass over charge ratio ( m/z) of the constituent\\nproteins in the blood. Hence the identiﬁcation of a peak in th e spectrum\\nat a certain tjtells us that there is a protein with a corresponding mass\\nand charge. The identity of this protein can then be determin ed by other\\nmeans.\\nFigure 18.7 shows an example taken from Adam et al. (2003). It shows\\nthe average spectra for healthy patients and those with pros tate cancer.\\nThere are 16,898 m/zsites in total, ranging in value from 2000 to 40,000.\\nThe full dataset consists of 157 healthy patients and 167 wit h cancer, and\\nthe goal is to ﬁnd m/zsites that discriminate between the two groups.\\nThis is an example of functional data; the predictors can be viewed as a\\nfunction of m/z. There has been much interest in this problem in the past\\nfew years; see e.g. Petricoin et al. (2002).\\nThe data were ﬁrst standardized (baseline subtraction and n ormaliza-\\ntion), and we restricted attention to m/zvalues between 2000 and 40,000\\n(spectra outside of this range were not of interest). We then applied near-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 682}),\n",
       " Document(page_content='18.4 Linear Classiﬁers with L1Regularization 665\\n2e+03 5e+03 1e+04 2e+04 5e+04 1e+05 2e+0510 20 30 40\\nm/zIntensityNormal\\nCancer\\nFIGURE 18.7. Protein mass spectrometry data: average proﬁles from normal\\nand prostate cancer patients.\\nest shrunken centroids and lasso regression to the data, wit h the results for\\nboth methods shown in Table 18.2.\\nBy ﬁtting harder to the data, the lasso achieves a considerab ly lower\\ntest error rate. However, it may not provide a scientiﬁcally useful solu-\\ntion. Ideally, protein mass spectrometry resolves a biolog ical sample into\\nits constituent proteins, and these should appear as peaks i n the spectra.\\nThe lasso doesn’t treat peaks in any special way, so not surpr isingly only\\nsome of the non-zero lasso weights were situated near peaks i n the spectra.\\nFurthermore, the same protein may yield a peak at slightly di ﬀerentm/z\\nvalues in diﬀerent spectra. In order to identify common peak s, some kind\\nofm/zwarping is needed from sample to sample.\\nTo address this, we applied a standard peak-extraction algo rithm to each\\nspectrum, yielding a total of 5178 peaks in the 217 training s pectra. Our\\nidea was to pool the collection of peaks from all patients, an d hence con-\\nstruct a set of common peaks. For this purpose, we applied hie rarchical\\nclustering to the positions of these peaks along the log m/zaxis. We cut\\nthe resulting dendrogram horizontally at height log(0 .005)3, and computed\\naverages ofthepeakpositions ineachresultingcluster.Th isprocessyielded\\n728 common clusters and their corresponding peak centers.\\nGiven these 728 common peaks, we determined which of these we re\\npresent in each individual spectrum, and if present, the hei ght of the peak.\\nA peak height of zero was assigned if that peak was not found. T his pro-\\nduced a 217×728 matrix of peak heights as features, which was used in a\\nlasso regression. We scored the test spectra for the same 728 peaks.\\n3Use of the value 0 .005 means that peaks with positions less than 0.5% apart are\\nconsidered the same peak, a fairly common assumption.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 683}),\n",
       " Document(page_content='666 18. High-Dimensional Problems: p≫N\\nTABLE 18.2. Results for the prostate data example. The standard deviation for\\nthe test errors is about 4.5.\\nMethod Test Errors/108 Number of Sites\\n1. Nearest shrunken centroids 34 459\\n2. Lasso 22 113\\n3. Lasso on peaks 28 35\\nThe prediction results for this application of the lasso to t he peaks are\\nshown in the last line of Table 18.2: it does fairly well, but n ot as well\\nas the lasso on the raw spectra. However, the ﬁtted model may b e more\\nuseful to the biologist as it yields 35 peak positions for fur ther study. On\\nthe other hand, the results suggest that there may be useful d iscriminatory\\ninformation between the peaks of the spectra, and the positi ons of the lasso\\nsites from line (2) of the table also deserve further examina tion.\\n18.4.2 The Fused Lasso for Functional Data\\nIn the previous example, the features had a natural order, de termined by\\nthe mass-to-charge ratio m/z. More generally, we may have functional fea-\\nturesxi(t) that are ordered according to some index variable t. We have\\nalready discussed several approaches for exploiting such s tructure.\\nWe can represent xi(t) by their coeﬃcients in a basis of functions in t,\\nsuch as splines, wavelets or Fourier bases, and then apply a r egression using\\nthese coeﬃcients as predictors. Equivalently, one can inst ead represent the\\ncoeﬃcients of the original features in these bases. These ap proaches are\\ndescribed in Section 5.3.\\nIn the classiﬁcation setting, we discuss the analogous appr oach of penal-\\nizeddiscriminantanalysisinSection12.6.Thisusesapena ltythatexplicitly\\ncontrols the resulting smoothness of the coeﬃcient vector.\\nThe above methods tend to smooth the coeﬃcients uniformly. H ere we\\npresent a more adaptive strategy that modiﬁes the lasso pena lty to take\\ninto account the ordering of the features. The fused lasso (Tibshirani et\\nal., 2005) solves\\nmin\\nβ∈IRp{N∑\\ni=1(yi−β0−p∑\\nj=1xijβj)2+λ1p∑\\nj=1|βj|+λ2p−1∑\\nj=1|βj+1−βj|}\\n.(18.22)\\nThis criterion is strictly convex in β, so a unique solution exists. The ﬁrst\\npenalty encourages the solution to be sparse, while the seco nd encourages\\nit to be smooth in the index j.\\nThe diﬀerence penalty in (18.22) assumes an uniformly space d indexj. If\\ninstead the underlying index variable thas nonuniform values tj, a natural\\ngeneralization of (18.22) would be based on divided diﬀeren ces', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 684}),\n",
       " Document(page_content='18.4 Linear Classiﬁers with L1Regularization 667\\n0 200 400 600 800 1000−2 0 2 4\\nGenome orderlog2 ratio\\nFIGURE 18.8. Fused lasso applied to CGH data. Each point represents the\\ncopy-number of a gene in a tumor sample, relative to that of a con trol (on the log\\nbase-2 scale).\\nλ2p−1∑\\nj=1|βj+1−βj|\\n|tj+1−tj|. (18.23)\\nThis amounts to having a penalty modiﬁer for each of the terms in the\\nseries.\\nA particularly useful special case arises when the predicto r matrix X=\\nIN, theN×Nidentity matrix. This is a special case of the fused lasso,\\nused to approximate a sequence {yi}N\\n1. Thefused lasso signal approximator\\nsolves\\nmin\\nβ∈IRN{N∑\\ni=1(yi−β0−βi)2+λ1N∑\\ni=1|βi|+λ2N−1∑\\ni=1|βi+1−βi|}\\n.(18.24)\\nFigure 18.8 shows an example taken from Tibshirani and Wang ( 2007). The\\ndata in the panel come from a Comparative Genomic Hybridizat ion (CGH)\\narray, measuring the approximate log (base-two) ratio of th e number of\\ncopies of each gene in a tumor sample, as compared to a normal s ample.\\nThe horizontal axis represents the chromosomal location of each gene. The\\nideaisthatincancercells,genesareoftenampliﬁed(dupli cated)ordeleted,\\nand it is of interest to detect these events. Furthermore, th ese events tend\\nto occur in contiguous regions. The smoothed signal estimat e from the\\nfused lasso signal approximator is shown in dark red (with ap propriately\\nchosen values for λ1andλ2). The signiﬁcantly nonzero regions can be used\\nto detect locations of gains and losses of genes in the tumor.\\nThere is also a two-dimensional version of the fused lasso, i n which the\\nparameters are laid out in a grid of pixels, and a penalty is ap plied to the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 685}),\n",
       " Document(page_content='668 18. High-Dimensional Problems: p≫N\\nﬁrst diﬀerences to the left, right, above and below the targe t pixel. This\\ncan be useful for denoising or classifying images. Friedman et al. (2007)\\ndevelop fast generalized coordinate descent algorithms fo r the one- and\\ntwo-dimensional fused lasso.\\n18.5 Classiﬁcation When Features are Unavailable\\nIn some applications the objects under study are more abstra ct in nature,\\nand it is not obvious how to deﬁne a feature vector. As long as w e can ﬁll\\nin anN×Nproximity matrix of similarities between pairs of objects in our\\ndatabase,itturnsoutwecanputtousemanyoftheclassiﬁers inourarsenal\\nby interpreting the proximities as inner-products. Protei n structures fall\\ninto this category, and we explore an example in Section 18.5 .1 below.\\nIn other applications, such as document classiﬁcation, fea ture vectors are\\navailable but can be extremely high-dimensional. Here we ma y not wish\\nto compute with such high-dimensional data, but rather stor e the inner-\\nproducts between pairs of documents. Often these inner-pro ducts can be\\napproximated by sampling techniques.\\nPairwise distances serve a similar purpose, because they ca n be turned\\ninto centered inner-products. Proximity matrices are disc ussed in more de-\\ntail in Chapter 14.\\n18.5.1 Example: String Kernels and Protein Classiﬁcation\\nAn important problem in computational biology is to classif y proteins into\\nfunctional and structural classes based on their sequence s imilarities. Pro-\\ntein molecules are strings of amino acids, diﬀering in both l ength and com-\\nposition. In the example we consider, the lengths vary betwe en 75–160\\namino-acidmolecules,eachofwhichcanbeoneof20diﬀerent types,labeled\\nusing letters. Here are two examples, of length 110 and 153, r espectively:\\nIPTSALVKETLALLSTHRTLLIANETLRIPVPVHKNHQLCTEEIFQGIGTL ESQTVQGGTV\\nERLFKNLSLIKKYIDGQKKKCGEERRRVNQFLDY LQEFLGVMNTEWI\\nPHRRDLCSRSIWLARKIRSDLTALTESYVKHQGLWSELTEAER LQENLQAYRTFHVLLA\\nRLLEDQQVHFTPTEGDFHQAIHTLLLQVAAFAYQIEELMILLEYKIPRNEA DGMLFEKK\\nLWGLKV LQELSQWTVRSIHDLRFISSHQTGIP\\nThere have been many proposals for measuring the similarity between a\\npair of protein molecules. Here we focus on a measure based on the count\\nof matching substrings (Leslie et al., 2004), such as the LQEabove.\\nTo construct our features, we count the number of times that a given\\nsequence of length moccurs in our string, and we compute this number', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 686}),\n",
       " Document(page_content='18.5 Classiﬁcation When Features are Unavailable 669\\nfor all possible sequences of length m. Formally, for a string x, we deﬁne a\\nfeature map\\nΦm(x) ={φa(x)}a∈Am (18.25)\\nwhereAmis the set of subsequences of length m, andφa(x) is the number\\nof times that “ a” occurs in our string x. Using this, we deﬁne the inner\\nproduct\\nKm(x1,x2) =⟨Φm(x1),Φm(x2)⟩, (18.26)\\nwhich measures the similarity between the two strings x1, x2.This can be\\nused to drive, for example, a support vector classiﬁer for cl assifying strings\\ninto diﬀerent protein classes.\\nNow the number of possible sequences ais|Am|= 20m, which can be\\nvery large for moderate m, and the vast majority of the subsequences do\\nnot match the strings in our training set. It turns out that we can compute\\ntheN×Ninner-product matrix or string kernel Km(18.26) eﬃciently\\nusing tree-structures, without actually computing the ind ividual vectors.\\nThis methodology, and the data to follow, come from Leslie et al. (2004).4\\nThe data consist of 1708 proteins in two classes— negative (1 663) and\\npositive (45). The two examples above, which we will call “ x1” and “x2”,\\nare from this set. We have marked the occurrences of subseque nceLQE,\\nwhich appears in both proteins. There are 203possible subsequences, so\\nΦ3(x) will be a vector of length 8000. For this example φLQE(x1) = 1 and\\nφLQE(x2) = 2.\\nUsing software from Leslie et al. (2004), we computed the str ing kernel\\nform= 4, which was then used in a support vector classiﬁer to ﬁnd th e\\nmaximal margin solution in this 204= 160,000-dimensional feature space.\\nWe used 10-fold cross-validation to compute the SVM predict ions on all of\\nthetrainingdata.TheorangecurveinFigure18.9showsthec ross-validated\\nROC curve for the support vector classiﬁer, computed by vary ing the cut-\\npoint on the real-valued predictions from the cross-valida ted support vector\\nclassiﬁer. The area under the curve is 0.84. Leslie et al. (20 04) show that\\nthe string kernel method is competitive with, but perhaps no t as accurate\\nas, more specialized methods for protein string matching.\\nMany other classiﬁers can be computed using only the informa tion in the\\nkernel matrix; some details are given in the next section. Th e results for\\nthe nearest centroid classiﬁer (green), and distance-weig hted one-nearest\\nneighbors (blue) are shown in Figure 18.9. Their performanc e is similar to\\nthat of the support vector classiﬁer.\\n4We thank Christina Leslie for her help and for providing the data, w hich is available\\non our book website.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 687}),\n",
       " Document(page_content='670 18. High-Dimensional Problems: p≫N\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0ROC Curves for String Kernel\\nSpecificitySensitivity\\nSVM  0.84\\nNearest Centroid  0.84\\nOne−Nearest Neighbor 0.86\\nFIGURE 18.9. Cross-validated ROC curves for protein example using the stri ng\\nkernel. The numbers next to each method in the legend give the ar ea under the\\ncurve, an overall measure of accuracy. The SVM achieves bett er sensitivities than\\nthe other two, which achieve better speciﬁcities.\\n18.5.2 Classiﬁcation and Other Models Using Inner-Product\\nKernels and Pairwise Distances\\nThere are a number of other classiﬁers, besides the support- vector ma-\\nchine, that can be implemented using only inner-product mat rices. This\\nalso implies they can be “kernelized” like the SVM.\\nAnobviousexampleisnearest-neighborclassiﬁcation,sin cewecantrans-\\nform pairwise inner-products to pairwise distances:\\n||xi−xi′||2=⟨xi,xi⟩+⟨xi′,xi′⟩−2⟨xi,xi′⟩. (18.27)\\nA variation of 1-NN classiﬁcation is used in Figure 18.9, whi ch produces\\na continuous discriminant score needed to construct a ROC cu rve. This\\ndistance-weighted 1-NN makes use of the distance of a test po ints to the\\nclosest member of each class; see Exercise 18.14.\\nNearest-centroid classiﬁcation follows easily as well. Fo r training pairs\\n(xi,gi), i= 1,...,N, a test point x0, and class centroids ¯ xk,k= 1,...,K\\nwe can write\\n||x0−¯xk||2=⟨x0,x0⟩−2\\nNk∑\\ngi=k⟨x0,xi⟩+1\\nN2\\nk∑\\ngi=k∑\\ngi′=k⟨xi,xi′⟩,(18.28)', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 688}),\n",
       " Document(page_content='18.5 Classiﬁcation When Features are Unavailable 671\\nHence we can compute the distance of the test point to each of t he cen-\\ntroids, and perform nearest centroid classiﬁcation. This a lso implies that\\nmethods like K-means clustering can also be implemented, us ing only the\\ninner products of the data points.\\nLogistic and multinomial regression with quadratic regula rization can\\nalso be implemented with inner-product kernels; see Sectio n 12.3.3 and\\nExercise 18.13. Exercise 12.10 derives linear discriminan t analysis using an\\ninner-product kernel.\\nPrincipal components can be computed using inner-product k ernels as\\nwell; since this is frequently useful, we give some details. Suppose ﬁrst\\nthat we have a centered data matrix X, and let X=UDVTbe its SVD\\n(18.12). Then Z=UDis the matrix of principal component variables (see\\nSection 14.5.1). But if K=XXT, then it follows that K=UD2UT, and\\nhence we can compute Zfrom the eigen decomposition of K. IfXisnot\\ncentered, then we can center it using ˜X= (I−M)X, whereM=1\\nN11T\\nis the mean operator. Thus we compute the eigenvectors of the double-\\ncentered kernel (I−M)K(I−M) for the principal components from an\\nuncentered inner-product matrix. Exercise 18.15 explores this further, and\\nSection 14.5.4 discusses in more detail kernel PCA for gener al kernels, such\\nas the radial kernel used in SVMs.\\nIf instead we had available only the pairwise (squared) Eucl idean dis-\\ntances between observations,\\n∆2\\nii′=||xi−xi′||2, (18.29)\\nit turns out we can do all of the above as well. The trick is to co nvert the\\npairwise distances to centered inner-products, and then pr oceed as before.\\nWe write\\n∆2\\nii′=||xi−¯x||2+||xi′−¯x||2−2⟨xi−¯x,xi′−¯x⟩. (18.30)\\nDeﬁning B={−∆2\\nii′/2}, we double center B:\\n˜K= (I−M)B(I−M); (18.31)\\nit is easy to check that ˜Kii′=⟨xi−¯x,xi′−¯x⟩, the centered inner-product\\nmatrix.\\nDistancesandinner-productsalsoallowustocomputetheme doidineach\\nclass—the observation with smallest average distance to oth er observations\\nin that class. This can be used for classiﬁcation (closest me doids), as well as\\nto drivek-medoids clustering (Section 14.3.10). With abstract data objects\\nlikeproteins,medoidshaveapracticaladvantageovermean s.Themedoidis\\noneofthetrainingexamples,andcanbedisplayed.Wetriedc losestmedoids\\nin the example in the next section (see Table 18.3), and its pe rformance is\\ndisappointing.\\nIt is useful to consider what we cannotdo with inner-product kernels and\\ndistances:', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 689}),\n",
       " Document(page_content='672 18. High-Dimensional Problems: p≫N\\nTABLE 18.3. Cross-validated error rates for the abstracts example. The ne arest\\nshrunken centroids ended up using no-shrinkage, but does us e a word-by-word\\nstandardization (section 18.2). This standardization giv es it a distinct advantage\\nover the other methods.\\nMethod CV Error (SE)\\n1. Nearest shrunken centroids 0.17 (0.05)\\n2. SVM 0.23 (0.06)\\n3. Nearest medoids 0.65 (0.07)\\n4. 1-NN 0.44 (0.07)\\n5. Nearest centroids 0.29 (0.07)\\n•Wecannotstandardizethevariables;standardizationsign iﬁcantlyim-\\nproves performance in the example in the next section.\\n•We cannot assess directly the contributions of individual v ariables.\\nIn particular, we cannot perform individual t-tests, ﬁt the nearest\\nshrunkencentroidsmodel,orﬁtanymodelthatusesthelasso penalty.\\n•Wecannotseparatethegoodvariablesfromthenoise:allvar iablesget\\nan equal say. If, as is often the case, the ratio of relevant to irrelevant\\nvariables is small, methods that use kernels are not likely t o work as\\nwell as methods that do feature selection.\\n18.5.3 Example: Abstracts Classiﬁcation\\nThis somewhat whimsical example serves to illustrate a limi tation of ker-\\nnel approaches. We collected the abstracts from 48 papers, 1 6 each from\\nBradley Efron (BE), Trevor Hastie and Rob Tibshirani (HT) (f requent co-\\nauthors), and Jerome Friedman (JF). We extracted all unique words from\\nthese abstracts, and deﬁned features xijto be the number of times word\\njappears in abstract i. This is the so-called bag of words representation.\\nQuotations, parentheses and special characters were ﬁrst r emoved from the\\nabstracts, and all characters were converted to lower case. We also removed\\nthe word “we”, which could unfairly discriminate HT abstrac ts from the\\nothers.\\nThere were 4492 total words, of which p= 1310 were unique. We sought\\nto classify the documents into BE, HT or JF on the basis of the f eatures\\nxij. Although it is artiﬁcial, this example allows us to assess t he possible\\ndegradation in performance if information speciﬁc to the ra w features is\\nnot used.\\nWeﬁrstappliedthenearestshrunkencentroidclassiﬁertot hedata,using\\n10-foldcross-validation.Itessentiallychosenoshrinka ge,andsousedallthe\\nfeatures; see the ﬁrst line of Table 18.3. The error rate is 17 %; the number\\nof features can be reduced to about 500 without much loss in ac curacy.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 690}),\n",
       " Document(page_content='18.5 Classiﬁcation When Features are Unavailable 673\\nNote that the nearest shrunken classiﬁer requires the raw fe ature matrix\\nXin order to standardize the features individually. Figure 1 8.10 shows the\\npredictivebayesusingthanalgorithmareproceduretechnologyvaluesaccuracyvariableswheninferencethosebayesianfrequentistproposepresentedmethodproblemsBE HT JF\\nFIGURE 18.10. Abstracts example: top 20scores from nearest shrunken cen-\\ntroids. Each score is the standardized diﬀerence in frequen cy for the word in the\\ngiven class (BE, HT or JF) versus all classes. Thus a positive sc ore (to the right\\nof the vertical grey zero lines) indicates a higher frequency in that class; a negative\\nscore indicates a lower relative frequency.\\ntop 20 discriminating words, with a positive score indicati ng that a word\\nappears more in that class than in the other classes.\\nSomeofthesetermsmakesense:forexample“frequentist”an d“Bayesian”\\nreﬂectEfron’sgreateremphasisonstatisticalinference. However,manyoth-\\ners are surprising, and reﬂect personal writing styles: for example, Fried-\\nman’s use of “presented” and HT’s use of “propose”.\\nWe then applied the support vector classiﬁer with linear ker nel and no\\nregularization, using the “all pairs” ( ovo) method to handle the three\\nclasses (regularization of the SVM did not improve its perfo rmance). The\\nresult is shown in Table 18.3. It does somewhat worse than the nearest\\nshrunken centroid classiﬁer.\\nAsmentioned,theﬁrstlineofTable18.3representsnearest shrunkencen-\\ntroids (with no shrinkage). Denote by sjthe pooled within-class standard\\ndeviation for feature j, ands0the median of the sjvalues. Then line (1)\\nalso corresponds to nearest centroid classiﬁcation, after ﬁrst standardizing\\neach feature by sj+s0[recall (18.4) on page 652].\\nLine (3) shows that the performance of nearest medoids is ver y poor,\\nsomething which surprised us. It is perhaps due to the small s ample sizes', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 691}),\n",
       " Document(page_content='674 18. High-Dimensional Problems: p≫N\\nand high dimensions, with medoids having much higher varian ce than\\nmeans. The performance of the one-nearest neighbor classiﬁ er is also poor.\\nThe performance of the nearest centroid classiﬁer is also sh own in Ta-\\nble 18.3 in line (5): it is better than nearest medoids, but wo rse than that\\nof nearest shrunken centroids, even with no shrinkage. The d iﬀerence seems\\nto be the standardization of each feature that is done in near est shrunken\\ncentroids. This standardization is important here, and req uires access to\\nthe individual feature values. Nearest centroids uses a sph erical metric, and\\nrelies on the fact that the features are in similar units. The support vector\\nmachine estimates a linear combination of the features and c an better deal\\nwith unstandardized features.\\n18.6 High-Dimensional Regression: Supervised\\nPrincipal Components\\nIn this section we describe a simple approach to regression a nd generalized\\nregression that is especially useful when p≫N. We illustrate the method\\non another microarray data example. The data is taken from Ro senwald\\net al. (2002) and consists of 240 samples from patients with d iﬀuse large\\nB-cell lymphoma (DLBCL), with gene expression measurement s for 7399\\ngenes. The outcome is survival time, either observed or righ t censored. We\\nrandomly divided the lymphoma samples into a training set of size 160 and\\na test set of size 80.\\nAlthough supervised principal components is useful for lin ear regression,\\nits most interesting applications may be in survival studie s, which is the\\nfocus of this example.\\nWe have not yet discussed regression with censored survival data in this\\nbook; it represents a generalized form of regression in whic h the outcome\\nvariable (survival time) is only partly observed for some in dividuals. Sup-\\npose for example we carry out a medical study that lasts for 36 5 days, and\\nfor simplicity all subjects are recruited on day one. We migh t observe one\\nindividual to die 200 days after the start of the study. Anoth er individ-\\nual might still be alive at 365 days when the study ends. This i ndividual\\nis said to be “right censored” at 365 days. We know only that he or she\\nlivedat least365 days. Although we do not know how long past 365 days\\nthe individual actually lived, the censored observation is still informative.\\nThis is illustrated in Figure 18.11. Figure 18.12 shows the s urvival curve\\nestimated by the Kaplan–Meier method for the 80 patients in t he test set.\\nSee for example Kalbﬂeisch and Prentice (1980) for a descrip tion of the\\nKaplan–Meier method.\\nOur objective in this example is to ﬁnd a set of features (gene s) that\\ncan predict the survival of an independent set of patients. T his could be', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 692}),\n",
       " Document(page_content='18.6 High-Dimensional Regression: Supervised Principal Co mponents 675\\nTime(days)Patient\\n0 100 200 300 365 1234\\nFIGURE 18.11. Censored survival data. For illustration there are four pati ents.\\nThe ﬁrst and third patients die before the study ends. The sec ond patient is alive\\nat the end of the study ( 365days), while the fourth patient is lost to follow-up\\nbefore the study ends. For example, this patient might have mo ved out of the\\ncountry. The survival times for patients two and four are sai d to be “censored.”\\n0 5 10 15 200.0 0.2 0.4 0.6 0.8 1.0l\\nlllllllll lll\\nll ll ll l lllll\\nl ll l l l lPr(T≥t)\\nMonthstSurvival Function\\nFIGURE 18.12. Lymphoma data. The Kaplan–Meier estimate of the survival\\nfunction for the 80patients in the test set, along with one-standard-error curv es.\\nThe curve estimates the probability of surviving past tmonths. The ticks indicate\\ncensored observations.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 693}),\n",
       " Document(page_content='676 18. High-Dimensional Problems: p≫N\\nPoor Cell Type Good Cell Type\\nSurvival Time\\nFIGURE 18.13. Underlying conceptual model for supervised principal compo -\\nnents. There are two cell types, and patients with the good ce ll type live longer on\\nthe average. Supervised principal components estimate the cell type, by averaging\\nthe expression of genes that reﬂect it.\\nuseful as a prognostic indicator to aid in choosing treatmen ts, or to help\\nunderstand the biological basis for the disease.\\nThe underlying conceptual model for supervised principal c omponents\\nis shown in Figure 18.13. We imagine that there are two cell ty pes, and\\npatients with the good cell type live longer on the average. H owever there\\nis considerable overlap in the two sets of survival times. We might think\\nof survival time as a “noisy surrogate” for cell type. A fully supervised\\napproach would give the most weight to those genes having the strongest\\nrelationship with survival. These genes are partially, but not perfectly, re-\\nlated to cell type. If we could instead discover the underlyi ng cell types of\\nthe patients, often reﬂected by a sizable signature of genes acting together\\nin pathways, then we might do a better job of predicting patie nt survival.\\nAlthough the cell type in Figure 18.13 is discrete, it is usef ul to imagine\\na continuous cell type, deﬁne by some linear combination of t he features.\\nWe will estimate the cell type as a continuous quantity, and t hen discretize\\nit for display and interpretation.\\nHowcanweﬁndthelinearcombinationthatdeﬁnestheimporta ntunder-\\nlying cell types?Principal components analysis (Section 1 4.5) is an eﬀective\\nmethod for ﬁnding linear combinations of features that exhi bit large varia-\\ntion in a dataset. But what we seek here are linear combinatio ns with both\\nhigh variance andsigniﬁcant correlation with the outcome. The lower right\\npanel of Figure 18.14 shows the result of applying standard p rincipal com-\\nponents in this example; the leading component does not corr elate strongly\\nwith survival (details are given in the ﬁgure caption).\\nHence we want to encourage principal component analysis to ﬁ nd linear\\ncombinations of features that have high correlation with th e outcome. To\\ndo this, we restrict attention to features which by themselv es have a siz-\\nable correlation with the outcome. This is summarized in the supervised\\nprincipal components Algorithm 18.1, and illustrated in Figure 18.14.\\nThe details in steps (1) and (2b) will depend on the type of out come\\nvariable. For a standard regression problem, we use the univ ariate linear\\nleast squares coeﬃcients in step (1) and a linear least squar es model in', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 694}),\n",
       " Document(page_content='18.6 High-Dimensional Regression: Supervised Principal Co mponents 677\\n7399 7350 50 27 1 Genes\\nPatientsSupervised PC\\n1 80 160\\nAbsolute Cox Score0 2 40 5 10 15 200.0 0.2 0.4 0.6 0.8 1.0Probability of Survivallow score\\nhigh scoreBest Single Gene\\nP=0.15\\n0 5 10 15 200.0 0.2 0.4 0.6 0.8 1.0Probability of SurvivalSupervised Principal Component − 27 Genes\\nP=0.006\\n0 5 10 15 200.0 0.2 0.4 0.6 0.8 1.0\\nMonthsProbability of SurvivalPrincipal Component − 7399 Genes\\nP=0.14\\nFIGURE 18.14. Supervised principal components on the lymphoma data. The\\nleft panel shows a heatmap of a subset of the gene-expression t raining data. The\\nrows are ordered by the magnitude of the univariate Cox-scor e, shown in the mid-\\ndle vertical column. The top 50 and bottom 50genes are shown. The supervised\\nprincipal component uses the top 27genes (chosen by 10-fold CV). It is repre-\\nsented by the bar at the top of the heatmap, and is used to order the columns\\nof the expression matrix. In addition, each row is multiplied b y the sign of the\\nCox-score. The middle panel on the right shows the survival cu rves on the test\\ndata when we create a low and high group by splitting this superv ised PC at zero\\n(training data mean). The curves are well separated, as indi cated by the p-value\\nfor the log-rank test. The top panel does the same, using the to p-scoring gene on\\nthe training data. The curves are somewhat separated, but no t signiﬁcantly. The\\nbottom panel uses the ﬁrst principal component on all the gen es, and the separa-\\ntion is also poor. Each of the top genes can be interpreted as no isy surrogates for\\na latent underlying cell-type characteristic, and supervised principal components\\nuses them all to estimate this latent factor.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 695}),\n",
       " Document(page_content='678 18. High-Dimensional Problems: p≫N\\nAlgorithm 18.1 Supervised Principal Components.\\n1. Compute the standardized univariate regression coeﬃcie nts for the\\noutcome as a function of each feature separately.\\n2. Foreachvalueofthethreshold θfromthelist0≤θ1<θ2<···<θK:\\n(a) Form a reduced data matrix consisting of only those featu res\\nwhose univariate coeﬃcient exceeds θin absolute value, and\\ncompute the ﬁrst mprincipal components of this matrix.\\n(b) Use these principal components in a regression model to p redict\\nthe outcome.\\n3. Pickθ(andm) by cross-validation.\\nstep (2b). For survival problems, Cox’s proportional hazar ds regression\\nmodeliswidelyused;henceweusethescoretestfromthismod elinstep(1)\\nand the multivariate Cox model in step (2b). The details are n ot essential\\nforunderstandingthebasicmethod;theymaybefoundinBair etal.(2006).\\nFigure 18.14 shows the results of supervised principal comp onents in this\\nexample. We used a Cox-score cutoﬀ of 3.53, yielding 27 genes , where the\\nvalue 3.53 was found through 10-fold cross-validation. We t hen computed\\nthe ﬁrst principal component ( m= 1) using just this subset of the data,\\nas well as its value for each of the test observations. We incl uded this as\\na quantitative predictor in a Cox regression model, and its l ikelihood-ratio\\nsigniﬁcance was p= 0.005. When dichotomized (using the mean score on\\nthe training data as a threshold), it clearly separates the p atients in the\\ntest set into low and high risk groups (middle-right panel of Figure 18.14,\\np= 0.006).\\nThe top-right panel of Figure 18.14 uses the top scoring gene (dichot-\\nomized) alone as a predictor of survival. It is not signiﬁcan t on the test set.\\nLikewise, the lower-right panel shows the dichotomized pri ncipal compo-\\nnent using all the training data, which is also not signiﬁcan t.\\nOur procedure allows m>1 principal components in step (2a). However,\\nthe supervision in step (1) encourages the principal compon ents to align\\nwith the outcome, and thus in most cases only the ﬁrst or ﬁrst f ew com-\\nponents tend to be useful for prediction. In the mathematica l development\\nbelow, we consider only the ﬁrst component, but extensions t o more than\\none component can be derived in a similar way.\\n18.6.1 Connection to Latent-Variable Modeling\\nA formal connection between supervised principal componen ts and the un-\\nderlyingcelltypemodel(Figure18.13)canbeseenthrougha latentvariable\\nmodel for the data. Suppose we have a response variable Ywhich is related', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 696}),\n",
       " Document(page_content='18.6 High-Dimensional Regression: Supervised Principal Co mponents 679\\nto an underlying latent variable Uby a linear model\\nY=β0+β1U+ε. (18.32)\\nInaddition,wehavemeasurementsonasetoffeatures Xjindexedby j∈P\\n(for pathway), for which\\nXj=α0j+α1jU+ǫj, j∈P. (18.33)\\nThe errorsεandǫjare assumed to have mean zero and are independent of\\nall other random variables in their respective models.\\nWe also have many additional features Xk,k̸∈Pwhich are independent\\nofU. We would like to identify P, estimateU, and hence ﬁt the predic-\\ntion model (18.32). This is a special case of a latent-struct ure model, or\\nsingle-component factor-analysis model (Mardia et al., 19 79, see also Sec-\\ntion 14.7). The latent factor Uis a continuous version of the cell type\\nconceptualized in Figure 18.13.\\nThe supervised principal component algorithm can be seen as a method\\nfor ﬁtting this model:\\n•The screening step (1) estimates the set P.\\n•GivenˆP, the largest principal component in step (2a) estimates the\\nlatent factor U.\\n•Finally, the regression ﬁt in step (2b) estimates the coeﬃci ent in\\nmodel (18.32).\\nStep (1) is natural, since on average the regression coeﬃcie nt is nonzero\\nonly ifα1jis non-zero. Hence this step should select the features j∈P.\\nStep (2a) is natural if we assume that the errors ǫjhave a Gaussian dis-\\ntribution, with the same variance. In this case the principa l component is\\nthe maximum likelihood estimate for the single factor model (Mardia et\\nal., 1979). The regression in (2b) is an obvious ﬁnal step.\\nSupposethereareatotalof pfeatures,with p1featuresintherelevantset\\nP. Then ifpandp1grow butp1is small relative to p, one can show (under\\nreasonable conditions) that the leading supervised princi pal component\\nis consistent for the underlying latent factor. The usual le ading principal\\ncomponent may not be consistent, since it can be contaminate d by the\\npresence of a large number of “noise” features.\\nFinally, suppose that the threshold used in step (1) of the su pervised\\nprincipal component procedure yields a large number of feat ures for com-\\nputation of the principal component. Then for interpretati onal purposes, as\\nwell as for practical uses, we would like some way of ﬁnding a r educed a set\\nof features that approximates the model. Pre-conditioning (Section 18.6.3)\\nis one way of doing this.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 697}),\n",
       " Document(page_content='680 18. High-Dimensional Problems: p≫N\\n18.6.2 Relationship with Partial Least Squares\\nSupervised principal components is closely related to part ial least squares\\nregression (Section 3.5.2). Bair et al. (2006) found that th e key to the good\\nperformance of supervised principal components was the ﬁlt ering out of\\nnoisyfeaturesinstep(2a).Partialleastsquares(Section 3.5.2)downweights\\nnoisy features, but does not throw them away; as a result a lar ge number\\nof noisy features can contaminate the predictions. However , a modiﬁcation\\nof the partial least squares procedure has been proposed tha t has a similar\\nﬂavor to supervised principal components [Brown et al. (199 1),Nadler and\\nCoifman (2005), for example]. We select the features as in st eps (1) and\\n(2a) of supervised principal components, but then apply PLS (rather than\\nprincipal components) to these features. For our current di scussion, we call\\nthis “thresholded PLS.”\\nThresholded PLScan beviewed as anoisy version of supervise dprincipal\\ncomponents, and hence we might not expect it to work as well in practice.\\nAssume the variables are all standardized. The ﬁrst PLS vari ate has the\\nform\\nz=∑\\nj∈P⟨y,xj⟩xj, (18.34)\\nandcanbethoughtofasanestimateofthelatentfactor Uinmodel(18.33).\\nIn contrast, the supervised principal components directio nˆusatisﬁes\\nˆu=1\\nd2∑\\nj∈P⟨ˆu,xj⟩xj, (18.35)\\nwheredis the leading singular value of XP. This follows from the deﬁnition\\nof the leading principal component. Hence thresholded PLS u ses weights\\nwhich are the inner product of ywith each of the features, while supervised\\nprincipalcomponentsusesthefeaturestoderivea“self-co nsistent”estimate\\nˆu. Since many features contribute to the estimate ˆu, rather than just the\\nsingle outcome y, we can expect ˆuto be less noisy than z. In fact, if there\\narep1features in the set P, andN, pandp1go to inﬁnity with p1/N→0,\\nthen it can be shown using the techniques in Bair et al. (2006) that\\nz=u+Op(1)\\nˆu=u+Op(√\\np1/N), (18.36)\\nwhereuis the true (unobservable) latent variable in the model (18. 32),\\n(18.33).\\nWe now present a simulation example to compare the methods nu meri-\\ncally. There are N= 100 samples and p= 5000 genes. We generated the\\ndata as follows:', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 698}),\n",
       " Document(page_content='18.6 High-Dimensional Regression: Supervised Principal Co mponents 681\\nFIGURE 18.15. Heatmap of the outcome (left column) and ﬁrst 500genes from\\na realization from model (18.37). The genes are in the columns, and the samples\\nare in the rows.\\nxij={\\n3+ǫijifi≤50,\\n4+ǫijifi>50j= 1,...,50\\nxij={\\n1.5+ǫijif 1≤i≤25 or 51≤i≤75\\n5.5+ǫijif 26≤i≤50 or 76≤i≤100j= 51,...,250\\nxij=ǫij j= 251,...,5000\\nyi= 2·1\\n50∑50\\nj=1xij+εi\\n(18.37)\\nwhereǫijandεiare independent normal random variables with mean 0 and\\nstandard deviations 1 and 1.5, respectively. Thus in the ﬁrs t 50 genes, there\\nisanaveragediﬀerenceof1unitbetweensamples1–50and51– 100,andthis\\ndiﬀerence correlates with the outcome y. The next 200 genes have a large\\naverage diﬀerence of 4 units between samples (1–25, 51–75) a nd (26–50,\\n76–100), but this diﬀerence is uncorrelated with the outcom e. The rest of\\nthe genes are noise. Figure 18.15 shows a heatmap of a typical realization,\\nwith the outcome at the left, and the ﬁrst 500 genes to the righ t.\\nWe generated 100 simulations from this model, and summarize the test\\nerror results in Figure 18.16. The test errors of principal c omponents and\\npartial least squares are shown at the right of the plot; both are badly\\naﬀected by the noisy features in the data. Supervised princi pal components\\nand thresholded PLS work best over a wide range of the number o f selected\\nfeatures, with the former showing consistently lower test e rrors.\\nWhile this example seems “tailor-made” for supervised prin cipal com-\\nponents, its good performance seems to hold in other simulat ed and real\\ndatasets (Bair et al., 2006).\\n18.6.3 Pre-Conditioning for Feature Selection\\nSupervisedprincipalcomponentscanyieldlowertesterror sthancompeting\\nmethods, as shown in Figure 18.16. However, it does not alway s produce a\\nsparse model involving only a small number of features (gene s). Even if the\\nthresholding in Step (1) of the algorithm yields a relativel y small number', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 699}),\n",
       " Document(page_content='682 18. High-Dimensional Problems: p≫N1.00 1.05 1.10 1.15 1.20 1.25\\nNumber of FeaturesRelative Root Mean Square Test Error\\n0 50 100 150 200 250 300 ... 5000Thresholded PLS\\nSupervised Principal Components\\nFIGURE 18.16. Root mean squared test error ( ±one standard error), for\\nsupervised principal components and thresholded PLS on 100realizations from\\nmodel (18.37). All methods use one component, and the errors are relative to\\nthe noise standard deviation (the Bayes error is 1.0). For both methods, diﬀerent\\nvalues for the ﬁltering threshold were tried and the number of fe atures retained\\nis shown on the horizontal axis. The extreme right points cor respond to regular\\nprincipal components and partial least squares, using all th e genes.\\nof features, it may be that some of the omitted features have s izable inner\\nproducts with the supervised principal component (and coul d act as a good\\nsurrogate). In addition, highly correlated features will t end to be chosen\\ntogether, and there may be great deal of redundancy in the set of selected\\nfeatures.\\nThe lasso (Sections 18.4 and 3.4.2), on the other hand, produ ces a sparse\\nmodelfromthedata.Howdothetesterrorsofthetwomethodsc ompareon\\nthesimulatedexampleofthelastsection?Figure18.17show sthetesterrors\\nfor one realization from model (18.37) for the lasso, superv ised principal\\ncomponents, and the pre-conditioned lasso (described belo w).\\nWe see that supervised principal components (orange curve) reaches its\\nlowest error when about 50 features are included in the model , which is\\nthe correct number for the simulation. Although a linear mod el in the ﬁrst\\n50 features is optimal, the lasso (green) is adversely aﬀect ed by the large\\nnumber of noisy features, and starts overﬁtting when far few er are in the\\nmodel.\\nCan we get the low test error of supervised principal compone nts along\\nwith the sparsity of the lasso? This is the goal of pre-conditioning (Paul\\net al., 2008). In this approach, one ﬁrst computes the superv ised principal\\ncomponent predictor ˆ yifor each observation in the training set (with the', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 700}),\n",
       " Document(page_content='18.7 Feature Assessment and the Multiple-Testing Problem 6 83\\n0 50 100 150 200 2503.0 3.2 3.4 3.6 3.8 4.0 4.2 4.4\\nNumber of Features in ModelMean Test ErrorLasso\\nSupervised Principal Components\\nPreconditioned Lasso\\nFIGURE 18.17. Test errors for the lasso, supervised principal components,\\nand pre-conditioned lasso, for one realization from model (18 .37). Each model is\\nindexed by the number of non-zero features. The supervised p rincipal component\\npath is truncated at 250features. The lasso self-truncates at 100, the sample size\\n(see Section 18.4). In this case, the pre-conditioned lasso a chieves the lowest error\\nwith about 25features.\\nthreshold selected by cross-validation). Then we apply the lasso with ˆyias\\nthe outcome variable, in place of the usual outcome yi. All features are used\\nin the lasso ﬁt, not just those that were retained in the thres holding step\\nin supervised principal components. The idea is that by ﬁrst denoising the\\noutcome variable, the lasso should not be as adversely aﬀect ed by the large\\nnumber of noise features. Figure 18.17 shows that pre-condi tioning (purple\\ncurve) has been successful here, yielding much lower test er ror than the\\nusuallasso,andaslow(inthiscase)asforsupervisedprinc ipalcomponents.\\nIt also can achieve this using less features. The usual lasso , applied to\\nthe raw outcome, starts to overﬁt more quickly than the pre-c onditioned\\nversion. Overﬁtting is not a problem, since the outcome vari able has been\\ndenoised. We usually select the tuning parameter for the pre -conditioned\\nlasso on more subjective grounds, like parsimony.\\nPre-conditioning can be applied in a variety of settings, us ing initial\\nestimates other than supervised principal components and p ost-processors\\nother than the lasso. More details may be found in Paul et al. ( 2008).\\n18.7 Feature Assessment and the Multiple-Testing\\nProblem\\nIn the ﬁrst part of this chapter we discuss prediction models in thep≫N\\nsetting. Here we consider the more basic problem of assessin g the signif-', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 701}),\n",
       " Document(page_content='684 18. High-Dimensional Problems: p≫N\\nicance of each of the pfeatures. Consider the protein mass spectrometry\\nexample of Section 18.4.1. In that problem, the scientist mi ght not be inter-\\nested in predicting whether a given patient has prostate can cer. Rather the\\ngoal might be to identify proteins whose abundance diﬀers be tween nor-\\nmal and cancer samples, in order to enhance understanding of the disease\\nand suggest targets for drug development. Thus our goal is to assess the\\nsigniﬁcance of individual features. This assessment is usu ally done without\\nthe use of a multivariate predictive model like those in the ﬁ rst part of this\\nchapter. The feature assessment problem moves our focus fro m prediction\\nto the traditional statistical topic of multiple hypothesis testing . For the\\nremainder of this chapter we will use Minstead ofpto denote the number\\nof features, since we will frequently be referring to p-values.\\nTABLE 18.4. Subset of the 12,625genes from microarray study of radiation\\nsensitivity. There are a total of 44samples in the normal group and 14in the\\nradiation sensitive group; we only show three samples from eac h group.\\nNormal Radiation Sensitive\\nGene 1 7.85 29.74 29.50 ...17.20 -50.75 -18.89 ...\\nGene 2 15.44 2.70 19.37 ...6.57 -7.41 79.18 ...\\nGene 3 -1.79 15.52 -3.13 ...-8.32 12.64 4.75 ...\\nGene 4 -11.74 22.35 -36.11 ...-52.17 7.24 -2.32 ...\\n...........................\\nGene 12,625 -14.09 32.77 57.78 ...-32.84 24.09 -101.44 ...\\nConsider, for example, the microarray data in Table 18.4, ta ken from a\\nstudy on the sensitivity of cancer patients to ionizing radi ation treatment\\n(Rieger et al., 2004). Each row consists of the expression of genes in 58\\npatient samples: 44 samples were from patients with a normal reaction, and\\n14 from patients who had a severe reaction to radiation. The m easurements\\nwere made on oligo-nucleotide microarrays. The object of th e experiment\\nwas to ﬁnd genes whose expression was diﬀerent in the radiati on sensitive\\ngroup of patients. There are M= 12,625 genes altogether; the table shows\\nthe data for some of the genes and samples for illustration.\\nTo identify informative genes, we construct a two-sample t-statistic for\\neach gene.\\ntj=¯x2j−¯x1j\\nsej, (18.38)\\nwhere ¯xkj=∑\\ni∈Cℓxij/Nℓ. HereCℓare the indices of the Nℓsamples in\\ngroupℓ, whereℓ= 1 is the normal group and ℓ= 2 is the sensitive group.\\nThe quantity se jis the pooled within-group standard error for gene j:', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 702}),\n",
       " Document(page_content='18.7 Feature Assessment and the Multiple-Testing Problem 6 85\\n−4 −2 0 2 40 200 400 600 800\\nt−statistics\\nFIGURE 18.18. Radiation sensitivity microarray example. A histogram of th e\\n12,625t-statistics comparing the radiation-sensitive versus ins ensitive groups.\\nOverlaid in blue is the histogram of the t-statistics from 1000permutations of the\\nsample labels.\\nsej= ˆσj√\\n1\\nN1+1\\nN2; ˆσ2\\nj=1\\nN1+N2−2(∑\\ni∈C1(xij−¯x1j)2+∑\\ni∈C2(xij−¯x2j)2)\\n.\\n(18.39)\\nA histogram of the 12,625 t-statistics is shown in orange in F igure 18.18,\\nranging in value from −4.7 to 5.0. If thetjvalues were normally distributed\\nwe could consider any value greater than two in absolute valu e to be sig-\\nniﬁcantly large. This would correspond to a signiﬁcance lev el of about 5%.\\nHere there are 1189 genes with |tj|≥2. However with 12,625 genes we\\nwould expect many large values to occur by chance, even if the group-\\ning is unrelated to any gene. For example, if the genes were in dependent\\n(which they are surely not), the number of falsely signiﬁcan t genes would\\nhave a binomial distribution with mean 12 ,625·0.05 = 631.3 and standard\\ndeviation 24.5; the actual 1189 is way out of range.\\nHow do we assess the results for all 12,625 genes? This is call ed themul-\\ntiple testing problem. We can start as above by computing a p-value for\\neach gene. This can be done using the theoretical t-distribution probabil-\\nities, which assumes the features are normally distributed . An attractive\\nalternative approach is to use the permutation distributio n, since it avoids\\nassumptions about the distribution of the data. We compute ( in principle)\\nallK=(58\\n14)\\npermutations of the sample labels, and for each permutation\\nkcompute the t-statisticstk\\nj. Then the p-value for gene jis', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 703}),\n",
       " Document(page_content='686 18. High-Dimensional Problems: p≫N\\npj=1\\nKK∑\\nk=1I(|tk\\nj|>|tj|). (18.40)\\nOf course,(58\\n14)\\nis a large number (around 1013) and so we can’t enumer-\\nate all of the possible permutations. Instead we take a rando m sample of\\nthe possible permutations; here we took a random sample of K= 1000\\npermutations.\\nTo exploit the fact that the genes are similar (e.g., measure d on the\\nsame scale), we can instead pool the results for all genes in c omputing the\\np-values.\\npj=1\\nMKM∑\\nj′=1K∑\\nk=1I(|tk\\nj′|>|tj|). (18.41)\\nThis also gives more granular p-values than does (18.40), since there many\\nmore values in the pooled null distribution than there are in each individual\\nnull distribution.\\nUsing this set of p-values, we would like to test the hypotheses:\\nH0j= treatment has no eﬀect on gene j\\nversus (18.42)\\nH1j= treatment has an eﬀect on gene j\\nfor allj= 1,2,...,M. We reject H0jat levelαifpj< α. This test has\\ntype-I error equal to α; that is, the probability of falsely rejecting H0jisα.\\nNow with many tests to consider, it is not clear what we should use\\nas an overall measure of error. Let Ajbe the event that H0jis falsely\\nrejected; by deﬁnition Pr( Aj) =α. Thefamily-wise error rate (FWER)\\nis the probability of at least one false rejection, and is a co mmonly used\\noverall measure of error. In detail, if A=∪M\\nj=1Ajis the event of at least\\none false rejection, then the FWER is Pr( A). Generally Pr( A)≫αfor\\nlargeM, and depends on the correlation between the tests. If the tes ts are\\nindependent each with type-I error rate α, then the family-wise error rate\\nof the collection of tests is (1 −(1−α)M). On the other hand, if the tests\\nhave positive dependence, that is Pr( Aj|Ak)>Pr(Aj), then the FWER\\nwill be less than (1 −(1−α)M). Positive dependence between tests often\\noccurs in practice, in particular in genomic studies.\\nOne of the simplest approaches to multiple testing is the Bonferroni\\nmethod. It makes each individual test more stringent, in ord er to make the\\nFWER equal to at most α: we reject H0jifpj<α/M. It is easy to show\\nthat the resulting FWER is ≤α(Exercise 18.16). The Bonferroni method\\ncan be useful if Mis relatively small, but for large Mit is too conservative,\\nthat is, it calls too few genes signiﬁcant.\\nIn our example, if we test at level say α= 0.05, then we must use the\\nthreshold 0.05/12,625 = 3.9×10−6. None of the 12 ,625 genes had a p-value\\nthis small.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 704}),\n",
       " Document(page_content='18.7 Feature Assessment and the Multiple-Testing Problem 6 87\\nThere are variations to this approach that adjust the indivi dualp-values\\nto achieve an FWER of at most α, with some approaches avoiding the\\nassumption of independence; see, e.g., Dudoit et al. (2002b ).\\n18.7.1 The False Discovery Rate\\nA diﬀerent approach to multiple testing does not try to contr ol the FWER,\\nbut focuses instead on the proportion of falsely signiﬁcant genes. As we will\\nsee, this approach has a strong practical appeal.\\nTable 18.5 summarizes the theoretical outcomes of Mhypothesis tests.\\nNote that the family-wise error rate is Pr( V≥1). Here we instead focus\\nTABLE 18.5. Possible outcomes from Mhypothesis tests. Note that Vis the\\nnumber of false-positive tests; the type-I error rate is E(V)/M0. The type-II error\\nrate isE(T)/M1, and the power is 1−E(T)/M1.\\nCalled Called\\nNot Signiﬁcant Signiﬁcant Total\\nH0True U VM0\\nH0False T SM1\\nTotal M−R RM\\non thefalse discovery rate\\nFDR = E(V/R). (18.43)\\nIn the microarray setting, this is the expected proportion o f genes that\\nare incorrectly called signiﬁcant, among the Rgenes that are called signif-\\nicant. The expectation is taken over the population from whi ch the data\\nare generated. Benjamini and Hochberg (1995) ﬁrst proposed the notion of\\nfalse discovery rate, and gave a testing procedure (Algorit hm 18.2) whose\\nFDR is bounded by a user-deﬁned level α. The Benjamini–Hochberg (BH)\\nprocedure is based on p-values; these can be obtained from an asymptotic\\napproximation to the test statistic (e.g., Gaussian), or a p ermutation dis-\\ntribution, as is done here.\\nIf the hypotheses are independent, Benjamini and Hochberg ( 1995) show\\nthat regardless of how many null hypotheses are true and rega rdless of the\\ndistribution of the p-values when the null hypothesis is false, this procedure\\nhas the property\\nFDR≤M0\\nMα≤α. (18.45)\\nFor illustration we chose α= 0.15. Figure 18.19 shows a plot of the or-\\nderedp-valuesp(j), and the line with slope 0 .15/12625.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 705}),\n",
       " Document(page_content='688 18. High-Dimensional Problems: p≫N\\nAlgorithm 18.2 Benjamini–Hochberg (BH) Method.\\n1. Fix the false discovery rate αand letp(1)≤p(2)≤···≤p(M)denote\\nthe ordered p-values\\n2. Deﬁne\\nL= max{\\nj:p(j)<α·j\\nM}\\n. (18.44)\\n3. Reject all hypotheses H0jfor whichpj≤p(L), the BH rejection\\nthreshold.\\nGenes ordered by p−valuep−value\\n1 5 10 50 1005*10^−6 5*10^−5 5*10^−4 5*10^−3••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• ••••••••••••••••••••••• •••• ••••••• • •• • ••• •••••••••••••••••••• • ••••••• •••• •••• • ••• •• •••• •• •••••••• ••••••••• • ••••• •••••• ••••• ••••• • ••• •••• • •••••• •• ••• ••••• • •• • •• •• ••••• •• ••• ••••• ••• • •• • •• •• ••• •• • ••• •• ••••••• ••• • • • •• •• •• • • •• •• •••••• • ••• • •\\nFIGURE 18.19. Microarray example continued. Shown is a plot of the ordered\\np-valuesp(j)and the line 0.15·(j/12,625), for the Benjamini–Hochberg method.\\nThe largest jfor which the p-valuep(j)falls below the line, gives the BH threshold.\\nHere this occurs at j= 11, indicated by the vertical line. Thus the BH method\\ncalls signiﬁcant the 11genes (in red) with smallest p-values.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 706}),\n",
       " Document(page_content='18.7 Feature Assessment and the Multiple-Testing Problem 6 89\\nAlgorithm 18.3 The Plug-in Estimate of the False Discovery Rate.\\n1. CreateKpermutations of the data, producing t-statisticstk\\njfor fea-\\nturesj= 1,2,...,Mand permutations k= 1,2,...,K.\\n2. For a range of values of the cut-point C, let\\nRobs=M∑\\nj=1I(|tj|>C),ˆE(V) =1\\nKM∑\\nj=1K∑\\nk=1I(|tk\\nj|>C).(18.46)\\n3. Estimate the FDR by ˆFDR =ˆE(V)/Robs.\\nStarting at the left and moving right, the BH method ﬁnds the l ast time\\nthat thep-values fall below the line. This occurs at j= 11, so we reject\\nthe 11 genes with smallest p-values. Note that the cutoﬀ occurs at the 11th\\nsmallestp-value, 0.00012, and the 11th largest of the values |tj|is 4.101\\nThus we reject the 11 genes with |tj|≥4.101.\\nFrom our brief description, it is not clear how the BH procedu re works;\\nthat is, why the corresponding FDR is at most 0 .15, the value used for α.\\nIndeed,theproofofthisfactisquitecomplicated(Benjami niandHochberg,\\n1995).\\nA more direct way to proceed is a plug-inapproach. Rather than starting\\nwith a value for α, we ﬁx a cut-point for our t-statistics, say the value\\n4.101 that appeared above. The number of observed values |tj|equal or\\ngreater than 4 .101 is 11. The total number of permutation values |tk\\nj|equal\\nor greater than 4 .101 is 1518, for an average of 1518 /1000 = 1.518 per\\npermutation. Thus a direct estimate of the false discovery r ate isˆFDR =\\n1.518/11≈14%. Note that 14% is approximately equal to the value of\\nα= 0.15 used above (the diﬀerence is due to discreteness). This pr ocedure\\nis summarized in Algorithm 18.3. To recap:\\nThe plug-in estimate of FDR of Algorithm 18.3 is equivalent t o the BH\\nprocedure of Algorithm 18.2, using the permutation p-values (18.40).\\nThis correspondence between the BH method and the plug-in es timate is\\nnot a coincidence. Exercise 18.17 shows that they are equiva lent in general.\\nNote that this procedure makes no reference to p-values at all, but rather\\nworks directly with the test statistics.\\nThe plug-in estimate is based on the approximation\\nE(V/R)≈E(V)\\nE(R), (18.47)\\nand in general ˆFDR is a consistent estimate of FDR(Storey, 2002; Storey et\\nal., 2004). Note that the numerator ˆE(V) actually estimates ( M/M0)E(V),', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 707}),\n",
       " Document(page_content='690 18. High-Dimensional Problems: p≫N\\nsince the permutation distribution uses MratherM0null hypotheses.\\nHence if an estimate of M0is available, a better estimate of FDR can be\\nobtained from ( ˆM0/M)·ˆFDR. Exercise 18.19 shows a way to estimate M0.\\nThe most conservative (upwardly biased) estimate of FDR use sM0=M.\\nEquivalently, an estimate of M0can be used to improve the BH method,\\nthrough relation (18.45).\\nThe reader might be surprised that we chose a value as large as 0.15 for\\nα, the FDR bound. We must remember that the FDR is not the same as\\ntype-I error, for which 0.05 is the customary choice. For the scientist, the\\nfalse discovery rate is the expected proportion of false pos itive genes among\\nthe list of genes that the statistician tells him are signiﬁc ant. Microarray\\nexperiments with FDRs as high as 0.15 might still be useful, e specially if\\nthey are exploratory in nature.\\n18.7.2 Asymmetric Cutpoints and the SAM Procedure\\nIn the testing methods described above, we used the absolute value of the\\ntest statistic tj, and hence applied the same cut-points to both positive and\\nnegative values of the statistic. In some experiments, it mi ght happen that\\nmost or all of the diﬀerentially expressed genes change in th e positive direc-\\ntion (or all in the negative direction). For this situation i t is advantageous\\nto derive separate cut-points for the two cases.\\nThesigniﬁcance analysis of microarrays (SAM) approach oﬀers a way of\\ndoing this. The basis of the SAM method is shown in Figure 18.2 0. On the\\nvertical axis we have plotted the ordered test statistics t(1)≤t(2)≤···≤\\nt(M), while the horizontal axis shows the expected order statist ics from the\\npermutations of the data: ˜t(j)= (1/K)∑K\\nk=1tk\\n(j), wheretk\\n(1)≤tk\\n(2)≤···≤\\ntk\\n(M)are the ordered test statistics from permutation k.\\nTwo lines are drawn, parallel to the 45◦line, ∆ units away. Starting at\\nthe origin and moving to the right, we ﬁnd the ﬁrst place that t he genes\\nleave the band. This deﬁnes the upper cutpoint Chiand all genes beyond\\nthat point are called signiﬁcant (marked red). Similarly we ﬁnd the lower\\ncutpoint Clowfor genes in the bottom left corner. Thus each value of the\\ntuning parameter ∆ deﬁnes upper and lower cutpoints, and the plug-in\\nestimate ˆFDR for each of these cutpoints is estimated as before. Typic ally\\narangeofvaluesof∆andassociated ˆFDRvaluesarecomputed,fromwhich\\na particular pair are chosen on subjective grounds.\\nThe advantage of the SAM approach lies in the possible asymme try of\\nthe cutpoints. In the example of Figure 18.20, with ∆ = 0 .71 we obtain\\n11 signiﬁcant genes; they are all in the upper right. The data points in the\\nbottom left never leave the band, and hence Clow=−∞. Hence for this\\nvalue of ∆, no genes are called signiﬁcant on the left (negati ve) side. We\\ndo not impose symmetry on the cutpoints, as was done in Sectio n 18.7.1,\\nas there is no reason to assume similar behavior at the two end s.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 708}),\n",
       " Document(page_content='18.7 Feature Assessment and the Multiple-Testing Problem 6 91\\nExpected Order Statisticst−statistic\\n−4 −2 0 2 4−4 −2 0 2 4\\n•••••••••••••• • •••••••••••• • ••••••• ••••• •• •••••••••••••••• •• • •••••• •••• • ••• • • •••••• • •••• ••••• •••• •• ••• •••• • •••• ••• •• • •• • •• • ••• • ••••• • •• •••• •• •• •••• • ••• •• ••••••• •• •••••••• •• ••• • ••• ••• •••• •• •• •••• •• •• •• •• ••• • •• • •• •• ••• •• •••• ••• ••• •• • •• ••• •• •• •• ••• •• •• •• •• ••• ••• •••• •• •••• •• •• •• •• •• •• •• •• ••• •• •• ••• •• ••• •• ••• ••• •• ••• •••• •••• •• •• •• •• •• •• •• •• • ••••• •• ••• ••• ••• •• •• •• •• •• ••• •• ••• •• •• •• ••• • ••• ••• •• •• •• •• •• •• ••• •• •• •• ••• •• ••• •• •• •• • •• •• •• •••• •• •• •• •• •• •• ••• ••• •• •• ••• •• •• •• • •• •• •• ••• •• •• ••• •• •• •• ••• •• •• •• •• •• ••• •• •• •• •• •• •• ••• •• •• •• •• •• ••• •••• •• •• •• •• ••• •• •••• •• •• •• •• •• ••• •• ••• •• •• •• •• ••• ••• •• ••• •• •• •• •• •• •• •• ••• •• •••• •• ••• ••• •• • •••• •• •• •• •• •• •• •• •• •• •• ••• •• •• •• •• ••• •• •• •• •• ••• •• •• •• ••• •• •• •• ••• • •• •• •• •• •• ••• •• •• •• •• • ••• •• •• •• •• •• •• ••• •• •• ••• •• ••• •• •• •• •• ••• •• •• •• ••• •• •• •• •• •• ••• •• •• •• •• •• •• •• •• ••• •• ••• ••• •• ••• •• ••• •• •• ••• •• •• •• •• ••• •• •• •••• ••• •• •• •• •• ••• •• ••• •• •• •••• •• •• •• ••• •• •• •• •• •• •• •• •• •• ••• •• ••• ••• •• •• •• •• •• •• ••• •• •• ••• •• •• •• •• ••• •• •• •• •• ••• •• •• ••• ••• •• •• •• ••• •• •• •• •• •• •• •• ••• •• ••• •• •• •• ••• •• •• • ••• •• •• •• •• •• •••• •• •• •• ••• •• •• •• •• •• •• ••• •• •• •• •• •• ••• •• ••• •• •• •• •• ••• •• •• •• •• •• ••• •• •• ••• •• ••• •• •• ••• •• ••• •• •• ••• •• ••• •• •• •• •• ••• •• •• •• ••• •• •• •• •• ••• •• •• •• •••• •• •• ••• •• •• • ••• •• •• ••• •• •• •• •• •• •• ••• •• ••• •• •• •• •• •• ••• •• •••• •• •• ••• •• ••• •• •• •• •• •• •• ••• •• •••• •• ••• •• ••• •• •• •• •• •• ••• •• •• •• ••• •• •••••• •• •• ••• ••• •• •• ••• •• •• ••• •• ••• •• ••• ••• ••• •• •• ••• • ••• •• •• •• •••• •• •• •• •• ••• •• •• •• ••• • •••• •• ••• •• •• •• •• ••• •• •• •••• •••• •• •• ••• ••• •• •• •• ••• •• •• •• ••• •• •• •• •• •• •• •• •••• •• •• ••• •• •• ••••• •• •• •• ••• •• •• •••• •• ••• •• •• •• •• •• ••• •• • •• ••• •• ••• •• • ••• ••• •• •• ••• •• • •• •• ••• •• • ••• ••• ••• •• ••• ••• ••• •• • ••• •• •• • •• •• •• ••• ••• • ••• ••• •• •• •• •• •• •• •• •• ••• •• •• •• •• •• •• •• ••• ••• •• ••• ••• •• • •••••••• •• • •••• • ••• •• ••• •• •• •• •• •••• ••• •• •• • ••••••• •• ••• •• • ••••• •••• •• ••••• •• • •••• • ••••• • •• • ••• ••• •• •• • ••••• •• •• •• • •••••• ••• •••• ••••••• •• • ••• • •• • •••• ••• •• ••••••••••• •• • • • • ••••• • • ••••• •••••• •••••••••••• • ••••••• ••••••••••••••••••••••\\n••••••••••••\\n∆\\nChi\\nFIGURE 18.20. SAM plot for the radiation sensitivity microarray data. On th e\\nvertical axis we have plotted the ordered test statistics, wh ile the horizontal axis\\nshows the expected order statistics of the test statistics f rom permutations of the\\ndata. Two lines are drawn, parallel to the 45◦line,∆units away from it. Starting\\nat the origin and moving to the right, we ﬁnd the ﬁrst place that the genes leave\\nthe band. This deﬁnes the upper cut-point Chiand all genes beyond that point are\\ncalled signiﬁcant (marked in red). Similarly we deﬁne a lower cut pointClow. For\\nthe particular value of ∆ = 0.71in the plot, no genes are called signiﬁcant in the\\nbottom left.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 709}),\n",
       " Document(page_content='692 18. High-Dimensional Problems: p≫N\\nThereissomesimilaritybetweenthisapproachandtheasymm etrypossi-\\nblewithlikelihood-ratiotests.Supposewehavealog-like lihoodℓ0(tj)under\\nthe null-hypothesis of no eﬀect, and a log-likelihood ℓ(tj) under the alterna-\\ntive. Then a likelihood ratio test amounts to rejecting the n ull-hypothesis\\nif\\nℓ(tj)−ℓ0(tj)>∆, (18.48)\\nfor some ∆. Depending on the likelihoods, and particularly t heir relative\\nvalues, this can result in a diﬀerent threshold for tjthan for−tj. The SAM\\nprocedure rejects the null-hypothesis if\\n|t(j)−˜t(j)|>∆ (18.49)\\nAgain, the threshold for each t(j)depends on the corresponding value of\\nthe null value ˜t(j).\\n18.7.3 A Bayesian Interpretation of the FDR\\nThere is an interesting Bayesian view of the FDR, developed i n Storey\\n(2002) and Efron and Tibshirani (2002). First we need to deﬁn e thepositive\\nfalse discovery rate (pFDR) as\\npFDR = E[V\\nR⏐⏐⏐⏐R>0]\\n. (18.50)\\nThe additional term positiverefers to the fact that we are only interested\\nin estimating an error rate where positive ﬁndings have occu rred. It is\\nthis slightly modiﬁed version of the FDR that has a clean Baye sian inter-\\npretation. Note that the usual FDR [expression (18.43)] is n ot deﬁned if\\nPr(R= 0)>0.\\nLet Γ be a rejection region for a single test; in the example ab ove we used\\nΓ = (−∞,−4.10)∪(4.10,∞). Suppose that Midentical simple hypothe-\\nsis tests are performed with the i.i.d. statistics t1,...,tMand rejection\\nregion Γ. We deﬁne a random variable Zjwhich equals 0 if the jth null\\nhypothesis is true, and 1 otherwise. We assume that each pair (tj,Zj) are\\ni.i.d random variables with\\ntj|Zj∼(1−Zj)·F0+Zj·F1 (18.51)\\nfor some distributions F0andF1. This says that each test statistic tjcomes\\nfrom one of two distributions: F0if the null hypothesis is true, and F1\\notherwise. Letting Pr( Zj= 0) =π0, marginally we have:\\ntj∼π0·F0+(1−π0)·F1. (18.52)\\nThen it can be shown (Efron et al., 2001; Storey, 2002) that', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 710}),\n",
       " Document(page_content='18.8 Bibliographic Notes 693\\npFDR(Γ) = Pr( Zj= 0|tj∈Γ). (18.53)\\nHence under the mixture model (18.51), the pFDR is the poster ior proba-\\nbility that the null hypothesis it true, given that test stat istic falls in the\\nrejection region for the test; that is, given that we reject t he null hypothesis\\n(Exercise 18.20).\\nThe false discovery rate provides a measure of accuracy for t ests based\\non an entire rejection region, such as |tj|≥2. But if the FDR of such a test\\nis say 10%, then a gene with say tj= 5 will be more signiﬁcant than a gene\\nwithtj= 2. Thus it is of interest to derive a local (gene-speciﬁc) ve rsion\\nof the FDR. The q-value(Storey, 2003) of a test statistic tjis deﬁned to\\nbe the smallest FDR over all rejection regions that reject tj. That is, for\\nsymmetric rejection regions, the q-value fortj= 2 is deﬁned to be the\\nFDR for the rejection region Γ = {−(∞,−2)∪(2,∞)}. Thus the q-value\\nfortj= 5 will be smaller than that for tj= 2, reﬂecting the fact that tj= 5\\nis more signiﬁcant than tj= 2. The local false discovery rate (Efron and\\nTibshirani, 2002) at t=t0is deﬁned to be\\nPr(Zj= 0|tj=t0). (18.54)\\nThis is the (positive) FDR for an inﬁnitesimal rejection reg ion surrounding\\nthe valuetj=t0.\\n18.8 Bibliographic Notes\\nMany references were given at speciﬁc points in this chapter ; we give some\\nadditional ones here. Dudoit et al. (2002a) give an overview and compar-\\nison of discrimination methods for gene expression data. Le vina (2002)\\ndoes some mathematical analysis comparing diagonal LDA to f ull LDA, as\\np,N→∞withp>N. She shows that with reasonable assumptions diago-\\nnal LDA has a lower asymptotic error rate than full LDA. Tibsh irani et al.\\n(2001a)andTibshiranietal.(2003)proposedthenearestsh runken-centroid\\nclassiﬁer. Zhu and Hastie (2004) study regularized logisti c regression. High-\\ndimensional regression and the lasso are very active areas o f research, and\\nmany references are given in Section 3.8.5. The fused lasso w as proposed\\nby Tibshirani et al. (2005), while Zou and Hastie (2005) intr oduced the\\nelastic net. Supervised principal components is discussed in Bair and Tib-\\nshirani (2004) and Bair et al. (2006). For an introduction to the analysis\\nof censored survival data, see Kalbﬂeisch and Prentice (198 0).\\nMicroarray technology has led to a ﬂurry of statistical rese arch: see for\\nexample the books by Speed (2003), Parmigiani et al. (2003), Simon et al.\\n(2004), and Lee (2004).\\nThefalsediscoveryratewasproposedbyBenjaminiandHochb erg(1995),\\nand studied and generalized in subsequent papers by these au thors and', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 711}),\n",
       " Document(page_content='694 18. High-Dimensional Problems: p≫N\\nmany others. A partial list of papers on FDR may be found on Yoa v Ben-\\njamini’s homepage. Some more recent papers include Efron an d Tibshirani\\n(2002), Storey (2002), Genovese and Wasserman (2004), Stor ey and Tib-\\nshirani (2003) and Benjamini and Yekutieli (2005). Dudoit e t al. (2002b)\\nreview methods for identifying diﬀerentially expressed ge nes in microarray\\nstudies.\\nExercises\\nEx. 18.1 For a coeﬃcient estimate ˆβj, letˆβj/||ˆβj||2be the normalized ver-\\nsion. Show that as λ→∞, the normalized ridge-regression estimates con-\\nverge to the renormalized partial-least-squares one-comp onent estimates.\\nEx.18.2Nearest shrunken centroids and the lasso. Considera(naiveBayes)\\nGaussian model for classiﬁcation in which the features j= 1,2,...,pare\\nassumed to be independent within each class k= 1,2,...,K. With ob-\\nservationsi= 1,2,...,NandCkequal to the set of indices of the Nk\\nobservations in class k, we observe xij∼N(µj+µjk,σ2\\nj) fori∈Ckwith∑K\\nk=1µjk= 0. Set ˆσ2\\nj=s2\\nj, the pooled within-class variance for feature j,\\nand consider the lasso-style minimization problem\\nmin\\n{µj,µjk}\\uf8f1\\n\\uf8f2\\n\\uf8f31\\n2p∑\\nj=1K∑\\nk=1∑\\ni∈Ck(xij−µj−µjk)2\\ns2\\nj+λ√\\nNkp∑\\nj=1K∑\\nk=1|µjk|\\nsj.\\uf8fc\\n\\uf8fd\\n\\uf8fe(18.55)\\nShow that the solution is equivalent to the nearest shrunken centroid es-\\ntimator (18.7), with s0set to zero, and m2\\nkequal to 1/Nkinstead of\\n1/Nk−1/Nas before.\\nEx. 18.3 Show that the ﬁtted coeﬃcients for the regularized multicla ss\\nlogistic regression problem (18.10) satisfy∑K\\nk=1ˆβkj= 0, j= 1,...,p.\\nWhat about the ˆβk0? Discuss issues with these constant parameters, and\\nhow they can be resolved.\\nEx. 18.4 Derive the computational formula (18.15) for ridge regress ion.\\n[Hint: Use the ﬁrst derivative of the penalized sum-of-squares cr iterion to\\nshow that if λ>0, thenˆβ=XTsfor somes∈IRN.]\\nEx. 18.5 Prove the theorem (18.16)–(18.17) in Section 18.3.5, by dec om-\\nposingβand the rows of Xinto their projections into the column space of\\nVand its complement in IRp.\\nEx. 18.6 Show how the theorem in Section 18.3.5 can be applied to regu-\\nlarized discriminant analysis [Equations 4.14 and (18.9)] .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 712}),\n",
       " Document(page_content='Exercises 695\\nEx. 18.7 Consider a linear regression problem where p≫N, and assume\\nthe rank of XisN. Let the SVD of X=UDVT=RVT, whereRis\\nN×Nnonsingular, and Visp×Nwith orthonormal columns.\\n(a) Show that there are inﬁnitely many least-squares soluti ons all with\\nzero residuals.\\n(b) Show that the ridge-regression estimate for βcan be written\\nˆβλ=V(RTR+λI)−1RTy (18.56)\\n(c) Show that when λ= 0, the solution ˆβ0=VD−1UTyhas residuals\\nall equal to zero, and is unique in that it has the smallest Euc lidean\\nnorm amongst all zero-residual solutions.\\nEx. 18.8 Data Piling . Exercise 4.2 shows that the two-class LDA solution\\ncan be obtained by a linear regression of a binary response ve ctorycon-\\nsisting of−1s and +1s. The prediction ˆβTxfor anyxis (up to a scale and\\nshift) the LDA score δ(x). Suppose now that p≫N.\\n(a) Consider the linear regression model f(x) =α+βTxﬁt to a binary\\nresponseY∈ {−1,+1}. Using Exercise 18.7, show that there are\\ninﬁnitely many directions deﬁned by ˆβin IRponto which the data\\nproject to exactly two points, one for each class. These are known as\\ndata piling directions (Ahn and Marron, 2005).\\n(b) Show that the distance between the projected points is 2 /||ˆβ||, and\\nhence these directions deﬁne separating hyperplanes with t hat mar-\\ngin.\\n(c) Argue that there is a single maximal data piling direction for which\\nthis distance is largest, and is deﬁned by ˆβ0=VD−1UTy=X−y,\\nwhereX=UDVTis the SVD of X.\\nEx. 18.9 Compare the data piling direction of Exercise 18.8 to the dir ection\\nof the optimal separating hyperplane (Section 4.5.2) quali tatively. Which\\nmakes the widest margin, and why? Use a small simulation to de monstrate\\nthe diﬀerence.\\nEx. 18.10 Whenp≫N, linear discriminant analysis (see Section 4.3) is\\ndegenerate because the within-class covariance matrix Wis singular. One\\nversion of regularized discriminant analysis (4.14) repla cesWby a ridged\\nversionW+λI, leading to a regularized discriminant function δλ(x) =\\nxT(W+λI)−1(¯x1−¯x−1). Show that δ0(x) = lim λ↓0δλ(x) corresponds to\\nthe maximal data piling direction deﬁned in Exercise 18.8.\\nEx. 18.11 Suppose you have a sample of Npairs (xi,yi), withyibinary\\nandxi∈IR1. Suppose also that the two classes are separable; e.g., for e ach', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 713}),\n",
       " Document(page_content='696 18. High-Dimensional Problems: p≫N\\npairi,i′withyi= 0 andyi′= 1,xi′−xi≥Cfor someC >0. You wish\\nto ﬁt a linear logistic regression model logitPr( Y= 1|X) =α+βXby\\nmaximum-likelihood. Show that ˆβis undeﬁned.\\nEx. 18.12 Suppose we wish to select the ridge parameter λby 10-fold cross-\\nvalidation in a p≫Nsituation (for any linear model). We wish to use the\\ncomputational shortcuts described in Section 18.3.5. Show that we need\\nonly to reduce the N×pmatrixXto theN×NmatrixRonce, and can\\nuse it in all the cross-validation runs.\\nEx. 18.13 Suppose our p>Npredictors are presented as an N×Ninner-\\nproduct matrix K=XXT, and we wish to ﬁt the equivalent of a linear\\nlogistic regression model in the original features with qua dratic regulariza-\\ntion. Our predictions are also to be made using inner product s; a newx0\\nis presented as k0=Xx0. LetK=UD2UTbe the eigen-decomposition of\\nK. Show that the predictions are given by ˆf0=kT\\n0ˆα, where\\n(a) ˆα=UD−1ˆβ, and\\n(b)ˆβis the ridged logistic regression estimate with input matri xR=\\nUD.\\nArgue that the same approach can be used for any appropriate k ernel\\nmatrixK.\\nEx. 18.14 Distance weighted 1-NN classiﬁcation . Consider the 1-nearest-\\nneighbor method (Section 13.3) in a two-class classiﬁcatio n problem. Let\\nd+(x0) be the shortest distance to a training observation in class +1, and\\nlikewised−(x0) the shortest distance for class −1. LetN−be the number\\nof samples in class −1,N+the number in class +1, and N=N−+N+.\\n(a) Show that\\nδ(x0) = logd−(x0)\\nd+(x0)(18.57)\\ncan be viewed as a nonparametric discriminant function corr espond-\\ning to 1-NN classiﬁcation. [ Hint: Show that ˆf+(x0) =1\\nN+d+(x0)can\\nbe viewed as a nonparametric estimate of the density in class +1 at\\nx0].\\n(b) How would you modify this function to introduce class pri or probabil-\\nitiesπ+andπ−diﬀerent from the sample-priors N+/NandN−/N?\\n(c) How would you generalize this approach for K-NN classiﬁc ation?\\nEx. 18.15 Kernel PCA. In Section 18.5.2 we show how to compute the\\nprincipal component variables Zfrom an uncentered inner-product matrix\\nK. We compute the eigen-decomposition ( I−M)K(I−M) =UD2UT,\\nwithM=11T/N, and then Z=UD. Suppose we have the inner-product', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 714}),\n",
       " Document(page_content='Exercises 697\\nvectork0, containing the Ninner-products between a new point x0and\\neach of the xiin our training set. Show that the (centered) projections of\\nx0onto the principal-component directions are given by\\nz0=D−1UT(I−M)[k0−K1/N]. (18.58)\\nEx. 18.16 Bonferroni method for multiple comparisons. Suppose we are in\\na multiple-testing scenario with null hypotheses H0j,j= 1,2,...,M, and\\ncorresponding p-valuespj,i= 1,2,...,M. LetAbe the event that at least\\none null hypothesis is falsely rejected, and let Ajbe the event that the\\njth null hypothesis is falsely rejected. Suppose that we use t he Bonferroni\\nmethod, rejecting the jth null hypothesis if pj<α/M.\\n(a) Show that Pr( A)≤α. [Hint: Pr(Aj∪Aj′) = Pr(Aj) + Pr(Aj′)−\\nPr(Aj∩Aj′)]\\n(b) If the hypotheses H0j,j= 1,2,...,M, are independent, then Pr( A) =\\n1−Pr(AC) = 1−∏M\\nj=1Pr(AC\\nj) = 1−(1−α/M)M. Use this to show\\nthat Pr(A)≈αin this case.\\nEx. 18.17 Equivalence between Benjamini–Hochberg and plug-in metho ds.\\n(a) In the notation of Algorithm 18.2, show that for rejectio n threshold\\np0=p(L), a proportion of at most p0of the permuted values tk\\nj\\nexceed|T|(L)where|T|(L)is theLth largest value among the |tj|.\\nHence show that the plug-in FDR estimate ˆFDR is less than or equal\\ntop0·M/L=α.\\n(b) Show that the cut-point |T|(L+1)produces a test with estimated FDR\\ngreater than α.\\nEx. 18.18 Use result (18.53) to show that\\npFDR =π0·{Type I error of Γ}\\nπ0·{Type I error of Γ}+π1{Power of Γ}(18.59)\\n(Storey, 2003).\\nEx. 18.19 Consider the data in Table 18.4 of Section (18.7), available from\\nthe book website.\\n(a) Using a symmetric two-sided rejection region based on th et-statistic,\\ncompute the plug-in estimate of the FDR for various values of the\\ncut-point.\\n(b) Carry out the BH procedure for various FDR levels αand show the\\nequivalence of your results, with those from part (a).', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 715}),\n",
       " Document(page_content='698 18. High-Dimensional Problems: p≫N\\n(c) Let (q.25,q.75) be the quartiles of the t-statistics from the permuted\\ndatasets.Let ˆ π0={#tj∈(q.25,q.75)}/(.5M),andset ˆπ0= min(ˆπ0,1).\\nMultiply the FDR estimates from (a) by ˆ π0and examine the results.\\n(d) Give a motivation for the estimate in part (c).\\n(Storey, 2003)\\nEx. 18.20 Proof of result (18.53). Write\\npFDR = E(\\nV\\nR|R>0)\\n(18.60)\\n=M∑\\nk=1E[\\nV\\nR|R=k]\\nPr(R=k|R>0) (18.61)\\nUse the fact that given R=k,Vis a binomial random variable, with k\\ntrials and probability of success Pr( H= 0|T∈Γ), to complete the proof.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 716}),\n",
       " Document(page_content='This is page 699\\nPrinter: Opaque this\\nReferences\\nAbu-Mostafa, Y. (1995). Hints, Neural Computation 7: 639–671.\\nAckley, D. H., Hinton, G. and Sejnowski, T. (1985). A learnin g algorithm\\nfor Boltzmann machines, Trends in Cognitive Sciences 9: 147–169.\\nAdam, B.-L., Qu, Y., Davis, J. W., Ward, M. D., Clements, M. A. ,\\nCazares, L. H., Semmes, O. J., Schellhammer, P. F., Yasui, Y. ,\\nFeng, Z. and Wright, G. (2003). Serum protein ﬁngerprinting cou-\\npled with a pattern-matching algorithm distinguishes pros tate cancer\\nfrom benign prostate hyperplasia and healthy mean, Cancer Research\\n63(10): 3609–3614.\\nAgrawal, R., Mannila, H., Srikant, R., Toivonen, H. and Verk amo, A. I.\\n(1995). Fast discovery of association rules, Advances in Knowledge\\nDiscovery and Data Mining , AAAI/MIT Press, Cambridge, MA.\\nAgresti, A. (1996). An Introduction to Categorical Data Analysis , Wiley,\\nNew York.\\nAgresti, A. (2002). Categorical Data Analysis (2nd Ed.) , Wiley, New York.\\nAhn,J.andMarron,J.(2005). Thedirectionofmaximaldatap ilinginhigh\\ndimensional space, Technical report , Statistics Department, University\\nof North Carolina, Chapel Hill.\\nAkaike, H. (1973). Information theory and an extension of th e maximum\\nlikelihood principle, Second International Symposium on Information\\nTheory, pp. 267–281.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 717}),\n",
       " Document(page_content='700 References\\nAllen, D. (1974). The relationship between variable select ion and data\\naugmentation and a method of prediction, Technometrics 16: 125–7.\\nAmbroise, C. and McLachlan, G. (2002). Selection bias in gen e extraction\\non the basis of microarray gene-expression data, Proceedings of the\\nNational Academy of Sciences 99: 6562–6566.\\nAmit, Y. and Geman, D. (1997). Shape quantization and recogn ition with\\nrandomized trees, Neural Computation 9: 1545–1588.\\nAnderson, J. and Rosenfeld, E. (eds) (1988). Neurocomputing: Foundations\\nof Research , MIT Press, Cambridge, MA.\\nAnderson, T. (2003). An Introduction to Multivariate Statistical Analysis,\\n3rd ed., Wiley, New York.\\nBach, F. and Jordan, M. (2002). Kernel independent componen t analysis,\\nJournal of Machine Learning Research 3: 1–48.\\nBair, E. and Tibshirani, R. (2004). Semi-supervised method s to predict\\npatient survival from gene expression data, PLOS Biology 2: 511–522.\\nBair, E., Hastie, T., Paul, D. and Tibshirani, R. (2006). Pre diction by\\nsupervised principal components, Journal of the American Statistical\\nAssociation 101: 119–137.\\nBakin, S. (1999). Adaptive regression and model selection i n data mining\\nproblems, Technical report , PhD. thesis, Australian National Univer-\\nsity, Canberra.\\nBanerjee, O., Ghaoui, L. E. and d’Aspremont, A. (2008). Mode l selection\\nthrough sparse maximum likelihood estimation for multivar iate gaus-\\nsianorbinarydata, Journal of Machine Learning Research 9:485–516.\\nBarron, A. (1993). Universal approximation bounds for supe rpositions of a\\nsigmoid function, IEEE Transactions on Information Theory 39: 930–\\n945.\\nBartlett, P. and Traskin, M. (2007). Adaboost is consistent ,in\\nB.Sch¨ olkopf,J.PlattandT.Hoﬀman(eds), Advances in Neural Infor-\\nmation Processing Systems 19 , MIT Press, Cambridge, MA, pp. 105–\\n112.\\nBecker, R., Cleveland, W. and Shyu, M. (1996). The visual des ign and con-\\ntrol of trellis display, Journal of Computational and Graphical Statis-\\ntics5: 123–155.\\nBell, A. and Sejnowski, T. (1995). An information-maximiza tion approach\\nto blind separation and blind deconvolution, Neural Computation\\n7: 1129–1159.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 718}),\n",
       " Document(page_content='References 701\\nBellman, R. E. (1961). Adaptive Control Processes , Princeton University\\nPress.\\nBenjamini, Y. and Hochberg, Y. (1995). Controlling the fals e discovery\\nrate: a practical and powerful approach to multiple testing ,Journal of\\nthe Royal Statistical Society Series B. 85: 289–300.\\nBenjamini, Y. and Yekutieli, Y. (2005). False discovery rat e controlling\\nconﬁdence intervals for selected parameters, Journal of the American\\nStatistical Association 100: 71–80.\\nBickel, P. and Levina, E. (2004). Some theory for Fisher’s li near discrim-\\ninant function,“Naive Bayes”, and some alternatives when t here are\\nmany more variables than observations, Bernoulli 10: 989–1010.\\nBickel, P. J., Ritov, Y. and Tsybakov, A. (2008). Simultaneo us analysis of\\nlasso and Dantzig selector, Annals of Statistics . to appear.\\nBishop, C. (1995). Neural Networks for Pattern Recognition , Clarendon\\nPress, Oxford.\\nBishop, C. (2006). Pattern Recognition and Machine Learning , Springer,\\nNew York.\\nBishop, Y., Fienberg, S. and Holland, P. (1975). Discrete Multivariate\\nAnalysis, MIT Press, Cambridge, MA.\\nBoyd, S. and Vandenberghe, L. (2004). Convex Optimization , Cambridge\\nUniversity Press.\\nBreiman, L. (1992). The little bootstrap and other methods f or dimension-\\nality selection in regression: X-ﬁxed prediction error, Journal of the\\nAmerican Statistical Association 87: 738–754.\\nBreiman, L. (1996a). Bagging predictors, Machine Learning 26: 123–140.\\nBreiman, L. (1996b). Stacked regressions, Machine Learning 24: 51–64.\\nBreiman,L.(1998). Arcingclassiﬁers(withdiscussion), Annals of Statistics\\n26: 801–849.\\nBreiman, L. (1999). Prediction games and arcing algorithms ,Neural Com-\\nputation 11(7): 1493–1517.\\nBreiman, L. (2001). Random forests, Machine Learning 45: 5–32.\\nBreiman, L. and Friedman, J. (1997). Predicting multivaria te responses\\nin multiple linear regression (with discussion), Journal of the Royal\\nStatistical Society Series B. 59: 3–37.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 719}),\n",
       " Document(page_content='702 References\\nBreiman, L. and Ihaka, R. (1984). Nonlinear discriminant an alysis via\\nscaling and ACE, Technical report , University of California, Berkeley.\\nBreiman, L. and Spector, P. (1992). Submodel selection and e valuation\\nin regression: the X-random case, International Statistical Review\\n60: 291–319.\\nBreiman, L., Friedman, J., Olshen, R. and Stone, C. (1984). Classiﬁcation\\nand Regression Trees , Wadsworth, New York.\\nBremaud, P. (1999). Markov Chains: Gibbs Fields, Monte Carlo Simula-\\ntion, and Queues , Springer, New York.\\nBrown, P., Spiegelman, C. and Denham, M. (1991). Chemometri cs and\\nspectral frequency selection, Transactions of the Royal Society of Lon-\\ndon Series A. 337: 311–322.\\nBruce, A. and Gao, H. (1996). Applied Wavelet Analysis with S-PLUS ,\\nSpringer, New York.\\nB¨ uhlmann, P. and Hothorn, T. (2007). Boosting algorithms: regulariza-\\ntion, prediction and model ﬁtting (with discussion), Statistical Science\\n22(4): 477–505.\\nBuja, A., Hastie, T. and Tibshirani, R. (1989). Linear smoot hers and\\nadditive models (with discussion), Annals of Statistics 17: 453–555.\\nBuja,A.,Swayne,D.,Littman,M.,Hofmann,H.andChen,L.(2 008). Data\\nvizualization with multidimensional scaling, Journal of Computational\\nand Graphical Statistics . to appear.\\nBunea, F., Tsybakov, A. and Wegkamp, M. (2007). Sparsity ora cle inequal-\\nities for the lasso, Electronic Journal of Statistics 1: 169–194.\\nBurges, C. (1998). A tutorial on support vector machines for pattern recog-\\nnition,Knowledge Discovery and Data Mining 2(2): 121–167.\\nButte, A., Tamayo, P., Slonim, D., Golub, T. and Kohane, I. (2 000).\\nDiscovering functional relationships between RNA express ion and\\nchemotherapeutic susceptibility using relevance network s,Proceedings\\nof the National Academy of Sciences pp. 12182–12186.\\nCandes, E. (2006). Compressive sampling, Proceedings of the Interna-\\ntional Congress of Mathematicians , European Mathematical Society,\\nMadrid, Spain.\\nCandes, E. and Tao, T. (2007). The Dantzig selector: Statist ical estimation\\nwhen p is much larger than n, Annals of Statistics 35(6): 2313–2351.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 720}),\n",
       " Document(page_content='References 703\\nChambers, J. and Hastie, T. (1991). Statistical Models in S ,\\nWadsworth/Brooks Cole, Paciﬁc Grove, CA.\\nChaudhuri, S., Drton, M. and Richardson, T. S. (2007). Estim ation of a\\ncovariance matrix with zeros, Biometrika 94(1): 1–18.\\nChen, L. and Buja, A. (2008). Local multidimensional scalin g for nonlinear\\ndimension reduction, graph drawing and proximity analysis ,Journal\\nof the American Statistical Association .\\nChen, S. S., Donoho, D. and Saunders, M. (1998). Atomic decom position\\nby basis pursuit, SIAM Journal on Scientiﬁc Computing 20(1): 33–61.\\nCherkassky, V. and Ma, Y. (2003). Comparison of model select ion for\\nregression, Neural computation 15(7): 1691–1714.\\nCherkassky, V. and Mulier, F. (2007). Learning from Data (2nd Edition) ,\\nWiley, New York.\\nChui, C. (1992). An Introduction to Wavelets , Academic Press, London.\\nCliﬀord, P. (1990). Markov random ﬁelds in statistics, inG. R. Grimmett\\nand D. J. A. Welsh (eds), Disorder in Physical Systems. A Volume in\\nHonour of John M. Hammersley , Clarendon Press, Oxford, pp. 19–32.\\nComon, P. (1994). Independent component analysis—a new conc ept?,Sig-\\nnal Processing 36: 287–314.\\nCook, D. and Swayne, D. (2007). Interactive and Dynamic Graphics for\\nData Analysis; with R and GGobi , Springer, New York. With con-\\ntributions from A. Buja, D. Temple Lang, H. Hofmann, H. Wickh am\\nand M. Lawrence.\\nCook, N. (2007). Use and misuse of the receiver operating cha racteristic\\ncurve in risk prediction, Circulation 116(6): 928–35.\\nCopas, J. B. (1983). Regression, prediction and shrinkage ( with discus-\\nsion),Journal of the Royal Statistical Society, Series B, Methodo logical\\n45: 311–354.\\nCover, T. and Hart, P. (1967). Nearest neighbor pattern clas siﬁcation,\\nIEEE Transactions on Information Theory IT-11: 21–27.\\nCover, T. and Thomas, J. (1991). Elements of Information Theory , Wiley,\\nNew York.\\nCox, D. and Hinkley, D. (1974). Theoretical Statistics , Chapman and Hall,\\nLondon.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 721}),\n",
       " Document(page_content='704 References\\nCox, D. and Wermuth, N. (1996). Multivariate Dependencies: Models,\\nAnalysis and Interpretation , Chapman and Hall, London.\\nCressie, N. (1993). Statistics for Spatial Data (Revised Edition) , Wiley-\\nInterscience, New York.\\nCsiszar, I. and Tusn´ ady, G. (1984). Information geometry a nd alternat-\\ning minimization procedures, Statistics & Decisions Supplement Issue\\n1: 205–237.\\nCutler, A. and Breiman, L. (1994). Archetypal analysis, Technometrics\\n36(4): 338–347.\\nDasarathy, B. (1991). Nearest Neighbor Pattern Classiﬁcation Techniques ,\\nIEEE Computer Society Press, Los Alamitos, CA.\\nDaubechies, I. (1992). Ten Lectures on Wavelets , Society for Industrial and\\nApplied Mathematics, Philadelphia, PA.\\nDaubechies, I., Defrise, M. and De Mol, C. (2004). An iterati ve threshold-\\ning algorithm for linear inverse problems with a sparsity co nstraint,\\nCommunications on Pure and Applied Mathematics 57: 1413–1457.\\nde Boor, C. (1978). A Practical Guide to Splines , Springer, New York.\\nDempster, A. (1972). Covariance selection, Biometrics 28: 157–175.\\nDempster, A., Laird, N. and Rubin, D. (1977). Maximum likeli hood from\\nincomplete data via the EM algorithm (with discussion), Journal of\\nthe Royal Statistical Society Series B 39: 1–38.\\nDevijver, P. and Kittler, J. (1982). Pattern Recognition: A Statistical Ap-\\nproach, Prentice-Hall, Englewood Cliﬀs, N.J.\\nDietterich, T. (2000a). Ensemble methods in machine learni ng,Lecture\\nNotes in Computer Science 1857: 1–15.\\nDietterich, T. (2000b). An experimental comparison of thre e methods for\\nconstructing ensembles of decision trees: bagging, boosti ng, and ran-\\ndomization, Machine Learning 40(2): 139–157.\\nDietterich, T. and Bakiri, G. (1995). Solving multiclass le arning problems\\nvia error-correcting output codes, Journal of Artiﬁcial Intelligence Re-\\nsearch2: 263–286.\\nDonath, W. E. and Hoﬀman, A. J. (1973). Lower bounds for the pa rtition-\\ning of graphs, IBM Journal of Research and Development pp. 420–425.\\nDonoho, D. (2006a). Compressed sensing, IEEE Transactions on Informa-\\ntion Theory 52(4): 1289–1306.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 722}),\n",
       " Document(page_content='References 705\\nDonoho, D.(2006b). Formostlarge underdeterminedsystems ofequations,\\ntheminimal ℓ1-normsolutionisthesparsestsolution, Communications\\non Pure and Applied Mathematics 59: 797–829.\\nDonoho, D. and Elad, M. (2003). Optimally sparse representa tion from\\novercomplete dictionaries via ℓ1-norm minimization, Proceedings of\\nthe National Academy of Sciences 100: 2197–2202.\\nDonoho, D. and Johnstone, I. (1994). Ideal spatial adaptati on by wavelet\\nshrinkage, Biometrika 81: 425–455.\\nDonoho, D. and Stodden, V. (2004). When does non-negative ma trix\\nfactorization give a correct decomposition into parts?, inS. Thrun,\\nL. Saul and B. Sch¨ olkopf (eds), Advances in Neural Information Pro-\\ncessing Systems 16 , MIT Press, Cambridge, MA.\\nDuan, N. and Li, K.-C. (1991). Slicing regression: a link-fr ee regression\\nmethod, Annals of Statistics 19: 505–530.\\nDuchamp, T. and Stuetzle, W. (1996). Extremal properties of principal\\ncurves in the plane, Annals of Statistics 24: 1511–1520.\\nDuda, R., Hart, P. and Stork, D. (2000). Pattern Classiﬁcation (2nd Edi-\\ntion), Wiley, New York.\\nDudoit, S., Fridlyand, J. and Speed, T. (2002a). Comparison of discrimi-\\nnation methods for the classiﬁcation of tumors using gene ex pression\\ndata,Journal of the American Statistical Association 97(457): 77–87.\\nDudoit, S., Yang, Y., Callow, M. and Speed, T. (2002b). Stati stical meth-\\nods for identifying diﬀerentially expressed genes in repli cated cDNA\\nmicroarray experiments, Statistica Sinica pp. 111–139.\\nEdwards, D. (2000). Introduction to Graphical Modelling, 2nd Edition ,\\nSpringer, New York.\\nEfron, B. (1975). The eﬃciency of logistic regression compa red to normal\\ndiscriminant analysis, Journal of the American Statistical Association\\n70: 892–898.\\nEfron, B. (1979). Bootstrap methods: another look at the jac kknife,Annals\\nof Statistics 7: 1–26.\\nEfron, B. (1983). Estimating the error rate of a prediction r ule: some\\nimprovements on cross-validation, Journal of the American Statistical\\nAssociation 78: 316–331.\\nEfron, B. (1986). How biased is the apparent error rate of a pr ediction\\nrule?,Journal of the American Statistical Association 81: 461–70.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 723}),\n",
       " Document(page_content='706 References\\nEfron, B. and Tibshirani, R. (1991). Statistical analysis i n the computer\\nage,Science253: 390–395.\\nEfron, B. and Tibshirani, R. (1993). An Introduction to the Bootstrap ,\\nChapman and Hall, London.\\nEfron, B. and Tibshirani, R. (1996). Using specially design ed exponential\\nfamilies for density estimation, Annals of Statistics 24(6): 2431–2461.\\nEfron, B. and Tibshirani, R. (1997). Improvements on cross- validation: the\\n632+ bootstrap: method, Journal of the American Statistical Associ-\\nation92: 548–560.\\nEfron,B.andTibshirani,R.(2002). Microarrays,empirica lBayesmethods,\\nand false discovery rates, Genetic Epidemiology 1: 70–86.\\nEfron, B., Hastie, T. and Tibshirani, R. (2007). Discussion of “Dantzig\\nselector” by Candes and Tao, Annals of Statistics 35(6): 2358–2364.\\nEfron, B., Hastie, T., Johnstone, I. and Tibshirani, R. (200 4). Least angle\\nregression (with discussion), Annals of Statistics 32(2): 407–499.\\nEfron, B., Tibshirani, R., Storey, J. and Tusher, V. (2001). Empirical\\nBayes analysis of a microarray experiment, Journal of the American\\nStatistical Association 96: 1151–1160.\\nEvgeniou, T., Pontil, M. and Poggio, T. (2000). Regularizat ion networks\\nand support vector machines, Advances in Computational Mathemat-\\nics13(1): 1–50.\\nFan, J. and Fan, Y. (2008). High dimensional classiﬁcation u sing features\\nannealed independence rules, Annals of Statistics . to appear.\\nFan, J. and Gijbels, I. (1996). Local Polynomial Modelling and Its Appli-\\ncations, Chapman and Hall, London.\\nFan, J. and Li, R. (2005). Variable selection via nonconcave penalized\\nlikelihoodanditsoracleproperties, Journal of the American Statistical\\nAssociation 96: 1348–1360.\\nFiedler, M. (1973). Algebraic connectivity of graphs, Czechoslovak Mathe-\\nmatics Journal 23(98): 298–305.\\nFienberg, S. (1977). The Analysis of Cross-Classiﬁed Categorical Data ,\\nMIT Press, Cambridge.\\nFisher, R. A. (1936). The use of multiple measurements in tax onomic\\nproblems, Eugen.7: 179–188.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 724}),\n",
       " Document(page_content='References 707\\nFisher, W. (1958). On grouping for maximum homogeniety, Journal of the\\nAmerican Statistical Association 53(284): 789–798.\\nFix, E. and Hodges, J. (1951). Discriminatory analysis—nonp arametric\\ndiscrimination: Consistency properties, Technical Report 21-49-004,4 ,\\nU.S. Air Force, School of Aviation Medicine, Randolph Field , TX.\\nFlury, B. (1990). Principal points, Biometrika 77: 33–41.\\nForgy, E. (1965). Cluster analysis of multivariate data: eﬃ ciency vs. inter-\\npretability of classiﬁcations, Biometrics 21: 768–769.\\nFrank, I. and Friedman, J. (1993). A statistical view of some chemometrics\\nregression tools (with discussion), Technometrics 35(2): 109–148.\\nFreund, Y. (1995). Boosting a weak learning algorithm by maj ority,Infor-\\nmation and Computation 121(2): 256–285.\\nFreund, Y. and Schapire, R. (1996a). Experiments with a new b oosting\\nalgorithm, Machine Learning: Proceedings of the Thirteenth Interna-\\ntional Conference , Morgan Kauﬀman, San Francisco, pp. 148–156.\\nFreund, Y. and Schapire, R. (1996b). Game theory, on-line pr ediction and\\nboosting, Proceedings of the Ninth Annual Conference on Computa-\\ntional Learning Theory , Desenzano del Garda, Italy, pp. 325–332.\\nFreund, Y. and Schapire, R. (1997). A decision-theoretic ge neralization of\\nonline learning and an application to boosting, Journal of Computer\\nand System Sciences 55: 119–139.\\nFriedman, J. (1987). Exploratory projection pursuit, Journal of the Amer-\\nican Statistical Association 82: 249–266.\\nFriedman, J. (1989). Regularized discriminant analysis, Journal of the\\nAmerican Statistical Association 84: 165–175.\\nFriedman, J. (1991). Multivariate adaptive regression spl ines (with discus-\\nsion),Annals of Statistics 19(1): 1–141.\\nFriedman, J. (1994a). Flexible metric nearest-neighbor cl assiﬁcation, Tech-\\nnical report , Stanford University.\\nFriedman, J. (1994b). An overview of predictive learning an d function\\napproximation, inV. Cherkassky, J. Friedman and H. Wechsler (eds),\\nFrom Statistics to Neural Networks , Vol. 136 of NATO ISI Series F ,\\nSpringer, New York.\\nFriedman, J. (1996). Another approach to polychotomous cla ssiﬁcation,\\nTechnical report , Stanford University.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 725}),\n",
       " Document(page_content='708 References\\nFriedman, J. (1997). On bias, variance, 0-1 loss and the curs e of dimen-\\nsionality, Journal of Data Mining and Knowledge Discovery 1: 55–77.\\nFriedman, J. (1999). Stochastic gradient boosting, Technical report , Stan-\\nford University.\\nFriedman, J. (2001). Greedy function approximation: A grad ient boosting\\nmachine, Annals of Statistics 29(5): 1189–1232.\\nFriedman, J. and Fisher, N. (1999). Bump hunting in high dime nsional\\ndata,Statistics and Computing 9: 123–143.\\nFriedman, J. and Hall, P. (2007). On bagging and nonlinear es timation,\\nJournal of Statistical Planning and Inference 137: 669–683.\\nFriedman, J. and Popescu, B. (2003). Importance sampled lea rning ensem-\\nbles,Technical report , Stanford University, Department of Statistics.\\nFriedman, J. and Popescu, B. (2008). Predictive learning vi a rule ensem-\\nbles,Annals of Applied Statistics, to appear .\\nFriedman, J. and Silverman, B. (1989). Flexible parsimonio us smoothing\\nand additive modelling (with discussion), Technometrics 31: 3–39.\\nFriedman, J. and Stuetzle, W. (1981). Projection pursuit re gression, Jour-\\nnal of the American Statistical Association 76: 817–823.\\nFriedman, J. and Tukey, J. (1974). A projection pursuit algo rithm for\\nexploratory data analysis, IEEE Transactions on Computers, Series\\nC23: 881–889.\\nFriedman, J., Baskett, F. and Shustek, L. (1975). An algorit hm for ﬁnding\\nnearest neighbors, IEEE Transactions on Computers 24: 1000–1006.\\nFriedman, J., Bentley, J. and Finkel, R. (1977). An algorthm for ﬁnd-\\ning best matches in logarithmic expected time, ACM Transactions on\\nMathematical Software 3: 209–226.\\nFriedman, J., Hastie, T. and Tibshirani, R. (2000). Additiv e logistic re-\\ngression: a statistical view of boosting (with discussion) ,Annals of\\nStatistics 28: 337–307.\\nFriedman, J., Hastie, T. and Tibshirani, R. (2008a). Respon se to “Mease\\nand Wyner: Evidence contrary to the statistical view of boos ting”,\\nJournal of Machine Learning Research 9: 175–180.\\nFriedman, J., Hastie, T. and Tibshirani, R. (2008b). Sparse inverse covari-\\nance estimation with the graphical lasso, Biostatistics 9: 432–441.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 726}),\n",
       " Document(page_content='References 709\\nFriedman,J.,Hastie,T.andTibshirani,R.(2010). Regular izationpathsfor\\ngeneralized linear models via coordinate descent, Journal of Statistical\\nSoftware 33(1): 1–22.\\nFriedman, J., Hastie, T., Hoeﬂing, H. and Tibshirani, R. (20 07). Pathwise\\ncoordinate optimization, Annals of Applied Statistics 2(1): 302–332.\\nFriedman, J., Hastie, T., Rosset, S., Tibshirani, R. and Zhu , J. (2004).\\nDiscussion of three boosting papers by Jiang, Lugosi and Vay atis, and\\nZhang,Annals of Statistics 32: 102–107.\\nFriedman, J., Stuetzle, W. and Schroeder, A. (1984). Projec tion pursuit\\ndensity estimation, Journal of the American Statistical Association\\n79: 599–608.\\nFu, W. (1998). Penalized regressions: the bridge vs. the las so,Journal of\\nComputational and Graphical Statistics 7(3): 397–416.\\nFurnival, G. and Wilson, R. (1974). Regression by leaps and b ounds,Tech-\\nnometrics 16: 499–511.\\nGelfand, A. and Smith, A. (1990). Sampling based approaches to calculat-\\ning marginal densities, Journal of the American Statistical Association\\n85: 398–409.\\nGelman, A., Carlin, J., Stern, H. and Rubin, D. (1995). Bayesian Data\\nAnalysis, CRC Press, Boca Raton, FL.\\nGeman, S. and Geman, D. (1984). Stochastic relaxation, Gibb s distribu-\\ntions and the Bayesian restoration of images, IEEE Transactions on\\nPattern Analysis and Machine Intelligence 6: 721–741.\\nGenkin, A., Lewis, D. and Madigan, D. (2007). Large-scale Ba yesian logis-\\ntic regression for text categorization, Technometrics 49(3): 291–304.\\nGenovese, C. and Wasserman, L. (2004). A stochastic process approach to\\nfalse discovery rates, Annals of Statistics 32(3): 1035–1061.\\nGersho, A. and Gray, R. (1992). Vector Quantization and Signal Compres-\\nsion, Kluwer Academic Publishers, Boston, MA.\\nGirosi, F., Jones, M. and Poggio, T. (1995). Regularization theory and\\nneural network architectures, Neural Computation 7: 219–269.\\nGolub, G. and Van Loan, C. (1983). Matrix Computations , Johns Hopkins\\nUniversity Press, Baltimore.\\nGolub, G., Heath, M. and Wahba, G. (1979). Generalized cross -validation\\nas a method for choosing a good ridge parameter, Technometrics\\n21: 215–224.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 727}),\n",
       " Document(page_content='710 References\\nGolub, T., Slonim, D., Tamayo, P., Huard, C., Gaasenbeek, M. , Mesirov,\\nJ., Coller, H., Loh, M., Downing, J., Caligiuri, M., Bloomﬁe ld, C. and\\nLander, E. (1999). Molecular classiﬁcation of cancer: Clas s discovery\\nand class prediction by gene expression monitoring, Science286: 531–\\n536.\\nGoodall, C. (1991). Procrustes methods in the statistical a nalysis of shape,\\nJournal of the Royal Statistical Society, Series B 53: 285–321.\\nGordon, A. (1999). Classiﬁcation (2nd edition) , Chapman and Hall/CRC\\nPress, London.\\nGreen, P. and Silverman, B. (1994). Nonparametric Regression and Gener-\\nalized Linear Models: A Roughness Penalty Approach , Chapman and\\nHall, London.\\nGreenacre, M. (1984). Theory and Applications of Correspondence Analy-\\nsis, Academic Press, New York.\\nGreenshtein, E. and Ritov, Y. (2004). Persistence in high-d imensional lin-\\nearpredictorselectionandthevirtueofoverparametrizat ion,Bernoulli\\n10: 971–988.\\nGuo, Y., Hastie, T. and Tibshirani, R. (2006). Regularized l inear discrim-\\ninant analysis and its application in microarrays, Biostatistics 8: 86–\\n100.\\nGuyon, I., Gunn, S., Nikravesh, M. and Zadeh, L. (eds) (2006) .Feature\\nExtraction, Foundations and Applications , Springer, New York.\\nGuyon,I.,Weston,J.,Barnhill,S.andVapnik,V.(2002). Ge neselectionfor\\ncancer classiﬁcation using support vector machines, Machine Learning\\n46: 389–422.\\nHall, P. (1992). The Bootstrap and Edgeworth Expansion , Springer, New\\nYork.\\nHammersley, J. M. and Cliﬀord, P. (1971). Markov ﬁeld on ﬁnit e graphs\\nand lattices, unpublished.\\nHand, D. (1981). Discrimination and Classiﬁcation , Wiley, Chichester.\\nHanley, J. and McNeil, B. (1982). The meaning and use of the ar ea under\\na receiver operating characteristic (roc) curve, Radiology 143: 29–36.\\nHart, P. (1968). The condensed nearest-neighbor rule, IEEE Transactions\\non Information Theory 14: 515–516.\\nHartigan, J. A. (1975). Clustering Algorithms , Wiley, New York.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 728}),\n",
       " Document(page_content='References 711\\nHartigan, J. A. and Wong, M. A. (1979). [(Algorithm AS 136] A k-means\\nclustering algorithm (AS R39: 81v30 p355-356), Applied Statistics\\n28: 100–108.\\nHastie, T. (1984). Principal Curves and Surfaces , PhD thesis, Stanford\\nUniversity.\\nHastie, T. and Herman, A. (1990). An analysis of gestational age, neona-\\ntal size and neonatal death using nonparametric logistic re gression,\\nJournal of Clinical Epidemiology 43: 1179–90.\\nHastie, T. and Simard, P. (1998). Models and metrics for hand written digit\\nrecognition, Statistical Science 13: 54–65.\\nHastie, T. and Stuetzle, W. (1989). Principal curves, Journal of the Amer-\\nican Statistical Association 84(406): 502–516.\\nHastie, T. and Tibshirani, R. (1987). Nonparametric logist ic and propor-\\ntional odds regression, Applied Statistics 36: 260–276.\\nHastie, T. and Tibshirani, R. (1990). Generalized Additive Models , Chap-\\nman and Hall, London.\\nHastie, T. and Tibshirani, R. (1996a). Discriminant adapti ve nearest-\\nneighbor classiﬁcation, IEEE Pattern Recognition and Machine In-\\ntelligence 18: 607–616.\\nHastie, T. and Tibshirani, R. (1996b). Discriminant analys is by Gaussian\\nmixtures, Journal of the Royal Statistical Society Series B. 58: 155–\\n176.\\nHastie, T. and Tibshirani, R. (1998). Classiﬁcation by pair wise coupling,\\nAnnals of Statistics 26(2): 451–471.\\nHastie, T. and Tibshirani, R. (2003). Independent componen ts analysis\\nthrough product density estimation, inS. T. S. Becker and K. Ober-\\nmayer (eds), Advances in Neural Information Processing Systems 15 ,\\nMIT Press, Cambridge, MA, pp. 649–656.\\nHastie, T. and Tibshirani, R. (2004). Eﬃcient quadratic reg ularization for\\nexpression arrays, Biostatistics 5(3): 329–340.\\nHastie, T. and Zhu, J. (2006). Discussion of “Support vector machines\\nwith applications” by Javier Moguerza and Alberto Munoz, Statistical\\nScience21(3): 352–357.\\nHastie, T., Botha, J. and Schnitzler, C. (1989). Regression with an ordered\\ncategorical response, Statistics in Medicine 43: 884–889.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 729}),\n",
       " Document(page_content='712 References\\nHastie, T., Buja, A. and Tibshirani, R. (1995). Penalized di scriminant\\nanalysis, Annals of Statistics 23: 73–102.\\nHastie, T., Kishon, E., Clark, M. and Fan, J. (1992). A model f or\\nsignature veriﬁcation, Technical report , AT&T Bell Laboratories.\\nhttp://www-stat.stanford.edu/ ∼hastie/Papers/signature.pdf .\\nHastie, T., Rosset, S., Tibshirani, R. and Zhu, J. (2004). Th e entire reg-\\nularization path for the support vector machine, Journal of Machine\\nLearning Research 5: 1391–1415.\\nHastie, T., Taylor, J., Tibshirani, R. and Walther, G. (2007 ). Forward\\nstagewise regression and the monotone lasso, Electronic Journal of\\nStatistics 1: 1–29.\\nHastie, T., Tibshirani, R. and Buja, A. (1994). Flexible dis criminant analy-\\nsis by optimal scoring, Journal of the American Statistical Association\\n89: 1255–1270.\\nHastie, T., Tibshirani, R. and Buja, A. (2000). Flexible dis criminant and\\nmixture models, inJ. Kay and M. Titterington (eds), Statistics and\\nArtiﬁcial Neural Networks , Oxford University Press.\\nHastie, T., Tibshirani, R. and Friedman, J. (2003). A note on “Compari-\\nson of model selection for regression” by Cherkassky and Ma, Neural\\ncomputation 15(7): 1477–1480.\\nHathaway, R. J. (1986). Another interpretation of the EM alg orithm for\\nmixture distributions, Statistics & Probability Letters 4: 53–56.\\nHebb, D. (1949). The Organization of Behavior , Wiley, New York.\\nHertz, J., Krogh, A. and Palmer, R. (1991). Introduction to the Theory of\\nNeural Computation , Addison Wesley, Redwood City, CA.\\nHinton, G. (1989). Connectionist learning procedures, Artiﬁcial Intelli-\\ngence40: 185–234.\\nHinton, G. (2002). Training products of experts by minimizi ng contrastive\\ndivergence, Neural Computation 14: 1771–1800.\\nHinton, G., Osindero, S. and Teh, Y.-W. (2006). A fast learni ng algorithm\\nfor deep belief nets, Neural Computation 18: 1527–1554.\\nHo,T.K.(1995). Randomdecisionforests, inM.KavavaughandP.Storms\\n(eds),Proc. Third International Conference on Document Analysis\\nand Recognition , Vol. 1, IEEE Computer Society Press, New York,\\npp. 278–282.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 730}),\n",
       " Document(page_content='References 713\\nHoeﬂing, H. and Tibshirani, R. (2008). Estimation of sparse Markov net-\\nworks using modiﬁed logistic regression and the lasso, subm itted.\\nHoerl, A. E. and Kennard, R. (1970). Ridge regression: biase d estimation\\nfor nonorthogonal problems, Technometrics 12: 55–67.\\nHothorn, T. and B¨ uhlmann, P. (2006). Model-based boosting in high di-\\nmensions, Bioinformatics 22(22): 2828–2829.\\nHuber, P. (1964). Robust estimation of a location parameter ,Annals of\\nMathematical Statistics 53: 73–101.\\nHuber, P. (1985). Projection pursuit, Annals of Statistics 13: 435–475.\\nHunter, D. and Lange, K. (2004). A tutorial on MM algorithms, The\\nAmerican Statistician 58(1): 30–37.\\nHyv¨ arinen, A. and Oja, E. (2000). Independent component an alysis: algo-\\nrithms and applications, Neural Networks 13: 411–430.\\nHyv¨ arinen, A., Karhunen, J. and Oja, E. (2001). Independent Component\\nAnalysis, Wiley, New York.\\nIzenman, A. (1975). Reduced-rank regression for the multiv ariate linear\\nmodel,Journal of Multivariate Analysis 5: 248–264.\\nJacobs, R., Jordan, M., Nowlan, S. and Hinton, G. (1991). Ada ptive mix-\\ntures of local experts, Neural computation 3: 79–87.\\nJain, A. and Dubes, R. (1988). Algorithms for Clustering Data , Prentice-\\nHall, Englewood Cliﬀs, N.J.\\nJames, G. and Hastie, T. (1998). The error coding method and P ICTs,\\nJournal of Computational and Graphical Statistics 7(3): 377–387.\\nJancey, R. (1966). Multidimensional group analysis, Australian Journal of\\nBotany14: 127–130.\\nJensen, F. V., Lauritzen, S. and Olesen, K. G. (1990). Bayesi an updating\\nin recursive graphical models by local computation, Computational\\nStatistics Quarterly 4: 269–282.\\nJiang, W. (2004). Process consistency for Adaboost, Annals of Statistics\\n32(1): 13–29.\\nJirou´ sek, R. and Pˇ reuˇ cil, S. (1995). On the eﬀective impl ementation of the\\niterative proportional ﬁtting procedure, Computational Statistics and\\nData Analysis 19: 177–189.\\nJohnson, N. (2008). A study of the NIPS feature selection cha llenge, Sub-\\nmitted.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 731}),\n",
       " Document(page_content='714 References\\nJoliﬀe, I. T., Trendaﬁlov, N. T. and Uddin, M. (2003). A modiﬁ ed principal\\ncomponent technique based on the lasso, Journal of Computational\\nand Graphical Statistics 12: 531–547.\\nJones,L.(1992). AsimplelemmaongreedyapproximationinH ilbertspace\\nand convergence rates for projection pursuit regression an d neural\\nnetwork training, Annals of Statistics 20: 608–613.\\nJordan, M. (2004). Graphical models, Statistical Science (Special Issue on\\nBayesian Statistics) 19: 140–155.\\nJordan, M. and Jacobs, R. (1994). Hierachical mixtures of ex perts and the\\nEM algorithm, Neural Computation 6: 181–214.\\nKalbﬂeisch, J. and Prentice, R. (1980). The Statistical Analysis of Failure\\nTime Data , Wiley, New York.\\nKaufman, L. and Rousseeuw, P. (1990). Finding Groups in Data: An In-\\ntroduction to Cluster Analysis , Wiley, New York.\\nKearns, M. and Vazirani, U. (1994). An Introduction to Computational\\nLearning Theory , MIT Press, Cambridge, MA.\\nKittler, J., Hatef, M., Duin, R. and Matas, J. (1998). On comb ining classi-\\nﬁers,IEEE Transaction on Pattern Analysis and Machine Intellige nce\\n20(3): 226–239.\\nKleinberg,E.M.(1990). Stochasticdiscrimination, Annals of Mathematical\\nArtiﬁcial Intelligence 1: 207–239.\\nKleinberg, E. M. (1996). An overtraining-resistant stocha stic modeling\\nmethod for pattern recognition, Annals of Statistics 24: 2319–2349.\\nKnight, K. and Fu, W. (2000). Asymptotics for lasso-type est imators,\\nAnnals of Statistics 28(5): 1356–1378.\\nKoh, K., Kim, S.-J. and Boyd, S. (2007). An interior-point me thod\\nfor large-scale L1-regularized logistic regression, Journal of Machine\\nLearning Research 8: 1519–1555.\\nKohavi, R. (1995). A study of cross-validation and bootstra p for accu-\\nracy estimation and model selection, International Joint Conference\\non Artiﬁcial Intelligence (IJCAI) , Morgan Kaufmann, pp. 1137–1143.\\nKohonen, T. (1989). Self-Organization and Associative Memory (3rd edi-\\ntion), Springer, Berlin.\\nKohonen, T. (1990). The self-organizing map, Proceedings of the IEEE\\n78: 1464–1479.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 732}),\n",
       " Document(page_content='References 715\\nKohonen, T., Kaski, S., Lagus, K., Saloj¨ arvi, J., Paatero, A. and Saarela,\\nA. (2000). Self-organization of a massive document collect ion,IEEE\\nTransactions on Neural Networks 11(3): 574–585. Special Issue on\\nNeural Networks for Data Mining and Knowledge Discovery.\\nKoller, D. and Friedman, N. (2007). Structured Probabilistic Models , Stan-\\nford Bookstore Custom Publishing. (Unpublished Draft).\\nKressel, U. (1999). Pairwise classiﬁcation and support vec tor machines,\\ninB. Sch¨ olkopf, C. Burges and A. Smola (eds), Advances in Ker-\\nnel Methods - Support Vector Learning , MIT Press, Cambridge, MA.,\\npp. 255–268.\\nLambert, D. (1992). Zero-inﬂated Poisson regression, with an application\\nto defects in manufacturing, Technometrics 34(1): 1–14.\\nLange, K. (2004). Optimization , Springer, New York.\\nLauritzen, S. (1996). Graphical Models , Oxford University Press.\\nLauritzen, S. and Spiegelhalter, D. (1988). Local computat ions with proba-\\nbilitiesongraphicalstructuresandtheirapplicationtoe xpertsystems,\\nJ. Royal Statistical Society B. 50: 157–224.\\nLawson, C. and Hansen, R. (1974). Solving Least Squares Problems ,\\nPrentice-Hall, Englewood Cliﬀs, NJ.\\nLe Cun, Y. (1989). Generalization and network design strate gies,Techni-\\ncal Report CRG-TR-89-4 , Department of Computer Science, Univ. of\\nToronto.\\nLe Cun, Y., Boser, B., Denker, J., Henderson, D., Howard, R., Hubbard,\\nW. and Jackel, L. (1990). Handwritten digit recognition wit h a back-\\npropogation network, inD. Touretzky (ed.), Advances in Neural In-\\nformation Processing Systems , Vol. 2, Morgan Kaufman, Denver, CO,\\npp. 386–404.\\nLe Cun, Y., Bottou, L., Bengio, Y. and Haﬀner, P. (1998). Grad ient-based\\nlearning applied to document recognition, Proceedings of the IEEE\\n86(11): 2278–2324.\\nLeathwick, J., Elith, J., Francis, M., Hastie, T. and Taylor , P. (2006). Vari-\\nation in demersal ﬁsh species richness in the oceans surroun ding new\\nzealand: an analysis using boosted regression trees, Marine Ecology\\nProgress Series 77: 802–813.\\nLeathwick, J., Rowe, D., Richardson, J., Elith, J. and Hasti e, T. (2005).\\nUsing multivariate adaptive regression splines to predict the distribu-\\ntions of New Zealand’s freshwater diadromous ﬁsh, Freshwater Biology\\n50: 2034–2051.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 733}),\n",
       " Document(page_content='716 References\\nLeblanc, M. and Tibshirani, R. (1996). Combining estimates in regres-\\nsion and classiﬁcation, Journal of the American Statistical Association\\n91: 1641–1650.\\nLeCun, Y., Bottou, L., Bengio, Y. and Haﬀner, P. (1998). Grad ient-based\\nlearning applied to document recognition, Proceedings of the IEEE\\n86(11): 2278–2324.\\nLee,D.andSeung,H.(1999). Learningthepartsofobjectsby non-negative\\nmatrix factorization, Nature401: 788.\\nLee, D. and Seung, H. (2001). Algorithms for non-negative ma trix factor-\\nization,Advances in Neural Information Processing Systems, (NIPS\\n2001), Vol. 13, Morgan Kaufman, Denver., pp. 556–562.\\nLee, M.-L. (2004). Analysis of Microarray Gene Expression Data , Kluwer\\nAcademic Publishers.\\nLee, S.-I., Ganapathi, V. and Koller, D. (2007). Eﬃcient str ucture learning\\nof markov networks using l1-regularization, inB. Sch¨ olkopf, J. Platt\\nand T. Hoﬀman (eds), Advances in Neural Information Processing\\nSystems 19 , MIT Press, Cambridge, MA, pp. 817–824.\\nLeslie, C., Eskin, E., Cohen, A., Weston, J. and Noble, W. S. ( 2004). Mis-\\nmatch string kernels for discriminative protein classiﬁca tion,Bioinfor-\\nmatics20(4): 467–476.\\nLevina, E. (2002). Statistical issues in texture analysis , PhD thesis, De-\\npartment. of Statistics, University of California, Berkel ey.\\nLin, H., McCulloch, C., Turnbull, B., Slate, E. and Clark, L. (2000). A\\nlatent class mixed model for analyzing biomarker trajector ies in lon-\\ngitudinal data with irregularly scheduled observations, Statistics in\\nMedicine 19: 1303–1318.\\nLin, Y. and Zhang, H. (2006). Component selection and smooth ing in\\nsmoothing spline analysis of variance models, Annals of Statistics\\n34: 2272–2297.\\nLittle, R. and Rubin, D. (2002). Statistical Analysis with Missing Data\\n(2nd Edition) , Wiley, New York.\\nLloyd,S.(1957). LeastsquaresquantizationinPCM., Technical report ,Bell\\nLaboratories. Published in 1982 in IEEE Transactions on Inf ormation\\nTheory28128-137.\\nLoader, C. (1999). Local Regression and Likelihood , Springer, New York.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 734}),\n",
       " Document(page_content='References 717\\nLoh, W. and Vanichsetakul, N. (1988). Tree structured class iﬁcation via\\ngeneralized discriminant analysis, Journal of the American Statistical\\nAssociation 83: 715–728.\\nLugosi, G. and Vayatis, N. (2004). On the bayes-risk consist ency of regu-\\nlarized boosting methods, Annals of Statistics 32(1): 30–55.\\nMacnaughton Smith, P., Williams, W., Dale, M. and Mockett, L . (1965).\\nDissimilarity analysis: a new technique of hierarchical su bdivision, Na-\\nture202: 1034–1035.\\nMacKay, D. (1992). A practical Bayesian framework for backp ropagation\\nneural networks, Neural Computation 4: 448–472.\\nMacQueen, J. (1967). Some methods for classiﬁcation and ana lysis of mul-\\ntivariate observations, Proceedings of the Fifth Berkeley Symposium\\non Mathematical Statistics and Probability, eds. L.M. LeCa m and J.\\nNeyman, University of California Press, pp. 281–297.\\nMadigan, D. and Raftery, A. (1994). Model selection and acco unting for\\nmodel uncertainty using Occam’s window, Journal of the American\\nStatistical Association 89: 1535–46.\\nMardia,K.,Kent,J.andBibby,J.(1979). Multivariate Analysis ,Academic\\nPress.\\nMason, L., Baxter, J., Bartlett, P. and Frean, M. (2000). Boo sting algo-\\nrithms as gradient descent, 12: 512–518.\\nMassart, D., Plastria, F. and Kaufman, L. (1983). Non-hiera rchical clus-\\ntering with MASLOC, The Journal of the Pattern Recognition Society\\n16: 507–516.\\nMcCullagh, P. and Nelder, J. (1989). Generalized Linear Models , Chapman\\nand Hall, London.\\nMcCulloch, W. and Pitts, W. (1943). A logical calculus of the ideas immi-\\nnent in nervous activity, Bulletin of Mathematical Biophysics 5: 115–\\n133. Reprinted in Anderson and Rosenfeld (1988), pp 96-104.\\nMcLachlan, G. (1992). Discriminant Analysis and Statistical Pattern\\nRecognition , Wiley, New York.\\nMease, D. and Wyner, A. (2008). Evidence contrary to the stat istical view\\nof boosting (with discussion), Journal of Machine Learning Research\\n9: 131–156.\\nMeinshausen, N. (2007). Relaxed lasso, Computational Statistics and Data\\nAnalysis 52(1): 374–393.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 735}),\n",
       " Document(page_content='718 References\\nMeinshausen, N. and B¨ uhlmann, P. (2006). High-dimensiona l graphs and\\nvariable selection with the lasso, Annals of Statistics 34: 1436–1462.\\nMeir, R. and R¨ atsch, G. (2003). An introduction to boosting and leverag-\\ning,inS. Mendelson and A. Smola (eds), Lecture notes in Computer\\nScience,AdvancedLecturesinMachineLearning,Springer,NewYork .\\nMichie, D., Spiegelhalter, D. and Taylor, C. (eds) (1994). Machine Learn-\\ning, Neural and Statistical Classiﬁcation , Ellis Horwood Series in Ar-\\ntiﬁcial Intelligence, Ellis Horwood.\\nMorgan,J.N.andSonquist,J.A.(1963). Problemsintheanal ysisofsurvey\\ndata, and a proposal, Journal of the American Statistical Association\\n58: 415–434.\\nMurray, W., Gill, P. and Wright, M. (1981). Practical Optimization , Aca-\\ndemic Press.\\nMyles, J. and Hand, D. (1990). The multiclass metric problem in nearest\\nneighbor classiﬁcation, Pattern Recognition 23: 1291–1297.\\nNadler, B. and Coifman, R. R. (2005). An exact asymptotic for mula for the\\nerror in CLS and in PLS: The importance of dimensional reduct ion in\\nmultivariate calibration, Journal of Chemometrics 102: 107–118.\\nNeal, R. (1996). Bayesian Learning for Neural Networks , Springer, New\\nYork.\\nNeal, R. and Hinton, G. (1998). A view of the EM algorithm that justiﬁes\\nincremental, sparse, and other variants; in Learning in Gra phical Mod-\\nels, M. Jordan (ed.) , Dordrecht: Kluwer Academic Publishers, Boston,\\nMA., pp. 355–368.\\nNeal, R. and Zhang, J. (2006). High dimensional classiﬁcati on with\\nbayesian neural networks and dirichlet diﬀusion trees, inI. Guyon,\\nS. Gunn, M. Nikravesh and L. Zadeh (eds), Feature Extraction, Foun-\\ndations and Applications , Springer, New York, pp. 265–296.\\nOnton, J. and Makeig, S. (2006). Information-based modelin g of event-\\nrelated brain dynamics, inNeuper and Klimesch (eds), Progress in\\nBrain Research , Vol. 159, Elsevier, pp. 99–120.\\nOsborne, M., Presnell, B. and Turlach, B. (2000a). A new appr oach to\\nvariableselectioninleastsquaresproblems, IMA Journal of Numerical\\nAnalysis 20: 389–404.\\nOsborne, M., Presnell, B. and Turlach, B. (2000b). On the las so and its\\ndual,Journal of Computational and Graphical Statistics 9: 319–337.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 736}),\n",
       " Document(page_content='References 719\\nPace, R. K. and Barry, R. (1997). Sparse spatial autoregress ions,Statistics\\nand Probability Letters 33: 291–297.\\nPage, L., Brin, S., Motwani, R. and Winograd, T. (1998). The\\npagerank citation ranking: bringing order to the web, Tech-\\nnical report , Stanford Digital Library Technologies Project.\\nhttp://citeseer.ist.psu.edu/page98pagerank.html .\\nPark,M.Y.andHastie,T.(2007). l1-regularizationpathalgorithmforgen-\\neralized linear models, Journal of the Royal Statistical Society Series\\nB69: 659–677.\\nParker,D.(1985). Learninglogic, Technical Report TR-87 ,CambridgeMA:\\nMIT Center for Research in Computational Economics and Mana ge-\\nment Science.\\nParmigiani, G., Garett, E. S., Irizarry, R. A. and Zeger, S. L . (eds) (2003).\\nThe Analysis of Gene Expression Data , Springer, New York.\\nPaul, D., Bair, E., Hastie, T. and Tibshirani, R. (2008). “Pr e-conditioning”\\nfor feature selection and regression in high-dimensional p roblems, An-\\nnals of Statistics 36(4): 1595–1618.\\nPearl, J. (1986). On evidential reasoning in a hierarchy of h ypotheses,\\nArtiﬁcial Intelligence 28: 9–15.\\nPearl, J. (1988). Probabilistic reasoning in intelligent systems: networks of\\nplausible inference , Morgan Kaufmann, San Francisco, CA.\\nPearl, J. (2000). Causality: Models, Reasoning and Inference , Cambridge\\nUniversity Press.\\nPeterson and Anderson, J. R. (1987). A mean ﬁeld theory learn ing algo-\\nrithm for neural networks, Complex Systems 1: 995–1019.\\nPetricoin, E. F., Ardekani, A. M., Hitt, B. A., Levine, P. J., Fusaro, V.,\\nSteinberg, S. M., Mills, G. B., Simone, C., Fishman, D. A., Ko hn,\\nE. and Liotta, L. A. (2002). Use of proteomic patterns in seru m to\\nidentify ovarian cancer, Lancet359: 572–577.\\nPlatt, J. (1999). Fast Training of Support Vector Machines using Sequen-\\ntial Minimal Optimization; in Advances in Kernel Methods—S upport\\nVector Learning, B. Sch¨ olkopf and C. J. C. Burges and A. J. Sm ola\\n(eds), MIT Press, Cambridge, MA., pp. 185–208.\\nQuinlan, R. (1993). C4.5: Programs for Machine Learning , Morgan Kauf-\\nmann, San Mateo.\\nQuinlan, R. (2004). C5.0, www.rulequest.com .', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 737}),\n",
       " Document(page_content='720 References\\nRamaswamy, S., Tamayo, P., Rifkin, R., Mukherjee, S., Yeang , C., Angelo,\\nM.,Ladd,C.,Reich,M.,Latulippe,E.,Mesirov,J.,Poggio, T.,Gerald,\\nW., Loda, M., Lander, E. and Golub, T. (2001). Multiclass can cer\\ndiagnosis using tumor gene expression signature, PNAS98: 15149–\\n15154.\\nRamsay, J. and Silverman, B. (1997). Functional Data Analysis , Springer,\\nNew York.\\nRao, C. R. (1973). Linear Statistical Inference and Its Applications , Wiley,\\nNew York.\\nR¨ atsch, G. and Warmuth, M. (2002). Maximizing the margin wi th boost-\\ning,Proceedings of the 15th Annual Conference on Computational\\nLearning Theory , pp. 334–350.\\nRavikumar, P., Liu, H., Laﬀerty, J. and Wasserman, L. (2008) . Spam:\\nSparse additive models, inJ. Platt, D. Koller, Y. Singer and S. Roweis\\n(eds),Advances in Neural Information Processing Systems 20 , MIT\\nPress, Cambridge, MA, pp. 1201–1208.\\nRidgeway, G. (1999). The state of boosting, Computing Science and Statis-\\ntics31: 172–181.\\nRieger, K., Hong, W., Tusher, V., Tang, J., Tibshirani, R. an d Chu, G.\\n(2004). Toxicity from radiation therapy associated with ab normal\\ntranscriptional responses to DNA damage, Proceedings of the National\\nAcademy of Sciences 101: 6634–6640.\\nRipley,B.D.(1996). Pattern Recognition and Neural Networks ,Cambridge\\nUniversity Press.\\nRissanen, J. (1983). A universal prior for integers and esti mation by mini-\\nmum description length, Annals of Statistics 11: 416–431.\\nRobbins, H. and Munro, S. (1951). A stochastic approximatio n method,\\nAnnals of Mathematical Statistics 22: 400–407.\\nRoosen, C. and Hastie, T. (1994). Automatic smoothing splin e projection\\npursuit,Journal of Computational and Graphical Statistics 3:235–248.\\nRosenblatt, F. (1958). The perceptron: a probabilistic mod el for infor-\\nmation storage and organization in the brain, Psychological Review\\n65: 386–408.\\nRosenblatt, F. (1962). Principles of Neurodynamics: Perceptrons and the\\nTheory of Brain Mechanisms , Spartan, Washington, D.C.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 738}),\n",
       " Document(page_content='References 721\\nRosenwald, A., Wright, G., Chan, W. C., Connors, J. M., Campo , E.,\\nFisher, R. I., Gascoyne, R. D., Muller-Hermelink, H. K., Sme land,\\nE. B. and Staudt, L. M. (2002). The use of molecular proﬁling t o\\npredict survival after chemotherapy for diﬀuse large b-cel l lymphoma,\\nThe New England Journal of Medicine 346: 1937–1947.\\nRosset, S. and Zhu, J. (2007). Piecewise linear regularized solution paths,\\nAnnals of Statistics 35(3): 1012–1030.\\nRosset,S.,Zhu,J.andHastie,T.(2004a). Boostingasaregu larizedpathto\\na maximum margin classiﬁer, Journal of Machine Learning Research\\n5: 941–973.\\nRosset, S., Zhu, J. and Hastie, T. (2004b). Margin maximizin g loss func-\\ntions,inS. Thrun, L. Saul and B. Sch¨ olkopf (eds), Advances in Neural\\nInformation Processing Systems 16 , MIT Press, Cambridge, MA.\\nRousseauw, J., du Plessis, J., Benade, A., Jordaan, P., Kotz e, J., Jooste, P.\\nand Ferreira, J. (1983). Coronary risk factor screening in t hree rural\\ncommunities, South African Medical Journal 64: 430–436.\\nRoweis, S. T. and Saul, L. K. (2000). Locally linear embeddin g,Science\\n290: 2323–2326.\\nRumelhart, D., Hinton, G. and Williams, R. (1986). Learning internal rep-\\nresentations by error propagation, inD. Rumelhart and J. McClelland\\n(eds),Parallel Distributed Processing: Explorations in the Micr ostruc-\\nture of Cognition , The MIT Press, Cambridge, MA., pp. 318–362.\\nSachs, K., Perez, O., Pe’er, D., Lauﬀenburger, D. and Nolan, G. (2005).\\nCausalprotein-signalingnetworksderivedfrommultipara metersingle-\\ncell data, Science308: 523–529.\\nSchapire, R. (1990). The strength of weak learnability, Machine Learning\\n5(2): 197–227.\\nSchapire, R. (2002). The boosting approach to machine learn ing: an\\noverview, inD. Denison, M. Hansen, C. Holmes, B. Mallick and B. Yu\\n(eds),MSRI workshop on Nonlinear Estimation and Classiﬁcation ,\\nSpringer, New York.\\nSchapire, R. and Singer, Y. (1999). Improved boosting algor ithms using\\nconﬁdence-rated predictions, Machine Learning 37(3): 297–336.\\nSchapire, R., Freund, Y., Bartlett, P. and Lee, W. (1998). Bo osting the\\nmargin: a new explanation for the eﬀectiveness of voting met hods,\\nAnnals of Statistics 26(5): 1651–1686.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 739}),\n",
       " Document(page_content='722 References\\nSch¨ olkopf,B.,Smola,A.andM¨ uller,K.-R.(1999). Kernel principalcompo-\\nnentanalysis, inB.Sch¨ olkopf,C.BurgesandA.Smola(eds), Advances\\nin Kernel Methods—Support Vector Learning , MIT Press, Cambridge,\\nMA, USA, pp. 327–352.\\nSchwarz, G. (1978). Estimating the dimension of a model, Annals of Statis-\\ntics6(2): 461–464.\\nScott, D. (1992). Multivariate Density Estimation: Theory, Practice, and\\nVisualization , Wiley, New York.\\nSeber, G. (1984). Multivariate Observations , Wiley, New York.\\nSegal, M. (2004). Machine learning benchmarks and random fo rest regres-\\nsion,Technical report , eScholarship Repository, University of Califor-\\nnia.http://repositories.edlib.org/cbmb/bench rfregn.\\nShao, J. (1996). Bootstrap model selection, Journal of the American Sta-\\ntistical Association 91: 655–665.\\nShenoy, P. and Shafer, G. (1988). An axiomatic framework for Bayesian\\nand belief-function propagation, AAAI Workshop on Uncertainty in\\nAI, North-Holland, pp. 307–314.\\nShort,R.andFukunaga,K.(1981). Theoptimaldistancemeas urefornear-\\nest neighbor classiﬁcation, IEEE Transactions on Information Theory\\n27: 622–627.\\nSilverman, B. (1986). Density Estimation for Statistics and Data Analysis ,\\nChapman and Hall, London.\\nSilvey, S. (1975). Statistical Inference , Chapman and Hall, London.\\nSimard, P., Cun, Y. L. and Denker, J. (1993). Eﬃcient pattern recognition\\nusing a new transformation distance, Advances in Neural Information\\nProcessing Systems , Morgan Kaufman, San Mateo, CA, pp. 50–58.\\nSimon, R. M., Korn, E. L., McShane, L. M., Radmacher, M. D., Wr ight,\\nG. and Zhao, Y. (2004). Design and Analysis of DNA Microarray\\nInvestigations , Springer, New York.\\nSj¨ ostrand,K.,Rostrup,E.,Ryberg,C.,Larsen,R.,Studho lme,C.,Baezner,\\nH., Ferro, J., Fazekas, F., Pantoni, L., Inzitari, D. and Wal demar,\\nG. (2007). Sparse decomposition and modeling of anatomical shape\\nvariation, IEEE Transactions on Medical Imaging 26(12): 1625–1635.\\nSpeed, T. and Kiiveri, H. T. (1986). Gaussian Markov distrib utions over\\nﬁnite graphs, Annals of Statistics 14: 138–150.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 740}),\n",
       " Document(page_content='References 723\\nSpeed, T. (ed.) (2003). Statistical Analysis of Gene Expression Microarray\\nData, Chapman and Hall, London.\\nSpiegelhalter, D., Best, N., Gilks, W. and Inskip, H. (1996) . Hepatitis\\nB: a case study in MCMC methods, inW. Gilks, S. Richardson and\\nD. Spegelhalter (eds), Markov Chain Monte Carlo in Practice , Inter-\\ndisciplinary Statistics, Chapman and Hall, London, pp. 21– 43.\\nSpielman, D. A. and Teng, S.-H. (1996). Spectral partitioni ng works: Pla-\\nnar graphs and ﬁnite element meshes, IEEE Symposium on Founda-\\ntions of Computer Science , pp. 96–105.\\nStamey, T., Kabalin, J., McNeal, J., Johnstone, I., Freiha, F., Redwine, E.\\nand Yang, N. (1989). Prostate speciﬁc antigen in the diagnos is and\\ntreatment of adenocarcinoma of the prostate II radical pros tatectomy\\ntreated patients, Journal of Urology 16: 1076–1083.\\nStone, C., Hansen, M., Kooperberg, C. and Truong, Y. (1997). Polynomial\\nsplinesandtheirtensorproducts(withdiscussion), Annals of Statistics\\n25(4): 1371–1470.\\nStone, M. (1974). Cross-validatory choice and assessment o f statistical\\npredictions, Journal of the Royal Statistical Society Series B 36: 111–\\n147.\\nStone, M. (1977). An asymptotic equivalence of choice of mod el by cross-\\nvalidation and Akaike’s criterion, Journal of the Royal Statistical So-\\nciety Series B. 39: 44–7.\\nStone, M. and Brooks, R. J. (1990). Continuum regression: cr oss-validated\\nsequentially constructed prediction embracing ordinary l east squares,\\npartial least squares and principal components regression (Corr: V54\\np906-907), Journal of the Royal Statistical Society, Series B 52: 237–\\n269.\\nStorey, J. (2002). A direct approach to false discovery rate s,Journal of the\\nRoyal Statistical Society B. 64(3): 479–498.\\nStorey, J. (2003). The positive false discovery rate: A Baye sian interpreta-\\ntion and the q-value, Annals of Statistics 31: 2013–2025.\\nStorey,J.andTibshirani,R.(2003). Statisticalsigniﬁca nceforgenomewide\\nstudies,Proceedings of the National Academy of Sciences 100-: 9440–\\n9445.\\nStorey, J., Taylor, J. and Siegmund, D. (2004). Strong contr ol, conservative\\npoint estimation, and simultaneous conservative consiste ncy of false\\ndiscovery rates: A uniﬁed approach., Journal of the Royal Statistical\\nSociety, Series B 66: 187–205.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 741}),\n",
       " Document(page_content='724 References\\nSurowiecki, J. (2004). The Wisdom of Crowds: Why the Many are Smarter\\nthan the Few and How Collective Wisdom Shapes Business, Eco-\\nnomics, Societies and Nations. , Little, Brown.\\nSwayne, D., Cook, D. and Buja, A. (1991). Xgobi: Interactive dynamic\\ngraphics in the X window system with a link to S, ASA Proceedings\\nof Section on Statistical Graphics , pp. 1–8.\\nTanner, M. and Wong, W. (1987). The calculation of posterior distribu-\\ntionsbydataaugmentation(withdiscussion), Journal of the American\\nStatistical Association 82: 528–550.\\nTarpey, T. and Flury, B. (1996). Self-consistency: A fundam ental concept\\nin statistics, Statistical Science 11: 229–243.\\nTenenbaum, J. B., de Silva, V. and Langford, J. C. (2000). A gl obal\\ngeometric framework for nonlinear dimensionality reducti on,Science\\n290: 2319–2323.\\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso,\\nJournal of the Royal Statistical Society, Series B 58: 267–288.\\nTibshirani, R. and Hastie, T. (2007). Margin trees for high- dimensional\\nclassiﬁcation, Journal of Machine Learning Research 8: 637–652.\\nTibshirani, R. and Knight, K. (1999). Model search and infer ence by boot-\\nstrap “bumping, Journal of Computational and Graphical Statistics\\n8: 671–686.\\nTibshirani, R. and Wang, P. (2007). Spatial smoothing and ho t spot de-\\ntection for CGH data using the fused lasso, Biostatistics 9: 18–29.\\nTibshirani, R., Hastie, T., Narasimhan, B. and Chu, G. (2001 a). Diagnosis\\nof multiple cancer types by shrunken centroids of gene expre ssion,\\nProceedings of the National Academy of Sciences 99: 6567–6572.\\nTibshirani, R., Hastie, T., Narasimhan, B. and Chu, G. (2003 ). Class\\nprediction by nearest shrunken centroids, with applicatio ns to DNA\\nmicroarrays, Statistical Science 18(1): 104–117.\\nTibshirani, R., Saunders, M., Rosset, S., Zhu, J. and Knight , K. (2005).\\nSparsity and smoothness via the fused lasso, Journal of the Royal\\nStatistical Society, Series B 67: 91–108.\\nTibshirani, R., Walther, G. and Hastie, T. (2001b). Estimat ing the number\\nof clusters in a dataset via the gap statistic, Journal of the Royal\\nStatistical Society, Series B. 32(2): 411–423.\\nTropp, J. (2004). Greed is good: algorithmic results for spa rse approxima-\\ntion,IEEE Transactions on Information Theory 50: 2231– 2242.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 742}),\n",
       " Document(page_content='References 725\\nTropp, J. (2006). Just relax: convex programming methods fo r identify-\\ning sparse signals in noise, IEEE Transactions on Information Theory\\n52: 1030–1051.\\nValiant, L. G. (1984). A theory of the learnable, Communications of the\\nACM27: 1134–1142.\\nvan der Merwe, A. and Zidek, J. (1980). Multivariate regress ion analysis\\nand canonical variates, The Canadian Journal of Statistics 8: 27–39.\\nVapnik, V. (1996). The Nature of Statistical Learning Theory , Springer,\\nNew York.\\nVapnik, V. (1998). Statistical Learning Theory , Wiley, New York.\\nVidakovic, B. (1999). Statistical Modeling by Wavelets , Wiley, New York.\\nvon Luxburg, U. (2007). A tutorial on spectral clustering, Statistics and\\nComputing 17(4): 395–416.\\nWahba, G. (1980). Spline bases, regularization, and genera lized cross-\\nvalidation for solving approximation problems with large q uantities\\nof noisy data, Proceedings of the International Conference on Approx-\\nimation theory in Honour of George Lorenz , Academic Press, Austin,\\nTexas, pp. 905–912.\\nWahba, G. (1990). Spline Models for Observational Data , SIAM, Philadel-\\nphia.\\nWahba, G., Lin, Y. and Zhang, H. (2000). GACV for support vect or ma-\\nchines,inA. Smola, P. Bartlett, B. Sch¨ olkopf and D. Schuurmans\\n(eds),Advances in Large Margin Classiﬁers , MIT Press, Cambridge,\\nMA., pp. 297–311.\\nWainwright, M. (2006). Sharp thresholds for noisy and high- dimensional\\nrecovery of sparsity using ℓ1-constrained quadratic programming,\\nTechnical report , Department of Statistics, University of California,\\nBerkeley.\\nWainwright, M. J., Ravikumar, P. and Laﬀerty, J. D. (2007). H igh-\\ndimensional graphical model selection using ℓ1-regularized logistic re-\\ngression, inB. Sch¨ olkopf, J. Platt and T. Hoﬀman (eds), Advances\\nin Neural Information Processing Systems 19 , MIT Press, Cambridge,\\nMA, pp. 1465–1472.\\nWasserman, L. (2004). All of Statistics: a Concise Course in Statistical\\nInference , Springer, New York.\\nWeisberg, S. (1980). Applied Linear Regression , Wiley, New York.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 743}),\n",
       " Document(page_content='726 References\\nWerbos, P. (1974). Beyond Regression , PhD thesis, Harvard University.\\nWeston, J. and Watkins, C. (1999). Multiclass support vecto r machines, in\\nM.Verleysen(ed.), Proceedings of ESANN99 ,D.FactoPress,Brussels.\\nWhittaker, J. (1990). Graphical Models in Applied Multivariate Statistics ,\\nWiley, Chichester.\\nWickerhauser, M. (1994). Adapted Wavelet Analysis from Theory to Soft-\\nware, A.K. Peters Ltd, Natick, MA.\\nWidrow, B. and Hoﬀ, M. (1960). Adaptive switching circuits, IRE\\nWESCON Convention record , Vol. 4. pp 96-104; Reprinted in An-\\ndersen and Rosenfeld (1988).\\nWold, H. (1975). Soft modelling by latent variables: the non linear iterative\\npartial least squares (NIPALS) approach, Perspectives in Probability\\nand Statistics, In Honor of M. S. Bartlett , pp. 117–144.\\nWolpert, D. (1992). Stacked generalization, Neural Networks 5: 241–259.\\nWu, T. and Lange, K. (2007). The MM alternative to EM, unpubli shed.\\nWu, T. and Lange, K. (2008). Coordinate descent procedures f or lasso\\npenalized regression, Annals of Applied Statistics 2(1): 224–244.\\nYee, T. and Wild, C. (1996). Vector generalized additive mod els,Journal\\nof the Royal Statistical Society, Series B. 58: 481–493.\\nYuan, M. and Lin, Y. (2007). Model selection and estimation i n regression\\nwith grouped variables, Journal of the Royal Statistical Society, Series\\nB68(1): 49–67.\\nZhang, P. (1993). Model selection via multifold cross-vali dation,Annals of\\nStatistics 21: 299–311.\\nZhang, T. and Yu, B. (2005). Boosting with early stopping: co nvergence\\nand consistency, Annals of Statistics 33: 1538–1579.\\nZhao, P. and Yu, B. (2006). On model selection consistency of lasso,Jour-\\nnal of Machine Learning Research 7: 2541–2563.\\nZhao, P., Rocha, G. and Yu, B. (2008). The composite absolute penalties\\nfor grouped and hierarchichal variable selection, Annals of Statistics .\\n(to appear).\\nZhu, J. and Hastie, T. (2004). Classiﬁcation of gene microar rays by penal-\\nized logistic regression, Biostatistics 5(2): 427–443.\\nZhu, J., Zou, H., Rosset, S. and Hastie, T. (2005). Multiclas s adaboost,\\nUnpublished.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 744}),\n",
       " Document(page_content='References 727\\nZou, H. (2006). The adaptive lasso and its oracle properties ,Journal of\\nthe American Statistical Association 101: 1418–1429.\\nZou, H. and Hastie, T. (2005). Regularization and variable s election via\\nthe elastic net, Journal of the Royal Statistical Society Series B.\\n67(2): 301–320.\\nZou, H., Hastie, T. and Tibshirani, R. (2006). Sparse princi pal com-\\nponent analysis, Journal of Computational and Graphical Statistics\\n15(2): 265–28.\\nZou, H., Hastie, T. and Tibshirani, R. (2007). On the degrees of freedom\\nof the lasso, Annals of Statistics 35(5): 2173–2192.', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 745}),\n",
       " Document(page_content='728 References', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 746}),\n",
       " Document(page_content='This is page 729\\nPrinter: Opaque this\\nAuthor Index\\nAbu-Mostafa, Y. 95, 474\\nAckley, D. H. 645\\nAdam, B.-L. 664\\nAgrawal, R. 489–491, 578\\nAgresti, A. 385, 638, 640\\nAhn, J. 695\\nAkaike, H. 257\\nAllen, D. 257\\nAmbroise, C. 247\\nAmit, Y. 602\\nAnderson, J. R. 641\\nAnderson, T. 645\\nAngelo, M. 654, 658\\nArdekani, A. M. 664\\nBach, F. 569\\nBaezner, H. 551\\nBair, E. 676, 679–683, 693\\nBakin, S. 90\\nBakiri, G. 605, 606\\nBanerjee, O. 636\\nBarnhill, S. 658\\nBarron, A. 415\\nBarry, R. 371\\nBartlett, P. 384, 615Baskett, F. 480\\nBaxter, J. 384\\nBecker, R. 369\\nBell, A. 578\\nBellman, R. E. 22\\nBenade, A. 122\\nBengio, Y. 404, 407, 408, 414, 644\\nBenjamini, Y. 687, 689, 693\\nBentley, J. 480\\nBest, N. 292\\nBibby, J. 94, 135, 441, 539, 559,\\n578, 630, 679\\nBickel, P. 652\\nBickel, P. J. 89\\nBishop, C. 38, 233, 414, 623, 645\\nBishop, Y. 629, 638\\nBloomﬁeld, C. 663\\nBoser, B. 404, 414\\nBotha, J. 334\\nBottou, L. 404, 407, 408, 414, 644\\nBoyd, S. 125, 632\\nBreiman,L.85,243,251,257,292,\\n308, 310, 334, 339, 367,\\n384, 451, 453, 455, 554,\\n587, 602', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 747}),\n",
       " Document(page_content='730 Author Index\\nBremaud, P. 577\\nBrin, S. 577\\nBrooks, R. J. 81\\nBrown, P. 679\\nBruce, A. 181\\nB¨ uhlmann, P. 87, 361, 384\\nBuja, A. 110, 297, 441, 446, 451,\\n455, 565, 574, 576, 578\\nBunea, F. 91\\nBurges, C. 455\\nButte, A. 631\\nCaligiuri, M. 663\\nCallow, M. 686, 693\\nCampo, E. 674\\nCandes, E. 86, 89, 613\\nCarlin, J. 292\\nCazares, L. H. 664\\nChambers, J. 334\\nChan, W. C. 674\\nChaudhuri, S. 631, 633\\nChen, L. 574, 576, 578\\nChen, S. S. 68, 94\\nCherkassky, V. 38, 239, 257\\nChu, G. 684, 693\\nChui, C. 181\\nClark, L. 331\\nClark, M. 539\\nClements, M. A. 664\\nCleveland, W. 369\\nCliﬀord, P. 629\\nCohen, A. 668, 669\\nCoifman, R. R. 679\\nColler, H. 663\\nComon, P. 578\\nConnors, J. M. 674\\nCook, D. 565, 578\\nCook, N. 317\\nCopas, J. B. 94, 610\\nCover, T. 257, 465, 481\\nCox, D. 292, 645\\nCressie, N. 171\\nCsiszar, I. 292\\nCun, Y. L. 407, 471, 481\\nCutler, A. 554Dale, M. 526\\nDasarathy, B. 480, 481\\nd’Aspremont, A. 636\\nDaubechies, I. 92, 181\\nDavis, J. W. 664\\nde Boor, C. 181\\nDe Mol, C. 92\\nde Silva, V. 573\\nDefrise, M. 92\\nDempster, A. 292, 449, 633\\nDenham, M. 679\\nDenker, J. 404, 407, 414, 471, 481\\nDevijver, P. 480\\nDietterich, T. 286, 602, 605, 606,\\n623\\nDonath, W. E. 578\\nDonoho, D. 68, 86, 91, 94, 179,\\n181, 554, 613\\nDowning, J. 663\\nDrton, M. 631, 633\\ndu Plessis, J. 122\\nDuan, N. 480\\nDubes, R. 508, 522\\nDuchamp, T. 541\\nDuda, R. 38, 135\\nDudoit, S. 686, 693\\nDuin, R. 624\\nEdwards, D. 645\\nEfron, B. 73, 86, 90, 94, 97, 98,\\n128, 231, 254, 257, 292,\\n334, 568, 609, 692, 693\\nElad, M. 613\\nElith, J. 375, 376, 378\\nEskin, E. 668, 669\\nEvgeniou, T. 168, 181, 455\\nFan, J. 92, 216, 539, 654\\nFan, Y. 654\\nFazekas, F. 551\\nFeng, Z. 664\\nFerreira, J. 122\\nFerro, J. 551\\nFiedler, M. 578\\nFienberg, S. 585, 629, 638', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 748}),\n",
       " Document(page_content='Author Index 731\\nFinkel, R. 480\\nFisher, N. 334\\nFisher, R. A. 136, 455\\nFisher, R. I. 674\\nFisher, W. 310\\nFishman, D. A. 664\\nFix, E. 481\\nFlury, B. 578\\nForgy, E. 578\\nFrancis, M. 375, 376, 378\\nFrank, I. 81, 82, 94\\nFrean, M. 384\\nFreiha, F. 3, 49\\nFreund, Y. 337, 383, 384, 615\\nFridlyand, J. 693\\nFriedman,J.38,81,82,85,92–94,\\n111, 121, 126, 251, 257,\\n258, 308, 310, 334, 339,\\n345, 365, 367, 384, 391,\\n414, 437, 451, 453, 475,\\n480, 565, 578, 602, 611,\\n617–621, 623, 636, 657,\\n661, 667\\nFriedman, N. 629, 630, 645\\nFu, W. 91, 92\\nFukunaga, K. 475\\nFurnival, G. 57\\nFusaro, V. 664\\nGaasenbeek, M. 663\\nGanapathi, V. 642\\nGao, H. 181\\nGascoyne, R. D. 674\\nGelfand, A. 292\\nGelman, A. 292\\nGeman, D. 292, 602\\nGeman, S. 292\\nGenkin, A. 661\\nGenovese, C. 693\\nGerald, W. 654, 658\\nGersho, A. 514, 515, 526, 578\\nGhaoui, L. E. 636\\nGijbels, I. 216\\nGilks, W. 292\\nGill, P. 96, 421Girosi, F. 168, 174, 181, 415\\nGolub, G. 257, 335, 535\\nGolub, T. 631, 654, 658, 663\\nGoodall, C. 578\\nGordon, A. 578\\nGray, R. 514, 515, 526, 578\\nGreen, P. 181, 183, 334\\nGreenacre, M. 455\\nGreenshtein, E. 91\\nGuo, Y. 657\\nGuyon, I. 658\\nHaﬀner,P.404,407,408,414,644\\nHall, P. 292, 602, 619\\nHammersley, J. M. 629\\nHand, D. 135, 475\\nHanley, J. 317\\nHansen, M. 328\\nHansen, R. 93\\nHart, P. 38, 135, 465, 480, 481\\nHartigan, J. A. 510, 578\\nHastie, T. 72, 73, 78, 86, 88, 90,\\n92–94, 97, 98, 110, 121,\\n122, 126, 137, 174, 216,\\n257, 297, 299, 304, 334,\\n339, 345, 348, 349, 375,\\n376, 378, 384, 385, 414,\\n428, 431, 434, 437, 441,\\n446, 451, 455, 475, 478,\\n480, 481, 519, 539, 550,\\n565, 568, 578, 606, 609–\\n611, 614, 615, 636, 657,\\n658, 660–662, 664, 667,\\n676, 679–683, 693\\nHatef, M. 624\\nHathaway, R. J. 292\\nHeath, M. 257\\nHebb, D. 414\\nHenderson, D. 404, 414\\nHerman, A. 334\\nHertz, J. 414\\nHinkley, D. 292\\nHinton,G.292,334,408,414,644,\\n645\\nHitt, B. A. 664', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 749}),\n",
       " Document(page_content='732 Author Index\\nHo, T. K. 602\\nHochberg, Y. 687, 689, 693\\nHodges, J. 481\\nHoeﬂing, H. 92, 93, 642, 667\\nHoerl, A. E. 64, 94\\nHoﬀ, M. 396, 414\\nHoﬀman, A. J. 578\\nHofmann, H. 578\\nHolland, P. 629, 638\\nHong, W. 684\\nHothorn, T. 87, 361, 384\\nHoward, R. 404, 414\\nHuard, C. 663\\nHubbard, W. 404, 414\\nHuber, P. 349, 414, 435, 565, 578\\nHunter, D. 294\\nHyv¨ arinen, A. 560, 562, 578, 583\\nIhaka, R. 455\\nInskip, H. 292\\nInzitari, D. 551\\nIzenman, A. 84\\nJackel, L. 404, 414\\nJacobs, R. 334\\nJain, A. 508, 522\\nJames, G. 606\\nJancey, R. 578\\nJensen, F. V. 629\\nJiang, W. 384\\nJirou´ sek, R. 640\\nJohnson, N. 412\\nJohnstone, I. 3, 49, 73, 86, 94, 97,\\n98, 179, 181, 609, 613\\nJoliﬀe, I. T. 550\\nJones, L. 415\\nJones, M. 168, 174, 181, 415\\nJooste, P. 122\\nJordaan, P. 122\\nJordan, M. 334, 569, 645\\nKabalin, J. 3, 49\\nKalbﬂeisch, J. 674, 693\\nKarhunen, J. 583\\nKaski, S. 531, 532, 578Kaufman, L. 517, 526, 578\\nKearns, M. 380\\nKennard, R. 64, 94\\nKent, J. 94, 135, 441, 539, 559,\\n578, 630, 679\\nKiiveri, H. T. 632\\nKim, S.-J. 125\\nKishon, E. 539\\nKittler, J. 480, 624\\nKleinberg, E. M. 602\\nKnight, K. 91, 292, 666, 693\\nKoh, K. 125\\nKohane, I. 631\\nKohavi, R. 243, 257\\nKohn, E. 664\\nKohonen, T. 462, 481, 531, 532,\\n578\\nKoller, D. 629, 630, 642, 645\\nKooperberg, C. 328\\nKorn, E. L. 693\\nKotze, J. 122\\nKressel, U. 437\\nKrogh, A. 414\\nLadd, C. 654, 658\\nLaﬀerty, J. 90, 304\\nLaﬀerty, J. D. 642\\nLagus, K. 531, 532, 578\\nLaird, N. 292, 449\\nLambert, D. 376\\nLander, E. 654, 658, 663\\nLange, K. 92, 294, 583, 584\\nLangford, J. C. 573\\nLarsen, R. 551\\nLatulippe, E. 654, 658\\nLauﬀenburger, D. 625\\nLauritzen, S. 629, 632, 645\\nLawson, C. 93\\nLe Cun, Y. 404, 406–408, 414\\nLeathwick, J. 375, 376, 378\\nLeblanc, M. 292\\nLeCun, Y. 644\\nLee, D. 552, 553\\nLee, M.-L. 693\\nLee, S.-I. 642', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 750}),\n",
       " Document(page_content='Author Index 733\\nLee, W. 384, 615\\nLeslie, C. 668, 669\\nLevina, E. 652, 693\\nLevine, P. J. 664\\nLewis, D. 661\\nLi, K.-C. 480\\nLi, R. 92\\nLin, H. 331\\nLin, Y. 90, 304, 428, 455\\nLiotta, L. A. 664\\nLittle, R. 332, 647\\nLittman, M. 578\\nLiu, H. 90, 304\\nLloyd, S. 481, 578\\nLoader, C. 209, 216\\nLoda, M. 654, 658\\nLoh, M. 663\\nLoh, W. 310\\nLugosi, G. 384\\nMa, Y. 257\\nMacnaughton Smith, P. 526\\nMacKay, D. 623\\nMacQueen, J. 481, 578\\nMadigan, D. 257, 292, 661\\nMakeig, S. 564, 565\\nMannila, H. 489–491, 578\\nMardia, K. 94, 135, 441, 539, 559,\\n578, 630, 679\\nMarron, J. 695\\nMason, L. 384\\nMassart, D. 517\\nMatas, J. 624\\nMcCullagh, P. 638, 640\\nMcCulloch, C. 331\\nMcCulloch, W. 414\\nMcLachlan, G. 135, 247\\nMcNeal, J. 3, 49\\nMcNeil, B. 317\\nMcShane, L. M. 693\\nMease, D. 384, 603\\nMeinshausen, N. 91, 635, 642\\nMeir, R. 384\\nMesirov, J. 654, 658, 663\\nMills, G. B. 664Mockett, L. 526\\nMorgan, J. N. 334\\nMotwani, R. 577\\nMukherjee, S. 654, 658\\nMulier, F. 38, 239\\nMuller-Hermelink, H. K. 674\\nM¨ uller, K.-R. 547, 548\\nMunro, S. 397\\nMurray, W. 96, 421\\nMyles, J. 475\\nNadler, B. 679\\nNarasimhan, B. 693\\nNeal, R. 268, 292, 409–412, 414,\\n605, 623\\nNelder, J. 638, 640\\nNoble, W. S. 668, 669\\nNolan, G. 625\\nNowlan, S. 334\\nOja, E. 560, 562, 578, 583\\nOlesen, K. G. 629\\nOlshen,R.251,308,310,334,367,\\n451, 453\\nOnton, J. 564, 565\\nOsborne, M. 76, 94\\nOsindero, S. 644\\nPaatero, A. 531, 532, 578\\nPace, R. K. 371\\nPage, L. 577\\nPalmer, R. 414\\nPantoni, L. 551\\nPark, M. Y. 94, 126, 661\\nParker, D. 414\\nPaul, D. 676, 679–683, 693\\nPearl, J. 629, 645\\nPe’er, D. 625\\nPerez, O. 625\\nPeterson 641\\nPetricoin, E. F. 664\\nPitts, W. 414\\nPlastria, F. 517\\nPlatt, J. 453\\nPoggio,T.168,174,181,415,455,\\n654, 658', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 751}),\n",
       " Document(page_content='734 Author Index\\nPontil, M. 168, 181, 455\\nPopescu, B. 617–619, 621, 623\\nPrentice, R. 674, 693\\nPresnell, B. 76, 94\\nPˇ reuˇ cil, S. 640\\nQu, Y. 664\\nQuinlan, R. 312, 334, 624\\nRadmacher, M. D. 693\\nRaftery, A. 257, 292\\nRamaswamy, S. 654, 658\\nRamsay, J. 181, 578\\nRao, C. R. 455\\nR¨ atsch, G. 384, 615\\nRavikumar, P. 90, 304, 642\\nRedwine, E. 3, 49\\nReich, M. 654, 658\\nRichardson, J. 375\\nRichardson, T. S. 631, 633\\nRidgeway, G. 361\\nRieger, K. 684\\nRifkin, R. 654, 658\\nRipley, B. D. 38, 131, 135, 136,\\n234, 308, 310, 400, 414,\\n415, 455, 468, 480, 481,\\n641, 645\\nRissanen, J. 257\\nRitov, Y. 89, 91\\nRobbins, H. 397\\nRocha, G. 90\\nRoosen, C. 414\\nRosenblatt, F. 102, 129, 414\\nRosenwald, A. 674\\nRosset, S. 89, 98, 348, 349, 385,\\n426, 428, 434, 610, 611,\\n615, 657, 661, 664, 666,\\n693\\nRostrup, E. 551\\nRousseauw, J. 122\\nRousseeuw, P. 517, 526, 578\\nRowe, D. 375\\nRoweis, S. T. 573\\nRubin, D. 292, 332, 449, 647\\nRumelhart, D. 414Ryberg, C. 551\\nSaarela, A. 531, 532, 578\\nSachs, K. 625\\nSaloj¨ arvi, J. 531, 532, 578\\nSaul, L. K. 573\\nSaunders, M. 68, 94, 666, 693\\nSchapire, R. 337, 380, 383, 384,\\n615\\nSchellhammer, P. F. 664\\nSchnitzler, C. 334\\nSch¨ olkopf, B. 547, 548\\nSchroeder, A. 391\\nSchwarz, G. 233, 257\\nScott, D. 216\\nSeber, G. 94\\nSegal, M. 596\\nSejnowski, T. 578, 645\\nSemmes, O. J. 664\\nSeung, H. 552, 553\\nShafer, G. 629\\nShao, J. 257\\nShenoy, P. 629\\nShort, R. 475\\nShustek, L. 480\\nShyu, M. 369\\nSiegmund, D. 689\\nSilverman, B. 181, 183, 216, 334,\\n486, 567, 578\\nSilvey, S. 292\\nSimard, P. 407, 471, 480, 481\\nSimon, R. M. 693\\nSimone, C. 664\\nSinger, Y. 384\\nSj¨ ostrand, K. 551\\nSlate, E. 331\\nSlonim, D. 631, 663\\nSmeland, E. B. 674\\nSmith, A. 292\\nSmola, A. 547, 548\\nSonquist, J. A. 334\\nSpector, P. 243, 257\\nSpeed, T. 632, 686, 693\\nSpiegelhalter, D. 292, 629\\nSpiegelman, C. 679', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 752}),\n",
       " Document(page_content='Author Index 735\\nSpielman, D. A. 578\\nSrikant, R. 489–491, 578\\nStamey, T. 3, 49\\nStaudt, L. M. 674\\nSteinberg, S. M. 664\\nStern, H. 292\\nStodden, V. 554\\nStone, C. 251, 308, 310, 328, 334,\\n367, 451, 453\\nStone, M. 81, 257\\nStorey, J. 689, 692, 693, 697, 698\\nStork, D. 38, 135\\nStudholme, C. 551\\nStuetzle, W. 391, 414, 541, 578\\nSurowiecki, J. 286\\nSwayne, D. 565, 578\\nTamayo, P. 631, 654, 658, 663\\nTang, J. 684\\nTanner, M. 292\\nTao, T. 89, 613\\nTarpey, T. 578\\nTaylor, J. 88, 94, 610, 614, 689\\nTaylor, P. 375, 376, 378\\nTeh, Y.-W. 644\\nTenenbaum, J. B. 573\\nTeng, S.-H. 578\\nThomas, J. 257\\nTibshirani, R. 73, 78, 86, 88, 90,\\n92–94, 97, 98, 110, 121,\\n122, 126, 137, 216, 257,\\n292, 297, 299, 304, 334,\\n339, 345, 384, 428, 431,\\n434, 437, 441, 446, 451,\\n455, 475, 478, 480, 481,\\n519, 550, 565, 568, 609–\\n611, 614, 636, 642, 657,\\n658, 660, 661, 666, 667,\\n676, 679–684, 692, 693\\nToivonen, H. 489–491, 578\\nTraskin, M. 384\\nTrendaﬁlov, N. T. 550\\nTropp, J. 91\\nTruong, Y. 328\\nTsybakov, A. 89, 91Tukey, J. 414, 565, 578\\nTurlach, B. 76, 94\\nTurnbull, B. 331\\nTusher, V. 684, 692\\nTusn´ ady, G. 292\\nUddin, M. 550\\nValiant, L. G. 380\\nvan der Merwe, A. 84\\nVan Loan, C. 335, 535\\nVandenberghe, L. 632\\nVanichsetakul, N. 310\\nVapnik, V. 38, 102, 132, 135, 171,\\n257, 438, 455, 658\\nVayatis, N. 384\\nVazirani, U. 380\\nVerkamo, A. I. 489–491, 578\\nVidakovic, B. 181\\nvon Luxburg, U. 578\\nWahba,G.168,169,181,257,268,\\n428, 429, 455\\nWainwright, M. 91\\nWainwright, M. J. 642\\nWaldemar, G. 551\\nWalther, G. 88, 94, 519, 610, 614\\nWang, P. 667\\nWard, M. D. 664\\nWarmuth, M. 615\\nWasserman, L. 90, 304, 626, 645,\\n693\\nWatkins, C. 658\\nWegkamp, M. 91\\nWeisberg, S. 94\\nWerbos, P. 414\\nWermuth, N. 645\\nWeston, J. 658, 668, 669\\nWhittaker, J. 632, 633, 641, 645\\nWickerhauser, M. 181\\nWidrow, B. 396, 414\\nWild, C. 300\\nWilliams, R. 414\\nWilliams, W. 526\\nWilson, R. 57', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 753}),\n",
       " Document(page_content='736 Author Index\\nWinograd, T. 577\\nWold, H. 94\\nWolpert, D. 292\\nWong, M. A. 510\\nWong, W. 292\\nWright, G. 664, 674, 693\\nWright, M. 96, 421\\nWu, T. 92, 294, 583\\nWyner, A. 384, 603\\nYang, N. 3, 49\\nYang, Y. 686, 693\\nYasui, Y. 664\\nYeang, C. 654, 658\\nYee, T. 300\\nYekutieli, Y. 693\\nYu, B. 90, 91, 384\\nYuan, M. 90Zhang, H. 90, 304, 428, 455\\nZhang, J. 409–412, 605\\nZhang, P. 257\\nZhang, T. 384\\nZhao, P. 90, 91\\nZhao, Y. 693\\nZhu, J. 89, 98, 174, 348, 349, 385,\\n426, 428, 434, 610, 611,\\n615, 657, 661, 664, 666,\\n693\\nZidek, J. 84\\nZou, H. 72, 78, 92, 349, 385, 550,\\n662, 693', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 754}),\n",
       " Document(page_content='This is page 737\\nPrinter: Opaque this\\nIndex\\nL1regularization, seeLasso\\nActivation function, 392–395\\nAdaBoost, 337–346\\nAdaptive lasso, 92\\nAdaptive methods, 429\\nAdaptive nearest neighbor meth-\\nods, 475–478\\nAdaptive wavelet ﬁltering, 181\\nAdditive model, 295–304\\nAdjusted response, 297\\nAﬃne set, 130\\nAﬃne-invariant average, 482, 540\\nAIC,seeAkaike information cri-\\nterion\\nAkaikeinformationcriterion(AIC),\\n230\\nAnalysis of deviance, 124\\nApplications\\nabstracts, 672\\naorta, 204\\nbone, 152\\nCalifornia housing, 371–372,\\n591\\ncountries, 517demographics, 379–380\\ndocument, 532\\nﬂow cytometry, 637\\ngalaxy, 201\\nheart attack, 122, 146, 207\\nlymphoma, 674\\nmarketing, 488\\nmicroarray, 5, 505, 532\\nnested spheres, 590\\nNew Zealand ﬁsh, 375–379\\nnuclear magnetic resonance,\\n176\\nozone, 201\\nprostatecancer,3,49,61,608\\nproteinmassspectrometry,664\\nsatellite image, 470\\nskin of the orange, 429–432\\nspam, 2, 300–304, 313, 320,\\n328, 352, 593\\nvowel, 440, 464\\nwaveform, 451\\nZIP code, 4, 404, 536–539\\nArchetypal analysis, 554–557\\nAssociation rules, 492–495, 499–\\n501', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 755}),\n",
       " Document(page_content='738 Index\\nAutomaticrelevancedetermination,\\n411\\nAutomatic selectionofsmoothing\\nparameters , 156\\nB-Spline, 186\\nBack-propagation, 392–397, 408–\\n409\\nBackﬁtting, 297, 391\\nBackward\\nselection, 58\\nstepwise selection, 59\\nBackward pass, 396\\nBagging, 282–288, 409, 587\\nBasis expansions and regulariza-\\ntion, 139–189\\nBasisfunctions,141,186,189,321,\\n328\\nBatch learning, 397\\nBaum–Welch algorithm, 272\\nBayes\\nclassiﬁer, 21\\nfactor, 234\\nmethods, 233–235, 267–272\\nrate, 21\\nBayesian, 409\\nBayesianinformationcriterion(BIC),\\n233\\nBenjamini–Hochbergmethod,688\\nBest-subset selection, 57, 610\\nBetween class covariance matrix,\\n114\\nBias, 16, 24, 37, 160, 219\\nBias-variance decomposition, 24,\\n37, 219\\nBias-variance tradeoﬀ, 37, 219\\nBIC,seeBayesianInformationCri-\\nterion\\nBoltzmann machines, 638–648\\nBonferroni method, 686\\nBoosting, 337–386, 409\\nas lasso regression, 607–609\\nexponentiallossandAdaBoost,\\n343\\ngradient boosting, 358implementations, 360\\nmargin maximization, 613\\nnumerical optimization, 358\\npartial-dependenceplots,369\\nregularization path, 607\\nshrinkage, 364\\nstochastic gradient boosting,\\n365\\ntree size, 361\\nvariable importance, 367\\nBootstrap,249,261–264,267,271–\\n282, 587\\nrelationshiptoBayesianmethod,\\n271\\nrelationshiptomaximumlike-\\nlihood method, 267\\nBottom-up clustering, 520–528\\nBump hunting, seePatient rule\\ninduction method\\nBumping, 290–292\\nC5.0, 624\\nCanonical variates, 441\\nCART,seeClassiﬁcation and re-\\ngression trees\\nCategorical predictors, 10, 310\\nCensored data, 674\\nClassicalmultidimensionalscaling,\\n570\\nClassiﬁcation, 22, 101–137, 305–\\n317, 417–429\\nClassiﬁcationandregressiontrees\\n(CART), 305–317\\nClique, 628\\nClustering, 501–528\\nk-means, 509–510\\nagglomerative, 523–528\\nhierarchical, 520–528\\nCodebook, 515\\nCombinatorial algorithms, 507\\nCombining models, 288–290\\nCommittee, 289, 587, 605\\nComparison of learning methods,\\n350–352\\nComplete data, 276', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 756}),\n",
       " Document(page_content='Index 739\\nComplexity parameter, 37\\nComputational shortcuts\\nquadratic penalty, 659\\nCondensing procedure, 480\\nConditional likelihood, 31\\nConfusion matrix, 301\\nConjugate gradients, 396\\nConsensus, 285–286\\nConvolutional networks, 407\\nCoordinate descent, 92, 636, 668\\nCOSSO, 304\\nCost complexity pruning, 308\\nCovariance graph, 631\\nCpstatistic, 230\\nCross-entropy, 308–310\\nCross-validation, 241–245\\nCubic smoothing spline, 151–153\\nCubic spline, 151–153\\nCurse of dimensionality, 22–26\\nDantzig selector, 89\\nData augmentation, 276\\nDaubechies symmlet-8 wavelets,\\n176\\nDe-correlation, 597\\nDecision boundary, 13–15, 21\\nDecision trees, 305–317\\nDecoder, 515, seeencoder\\nDecomposable models, 641\\nDegrees of freedom\\nin an additive model, 302\\nin ridge regression, 68\\nof a tree, 336\\nofsmoothermatrices,153–154,\\n158\\nDelta rule, 397\\nDemmler-Reinschbasisforsplines,\\n156\\nDensity estimation, 208–215\\nDeviance, 124, 309\\nDiagonallineardiscriminantanal-\\nysis, 651–654\\nDimension reduction, 658\\nfor nearest neighbors, 479\\nDiscrete variables, 10, 310–311Discriminant\\nadaptivenearestneighborclas-\\nsiﬁer, 475–480\\nanalysis, 106–119\\ncoordinates, 108\\nfunctions, 109–110\\nDissimilarity measure, 503–504\\nDummy variables, 10\\nEarly stopping, 398\\nEﬀective degrees of freedom, 17,\\n68,153–154,158,232,302,\\n336\\nEﬀective number of parameters,\\n15,68,153–154,158,232,\\n302, 336\\nEigenvaluesofasmoothermatrix,\\n154\\nElastic net, 662\\nEM algorithm, 272–279\\nasamaximization-maximization\\nprocedure, 277\\nfor two component Gaussian\\nmixture, 272\\nEncoder, 514–515\\nEnsemble, 616–623\\nEnsemble learning, 605–624\\nEntropy, 309\\nEquivalent kernel, 156\\nError rate, 219–230\\nError-correcting codes, 606\\nEstimates of in-sample prediction\\nerror, 230\\nExpectation-maximizationalgorithm,\\nseeEM algorithm\\nExtra-sample error, 228\\nFalsediscoveryrate,687–690,692,\\n693\\nFeature, 1\\nextraction, 150\\nselection, 409, 658, 681–683\\nFeed-forwardneuralnetworks,392–\\n408', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 757}),\n",
       " Document(page_content='740 Index\\nFisher’s linear discriminant, 106–\\n119, 438\\nFlexiblediscriminantanalysis,440–\\n445\\nForward\\nselection, 58\\nstagewise, 86, 608\\nstagewise additive modeling,\\n342\\nstepwise, 73\\nForward pass algorithm, 395\\nFourier transform, 168\\nFrequentist methods, 267\\nFunction approximation, 28–36\\nFused lasso, 666\\nGap statistic, 519\\nGating networks, 329\\nGauss-Markov theorem, 51–52\\nGauss-Newton method, 391\\nGaussian(normal)distribution,16\\nGaussian graphical model, 630\\nGaussian mixtures, 273, 463, 492,\\n509\\nGaussian radial basis functions,\\n212\\nGBM,seeGradient boosting\\nGBMpackage, seeGradientboost-\\ning\\nGCV,seeGeneralizedcross-validation\\nGEM (generalized EM), 277\\nGeneralization\\nerror, 220\\nperformance, 220\\nGeneralized additive model, 295–\\n304\\nGeneralizedassociationrules,497–\\n499\\nGeneralized cross-validation, 244\\nGeneralizedlineardiscriminantanal-\\nysis, 438\\nGeneralized linear models, 125\\nGibbs sampler, 279–280, 641\\nfor mixtures, 280\\nGini index, 309Global Markov property, 628\\nGradient Boosting, 359–361\\nGradient descent, 358, 395–397\\nGraph Laplacian, 545\\nGraphical lasso, 636\\nGrouped lasso, 90\\nHaar basis function, 176\\nHammersley-Cliﬀordtheorem,629\\nHard-thresholding, 653\\nHat matrix, 46\\nHelix, 582\\nHessian matrix, 121\\nHidden nodes, 641–642\\nHidden units, 393–394\\nHierarchical clustering, 520–528\\nHierarchical mixtures of experts,\\n329–332\\nHigh-dimensional problems, 649\\nHints, 96\\nHyperplane, seeSeparating Hy-\\nperplane\\nICA,seeIndependentcomponents\\nanalysis\\nImportance sampling, 617\\nIn-sample prediction error, 230\\nIncomplete data, 332\\nIndependentcomponentsanalysis,\\n557–570\\nIndependent variables, 9\\nIndicator response matrix, 103\\nInference, 261–294\\nInformation\\nFisher, 266\\nobserved, 274\\nInformation theory, 236, 561\\nInner product, 53, 668, 670\\nInputs, 10\\nInstability of trees, 312\\nIntercept, 11\\nInvariance manifold, 471\\nInvariant metric, 471\\nInverse wavelet transform, 179', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 758}),\n",
       " Document(page_content='Index 741\\nIRLS,seeIteratively reweighted\\nleast squares\\nIrreducible error, 224\\nIsing model, 638\\nISOMAP, 572\\nIsometric feature mapping, 572\\nIterativeproportionalscaling,585\\nIterativelyreweightedleastsquares\\n(IRLS), 121\\nJensen’s inequality, 293\\nJoin tree, 629\\nJunction tree, 629\\nK-means clustering, 460, 509–514\\nK-medoid clustering, 515–520\\nK-nearestneighborclassiﬁers,463\\nKarhunen-Loevetransformation(prin-\\ncipal components), 66–\\n67, 79, 534–539\\nKarush-Kuhn-Tucker conditions,\\n133, 420\\nKernel\\nclassiﬁcation, 670\\ndensity classiﬁcation, 210\\ndensity estimation, 208–215\\nfunction, 209\\nlogistic regression, 654\\nprincipalcomponent,547–550\\nstring, 668–669\\ntrick, 660\\nKernelmethods,167–176,208–215,\\n423–438, 659\\nKnot, 141, 322\\nKriging, 171\\nKruskal-Shephard scaling, 570\\nKullback-Leibler distance, 561\\nLagrange multipliers, 293\\nLandmark, 539\\nLaplacian, 545\\nLaplacian distribution, 72\\nLAR,seeLeast angle regression\\nLasso,68–69,86–90,609,635,636,\\n661fused, 666\\nLatent\\nfactor, 674\\nvariable, 678\\nLearning, 1\\nLearning rate, 396\\nLearningvectorquantization,462\\nLeast angle regression, 73–79, 86,\\n610\\nLeast squares, 11, 32\\nLeave-one-outcross-validation,243\\nLeNet, 406\\nLikelihood function, 265, 273\\nLinear basis expansion, 139–148\\nLinear combination splits, 312\\nLineardiscriminantfunction,106–\\n119\\nLinear methods\\nfor classiﬁcation, 101–137\\nfor regression, 43–99\\nLinear models and least squares,\\n11\\nLinear regression of an indicator\\nmatrix, 103\\nLinear separability, 129\\nLinear smoother, 153\\nLink function, 296\\nLLE,seeLocal linear embedding\\nLocal false discovery rate, 693\\nLocal likelihood, 205\\nLocal linear embedding, 572\\nLocalmethodsinhighdimensions,\\n22–27\\nLocal minima, 400\\nLocal polynomial regression, 197\\nLocal regression, 194, 200\\nLocalizationintime/frequency,175\\nLoess (local regression), 194, 200\\nLog-linear model, 639\\nLog-odds ratio (logit), 119\\nLogistic (sigmoid) function, 393\\nLogistic regression, 119–128, 299\\nLogit (log-odds ratio), 119\\nLossfunction,18,21,219–223,346\\nLoss matrix, 310', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 759}),\n",
       " Document(page_content='742 Index\\nLossless compression, 515\\nLossy compression, 515\\nLVQ,seeLearning Vector Quan-\\ntization\\nMahalanobis distance, 441\\nMajority vote, 337\\nMajorization, 294, 553\\nMajorize-Minimizealgorithm,294,\\n584\\nMAP (maximum aposteriori) es-\\ntimate, 270\\nMargin, 134, 418\\nMarket basket analysis, 488, 499\\nMarkovchainMonteCarlo(MCMC)\\nmethods, 279\\nMarkov graph, 627\\nMarkov networks, 638–648\\nMARS,seeMultivariate adaptive\\nregression splines\\nMART,seeMultiple additive re-\\ngression trees\\nMaximum likelihood estimation,\\n31, 261, 265\\nMCMC, seeMarkovChainMonte\\nCarlo Methods\\nMDL,seeMinimum description\\nlength\\nMean ﬁeld approximation, 641\\nMean squared error, 24, 285\\nMemory-based method, 463\\nMetropolis-Hastingsalgorithm,282\\nMinimumdescriptionlength(MDL),\\n235\\nMinorization, 294, 553\\nMinorize-Maximizealgorithm,294,\\n584\\nMisclassiﬁcation error, 17, 309\\nMissing data, 276, 332–333\\nMissing predictor values, 332–333\\nMixing proportions, 214\\nMixturediscriminantanalysis,449–\\n455\\nMixture modeling, 214–215, 272–\\n275, 449–455, 692Mixture of experts, 329–332\\nMixtures and the EM algorithm,\\n272–275\\nMM algorithm, 294, 584\\nMode seekers, 507\\nModelaveragingandstacking,288\\nModel combination, 289\\nModel complexity, 221–222\\nModelselection,57,222–223,230–\\n231\\nModiﬁed regression, 634\\nMonte Carlo method, 250, 495\\nMother wavelet, 178\\nMultidimensionalscaling,570–572\\nMultidimensional splines, 162\\nMultiedit algorithm, 480\\nMultilayer perceptron, 400, 401\\nMultinomial distribution, 120\\nMultiple additive regression trees\\n(MART), 361\\nMultiple hypothesis testing, 683–\\n693\\nMultiple minima, 291, 400\\nMultiple outcome shrinkage and\\nselection, 84\\nMultiple outputs, 56, 84, 103–106\\nMultipleregressionfromsimpleuni-\\nvariate regression, 52\\nMultiresolution analysis, 178\\nMultivariate adaptive regression\\nsplines(MARS),321–327\\nMultivariatenonparametricregres-\\nsion, 445\\nNadaraya–Watson estimate, 193\\nNaive Bayes classiﬁer, 108, 210–\\n211, 694\\nNatural cubic splines, 144–146\\nNearest centroids, 670\\nNearest neighbor methods, 463–\\n483\\nNearest shrunken centroids, 651–\\n654, 694\\nNetwork diagram, 392\\nNeural networks, 389–416', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 760}),\n",
       " Document(page_content='Index 743\\nNewton’smethod(Newton-Raphson\\nprocedure), 120–122\\nNon-negativematrixfactorization,\\n553–554\\nNonparametriclogisticregression,\\n299–304\\nNormal (Gaussian) distribution,\\n16, 31\\nNormal equations, 12\\nNumerical optimization, 395–396\\nObject dissimilarity, 505–507\\nOnline algorithm, 397\\nOptimal scoring, 445, 450–451\\nOptimalseparatinghyperplane,132–\\n135\\nOptimismofthetrainingerrorrate,\\n228–230\\nOrderedcategorical(ordinal)pre-\\ndictor, 10, 504\\nOrdered features, 666\\nOrthogonal predictors, 53\\nOverﬁtting, 220, 228–230, 364\\nPageRank, 576\\nPairwise distance, 668\\nPairwise Markov property, 628\\nParametric bootstrap, 264\\nPartialdependenceplots,369–370\\nPartial least squares, 80–82, 680\\nPartition function, 638\\nParzen window, 208\\nPasting, 318\\nPathalgorithm,73–79,86–89,432\\nPatientruleinductionmethod(PRIM),\\n317–321, 499–501\\nPeeling, 318\\nPenalization, 607, seeregulariza-\\ntion\\nPenalizeddiscriminantanalysis,446–\\n449\\nPenalized polynomial regression,\\n171\\nPenalizedregression,34,61–69,171\\nPenalty matrix, 152, 189Perceptron, 392–416\\nPiecewisepolynomialsandsplines,\\n36, 143\\nPosterior\\ndistribution, 268\\nprobability, 233–235, 268\\nPower method, 577\\nPre-conditioning, 681–683\\nPrediction accuracy, 329\\nPrediction error, 18\\nPredictive distribution, 268\\nPRIM,seePatient rule induction\\nmethod\\nPrincipal components, 66–67, 79–\\n80, 534–539, 547\\nregression, 79–80\\nsparse, 550\\nsupervised, 674\\nPrincipalcurvesandsurfaces,541–\\n544\\nPrincipal points, 541\\nPrior distribution, 268–272\\nProcrustes\\naverage, 540\\ndistance, 539\\nProjection pursuit, 389–392, 565\\nregression, 389–392\\nPrototype classiﬁer, 459–463\\nPrototype methods, 459–463\\nProximity matrices, 503\\nPruning, 308\\nQR decomposition, 55\\nQuadraticapproximationsandin-\\nference, 124\\nQuadratic discriminant function,\\n108, 110\\nRadial basis function (RBF) net-\\nwork, 392\\nRadial basis functions, 212–214,\\n275, 393\\nRadial kernel, 548\\nRandom forest, 409, 587–604\\nalgorithm, 588', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 761}),\n",
       " Document(page_content='744 Index\\nbias, 596–601\\ncomparison to boosting, 589\\nexample, 589\\nout-of-bag ( oob), 592\\noverﬁt, 596\\nproximity plot, 595\\nvariable importance, 593\\nvariance, 597–601\\nRao score test, 125\\nRayleigh quotient, 116\\nReceiver operating characteristic\\n(ROC) curve, 317\\nReduced-rank linear discriminant\\nanalysis, 113\\nRegression,11–14,43–99,200–204\\nRegression spline, 144\\nRegularization, 34, 167–176\\nRegularizeddiscriminantanalysis,\\n112–113, 654\\nRelevance network, 631\\nRepresenter of evaluation, 169\\nReproducingkernelHilbertspace,\\n167–176, 428–429\\nReproducing property, 169\\nResponsibilities, 274–275\\nRidge regression, 61–68, 650, 659\\nRisk factor, 122\\nRobust ﬁtting, 346–350\\nRosenblatt’s perceptron learning\\nalgorithm, 130\\nRug plot, 303\\nRuleﬁt, 623\\nSAM,690–693, seeSigniﬁcanceAnal-\\nysis of Microarrays\\nSammon mapping, 571\\nSCAD, 92\\nScaling of the inputs, 398\\nSchwarz’s criterion, 230–235\\nScore equations, 120, 265\\nSelf-consistencyproperty,541–543\\nSelf-organizing map (SOM), 528–\\n534\\nSensitivity of a test, 314–317\\nSeparating hyperplane, 132–135Separatinghyperplanes,136,417–\\n419\\nSeparator, 628\\nShape average, 482, 540\\nShrinkage methods, 61–69, 652\\nSigmoid, 393\\nSigniﬁcance Analysis of Microar-\\nrays, 690–693\\nSimilarity measure, seeDissimi-\\nlarity measure\\nSingle index model, 390\\nSingular value decomposition, 64,\\n535–536, 659\\nsingular values, 535\\nsingular vectors, 535\\nSliced inverse regression, 480\\nSmoother, 139–156, 192–199\\nmatrix, 153\\nSmoothingparameter,37,156–161,\\n198–199\\nSmoothing spline, 151–156\\nSoft clustering, 512\\nSoft-thresholding, 653\\nSoftmax function, 393\\nSOM,seeSelf-organizing map\\nSparse, 175, 304, 610–613, 636\\nadditive model, 91\\ngraph, 625, 635\\nSpeciﬁcity of a test, 314–317\\nSpectral clustering, 544–547\\nSpline, 186\\nadditive, 297–299\\ncubic, 151–153\\ncubic smoothing, 151–153\\ninteraction, 428\\nregression, 144\\nsmoothing, 151–156\\nthin plate, 165\\nSquared error loss, 18, 24, 37, 219\\nSRM,seeStructuralriskminimiza-\\ntion\\nStacking(stackedgeneralization),\\n290\\nStarting values, 397\\nStatistical decision theory, 18–22', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 762}),\n",
       " Document(page_content='Index 745\\nStatistical model, 28–29\\nSteepest descent, 358, 395–397\\nStepwise selection, 60\\nStochastic approximation, 397\\nStochasticsearch(bumping),290–\\n292\\nStress function, 570–572\\nStructuralriskminimization(SRM),\\n239–241\\nSubset selection, 57–60\\nSupervised learning, 2\\nSupervisedprincipalcomponents,\\n674–681\\nSupportvectorclassiﬁer,417–421,\\n654\\nmulticlass, 657\\nSupport vector machine, 423–437\\nSURE shrinkage method, 179\\nSurvival analysis, 674\\nSurvival curve, 674\\nSVD,seeSingular value decom-\\nposition\\nSymmlet basis, 176\\nTangent distance, 471–475\\nTanh activation function, 424\\nTarget variables, 10\\nTensor product basis, 162\\nTest error, 220–223\\nTest set, 220\\nThin plate spline, 165\\nThinning strategy, 189\\nTrace of a matrix, 153\\nTraining epoch, 397\\nTraining error, 220–223\\nTraining set, 219–223\\nTree for regression, 307–308\\nTree-based methods, 305–317\\nTrees for classiﬁcation, 308–310\\nTrellis display, 202Undirected graph, 625–648\\nUniversal approximator, 390\\nUnsupervisedlearning,2,485–585\\nUnsupervised learning as super-\\nvised learning, 495–497\\nValidation set, 222\\nVapnik-Chervonenkis(VC)dimen-\\nsion, 237–239\\nVariable importance plot, 594\\nVariable types and terminology, 9\\nVariance, 16, 25, 37, 158–161, 219\\nbetween, 114\\nwithin, 114, 446\\nVariance reduction, 588\\nVarying coeﬃcient models, 203–\\n204\\nVCdimension, seeVapnik–Chervon-\\nenkis dimension\\nVector quantization, 514–515\\nVoronoi regions, 510\\nWald test, 125\\nWavelet\\nbasis functions, 176–179\\nsmoothing, 174\\ntransform, 176–179\\nWeak learner, 383, 605\\nWeakest link pruning, 308\\nWebpages, 576\\nWebsite for book, 8\\nWeight decay, 398\\nWeight elimination, 398\\nWeights in a neural network, 395\\nWithinclasscovariancematrix,114,\\n446', metadata={'source': '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf', 'page': 763})]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = '/Users/damienbenveniste/Projects/Teaching/Introduction_Langchain/data/mixed_data/element_of_SL.pdf'\n",
    "\n",
    "loader = PyPDFLoader(file_path=file_path)\n",
    "data = loader.load_and_split()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13b238e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "docsearch = Chroma.from_documents(\n",
    "    data, \n",
    "    embeddings, \n",
    "    collection_name=\"statistical_learning\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72395eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=docsearch.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6755fcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "This is page 1\n",
      "Printer: Opaque this\n",
      "1\n",
      "Introduction\n",
      "Statistical learning plays a key role in many areas of science, ﬁnance and\n",
      "industry. Here are some examples of learning problems:\n",
      "•Predict whether a patient, hospitalized due to a heart attac k, will\n",
      "have a second heart attack. The prediction is to be based on de mo-\n",
      "graphic, diet and clinical measurements for that patient.\n",
      "•Predict the price of a stock in 6 months from now, on the basis o f\n",
      "company performance measures and economic data.\n",
      "•Identify the numbers in a handwritten ZIP code, from a digiti zed\n",
      "image.\n",
      "•Estimate the amount of glucose in the blood of a diabetic pers on,\n",
      "from the infrared absorption spectrum of that person’s bloo d.\n",
      "•Identify the risk factors for prostate cancer, based on clin ical and\n",
      "demographic variables.\n",
      "The science of learning plays a key role in the ﬁelds of statis tics, data\n",
      "mining and artiﬁcial intelligence, intersecting with area s of engineering and\n",
      "other disciplines.\n",
      "This book is about learning from data. In a typical scenario, we have\n",
      "an outcome measurement, usually quantitative (such as a sto ck price) or\n",
      "categorical (such as heart attack/no heart attack), that we wish to predict\n",
      "based on a set of features (such as diet and clinical measurements). We\n",
      "have atraining set of data, in which we observe the outcome and feature\n",
      "\n",
      "This is page xi\n",
      "Printer: Opaque this\n",
      "Preface to the First Edition\n",
      "We are drowning in information and starving for knowledge.\n",
      "–Rutherford D. Roger\n",
      "The ﬁeld of Statistics is constantly challenged by the probl ems that science\n",
      "andindustrybringstoitsdoor.Intheearlydays,theseprob lemsoftencame\n",
      "from agricultural and industrial experiments and were rela tively small in\n",
      "scope. With the advent of computers and the information age, statistical\n",
      "problems have exploded both in size and complexity. Challen ges in the\n",
      "areas of data storage, organization and searching have led t o the new ﬁeld\n",
      "of “data mining”; statistical and computational problems i n biology and\n",
      "medicine have created “bioinformatics.” Vast amounts of da ta are being\n",
      "generated in many ﬁelds, and the statistician’s job is to mak e sense of it\n",
      "all: to extract important patterns and trends, and understa nd “what the\n",
      "data says.” We call this learning from data .\n",
      "The challenges in learning from data have led to a revolution in the sta-\n",
      "tisticalsciences.Sincecomputationplayssuchakeyrole, itisnotsurprising\n",
      "that much of this new development has been done by researcher s in other\n",
      "ﬁelds such as computer science and engineering.\n",
      "The learning problems that we consider can be roughly catego rized as\n",
      "eithersupervised orunsupervised . In supervised learning, the goal is to pre-\n",
      "dict the value of an outcome measure based on a number of input measures;\n",
      "in unsupervised learning, there is no outcome measure, and t he goal is to\n",
      "describe the associations and patterns among a set of input m easures.\n",
      "\n",
      "586 14. Unsupervised Learning\n",
      "\n",
      "42 2. Overview of Supervised Learning\n",
      "Human: What is Machine Learning?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The given context does not directly provide a definition of machine learning. However, based on the information provided, machine learning can be understood as a subfield of statistical learning, data mining, and artificial intelligence. It involves the use of algorithms and statistical models to enable computers to learn and make predictions or decisions without being explicitly programmed. Machine learning can be used to solve various learning problems, such as predicting outcomes based on input features and identifying patterns and associations in data.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run('What is Machine Learning?', callbacks=[handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0809b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "\n",
    "description = \"\"\"\n",
    "useful for when you need to answer questions \n",
    "about Machine Learning and Statistical Learning. \n",
    "Input should be a fully formed question.\n",
    "\"\"\"\n",
    "\n",
    "retrieval_tool = Tool(\n",
    "    name=\"Statistical Learning Book\",\n",
    "    func=chain.run,\n",
    "    description=description,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b564350",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools(['wikipedia', \"llm-math\"], llm=llm)\n",
    "\n",
    "tools.append(retrieval_tool)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools, \n",
    "    llm=llm, \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1571182d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "Wikipedia: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\n",
      "Calculator: Useful for when you need to answer questions about math.\n",
      "Statistical Learning Book: \n",
      "useful for when you need to answer questions \n",
      "about Machine Learning and Statistical Learning. \n",
      "Input should be a fully formed question.\n",
      "\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Wikipedia, Calculator, Statistical Learning Book]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: \n",
      "Can we use machine learning to predict the age of the president?\n",
      "\n",
      "Thought:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI'm not sure if machine learning can be used to predict the age of the president. I should consult the Statistical Learning Book to see if it provides any insights on this topic.\n",
      "\n",
      "Action: Statistical Learning Book\n",
      "Action Input: Can machine learning be used to predict someone's age?\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "This is page 1\n",
      "Printer: Opaque this\n",
      "1\n",
      "Introduction\n",
      "Statistical learning plays a key role in many areas of science, ﬁnance and\n",
      "industry. Here are some examples of learning problems:\n",
      "•Predict whether a patient, hospitalized due to a heart attac k, will\n",
      "have a second heart attack. The prediction is to be based on de mo-\n",
      "graphic, diet and clinical measurements for that patient.\n",
      "•Predict the price of a stock in 6 months from now, on the basis o f\n",
      "company performance measures and economic data.\n",
      "•Identify the numbers in a handwritten ZIP code, from a digiti zed\n",
      "image.\n",
      "•Estimate the amount of glucose in the blood of a diabetic pers on,\n",
      "from the infrared absorption spectrum of that person’s bloo d.\n",
      "•Identify the risk factors for prostate cancer, based on clin ical and\n",
      "demographic variables.\n",
      "The science of learning plays a key role in the ﬁelds of statis tics, data\n",
      "mining and artiﬁcial intelligence, intersecting with area s of engineering and\n",
      "other disciplines.\n",
      "This book is about learning from data. In a typical scenario, we have\n",
      "an outcome measurement, usually quantitative (such as a sto ck price) or\n",
      "categorical (such as heart attack/no heart attack), that we wish to predict\n",
      "based on a set of features (such as diet and clinical measurements). We\n",
      "have atraining set of data, in which we observe the outcome and feature\n",
      "\n",
      "10.14 Illustrations 383\n",
      "agePartial Dependence\n",
      "1 2 3 4 5 6 70 1 2 3 4Retired\n",
      "agePartial Dependence\n",
      "1 2 3 4 5 6 7-2 -1 0 1 2Student\n",
      "agePartial Dependence\n",
      "1 2 3 4 5 6 7-2 -1 0 1 2Prof/Man\n",
      "FIGURE 10.25. Partial dependence of the odds of three diﬀerent occupation s\n",
      "on age, for the demographics data.\n",
      "showed that a weak learner could always improve its performance by train-\n",
      "ing two additional classiﬁers on ﬁltered versions of the inp ut data stream.\n",
      "A weak learner is an algorithm for producing a two-class clas siﬁer with\n",
      "performance guaranteed (with high probability) to be signi ﬁcantly better\n",
      "than a coin-ﬂip. After learning an initial classiﬁer G1on the ﬁrst Ntraining\n",
      "points,\n",
      "•G2is learned on a new sample of Npoints, half of which are misclas-\n",
      "siﬁed byG1;\n",
      "•G3is learned on Npoints for which G1andG2disagree;\n",
      "•the boosted classiﬁer is GB=majority vote (G1,G2,G3).\n",
      "Schapire’s “Strength of Weak Learnability” theorem proves thatGBhas\n",
      "improved performance over G1.\n",
      "Freund (1995) proposed a “boost by majority” variation whic h combined\n",
      "many weak learners simultaneously and improved the perform ance of the\n",
      "simple boosting algorithm of Schapire. The theory supporti ng both of these\n",
      "\n",
      "This is page 699\n",
      "Printer: Opaque this\n",
      "References\n",
      "Abu-Mostafa, Y. (1995). Hints, Neural Computation 7: 639–671.\n",
      "Ackley, D. H., Hinton, G. and Sejnowski, T. (1985). A learnin g algorithm\n",
      "for Boltzmann machines, Trends in Cognitive Sciences 9: 147–169.\n",
      "Adam, B.-L., Qu, Y., Davis, J. W., Ward, M. D., Clements, M. A. ,\n",
      "Cazares, L. H., Semmes, O. J., Schellhammer, P. F., Yasui, Y. ,\n",
      "Feng, Z. and Wright, G. (2003). Serum protein ﬁngerprinting cou-\n",
      "pled with a pattern-matching algorithm distinguishes pros tate cancer\n",
      "from benign prostate hyperplasia and healthy mean, Cancer Research\n",
      "63(10): 3609–3614.\n",
      "Agrawal, R., Mannila, H., Srikant, R., Toivonen, H. and Verk amo, A. I.\n",
      "(1995). Fast discovery of association rules, Advances in Knowledge\n",
      "Discovery and Data Mining , AAAI/MIT Press, Cambridge, MA.\n",
      "Agresti, A. (1996). An Introduction to Categorical Data Analysis , Wiley,\n",
      "New York.\n",
      "Agresti, A. (2002). Categorical Data Analysis (2nd Ed.) , Wiley, New York.\n",
      "Ahn,J.andMarron,J.(2005). Thedirectionofmaximaldatap ilinginhigh\n",
      "dimensional space, Technical report , Statistics Department, University\n",
      "of North Carolina, Chapel Hill.\n",
      "Akaike, H. (1973). Information theory and an extension of th e maximum\n",
      "likelihood principle, Second International Symposium on Information\n",
      "Theory, pp. 267–281.\n",
      "\n",
      "720 References\n",
      "Ramaswamy, S., Tamayo, P., Rifkin, R., Mukherjee, S., Yeang , C., Angelo,\n",
      "M.,Ladd,C.,Reich,M.,Latulippe,E.,Mesirov,J.,Poggio, T.,Gerald,\n",
      "W., Loda, M., Lander, E. and Golub, T. (2001). Multiclass can cer\n",
      "diagnosis using tumor gene expression signature, PNAS98: 15149–\n",
      "15154.\n",
      "Ramsay, J. and Silverman, B. (1997). Functional Data Analysis , Springer,\n",
      "New York.\n",
      "Rao, C. R. (1973). Linear Statistical Inference and Its Applications , Wiley,\n",
      "New York.\n",
      "R¨ atsch, G. and Warmuth, M. (2002). Maximizing the margin wi th boost-\n",
      "ing,Proceedings of the 15th Annual Conference on Computational\n",
      "Learning Theory , pp. 334–350.\n",
      "Ravikumar, P., Liu, H., Laﬀerty, J. and Wasserman, L. (2008) . Spam:\n",
      "Sparse additive models, inJ. Platt, D. Koller, Y. Singer and S. Roweis\n",
      "(eds),Advances in Neural Information Processing Systems 20 , MIT\n",
      "Press, Cambridge, MA, pp. 1201–1208.\n",
      "Ridgeway, G. (1999). The state of boosting, Computing Science and Statis-\n",
      "tics31: 172–181.\n",
      "Rieger, K., Hong, W., Tusher, V., Tang, J., Tibshirani, R. an d Chu, G.\n",
      "(2004). Toxicity from radiation therapy associated with ab normal\n",
      "transcriptional responses to DNA damage, Proceedings of the National\n",
      "Academy of Sciences 101: 6634–6640.\n",
      "Ripley,B.D.(1996). Pattern Recognition and Neural Networks ,Cambridge\n",
      "University Press.\n",
      "Rissanen, J. (1983). A universal prior for integers and esti mation by mini-\n",
      "mum description length, Annals of Statistics 11: 416–431.\n",
      "Robbins, H. and Munro, S. (1951). A stochastic approximatio n method,\n",
      "Annals of Mathematical Statistics 22: 400–407.\n",
      "Roosen, C. and Hastie, T. (1994). Automatic smoothing splin e projection\n",
      "pursuit,Journal of Computational and Graphical Statistics 3:235–248.\n",
      "Rosenblatt, F. (1958). The perceptron: a probabilistic mod el for infor-\n",
      "mation storage and organization in the brain, Psychological Review\n",
      "65: 386–408.\n",
      "Rosenblatt, F. (1962). Principles of Neurodynamics: Perceptrons and the\n",
      "Theory of Brain Mechanisms , Spartan, Washington, D.C.\n",
      "Human: Can machine learning be used to predict someone's age?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mBased on the information provided, it is possible to use machine learning to predict someone's age. Machine learning algorithms can be trained on demographic and clinical measurements to make predictions about a person's age. However, it is important to note that the accuracy of these predictions would depend on the quality and relevance of the data used for training the algorithm.\u001b[0m\n",
      "Thought:\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "Wikipedia: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\n",
      "Calculator: Useful for when you need to answer questions about math.\n",
      "Statistical Learning Book: \n",
      "useful for when you need to answer questions \n",
      "about Machine Learning and Statistical Learning. \n",
      "Input should be a fully formed question.\n",
      "\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Wikipedia, Calculator, Statistical Learning Book]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: \n",
      "Can we use machine learning to predict the age of the president?\n",
      "\n",
      "Thought:I'm not sure if machine learning can be used to predict the age of the president. I should consult the Statistical Learning Book to see if it provides any insights on this topic.\n",
      "\n",
      "Action: Statistical Learning Book\n",
      "Action Input: Can machine learning be used to predict someone's age?\n",
      "Observation: Based on the information provided, it is possible to use machine learning to predict someone's age. Machine learning algorithms can be trained on demographic and clinical measurements to make predictions about a person's age. However, it is important to note that the accuracy of these predictions would depend on the quality and relevance of the data used for training the algorithm.\n",
      "Thought:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mMachine learning can be used to predict the age of the president, but the accuracy of the predictions would depend on the quality and relevance of the data used for training the algorithms.\n",
      "\n",
      "Final Answer: Yes, machine learning can be used to predict the age of the president, but the accuracy of the predictions would vary depending on the data used.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '\\nCan we use machine learning to predict the age of the president?\\n',\n",
       " 'output': 'Yes, machine learning can be used to predict the age of the president, but the accuracy of the predictions would vary depending on the data used.',\n",
       " 'intermediate_steps': [(AgentAction(tool='Statistical Learning Book', tool_input=\"Can machine learning be used to predict someone's age?\", log=\"I'm not sure if machine learning can be used to predict the age of the president. I should consult the Statistical Learning Book to see if it provides any insights on this topic.\\n\\nAction: Statistical Learning Book\\nAction Input: Can machine learning be used to predict someone's age?\"),\n",
       "   \"Based on the information provided, it is possible to use machine learning to predict someone's age. Machine learning algorithms can be trained on demographic and clinical measurements to make predictions about a person's age. However, it is important to note that the accuracy of these predictions would depend on the quality and relevance of the data used for training the algorithm.\")]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "Can we use machine learning to predict the age of the president?\n",
    "\"\"\"\n",
    "\n",
    "response = agent(\n",
    "    {\"input\": question}, \n",
    "    callbacks=[handler]\n",
    ")\n",
    "\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langchain)",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

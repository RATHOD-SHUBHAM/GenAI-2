{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RZztgktvYov",
        "outputId": "673f9e0d-e548-4498-fa52-9e6323070b95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.4/124.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet langchain langchain-openai langchain_core"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Latest preview API releases\n",
        "\n",
        "[API Version](https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation)\n",
        "\n",
        "# How can I select the proper openai.api_version?\n",
        "[Stackoverflow](https://stackoverflow.com/questions/76475419/how-can-i-select-the-proper-openai-api-version)"
      ],
      "metadata": {
        "id": "sSIjUr0kJn6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n",
        "os.environ[\"OPENAI_API_VERSION\"] = \"2024-05-01-preview\"\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"This form the Azure portal - model deployment\"\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = 'This form the Azure portal - model deployment'"
      ],
      "metadata": {
        "id": "x098q3Tewj7G"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Talk to LLM Directly\n"
      ],
      "metadata": {
        "id": "aCLsUQU9wc0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "\n",
        "llm = AzureChatOpenAI(temperature=0,\n",
        "                      azure_deployment=\"gpt-4o\",)\n",
        "\n",
        "messages = [\n",
        "    (\"system\", \"You are a helpful assistant that translates English to German.\"),\n",
        "    (\"human\", \"Translate this sentence from English to German. I love programming.\"),\n",
        "]\n",
        "\n",
        "llm.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UF78JGK2wfmc",
        "outputId": "bca3b50e-522f-46fd-ea59-79cd859e6858"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Ich liebe Programmieren.', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 34, 'total_tokens': 39}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_5f4bad809a', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-f24f2e62-c206-4445-a01e-bf1ac48e5140-0', usage_metadata={'input_tokens': 34, 'output_tokens': 5, 'total_tokens': 39})"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Prompt Template."
      ],
      "metadata": {
        "id": "uxqC0rl1wSML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 2 ways to have a prompt template.\n",
        "\n",
        "1.   ChatPromptTemplate\n",
        "2.   PromptTemplate\n",
        "\n"
      ],
      "metadata": {
        "id": "RPebjEFT0K2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ChatPromptTemplate"
      ],
      "metadata": {
        "id": "0ARxQXvr1w9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
        "        ),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# chaining\n",
        "chain = prompt | llm\n",
        "\n",
        "# Running the chain\n",
        "chain.invoke(\n",
        "    {\n",
        "        \"input_language\" : \"English\",\n",
        "        \"output_language\" : \"German\",\n",
        "        \"input\" : \"I Love you\",\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubFnw1d6vhKU",
        "outputId": "19973968-80ef-4012-eb84-e290ba000a66"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Ich liebe dich', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 25, 'total_tokens': 28}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_5f4bad809a', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-55fcaf4d-f785-47df-afca-afa54480301d-0', usage_metadata={'input_tokens': 25, 'output_tokens': 3, 'total_tokens': 28})"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Template\n",
        "\n",
        "\n",
        "\n",
        "1.   Template.\n",
        "2.   Prompt.\n",
        "3.   Chain.\n",
        "\n"
      ],
      "metadata": {
        "id": "rfNtqgrM10FD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "#### 1] With one input.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vkgQwj0l3n3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "template = '''\n",
        "  Act as a naming consultant for new companies.\n",
        "  What is a good name for a company that makes {products} ?\n",
        "'''\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=['products'],\n",
        "    template = template\n",
        ")\n",
        "\n",
        "prompt.format(products = 'Socks')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9Ihlh2m2yZvA",
        "outputId": "70c510b9-6ede-4e25-9fd6-adf22a285444"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  Act as a naming consultant for new companies.\\n  What is a good name for a company that makes Socks ?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = AzureChatOpenAI(temperature=0,\n",
        "                      azure_deployment=\"gpt-4o\",)\n",
        "\n",
        "chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt = prompt,\n",
        "    verbose = True\n",
        ")\n",
        "\n",
        "\n",
        "chain.invoke({\n",
        "    'products' : \"socks\"\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcQqhwUe2gXP",
        "outputId": "97e4cb94-4e5f-4abe-bbcd-fac78eec4b57"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "  Act as a naming consultant for new companies.\n",
            "  What is a good name for a company that makes socks ?\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'products': 'socks',\n",
              " 'text': \"Sure, I'd be happy to help! Here are a few suggestions for a company that makes socks:\\n\\n1. **SockSational**\\n2. **CozyToes**\\n3. **SockHaven**\\n4. **FootFinesse**\\n5. **SnugSteps**\\n6. **HappyFeet Co.**\\n7. **SockSymphony**\\n8. **ToastyThreads**\\n9. **SoleComfort**\\n10. **SockSerenity**\\n\\nWhen choosing a name, consider your target audience, brand values, and the unique selling points of your socks. Make sure the name is easy to remember, spell, and pronounce. Good luck with your new company!\"}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2] With Multiple input."
      ],
      "metadata": {
        "id": "TuSwFhNy3m3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = '''\n",
        "  Tell me a {adjective} joke about {content}.\n",
        "'''\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=['adjective', 'content'],\n",
        "    template = template\n",
        ")\n",
        "\n",
        "prompt.format(adjective = 'Funny', content = 'Cats')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Lg6QbDlL3LTe",
        "outputId": "dae7d797-9449-427c-e308-1b4a0c0e8827"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  Tell me a Funny joke about Cats.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = AzureChatOpenAI(temperature=0,\n",
        "                      azure_deployment=\"gpt-4o\",)\n",
        "\n",
        "chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt = prompt,\n",
        "    verbose = True\n",
        ")\n",
        "\n",
        "\n",
        "chain.invoke({\n",
        "    'adjective' : 'Funny',\n",
        "    'content' : 'Cats'\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00sFXFPm6soO",
        "outputId": "04c65651-d002-427f-e734-1df7e6082c1b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "  Tell me a Funny joke about Cats.\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'adjective': 'Funny',\n",
              " 'content': 'Cats',\n",
              " 'text': \"Sure, here's a cat joke for you:\\n\\nWhy was the cat sitting on the computer?\\n\\nBecause it wanted to keep an eye on the mouse! 🐱💻🖱️\"}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few Shot Template.\n",
        "\n",
        "\n",
        "\n",
        "1.   Example.\n",
        "2.   Example Template.\n",
        "3.   Example Prompt.\n",
        "\n",
        "4.   Prefix.\n",
        "5.   Suffix.\n",
        "\n",
        "6.   Few Shot Prompt Template.\n",
        "7.   Chain.\n",
        "\n"
      ],
      "metadata": {
        "id": "VjM5fiAdGJCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_openai import AzureChatOpenAI"
      ],
      "metadata": {
        "id": "7uc5OBR661EZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = AzureChatOpenAI(temperature=0,\n",
        "                      azure_deployment=\"gpt-4o\",)"
      ],
      "metadata": {
        "id": "XQ3gWLyiHTVv"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"query\": \"How are you?\",\n",
        "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
        "    }, {\n",
        "        \"query\": \"What time is it?\",\n",
        "        \"answer\": \"It's time to get a watch.\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "rGaNyUXHHftj"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_template = \"\"\"\n",
        "  user: {query}\n",
        "  AI: {answer}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7zXCyyyJHpXO"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt = PromptTemplate(\n",
        "    input_variables=['query', 'answer'],\n",
        "    template=example_template\n",
        ")"
      ],
      "metadata": {
        "id": "cXDzfigrIFVL"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"\"\"\n",
        "  The following are excerpts from conversations with an AI\n",
        "    assistant. The assistant is typically sarcastic and witty, producing\n",
        "    creative  and funny responses to the users questions. Here are some\n",
        "    examples:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nZyhfcZuHv75"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "suffix = \"\"\"\n",
        "  User: {query}\n",
        "  AI:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7j0JajtGH1AR"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=['query'],\n",
        "    example_separator='\\n\\n'\n",
        ")"
      ],
      "metadata": {
        "id": "2pQSXWSoH5nv"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the meaning of life\"\n",
        "\n",
        "chain = LLMChain(llm=llm,\n",
        "                 prompt = few_shot_prompt_template,\n",
        "                 verbose = False)\n",
        "\n",
        "chain.invoke({'query' : query})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4V8rQIgIP9s",
        "outputId": "e1e44cc8-b966-489f-8690-d5423a28bbc3"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What is the meaning of life',\n",
              " 'text': \"42. But if you're looking for a more detailed answer, I'd suggest asking a philosopher or maybe a really wise cat.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n3B6HnHyJXZk"
      },
      "execution_count": 55,
      "outputs": []
    }
  ]
}